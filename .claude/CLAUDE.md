# Podcast Summarizer System

A comprehensive podcast downloading, transcription, and summarization system that intelligently manages YouTube playlists with interactive episode selection.

## Overview

This system automates the process of:
1. Discovering new episodes from YouTube podcast playlists (including Chinese Xiaoyuzhou platform)
2. Processing one-off episodes from individual URLs (with automatic cleanup after completion)
3. Allowing you to interactively select which episodes to process
4. Downloading audio files
5. Transcribing audio to text using OpenAI's Whisper
6. Identifying speakers using voice diarization and Claude AI
7. Generating comprehensive AI-powered summaries using Claude Sonnet 4.5 (16K tokens)
8. Automatically translating Chinese titles to English and tracking content region
9. Tracking episode status through the entire pipeline
10. Generating AI-powered vintage newspaper-style header images for weekly summaries
11. Publishing to the web via GitHub ‚Üí teahose.com automatic deployment

## Web Publishing

**All content is automatically published online:**
- When you push to GitHub, it deploys automatically to **teahose.com**
- All podcast summaries, transcripts, and weekly digests are publicly accessible
- Images, including the AI-generated header images, are displayed on the website
- No manual deployment steps required - just `git push` and it goes live

## Architecture

### Directory Structure

```
project_root/
‚îú‚îÄ‚îÄ podcast_summarizer.py          # Main application script
‚îú‚îÄ‚îÄ podcast_urls.txt               # Configuration: list of podcast URLs
‚îú‚îÄ‚îÄ one_off_episodes.txt           # Configuration: one-off episodes to process
‚îú‚îÄ‚îÄ podcast_status.json            # State file: episode tracking
‚îú‚îÄ‚îÄ summarization_prompt.md        # Configuration: summary prompt template
‚îú‚îÄ‚îÄ .env                           # Configuration: API keys
‚îú‚îÄ‚îÄ processing.lock                # Runtime: lock file for parallel protection
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îî‚îÄ‚îÄ CLAUDE.md                  # This file - project memory
‚îî‚îÄ‚îÄ podcast_work/                  # Output directory
    ‚îú‚îÄ‚îÄ [podcast_name]/            # One folder per podcast
    ‚îÇ   ‚îî‚îÄ‚îÄ [episode_name]/        # One folder per episode
    ‚îÇ       ‚îú‚îÄ‚îÄ raw_podcast.mp3    # Downloaded audio file
    ‚îÇ       ‚îú‚îÄ‚îÄ transcript_raw.md  # Whisper transcription (before speaker ID)
    ‚îÇ       ‚îú‚îÄ‚îÄ transcript.md      # Transcript with speaker names
    ‚îÇ       ‚îî‚îÄ‚îÄ summary.md         # AI-generated summary with metadata
    ‚îî‚îÄ‚îÄ one off episodes/          # Special folder for one-off episodes
        ‚îî‚îÄ‚îÄ [episode_name]/        # One folder per one-off episode
            ‚îú‚îÄ‚îÄ raw_podcast.mp3
            ‚îú‚îÄ‚îÄ transcript_raw.md
            ‚îú‚îÄ‚îÄ transcript.md
            ‚îî‚îÄ‚îÄ summary.md
```

### Episode Folder Structure Example

```
podcast_work/
‚îú‚îÄ‚îÄ 20VC/
‚îÇ   ‚îî‚îÄ‚îÄ Joelle_Pineau_Scaling_Laws_AI_Success/
‚îÇ       ‚îú‚îÄ‚îÄ raw_podcast.mp3
‚îÇ       ‚îú‚îÄ‚îÄ transcript_raw.md         # Before speaker identification
‚îÇ       ‚îú‚îÄ‚îÄ transcript.md             # After speaker identification
‚îÇ       ‚îî‚îÄ‚îÄ summary.md
‚îî‚îÄ‚îÄ one off episodes/
    ‚îî‚îÄ‚îÄ Interesting_Interview_Title/
        ‚îú‚îÄ‚îÄ raw_podcast.mp3
        ‚îú‚îÄ‚îÄ transcript_raw.md
        ‚îú‚îÄ‚îÄ transcript.md
        ‚îî‚îÄ‚îÄ summary.md
```

### Standardized summary.md Format

All summary files follow this standardized format:

**YouTube Episodes:**
```markdown
# [Episode Title](https://www.youtube.com/watch?v=VIDEO_ID)

**Podcast:** Podcast Name
**Date:** Month DD, YYYY
**Participants:** Speaker 1, Speaker 2
**Region:** Western
**Video ID:** VIDEO_ID
**Video URL:** https://www.youtube.com/watch?v=VIDEO_ID
**Transcript:** [View Transcript](./transcript.md)

---

[Summary content with clickable timestamps]
```

**Xiaoyuzhou Episodes (Chinese):**
```markdown
# Episode Title

**Podcast:** Podcast Name
**Date:** Month DD, YYYY
**Participants:** Speaker 1, Speaker 2
**Region:** Chinese
**Video ID:** EPISODE_ID
**Video URL:** https://www.xiaoyuzhoufm.com/episode/EPISODE_ID
**Transcript:** [View Transcript](./transcript.md)

---

[Summary content]
```

**Key Features:**
- **Clickable title** (YouTube only) - Links directly to video
- **Clickable timestamps** (YouTube only) - All timestamps like `[00:15:30]` link to video at that moment (minus 10 seconds)
- **Standardized metadata** - Consistent fields across all episodes
- **Date formatting** - Human-readable format (e.g., "November 10, 2025")
- **Region tracking** - "Western" or "Chinese" for content categorization
- **Clickable transcript link** - "View Transcript" links to transcript.md

**Timestamp Linking:**
- YouTube timestamps automatically link with 10-second offset
- Format: `[[00:15:30]](https://www.youtube.com/watch?v=VIDEO_ID&t=15m20s)`
- Renders as clickable: [00:15:30] (user clicks timestamp to jump to video)
- Xiaoyuzhou timestamps remain as plain text (platform limitations)

## Configuration Files

### `podcast_urls.txt`

List of podcast URLs to track. Format: one URL per line with optional podcast name in comment.

**Example:**
```
https://www.youtube.com/playlist?list=PLnJFlI3aINuUNaznonoFEgMUGLBiLmB-w # 20VC
https://www.youtube.com/playlist?list=PLxxx # Tech Talk Weekly
https://www.youtube.com/watch?v=abc123 # Single Video
```

**Features:**
- Supports YouTube playlists
- Supports single video URLs
- Optional podcast name after `#` comment
- Lines starting with `#` are ignored
- Custom podcast names override automatic extraction

### `one_off_episodes.txt`

List of individual podcast episodes to process once. These episodes will be processed through the full pipeline and automatically removed from the file after successful completion.

**Example:**
```
# One-off episodes to process
https://www.youtube.com/watch?v=abc123xyz
https://www.youtube.com/watch?v=def456ghi # Great interview with X
https://www.xiaoyuzhoufm.com/episode/EPISODE_ID
```

**Features:**
- Supports YouTube single video URLs
- Supports Xiaoyuzhou episode URLs
- Optional comments after `#`
- Lines starting with `#` are ignored (treated as comments)
- Episodes stored in `podcast_work/one off episodes/` folder
- Episodes tracked under "One-off Episodes" podcast in status
- **Automatic cleanup**: Episodes are removed from file after summarization completes
- Useful for processing specific episodes without tracking entire playlists

**Workflow:**
1. Add episode URLs to `one_off_episodes.txt`
2. Run `python podcast_summarizer.py`
3. Select and process episodes from the interactive menu
4. Episodes are stored in `podcast_work/one off episodes/[episode_name]/`
5. After successful summarization, URLs are automatically removed from the file
6. Episodes remain in `podcast_status.json` for history tracking

### Transcript Format

Transcripts now include **timestamps for each segment**:

**Example:**
```
[00:00:15] The scaling laws have been remarkably robust.
[00:00:45] There's a lot we don't know yet in terms of vulnerability.
[00:01:20] Today we have one of the leading minds in AI.
```

**Features:**
- `[HH:MM:SS]` timestamps for each transcribed segment
- Segments extracted from Whisper's JSON output
- Helps locate specific topics in the original audio
- Enables easy reference between transcript and video

### `summarization_prompt.md`

Customizable prompt that defines how episodes should be summarized. Designed for investor and entrepreneur audiences.

**Default Structure:**
- **Key Themes (3-10 themes)** - Main insights with substantiation
- **Contrarian Perspectives (3-5 perspectives)** - Non-obvious insights that challenge conventional wisdom
- **Companies Identified (3-10 companies)** - Companies mentioned for excellence or relevance
- **Operating Insights (1-5 insights)** - Tactical takeaways for operations and decision-making

**Recent Updates:**
- Chinese content is automatically translated to English in summaries
- Timestamps are now required for all quotes in format: "Quote here" [HH:MM:SS]
- Summaries can now be up to 16K tokens (previously 2K) for more comprehensive coverage

### `podcast_status.json` & `podcast_status.md`

Master status tracking uses JSON for efficient processing and auto-generates a human-readable markdown file.

**JSON Structure:**
```json
{
  "podcasts": {
    "https://www.youtube.com/playlist?list=...": {
      "podcast_name": "20VC",
      "episodes": {
        "51y4KatMBFI": {
          "title": "Episode Title",
          "upload_date": "20241106",
          "discovered_date": "2024-11-06 22:31:00",
          "status": "summarized",
          "region": "Western"
        }
      }
    }
  }
}
```

**New Fields:**
- `region` - Either "Chinese" or "Western", automatically detected based on title language
  - Chinese titles are automatically translated to English and stored in the `title` field
  - Original Chinese titles are used for folder naming but translated for display

**Markdown File (`podcast_status.md`):**
- Auto-generated whenever status is saved
- Shows podcasts with episode counts and status breakdowns
- Each episode listed with:
  - Emoji indicators (‚úÖ summarized, üìù transcribed, ‚¨áÔ∏è downloaded, üîç discovered)
  - Publication date (from YouTube upload_date)
  - Processing status and discovery timestamp
- Human-readable for quick status checks

**Episode Status Values:**
- `discovered` - Found in playlist, not yet downloaded
- `downloaded` - Audio file downloaded to raw_podcast.mp3
- `transcribed` - Transcript generated via Whisper
- `summarized` - Summary generated via Claude Sonnet

### `.env`

Contains API keys and configuration.

**Required:**
```
OPENAI_API_KEY=sk-proj-...       # For Whisper transcription only
ANTHROPIC_API_KEY=sk-ant-...     # For speaker identification (Haiku) and summarization (Sonnet 4.5)
```

The system requires both API keys:
- **OPENAI_API_KEY**: Used exclusively for Whisper audio transcription
- **ANTHROPIC_API_KEY**: Used for both speaker identification (Claude Haiku 3.5) and summarization (Claude Sonnet 4.5)

The system will warn you at startup if either key is missing.

## Workflow

### Three-Phase Execution

When you run `python podcast_summarizer.py`:

#### Phase 1: Discovery (üîç)
- Scans all podcasts in `podcast_urls.txt`
- Fetches playlist contents
- Identifies new episodes:
  - **New podcasts**: Suggests only the latest episode
  - **Existing podcasts**: Suggests all episodes after the last downloaded one
- Updates `podcast_status.json` with all discovered episodes
- **No files are downloaded in this phase**

#### Phase 2: Interactive Selection (üìã)
Shows incomplete episodes in two separate sections:

- **One-off Episodes Section** (üìå):
  - Shows ALL incomplete one-off episodes (no date limit)
  - Episodes from `one_off_episodes.txt` that haven't been summarized yet
  - Status indicator emoji (üîç discovered, ‚¨áÔ∏è downloaded, üìù transcribed)
  - Numbered list for easy reference
  - Shows translated titles for Chinese content

- **Recent Incomplete Episodes Section** (üìÖ):
  - Shows incomplete episodes from regular podcasts in the **last 30 days**
  - Incomplete episodes include: discovered, downloaded, or transcribed (not yet summarized)
  - Status indicator emoji (üîç discovered, ‚¨áÔ∏è downloaded, üìù transcribed)
  - Numbered list for easy reference (continuing from one-off episodes)
  - Grouped by podcast name
  - Sorted by publication date (newest first)
  - Shows translated titles for Chinese content

- Prompts for user selection:
  - `all` - Select all episodes from both sections
  - `1,2,3` - Select specific episodes (comma-separated numbers)
  - `cancel` - Exit without processing

#### Phase 3: Processing (‚è±Ô∏è) - 4-Step Pipeline

For each selected episode:

**Step 1: Download**
- Extracts audio from YouTube video
- Saves as `raw_podcast.mp3`

**Step 2: Transcribe**
- Uses OpenAI's Whisper for transcription
- Includes segment timestamps `[HH:MM:SS]`
- Adds metadata header (title, podcast, date, URL)
- Saves initial transcript to `transcript_raw.md`

**Step 2a: Voice-Based Speaker Diarization** (if HuggingFace token configured)
- Uses pyannote.audio for voice-based speaker detection
- Adds speaker labels (SPEAKER_00, SPEAKER_01, etc.) based on voice analysis
- Runs locally on your machine (no API costs)
- Gracefully skips if not configured

**Step 3: Identify Speaker Names**
- Extracts guest names from YouTube video description via regex patterns (used as hints)
- Uses Claude Haiku to analyze first 15 minutes of transcript
- Maps SPEAKER_XX labels to actual full names using context and metadata
- Replaces speaker labels with actual names where confident
- Returns both updated transcript AND list of identified speaker names
- Updates transcript.md with speaker-identified version
- Identified speakers are used for the Participants field in summary

**Step 4: Summarize**
- Generates comprehensive AI summary using Claude Sonnet 4.5 (16,000 max tokens)
- Includes speaker attribution with **full names** in quotes and key points
- Includes timestamps in quotes (e.g., "Quote here" [00:15:30])
- Formats with metadata header including Participants and Region fields
- Metadata includes: title, podcast, date, participants, region (Chinese/Western), video ID, URL, transcript link
- Saves as `summary.md`

Updates `podcast_status.json` after each episode completes.

### Chinese Podcast Support

**Automatic Title Translation:**
- When an episode title contains Chinese characters, it is automatically detected
- The title is translated to English using Claude Haiku (cost-efficient)
- The translated title is stored in `podcast_status.json` for consistency
- Original Chinese title may still appear in folder names

**Region Tracking:**
- Each episode is tagged as either "Chinese" or "Western" based on title
- Region field is stored in `podcast_status.json` and `summary.md`
- Helps filter and organize content by language/region

**Xiaoyuzhou Platform:**
- Full support for Xiaoyuzhou (Â∞èÂÆáÂÆô) Chinese podcast platform
- Audio download via direct URL extraction
- Episode metadata and listing support
- See `xiaoyuzhou_helper.py` for implementation details

### Smart Download Logic

**First Run (New Podcast)**
- Fetches all episodes from playlist
- Marks all as "discovered"
- **Suggests only the latest episode** to avoid overwhelming downloads
- Allows user to download just that 1 or modify selection

**Subsequent Runs (Existing Podcast)**
- Fetches all episodes from playlist
- Finds the latest episode with status ‚â• "downloaded"
- **Suggests all episodes after that point**
- Allows user to select which new episodes to process

**Example Timeline:**
```
Day 1: Podcast has 10 episodes
  ‚Üí Suggests episode 10 only
  ‚Üí User downloads episode 10

Day 8: Podcast now has 13 episodes
  ‚Üí Suggests episodes 11, 12, 13 (the new ones)
  ‚Üí User downloads all 3 new episodes
```

## Dependencies

### Python Packages
```
openai           # Whisper transcription API
anthropic        # Claude API for speaker identification and summarization
python-dotenv    # Load .env configuration
pyannote.audio   # Voice-based speaker diarization (optional but recommended)
```

### System Tools
- `yt-dlp` - Download audio from YouTube
- `whisper` - Transcribe audio to text (OpenAI)
- `ffmpeg` - Audio processing

### API Keys
- **OpenAI API key** - For Whisper transcription
- **Anthropic API key** - For speaker identification (Haiku 3.5) and summarization (Sonnet 4.5)
- **HuggingFace token** (optional) - For pyannote speaker diarization

## Core Functions

### Status Management
- `load_podcast_status()` - Load status from JSON file
- `save_podcast_status(status_data)` - Save status to JSON file
- `get_episode_status(status_data, podcast_url, video_id)` - Get current status
- `set_episode_status(status_data, podcast_url, video_id, title, new_status)` - Update status with timestamp
- `get_latest_downloaded_episode(status_data, podcast_url)` - Find most recent completed episode

### Processing
- `process_single_video(video_info, video_index, podcast_name, podcast_url, custom_prompt, status_data)` - Download, transcribe, identify speakers, and summarize one episode (4-step pipeline)
- `download_video_audio(video_url, output_audio_path)` - Download audio via yt-dlp
- `transcribe_audio_to_text(audio_file_path, transcript_file_path)` - Transcribe via Whisper with segment timestamps
- `format_transcript_with_timestamps(whisper_data)` - Format Whisper JSON output with [HH:MM:SS] timestamps
- `extract_video_metadata(video_url)` - Extract title, description, channel, and uploader from YouTube video
- `parse_guest_names_from_description(description, title)` - Parse guest names from video description using regex patterns
- `identify_and_replace_speakers(transcript, video_metadata, video_title)` - Use Claude to identify speaker names from transcript context
- `summarize_transcript_with_ai(transcript_text, video_title, custom_prompt)` - Summarize via Claude Sonnet 4.5 with the custom prompt (16K token output)

### Chinese Language Support
- `has_chinese_characters(text)` - Detect if text contains Chinese characters (CJK Unicode range)
- `translate_chinese_to_english(text)` - Translate Chinese text to English using Claude Haiku
- Region detection and tracking automatically applied during episode discovery

### Utility
- `extract_podcast_name_from_url(playlist_url)` - Extract podcast name from URL
- `sanitize_filename(filename)` - Remove invalid filename characters
- `show_episode_selection_menu(all_episodes_to_process)` - Interactive selection interface (legacy, for new episodes)
- `show_recent_incomplete_episodes(status_data, selected_podcast_urls)` - Show incomplete episodes from last 30 days
- `get_playlist_videos(playlist_url, first_run)` - Fetch videos from playlist via yt-dlp

### Xiaoyuzhou Platform Support
See `xiaoyuzhou_helper.py` for Chinese podcast platform integration:
- `is_xiaoyuzhou_url(url)` - Check if URL is from Xiaoyuzhou platform
- `get_xiaoyuzhou_podcast_episodes(podcast_url, first_run)` - Fetch episode list
- `download_xiaoyuzhou_audio(episode_url, output_path)` - Download audio file
- `extract_xiaoyuzhou_podcast_name(podcast_url)` - Extract podcast name

### Weekly Header Image Generation
See `generate_weekly_header_image.py` for header image generation:
- `extract_themes_from_summary(summary_file_path, num_panels)` - Extract key themes from weekly summary markdown
- `create_image_prompt(panel_titles, date_str)` - Create detailed prompt for vintage newspaper style
- `generate_header_image(panel_titles, date_str, output_path)` - Generate image via OpenAI GPT-image-1
- `format_date_string(date_range_str)` - Format date string for header display

### File Management
- `load_summarization_prompt()` - Load custom prompt from file
- `acquire_lock()` / `release_lock()` - Prevent parallel processing conflicts

## Key Features

### Atomic Operations
- Status saved after every episode completes
- Can safely interrupt and resume without losing work
- No duplicate processing possible

### Smart Defaults
- First run: Downloads only latest to avoid overwhelming
- Subsequent runs: Only suggests new episodes
- Can always override and select manually

### Parallel Safety
- File-based locking prevents duplicate downloads
- 30-second lock timeout for stale locks
- Single instance guarantee

### Date Tracking
- Records when each episode was first discovered
- Provides audit trail of processing timeline
- Helps understand processing history

### Flexible Selection
- Download all new episodes at once
- Cherry-pick specific episodes
- Skip low-interest content
- Cancel anytime before processing starts

### Resumable Processing
- If download fails, transcript is already saved
- If transcription fails, download is already saved
- Resume from any point without re-processing

## Usage Examples

### Basic Usage
```bash
python podcast_summarizer.py
```

Then respond to the interactive prompts:
1. View discovered episodes
2. Enter selection (e.g., `all`, `1,3,5`, or `cancel`)
3. Wait for processing to complete

### First Time with a Podcast
```
1. Add URL to podcast_urls.txt with # Name
2. Run script
3. Script discovers all episodes, suggests latest
4. User can accept the 1 episode or modify selection
5. Episode gets downloaded, transcribed, summarized
```

### Adding New Episodes to Existing Podcast
```
1. Run script (no changes to podcast_urls.txt needed)
2. Script discovers all episodes, suggests only new ones
3. User selects which new episodes to process
4. Only those episodes are downloaded and processed
```

### Selective Processing
```
1. Run script (discovers 5 new episodes across 2 podcasts)
2. See all 5 listed with podcast names
3. Enter "1,3,5" to process only those 3
4. Episodes 2 and 4 are skipped
5. Status updated only for episodes 1, 3, 5
```

### Processing One-off Episodes
```
1. Add specific episode URLs to one_off_episodes.txt:
   https://www.youtube.com/watch?v=abc123xyz
   https://www.youtube.com/watch?v=def456ghi # Great interview

2. Run script
3. Script discovers the one-off episodes and lists them in a separate section
4. Select episodes to process (or 'all')
5. Episodes are downloaded, transcribed, and summarized
6. All files saved to podcast_work/one off episodes/[episode_name]/
7. After successful summarization, URLs are automatically removed from one_off_episodes.txt
8. Episodes remain tracked in podcast_status.json under "One-off Episodes"
```

### Generating Weekly Summary with Header Image
```
1. Run python generate_weekly_summary.py
2. Script finds all episodes published in last 7 days
3. Generates comprehensive weekly digest with Claude Sonnet 4.5
4. Extracts top 10 themes from the summary
5. Generates vintage newspaper-style header image with GPT-image-1
6. Saves summary with header image at top (email-ready format)
7. Files saved to weekly_summaries/weekly_summary_YYYY-MM-DD_to_YYYY-MM-DD.md
8. Header image saved as weekly_summary_YYYY-MM-DD_to_YYYY-MM-DD_header.png
```

## Weekly Header Image Generator

**Script:** `generate_weekly_header_image.py`

Generates AI-powered vintage newspaper-style comic headers for weekly podcast summaries.

### Features
- **10-panel layout** (5 rows x 2 columns) displaying key themes
- **Portrait orientation** (1024x1536) - optimized for mobile and email viewing
- **Vintage aesthetic** - 1950s comic book style with Ben Day dots and Roy Lichtenstein pop art
- **Automatic theme extraction** - Pulls top themes from weekly summary markdown
- **OpenAI GPT-image-1** - Latest image generation model from OpenAI
- **Email-ready** - Image appears at top of markdown file for copy-paste to email

### Usage

**Automatic (Integrated into Weekly Summary):**
```bash
python generate_weekly_summary.py
# Header image is automatically generated and inserted at top
```

**Manual (Standalone):**
```bash
# From existing summary file
python generate_weekly_header_image.py \
  --summary weekly_summaries/weekly_summary_2025-11-04_to_2025-11-11.md \
  --output header.png

# From custom themes
python generate_weekly_header_image.py \
  --themes "AI Development" "Media Industry" "Political Division" \
           "Tech Policy" "Economic Trends" "Cultural Issues" \
           "VC Funding" "Product Strategy" "Talent Acquisition" "Robotics" \
  --date "2025-11-04_to_2025-11-11" \
  --output header.png
```

### Configuration
In `generate_weekly_header_image.py`:
```python
IMAGE_MODEL = "gpt-image-1"     # OpenAI's latest image model
IMAGE_SIZE = "1024x1536"        # Portrait orientation
IMAGE_QUALITY = "high"          # Quality setting
NUM_PANELS = 10                 # Number of theme panels
```

### How It Works
1. **Theme Extraction**: Regex patterns extract key themes from markdown sections:
   - `## A. Key Topics` or `**A. Topics Discussed**`
   - `## B. Contrarian Perspectives` or `**B. Contrarian Perspectives**`
2. **Prompt Generation**: Creates detailed prompt specifying:
   - 5x2 grid layout with all 10 panels fully visible
   - Vintage 1950s comic book aesthetic
   - Panel-specific themes and visual metaphors
   - Roy Lichtenstein style with Ben Day dots
3. **Image Generation**: Calls OpenAI API with GPT-image-1 model
4. **Image Embedding**: Inserts image at top of markdown file with `![...]` syntax

### Output Format
The header image is embedded at the top of the weekly summary:
```markdown
![Podcast Weekly Digest Header](./weekly_summary_2025-11-04_to_2025-11-11_header.png)

# Weekly Podcast Summary

**Period:** November 04, 2025 to November 11, 2025
...
```

### Cost
- ~$0.04-0.08 per image with GPT-image-1 (high quality, 1024x1536)
- Included automatically in weekly summary generation

## File Naming

Episode folders use human-readable names derived from video titles:
- Spaces and special characters converted to underscores
- Invalid filename characters removed
- Example: "Joelle Pineau: Why Scaling Laws..." ‚Üí "Joelle_Pineau_Scaling_Laws_AI_Success"

## Monitoring Progress

Check `podcast_status.json` to see current state:
```json
{
  "20VC": {
    "51y4KatMBFI": {
      "status": "summarized",
      "discovered_date": "2024-11-06 22:31:00"
    },
    "abc123xyz": {
      "status": "transcribed",
      "discovered_date": "2024-11-07 10:15:00"
    }
  }
}
```

## Configuration Customization

### Change Summary Format
Edit `summarization_prompt.md` to modify:
- What sections to include
- Level of detail
- Output structure
- Focus areas

The changes apply to all future summarizations.

### Change Whisper Model
In `podcast_summarizer.py`, modify:
```python
WHISPER_MODEL = "base"  # Options: tiny, base, small, medium, large
```

Options trade off accuracy vs. speed/memory.

### Change Summary Output Length
In `podcast_summarizer.py`, modify the `summarize_transcript_with_ai()` function:
```python
max_tokens=16000  # Current: 16K for comprehensive summaries
                  # Previous: 2K (increase provides much more detail)
```

Note: The system now uses Claude Sonnet 4.5 with 16K max tokens for more comprehensive summaries compared to the previous 2K limit.

## Error Handling

- Failed downloads don't block transcription of other episodes
- Failed transcriptions don't block summaries of other episodes
- Errors are logged with full stack traces
- Status remains consistent even if errors occur
- Can safely retry failed episodes by re-running script

## Logging & Output

The script provides clear output:
- Discovery phase shows what's being checked
- Selection phase shows numbered list with details
- Processing phase shows progress for each step
- Errors include context and suggest fixes

## Backfilling Publication Dates

Episodes discovered before the upload_date feature was added don't have publication dates in the status file. To backfill these:

```bash
# Run the backfill script to fetch upload_date from YouTube for all episodes
python3 backfill_dates.py
```

This will:
- Iterate through all episodes without upload_date
- Fetch metadata from YouTube for each
- Store the upload_date (YYYYMMDD format)
- Update the markdown file with publication dates

Note: This can take a while for large podcast libraries (438+ episodes). The script is safe to interrupt and resume.

## Troubleshooting

### "OPENAI_API_KEY not set"
- Add `OPENAI_API_KEY=sk-...` to `.env` file
- Ensure `.env` is in the project root

### "yt-dlp not found"
- Install: `pip install yt-dlp`
- Or: `brew install yt-dlp` (macOS)

### "ffmpeg not found"
- Install: `brew install ffmpeg` (macOS)
- Or appropriate package manager for your OS

### "whisper not found"
- Install: `pip install openai-whisper`

### Script appears to hang
- Check if it's waiting for user input in Phase 2
- Press Ctrl+C to cancel

### Duplicate downloads despite running script
- Ensure no other instances are running
- Delete `processing.lock` if stale

## Advanced Usage

### Process Specific Episodes Only
1. Run script normally
2. At selection prompt, enter: `1,3,5` (just these episode numbers)
3. Script processes only those episodes

### Modify Individual Prompts
Edit `summarization_prompt.md` to change summary behavior for next run. Changes apply globally.

### Inspect Episode Status
```bash
cat podcast_status.json | python -m json.tool
```

View which episodes are at which stage of processing.

### Manual Resume
If a run is interrupted:
1. Check `podcast_status.json` for current state
2. Run script again
3. It will skip already-completed steps
4. Can modify selection to focus on incomplete episodes

## Cost Estimates

### Per Episode Processing Costs

**With Voice-Based Diarization (Recommended):**
- Transcription (Whisper): ~$0.10-0.30
- Diarization (pyannote, local): $0.00 (runs locally)
- Speaker ID (Claude Haiku, 15 min): ~$0.05-0.20
- Title Translation (Claude Haiku, if Chinese): ~$0.01-0.03
- Summarization (Claude Sonnet 4.5, 16K tokens): ~$0.30-1.50
- **Total: ~$0.45-2.00 per episode**

**Without Voice-Based Diarization:**
- Transcription (Whisper): ~$0.10-0.30
- Speaker ID (Claude Haiku, 15 min): ~$0.05-0.20 (less accurate without SPEAKER_XX labels)
- Title Translation (Claude Haiku, if Chinese): ~$0.01-0.03
- Summarization (Claude Sonnet 4.5, 16K tokens): ~$0.30-1.50
- **Total: ~$0.45-2.00 per episode**

**Cost Considerations:**
- Using Haiku (not Sonnet) for speaker ID and translation: ~10x cheaper
- Using first 15 minutes only for speaker ID: ~3-4x fewer tokens
- Voice-based diarization: More accurate, no added API costs
- Increased summary length (2K ‚Üí 16K tokens): More comprehensive, ~50% higher cost per episode
- Chinese title translation adds minimal cost (~$0.01-0.03 per episode)

### Weekly Summary Generation Costs

**Per Weekly Summary:**
- Claude Sonnet 4.5 summarization (10-20 episodes): ~$0.50-2.00
- Header image generation (GPT-image-1, high quality): ~$0.04-0.08
- **Total per weekly summary: ~$0.54-2.08**

### Monthly Cost Examples

**Episode Processing:**
- **Light usage** (10 episodes/month): $4.50-20.00
- **Medium usage** (50 episodes/month): $22.50-100.00
- **Heavy usage** (200 episodes/month): $90-400

**Weekly Summaries:**
- 4 weekly summaries/month: ~$2.16-8.32

**Total Monthly (Medium Usage):**
- 50 episodes + 4 weekly summaries: ~$24.66-108.32

Note: Costs vary based on:
- Episode length (longer = more transcription cost)
- Transcript complexity (affects summarization tokens)
- Number of speakers (minimal impact with voice diarization)
- Number of episodes per weekly summary

## Future Enhancements

Potential improvements:
- Web UI for selection and monitoring
- Batch scheduling (process every N days)
- Email notifications on completion
- Support for RSS feeds
- Integration with note-taking apps
- Advanced filtering and search
- Statistics and analytics

## License & Attribution

Built with:
- OpenAI (Whisper, GPT-4o-mini)
- yt-dlp (YouTube downloading)
- Python community libraries
