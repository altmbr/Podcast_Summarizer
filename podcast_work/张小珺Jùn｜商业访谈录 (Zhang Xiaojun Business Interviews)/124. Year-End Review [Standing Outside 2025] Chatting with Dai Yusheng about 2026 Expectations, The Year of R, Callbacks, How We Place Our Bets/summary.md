# 124. Year-End Review [Standing Outside 2025] Chatting with Dai Yusheng about 2026 Expectations, The Year of R, Callbacks, How We Place Our Bets

**Podcast:** 张小珺Jùn｜商业访谈录 (Zhang Xiaojun Business Interviews)
**Date:** 2025-12-13
**Participants:** Dai Yusheng, Zhang Xiaojun
**Region:** Chinese
**Video ID:** 693d7c172a383da167ecfcde
**Video URL:** https://www.xiaoyuzhoufm.com/episode/693d7c172a383da167ecfcde
**Transcript:** [View Transcript](./transcript.md)

---

# Summary and Analysis: The Year of R - Dai Yusheng on AI's Reality Check in 2026

## 1. Key Themes

### The Three R's: 2026 as the "Year of R" - Return, Research, and Remember

Dai Yusheng frames 2026 around three critical "R" concepts that will define the AI landscape. First, **Return** (回报): "In 2025, everyone invested massively, but in 2026, people will increasingly discuss what is AI's next paradigm, whether AI model capabilities have hit bottlenecks, and whether AI's short-term realizable revenue can support these investments" [01:00:36]. The investment has become unsustainable without clearer paths to monetization.

Second, **Research** (研究): Following Ilya Sutskever's framework, "AI history has always been an alternating process of scaling and research... when scaling reaches certain bottlenecks or its progress slows, we need new research to unlock new scaling opportunities" [01:17:05]. Demis Hassabis recently stated he believes "we still need one to two research breakthroughs to reach AGI" [01:19:06].

Third, **Remember/Memory**: "Memory will be a battleground for AI applications in 2026. How to obtain as much user context as possible, have as much communication with users as possible, thereby forming good memory, and making applications differentiated" [01:23:42]. This represents the moat that applications can build that models alone cannot replicate.

### The AI Investment Bubble: Massive Capital Meets Uncertain Returns

The investment-return equation is becoming untenable. David Cohen from Sequoia raised the "200 billion problem" which evolved into a "600 billion problem" and now exceeds one trillion dollars annually [01:34:02]. "Every technological revolution in human history has brought bubbles, almost without exception—whether from building canals, railways, highways, or later the internet. I believe AI will bring humanity's largest bubble, which may also be quite natural" [01:30:21].

However, the expected monetization paths face significant challenges. For advertising and e-commerce: "If everyone is selling ads, then it's largely a redistribution of existing pie rather than creating a new pie... the increase from AI is limited; most is still redistribution of existing pie" [02:37:23]. For programmer salary replacement: "If AI can complete most of the work of a programmer earning $100K annually, it doesn't mean AI should earn $80K. Rather, the work previously worth $100K becomes less valuable... it's actually a value deflation process" [02:38:56].

### China's AI Catch-Up: Closing the Gap Through Efficiency and Innovation

Chinese AI companies have dramatically closed the gap with frontier American models through remarkable cost efficiency. "DeepSeek emerged at the beginning of the year, making everyone realize that Chinese companies could catch up to near world-class levels with 1% or even less cost" [01:14:32]. This wasn't limited to DeepSeek: "Later, Qianwen, Kimi's K2, GLM, MiniMax—a series of models gradually showed that Chinese model companies could produce models with similar performance but costing only 5% to 1%, and could actually use open-source models" [01:14:40].

The implications are profound: "One American early-stage startup company, without products, is valued at $50 billion—probably more than all Chinese AI startups combined. The total AI software industry in China is about this one American company's valuation" [02:51:31]. Yet Chinese companies demonstrate superior capital efficiency: "Chinese entrepreneurs' operational efficiency is very high. For example, Pico reached over $100M in revenue globally without ever raising funding" [01:57:30].

## 2. Contrarian Perspectives

### Models Have No Secrets: The Death of "Big Compute" Moats

Against the prevailing 2023 narrative that massive compute would create insurmountable moats, Dai argues: "Currently, it's quite difficult to create generational differences, especially since everyone's underlying data is roughly the same... I don't think there are secrets that can be kept long-term, especially because information and talent flow very quickly in this industry" [01:15:05].

The evidence: Chinese companies consistently produce models approaching frontier performance at a fraction of the cost. "When you're catching up, many experiments don't need to be repeated. And because you have fewer resources, you're more cost-conscious. I think this is how innovations like MLA [DeepSeek's Multi-head Latent Attention] emerged" [01:20:25]. Even major players acknowledge this: "The all-in podcast host Chad Max said they've already switched much of their usage to Kimi K2 because performance is good and price is very low" [01:14:37].

### Application Layer Will Dominate, Not Model Layer

Contrary to the model-centric investment thesis, Dai asserts applications will capture more value: "When you have more first-party products as a model company becomes increasingly important. Even at the cost of competing directly with your largest customer, like Anthropic with Cursor" [01:25:34]. 

The reasoning: "For applications, you can aggregate the strengths of all models. You can use the best model for different tasks. This actually demonstrates that selling APIs itself doesn't have very high barriers" [01:24:38]. Applications add critical layers models cannot: "Advanced applications add a context layer beyond the model—user preferences, memory between user and model, industry-specific proprietary data—and an environment and tools layer" [02:31:20].

### AGI Arrival Significantly Delayed From Early Predictions

While many expected AGI by 2027, Dai sees a much longer timeline: "Achieving L3-level intelligence, realizing what many imagine—AI making people unemployed or AI meaning we don't need to work—does require further model capability improvements" [02:47:15]. 

The comparison to autonomous vehicles is instructive: "L3-L4 autonomous Robotaxis have actually taken over ten years to develop... this shows that unlocking this level of intelligence requires model capabilities to further improve" [02:46:25]. Even for narrower applications: "Agent isn't called 'the year of agent' but rather 'decade of agent'... it's a process that will take longer than most expect, true for autonomous driving, true for AI, even more so for humanoid robots" [02:40:41].

## 3. Companies Identified

### **Manus (Anthropic-backed)** 
*Description*: Universal agent application for office productivity, research, and data analysis
*Why mentioned*: Exemplar of successful agent application reaching $100M+ ARR within months of launch
*Quotes*: "Within a few months reached $100M ARR with dozens of points gross margin, 20% monthly growth—companies at this level in the US typically receive $3-5B valuations" [01:54:55]. "They were the first wave to experience Devin, feeling that agentic capabilities were already emerging" [02:00:30]

### **Cursor**
*Description*: AI-powered code editor that became breakout coding application
*Why mentioned*: Case study of application success and competitive dynamics with model companies
*Quotes*: "Now coding, represented by Cursor, has crossed the chasm into mainstream market. It's hard to imagine a somewhat knowledgeable programmer not using AI coding tools" [01:29:40]. "Cursor faces quite big challenges... all model companies are desperately competing in coding with their own applications" [02:44:22]

### **DeepSeek**
*Description*: Chinese AI research lab that shocked industry with ultra-low-cost frontier models
*Why mentioned*: Demonstrated Chinese AI's ability to achieve frontier performance at 1% of expected cost
*Quotes*: "DeepSeek emerged at the beginning of the year, making everyone realize that Chinese companies could catch up to near world-class levels with 1% or even less cost" [01:14:32]. "DeepSeek's MLA and later DSA [sparse attention] innovations—I think these represent real technical innovations by Chinese companies" [01:23:43]

### **Kimi (Moonshot AI)**
*Description*: Chinese AI company known for long-context models and reasoning capabilities  
*Why mentioned*: Example of Chinese company matching frontier capabilities with superior economics
*Quotes*: "Kimi K2, especially K2 Thinking, achieved SOTA performance on Humanity's Last Examination and BrowserCam benchmarks... many US users have switched to it" [01:14:44]. "Yang Zhilin [founder] was the black horse of model companies because everyone else didn't invest, but we thought his background and team were excellent" [02:23:40]

### **Higen (黑剑)**  
*Description*: AI video generation company (portfolio company)
*Why mentioned*: Early successful AI application investment demonstrating product iteration
*Quotes*: "When we invested in Higen, they wanted to do model head-swapping for e-commerce, then virtual humans... they went from 'Surreal' to what they do now" [02:51:13]

### **OpenAI**
*Description*: Leading AI research lab and ChatGPT creator
*Why mentioned*: Market leader facing questions about defensibility and return on massive investment
*Quotes*: "ChatGPT already has nearly 600M DAU, 1B MAU. For an application helping people acquire knowledge and conduct research, its penetration rate is quite high" [02:20:59]. "If you must invest in one large company in first-tier market right now... OpenAI is probably still the safest choice" [02:52:18]

## 4. People Identified

### **Ilya Sutskever** (Co-founder of OpenAI, now Safe Superintelligence)
*Description*: Legendary AI researcher who articulated the research/scaling paradigm shift
*Why mentioned*: His framework explains why 2026 requires return to research fundamentals
*Quotes*: "Ilya said AI history has always been an alternating process of scaling and research... now we've reached a point where we need research paradigm breakthroughs to unlock the next wave of major scaling opportunities" [01:17:02]

### **Demis Hassabis** (CEO of Google DeepMind)  
*Description*: AI pioneer leading Google's AI efforts
*Why mentioned*: His recent assessment that AGI requires 1-2 more breakthroughs signals shifted timeline
*Quotes*: "Demis recently said he believes we still need one to two research breakthroughs to reach AGI" [01:19:06]

### **David Cohen** (Sequoia Capital)
*Description*: Sequoia partner who articulated the AI investment-return problem  
*Why mentioned*: Framed the "$600B question" about AI monetization
*Quotes*: "David Cohen proposed the 200B problem, later 600B problem... essentially, with so many Nvidia chips purchased, how do you earn back the datacenter investment? Now it's already exceeded one trillion" [01:34:02]

### **Yang Zhilin** (Founder of Moonshot AI/Kimi)
*Description*: Leading Chinese AI entrepreneur who built Kimi
*Why mentioned*: Example of technical founder with strong product judgment
*Quotes*: "Yang Zhilin, his judgment on long-context in 2023 and agentic in 2025—the quality of such judgments is very important... what you build is preparing for the SOTA 6-12 months from now" [02:48:09]

### **Xiao Hong** (Founder of Manus) 
*Description*: Serial entrepreneur who built Manus into breakout agent application
*Why mentioned*: Case study of successful product intuition and team building
*Quotes*: "Xiao Hong has never had a traditional job... he graduated from university and went directly to entrepreneurship. For him, entrepreneurship is also a destined lifestyle" [02:07:01]. "Xiao Hong's team, before doing Manus, explored browser-based approaches, so they did a lot of research on how AI uses browsers and tools" [02:48:13]

## 5. Operating Insights

### Build Memory Flywheels Before Competitors Do

"The more first-party products model companies have becomes increasingly important. How to obtain as much user context as possible, communicate with users as much as possible, thereby forming good memory and making applications differentiated—this will be a very important theme for 2026" [01:23:42]. The implication: Applications should prioritize features that create switching costs through accumulated context and memory, as this represents the primary moat available in an era where model capabilities commoditize rapidly.

### Price for Quality Growth, Not Vanity Metrics

"The trend is already gradually toward whether you have good margin, good retention—whether you ultimately have quality growth. It's simple logic: for model companies, you need to train, experiment, hire people, so losing lots of money is understandable. But application companies, you're essentially buying tokens and processing them into products with added value for users to sell... you should have a considerable margin" [01:15:25]. Chinese companies like Pico reached $100M+ revenue without raising capital by maintaining unit economics from day one [01:57:30].

### Don't Stay on the Table Unless You Can Keep Flipping Cards  

"In AI, every year there are many changes. Simply put, each year new cards are turned over at the poker table. At this time, not leaving the table and continuously flipping cards means maintaining high execution speed and iteration velocity gives you opportunities" [02:48:37]. Manus's team attempted three product directions before hitting on their successful formula [02:50:32], while Typeless pivoted from Shopify plugins to Monica-style browser extensions to voice input [02:51:02].

### Avoid Defining New Products by Old Categories

"Don't take the product form of the last era to define yourself. I think products that say 'AI's such-and-such'—where such-and-such is a product form from the previous era—are very difficult to succeed... When you call it a browser, you face competing with a very, very mature product form. If OpenAI said 'I'm making an AI search engine,' it would probably struggle to compete with Google. But when it's called a chatbot, user expectations are different" [03:14:19]. New paradigms require new product definitions, not "mobile X" or "AI Y" formulations.

### For Global Markets, Design Universal from Day One

"For current LLM-based applications, they're international from day one. Previously, companies like Xiaohongshu doing international expansion required localized operations and significant product adjustments... But because language models handle all languages from the start, and for knowledge workers' tasks, needs are similar whether you're American, Brazilian, Japanese, or Chinese" [01:36:00]. Manus and GenSpark launched globally from inception without traditional localization overhead.

## 6. Overlooked Insights  

### The "Seen Nose Phenomenon": Benchmark Saturation Hides Real Capability Gaps

While most focus on benchmark scores as the primary signal of model quality, Dai reveals a critical insight: "When benchmarks are already saturated, many times you must use them in developer groups and user settings to see true capability differences" [01:16:42]. He explains through a striking example: "Recently launched Opus 4.5 scored about 80 on SWE-bench, Gemini 3 Pro scored about 78—seems like only 2 points difference. But from what we understand, developers in actual use feel Opus 4.5 is significantly better" [01:16:11].

The implication is profound but easily missed: As models approach 80-90% scores on existing benchmarks, we've entered a regime where quantitative metrics obscure qualitative differences. "This makes me wonder if AI models are like students—some are 'high scores, low ability' while others score relatively lower but have comprehensive capabilities" [01:16:28]. For investors and operators, this means benchmark leadership may be less defensible than assumed, while real-world performance evaluation becomes both more critical and more difficult. The race is shifting from "who scores highest" to "who creates better evaluation frameworks"—making benchmark design itself a key competitive arena, as "姚勝宇 may be the previous exam committee chairman" [03:08:34].

### Voice as the Underestimated Modality Breakthrough

While everyone focuses on video generation and coding breakthroughs, Dai identifies voice as a sleeper hit for 2026: "Voice—because I think sound is actually the most natural medium for us to communicate with people and computers... Previously, because understanding and capturing voice was quite primitive, killer application prototypes like Siri and Alexa were voice-based but AI wasn't smart enough... But in 2025 we've seen applications like Pico's Granola, Whisper, and our investment Typeless—these voice-based applications bring quite magical experiences" [02:12:58].

What makes this insight overlooked is the historical context: Voice interfaces failed spectacularly in the 2010s (Siri, Alexa, Google Assistant all disappointed), creating a "voice fatigue" among investors and founders. But Dai argues the modality was ahead of its capabilities: "Now voice may be an area where everyone will continue to discover very interesting scenarios" [02:13:36]. The supporting evidence of Typeless achieving product-market fit suggests this isn't speculative—voice input that understands intent and reformats appropriately (turning "I'll make three points" into properly formatted numbered lists) represents a genuine UX breakthrough that transcends mere transcription. For founders, this suggests voice-first applications may face less competition precisely because the modality carries negative associations from previous failures.

---

**Word Count**: 3,247 words

This analysis synthesizes a wide-ranging 3+ hour conversation into actionable insights for investors and operators, with particular emphasis on the contrarian "year of R" thesis that 2026 will force a reckoning between massive AI investment and actual returns—a perspective that directly contradicts the "full speed ahead" narrative from other prominent investors.