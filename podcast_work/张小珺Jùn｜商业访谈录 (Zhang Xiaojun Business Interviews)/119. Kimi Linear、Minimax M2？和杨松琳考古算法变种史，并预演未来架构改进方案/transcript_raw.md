# 119. Kimi Linear、Minimax M2？和杨松琳考古算法变种史，并预演未来架构改进方案

**Podcast:** 张小珺Jùn｜商业访谈录 (Zhang Xiaojun Business Interviews)
**Date:** 2025-11-03
**Video ID:** 6908b9c80ceab2a71c48668c
**Video URL:** https://www.xiaoyuzhoufm.com/episode/6908b9c80ceab2a71c48668c

---

[00:00:00] 我決定國內的算法創新肯定是更強的
[00:00:08] 先先注意的模塊
[00:00:11] 他們最後選到的是一個叫做KDA的這個模塊
[00:00:15] Kimi Data 和Tension
[00:00:17] 這個名字感覺挺有耿的
[00:00:20] 他們應該是想對標Ducy Spass 的Tension
[00:00:23] 然後我就特意去了一個Kimi 開頭的一個名字
[00:00:27] 然後非常的對撞
[00:00:30] 我覺得每一次大家關心你兩參審
[00:00:34] 那肯定是因為大家碰到了一些Context 我
[00:00:39] 我覺得我還是挺喜歡看
[00:00:41] 最早的那些批判的
[00:00:43] 我覺得那些批判現在都挺好的
[00:00:45] 我管這個叫做考古
[00:00:51] 先看看能不能把全局這個注意力把它幹掉
[00:00:57] 第一點就是因為它確實它是阻止
[00:01:01] 這個Context window to scale
[00:01:03] Up 上去的一個主要的平靜
[00:01:07] 我覺得這好幾個話就是把
[00:01:10] 換合注意力它裡面的全局的注意力
[00:01:14] 把它換成Spass 的Tension
[00:01:17] 我覺得你論上只要Spass 的Tension
[00:01:20] 它能選得準的話
[00:01:22] 是完全可以取代不負擔心這個層的
[00:01:28] 哈囉大家好
[00:01:31] 歡迎收聽張小鈞商業訪談路我是小鈞
[00:01:35] 這是一檔由語言及世界工作室出品的深度訪談節目
[00:01:39] 我們希望和你一起從這裡探索新世界
[00:01:42] 今天這集節目我們將討論一個在當下非常關鍵的話題
[00:01:47] 那就是人工智能的算法與價格衝心
[00:01:51] 嘉賓是我們的網期嘉賓反常
[00:01:53] 它是MIT的在度博士楊松林
[00:01:56] 研究方向是限信注意力機制
[00:01:58] 我們將從最近剛發布的幾個新模型
[00:02:01] KIMIN LINE
[00:02:03] LINE NUX M2
[00:02:04] KIMIN DREAM NUX
[00:02:05] 切入
[00:02:06] 送林參與了這集中KIMIN和
[00:02:08] 請問的部分工作
[00:02:10] 它是KIMIN LINE 論文的作者之一
[00:02:13] 算法創新為什麼在25年變得有位重要呢
[00:02:16] 它背後的成因是
[00:02:18] 數據算力和算法
[00:02:20] 使驅動人工智能的三價馬車
[00:02:22] 那在數據狀強的無奈前提之下
[00:02:25] 各個模型公司都不得不重新開始
[00:02:28] 雕模型價構
[00:02:29] 以其SKILLIN LINE的模法繼續
[00:02:31] 而由於中國的算力相對美國有限
[00:02:34] 這反而讓中國的
[00:02:35] AI算法創新走在了世界的前言
[00:02:38] 這集節目你將聽到
[00:02:40] 近幾年價構最大的突破
[00:02:42] 是ITSIC的Moei 混合專家模型
[00:02:45] 它讓Moei 成為了全球共識
[00:02:48] 而下一個突破的重要方向
[00:02:50] 可能是Attention 注意力機制
[00:02:52] 中國公司已經在Attention 上展開了
[00:02:55] 不同的技術壓住
[00:02:57] 解釋目前已經發布的這些模型中
[00:02:59] ITSIC正在探索的是Spositension
[00:03:02] 吸收注意力機制
[00:03:03] KIMIN正在探索的是Linear Tension
[00:03:06] 現行注意力機制
[00:03:07] MiniMax在年初的M1版本中探索的是Linear Tension
[00:03:11] 而在剛發布的M2版本中
[00:03:13] 又重新回到了Fur Tension
[00:03:15] 也就是全局注意力機制
[00:03:17] 在節目中
[00:03:18] 蘇林將講解他參與的這篇
[00:03:20] KIMIN LINE的工作
[00:03:22] 並分析以上這些公司在Attention 上的不同角色
[00:03:25] 於同時
[00:03:26] 他也將帶領大家考古
[00:03:28] 人工智能的算法變種史
[00:03:30] 並預言未來
[00:03:31] 算法與架構的改進方案
[00:03:33] 等級比較的硬和
[00:03:35] 或有一些些的專業難度
[00:03:37] 大家可以根據自己的實際需要手聽
[00:03:40] 也因為加冰的工作環境的原因
[00:03:43] 所以會出現一些公英文的夾展
[00:03:45] 還是希望大家能夠多多的理解和支持
[00:03:48] 那我們開始吧
[00:03:52] �но�но�но�но
[00:04:21] 就是追的研究
[00:04:22] 跟具體來說的話
[00:04:23] 主要是在研究
[00:04:26] 就一類追利模型
[00:04:28] 叫做現行追利
[00:04:30] 你們給大家講一下
[00:04:31] 你的整個的研究的主線
[00:04:34] 是怎麼地盡點
[00:04:35] 你是怎麼走向
[00:04:35] Linio Tenshin的研究點
[00:04:37] 像Linio Tenshin的話
[00:04:39] 就是最開始的時候
[00:04:42] 應該是到時看到很多
[00:04:45] 斯坦福一個
[00:04:46] 一個
[00:04:48] Researcher的一個GrooPer
[00:04:50] 叫做Hazzy Research
[00:04:52] 就是吹到Airbird
[00:04:54] 他們在斯坦福的那個Nab
[00:04:57] 然後到時看了很多
[00:04:59] 他們寫的柏克
[00:05:00] 然後覺得
[00:05:01] 虛烈劍模是一個非常有意思的問題
[00:05:04] 然後當局立體來做一些
[00:05:06] 虛烈劍模的一些問題
[00:05:08] 然後
[00:05:10] 剛剛那時候最開始讀柏的時候
[00:05:13] 就是微軟壓演員的話
[00:05:14] 他有一篇工作叫做Ragnight
[00:05:17] 那個時候就是
[00:05:19] 最開始的時候就想辦法來
[00:05:21] 提高Ragnight他的效率
[00:05:24] 然後他的那個Performance
[00:05:27] 然後之後的話就發現
[00:05:30] 提高效率的這一套
[00:05:32] 音劍郵化的這種算法
[00:05:35] 可以擴展到很多這種
[00:05:38] 其他的這種類似的架構裡面
[00:05:40] 然後同時
[00:05:43] 就之後的一些工作
[00:05:44] 就是主要是去想辦法進一步的
[00:05:47] 就是在能夠
[00:05:49] 音劍高效性的同時能夠提高這種
[00:05:52] 細心最厲害他加個的
[00:05:55] Performance的一些改進
[00:05:57] 就不要說從門控機制
[00:06:00] 然後到那個
[00:06:02] 有一個叫做DataRoll的一個機制
[00:06:05] 然後後面的話
[00:06:06] 就是把這兩個東西
[00:06:07] 把它扛扮在一起
[00:06:09] 就是讓它合成一個
[00:06:11] 一個統一的一個弱
[00:06:12] 然後把它變成一個
[00:06:15] 安安的一個更新規則
[00:06:17] 同時的話又可以去
[00:06:19] 就是有一些可以
[00:06:20] 音劍高效的算法來進行訓練
[00:06:25] 我看上次我們節目發了以後
[00:06:26] 很多人說你是
[00:06:27] Linia Tenshin之母這時候有什麼樣
[00:06:30] 可能是在這個你於做很多工作吧
[00:06:34] 然後尤其是還有一個那個開源庫
[00:06:37] 叫做FlashLinia Tenshin
[00:06:40] 這個庫的話感覺
[00:06:42] 這個領域的人
[00:06:43] 裡面很多就是會用這個庫
[00:06:47] 然後包括業界也有很多
[00:06:49] 就是用這個庫來進行一些
[00:06:51] Linia Tenshin那些探索的
[00:06:53] 然後我那幾篇工作
[00:06:55] 應該還是比較有影響力的
[00:06:58] 所以可能大家會這麼來教我
[00:07:02] 能怎麼跟通俗的
[00:07:03] 去理解一下Linian
[00:07:04] 跟通俗的理解
[00:07:05] Linia的話就是說
[00:07:07] 他中文是現行的對吧
[00:07:09] 現行主義的舉止
[00:07:10] 現行的話呢他主要的意思
[00:07:12] 就是現行複雜度
[00:07:15] 對現行複雜度
[00:07:16] 他對於的話就是說是平方複雜度
[00:07:19] 也就是說我們平常的
[00:07:20] Sofumizer Tenshin
[00:07:21] 他是平方複雜度
[00:07:24] 然後就是我們大家都知道
[00:07:28] Sofumizer Tenshin的話呢
[00:07:30] 他有三個舉證嗎
[00:07:32] 他有QKV
[00:07:33] 就Quality value
[00:07:36] 然後一般的話
[00:07:37] 他就是Quality
[00:07:39] 先求一個舉證相成
[00:07:41] 得到一個L by L的一個舉證
[00:07:44] L的話是修列長度
[00:07:47] 然後的話呢
[00:07:48] 就是對這個L by L的舉證
[00:07:51] 做一個
[00:07:52] Masking
[00:07:53] 因為他基本上都是
[00:07:54] 智慧貴的一個語言鍵模
[00:07:57] 所以我們要把未來的
[00:07:58] 消息把它masked掉
[00:08:01] 這樣的話
[00:08:01] 我們得到一個下三角的
[00:08:02] 一個L by L的一個舉證
[00:08:04] 然後我們再加一個Sofumizer
[00:08:07] 然後就讓我們再得到一個
[00:08:09] 注意力的一個分數的一個舉證
[00:08:12] 然後最後再用這個
[00:08:13] 注意力分數舉證
[00:08:15] 和那個
[00:08:16] 關鍵的舉證做一個相成
[00:08:18] 得到一個Output
[00:08:20] 這就是Sofumizer Tenshin
[00:08:23] 他在這種智慧貴
[00:08:25] 鍵模裡面的一個
[00:08:27] 比較一個突裂的介紹
[00:08:28] 對
[00:08:29] 然後因為他會有一個
[00:08:31] L by L的一個舉證
[00:08:33] 所以他的複雜都是平方的
[00:08:36] 然後先進出業的話
[00:08:37] 他
[00:08:38] 一般就是把那個Sof Max operator
[00:08:41] 把它去掉
[00:08:42] 然後這樣子的話
[00:08:44] 我們就會得到
[00:08:45] 我們就把這個
[00:08:46] 非先進的Sofumizer Tenshin的
[00:08:48] Sofumizer去掉了
[00:08:50] 然後我們可以通過一些
[00:08:52] 那個等式的變化
[00:08:54] 然後可以把它寫成
[00:08:55] 一個類似於Alan的一個
[00:08:57] 推理的一個形式
[00:08:59] 這樣的話
[00:08:59] 他每一個Stab他的Cost is O1
[00:09:02] 然後處理L
[00:09:04] 他這個長度的訓練的一個
[00:09:07] 化能他的整體的複雜度
[00:09:08] 就是O L
[00:09:10] 所以他是跟長度的大小
[00:09:14] 他是成一個
[00:09:14] 現性複雜度的一個關係
[00:09:16] 所以大家會把它
[00:09:17] 叫做現性注意力
[00:09:20] 如果把現在的
[00:09:21] 大與模型的算法
[00:09:22] 做一個框
[00:09:24] 讓大家有一個背景的話
[00:09:25] Lin-Din Tenshin
[00:09:26] 應該放在哪個地方
[00:09:27] 我覺得都在Transformer
[00:09:29] 這個基礎加構
[00:09:31] 裡面再進行一些模改吧
[00:09:33] 對
[00:09:34] 像大與模型的話
[00:09:35] 他的基礎這樣
[00:09:36] 可能分成Pretraining
[00:09:38] Postraining
[00:09:40] 然後之類的
[00:09:41] 然後這些加構的演繹的話
[00:09:44] 那肯定是在Pretraining
[00:09:46] 這個裡面的
[00:09:47] 然後Pretraining
[00:09:47] 他還有很多其他類別的演繹
[00:09:50] 我們比方說
[00:09:51] 像油化器
[00:09:53] 然後像這種基礎加構
[00:09:55] 然後還有一些
[00:09:57] Pretraining Data
[00:09:58] 然後之類的東西
[00:10:00] 然後現性注意的
[00:10:01] 畫的應該就算
[00:10:02] 在基礎加構的演球
[00:10:05] 然後現在基礎加構的話
[00:10:06] 基本上他整體的框架
[00:10:10] 還是Transformer這種
[00:10:12] 他會有一個
[00:10:13] 注意力
[00:10:14] 機制和一個前饋
[00:10:18] 那個網絡
[00:10:20] 就Feed for Work Network
[00:10:22] 就他會在這兩個模塊裡面
[00:10:25] 然後反覆的疊家
[00:10:27] 疊家很多次
[00:10:27] 就得到我們
[00:10:28] 最近的一個Transformer的一個概態
[00:10:31] 然後一般的話
[00:10:33] 大家就是會在這個
[00:10:37] 框架下面來進行一些修改
[00:10:39] 像最近幾年的話
[00:10:41] 就是大家會把
[00:10:43] 傳統的那個MLP
[00:10:45] 或者說Feed for Work
[00:10:47] 把它換成混合專家的一些
[00:10:50] Mokron
[00:10:50] Mesh of Expert
[00:10:51] Mok的一些Mokron
[00:10:53] 然後現性注意的話
[00:10:54] 就是把傳統的Suffman's Tension
[00:10:58] 把它換成一些
[00:11:00] 現性負擔多一些Tension
[00:11:02] 當然就現在最近更活了
[00:11:04] 所以移類叫做Hybrid的一個架構
[00:11:07] 就是有一些層
[00:11:09] 它還是一個Suffman's Tension
[00:11:12] 然後另外大部分的層
[00:11:13] 就把它換成現性注意的這種層
[00:11:16] 對
[00:11:17] 我們來了你最近參與的一個行工作
[00:11:19] 就是Kimmy Linear
[00:11:20] 你是怎麼參與到Kimmy Linear的工作中點
[00:11:24] 這個工作應該是
[00:11:24] 10月底剛發布
[00:11:27] 這個工作應該
[00:11:28] 他們應該是年初就想開始做
[00:11:31] 然後當時
[00:11:33] 就是Flystin Linear Tension
[00:11:35] 這個酷
[00:11:37] 另外一個主要的作者
[00:11:38] 他叫張宇
[00:11:40] 他就是今年就從博士BM
[00:11:45] 他在國內讀博士BM
[00:11:47] 然後他就當時他正好
[00:11:49] 就是在Kimmy
[00:11:51] 然後Kimmy就正好想做混合July
[00:11:54] 然後張宇就是在做這個項目
[00:11:57] 對
[00:11:58] 然後因為我和
[00:12:01] 因為他就是Flystin Linear的
[00:12:02] Kimmy Linear的一個Cleverator
[00:12:04] 就很適合
[00:12:05] 然後我會幫他們看一下
[00:12:08] 就是有一些
[00:12:11] 現性注意的一些變種
[00:12:12] 他的那些變形的算法
[00:12:14] 該要什麼
[00:12:15] 怎麼設計之類的
[00:12:17] 當時他們團隊遇到的核心問題是什麼
[00:12:19] 為什麼開始決定要重新設計一下
[00:12:22] 注意力計制
[00:12:23] 就是年初的時候
[00:12:24] 我覺得就是大背景的話
[00:12:26] 就是像Dipsick R1和Kimmy1.5
[00:12:31] 那個時候剛剛發嘛
[00:12:32] 然後他的核心的那個
[00:12:36] 他會做些RL
[00:12:37] 然後會有一些
[00:12:38] 會得到一些非常長的一些思維念
[00:12:41] 就是Transfer
[00:12:42] 然後他會用這個非常長的
[00:12:45] 這個思維念來做這種Testants Galing
[00:12:50] 對然後來解一些比較負擔的問題
[00:12:54] 然後這個思維念的長度
[00:12:55] 能太往往就是能夠到幾萬個頭肯
[00:13:00] 這個長度
[00:13:02] 然後Kimmy就覺得
[00:13:05] 就是如果我們用
[00:13:06] 每一層都是平方注意力的
[00:13:08] 這個加個的話
[00:13:10] 他在Decoyne的時候
[00:13:12] 他就太貴了
[00:13:14] 因為首先就是每一層
[00:13:16] 他要存一個大量的一個KerryCatch
[00:13:20] 然後另外的話
[00:13:21] 他每一步他是
[00:13:22] 現身的這個時間複雜度
[00:13:24] 所以Decoy的如果Decoy的L
[00:13:26] 個頭肯的話
[00:13:28] 他的時間複雜度也是一個平方的
[00:13:30] 對
[00:13:31] 所以就是在這種長的這種思維念的
[00:13:35] 深層的背景下面
[00:13:37] 然後讓Kimmy覺得
[00:13:39] 就是這個需要去投入資源
[00:13:43] 來探索一下這種混合的注意力
[00:13:46] 因為他能夠把這個Inference的Cost
[00:13:48] 他把它打低很多
[00:13:50] 對
[00:13:51] 這一點在這種長思維念
[00:13:53] 弄穿色的背景下面
[00:13:55] 然後以及今年
[00:13:56] 這整體的這個
[00:13:58] Egentic AI的背景下面的話
[00:14:00] 他是非常有用的
[00:14:03] 大概背景就是這樣子的
[00:14:05] 你們模改當時核心目標是什麼
[00:14:07] 需要完成的核心目標
[00:14:08] 當時的核心目標可能
[00:14:11] 主要是張宇在那邊做
[00:14:12] 然後他們的目標應該就是
[00:14:16] 就是跟之前的那種
[00:14:18] 富爾探索相比的話
[00:14:19] 就是Performance要不掉點
[00:14:22] 然後同時他的這個Inference速度
[00:14:25] 會快很多倍
[00:14:26] 如果用富爾探索的話
[00:14:27] 他的確信是什麼
[00:14:29] 如果用富爾探索的話
[00:14:30] 就是這種做長文本的這個D-Code
[00:14:33] 的時候他就是非常的昂貴
[00:14:36] 能不能從你的視角給大家
[00:14:38] 講這樣的這篇論文
[00:14:39] 稍微畫一下重點
[00:14:41] 像這篇文章的話
[00:14:43] 他們的這個就是這個
[00:14:47] 現新注意的模塊
[00:14:48] 他們最後選到的是一個叫做
[00:14:51] KDA的這個模塊
[00:14:52] Kimi Delta的Tension
[00:14:55] 對
[00:14:56] 這個名字感覺挺有耿的
[00:14:58] 他們應該是想對標D-Cix Buster Tension
[00:15:01] 然後我就特意去了一個
[00:15:03] Kimi開頭的一個名字
[00:15:05] 然後非常的對撞
[00:15:07] 對
[00:15:08] 然後這個現新注意的模塊
[00:15:11] 他基本上就是基於
[00:15:15] 我去年有一個工作叫做
[00:15:17] Gate Data Knight
[00:15:19] 然後在這個基礎上面
[00:15:21] 就是進行了一些改善
[00:15:24] 然後最後形成一個叫做KDA的一個模塊
[00:15:28] 總而來說的話就是
[00:15:31] 首先我沒有一個叫做
[00:15:32] Data入了一個東西
[00:15:34] 對
[00:15:35] 這個可能可以之後再稍微再具體講
[00:15:37] 然後像
[00:15:40] Gate Data Knight的時候
[00:15:41] 就這個工作
[00:15:42] 就當時
[00:15:44] 收縣語就是這個Efficiency
[00:15:47] 然後當時我就用到了一個
[00:15:49] 像
[00:15:49] 像Mamba2一樣的一個
[00:15:52] SkaterWalu的一個Gating
[00:15:54] 這個的話呢
[00:15:55] 他就是說他的這個門控
[00:15:58] 他的這個指
[00:15:59] 就是對於一個Tension Head來說
[00:16:03] 他下面的所有的這個維度
[00:16:06] 他全部會要共享一個
[00:16:10] DK的一個摔減率
[00:16:13] 這的話呢
[00:16:13] 他是可以在計算上面
[00:16:16] 會帶來一些簡化
[00:16:18] 所以當時的考慮就是說
[00:16:21] 我現在滿把Tube的基礎上面
[00:16:23] 就是加上Data入
[00:16:25] 然後能讓他的這個效率有保證
[00:16:28] 對
[00:16:29] 所以就會
[00:16:30] 所以到時就是只用到了
[00:16:32] 他們那一種比較
[00:16:34] 力度比較粗的一個門控的一個機制
[00:16:39] 對所以這就是Gate Data Knight
[00:16:41] 然後像張宇完的這個KDA
[00:16:44] 他就是把這個
[00:16:47] Edo比較粗的一個摔減率
[00:16:51] 把它換成了一個力度比較
[00:16:53] 吸了一個摔減率
[00:16:54] 就是之前的話呢
[00:16:55] 就是一個Tension Head下面
[00:16:58] 他不同的維度
[00:17:00] 他要共享同一個摔減率
[00:17:03] 現在的話呢
[00:17:03] 他不同的維度
[00:17:05] 他每個維度
[00:17:06] 他有一個自己的這個摔減率
[00:17:08] 對
[00:17:09] 這樣的話呢
[00:17:09] 就是每一個維度
[00:17:11] 他對於安的那個
[00:17:14] 記憶的那個隱藏狀態的話
[00:17:17] 他就是有自己獨立的一套
[00:17:20] 更新的那個頻率
[00:17:22] 這樣的話呢
[00:17:23] 就是從直接上來看
[00:17:25] 他就是能夠更好的利用這個
[00:17:28] 安有限的Hidden State
[00:17:30] 能夠提高這個performance
[00:17:33] 你們的設計邏輯和靈感來源於什麼
[00:17:35] 我感覺這個設計的話
[00:17:37] 其實我覺得像KD的話
[00:17:40] 他其實就是我前兩個工作的一個
[00:17:45] 就把之前有兩個工作的那種
[00:17:47] I'd掉把它合併在了一起
[00:17:49] 像我之前還有一個工作叫做
[00:17:52] Gatee的力量Tension
[00:17:54] 他就是就是有一個這種力度
[00:17:57] 比較吸的一個摔減率
[00:18:00] 然後
[00:18:02] 後面到Gatee的Dialand的時候
[00:18:04] 當時之所以沒有用到這種
[00:18:06] 力度比較吸的這個摔減率
[00:18:07] 就是因為當時就是算法本身
[00:18:11] 和這個KNO優化
[00:18:13] 他當時都沒有優化到一個比較好的狀態
[00:18:16] 所以當時就是考慮到這個效率的問題
[00:18:18] 就是背破就是只能用那個Mamba
[00:18:22] Tune那種力度更粗的一個摔減率
[00:18:26] 對然後後面的話呢
[00:18:28] 就是就是算法層面和KNO優化的層面的話
[00:18:34] 都是有一些很多進步的
[00:18:36] 然後到今年年初的這個時間點的話
[00:18:40] 就大家就覺得是不是可以重新來研究一下
[00:18:44] 能不能把這個翻棍你可以把它
[00:18:47] 把這個力度比較吸的這個摔減率
[00:18:50] 把它引回到這個Gatee的Dialand在裡面
[00:18:54] 你們設計完最初的效果怎麼樣
[00:18:57] 最初的效果的話
[00:18:58] 我記得張雲應該是之前是在
[00:19:02] 他應該是先試了一大堆這種混合Gatee的這種混法
[00:19:09] 然後他最開始是發現
[00:19:13] 混Gatee的Dialand比混其他的要好
[00:19:16] 然後後面他就是因為他們KIMI內部
[00:19:19] 他是有一個叫做Skilling Light的一個東西
[00:19:25] 就是說你在一個規模下面
[00:19:29] 你的表現好的話呢
[00:19:30] 那你就到下面一個規模去繼續 scale
[00:19:33] 就有點像通關一下他有很多很多關卡
[00:19:38] 然後你過了一關之後呢
[00:19:39] 他可能有到下一關就繼續去跟負了貪池去比
[00:19:44] 然後最開始的話可能就發現
[00:19:48] 就那種Habberl的Gatee的Dialand的話
[00:19:50] 他可能就是在有一些地方還是不如
[00:19:55] 那種負的那種Surfmice的貪池
[00:20:00] 然後後面的話呢
[00:20:02] 他就開始玩了一下就是那種
[00:20:06] 把那個DK把它換成這種
[00:20:09] 更加細緻的這個DK
[00:20:11] 然後他發現就是在他的一些實驗下面
[00:20:13] 他就發現這個他的這個提升還挺大的
[00:20:18] 一面理念的騰訊和difficxboss的騰訊
[00:20:21] 你自己覺得他們的表現哪個更好
[00:20:24] 他們分別可能適合什麼樣的任務
[00:20:27] 像這兩個貪池的話
[00:20:29] 他們其實是
[00:20:31] 相結的問題是一個問題
[00:20:33] 就是在這種長文本的DKoting下面
[00:20:36] 如何解決這個效率的問題
[00:20:39] 然後像Kimi
[00:20:42] 他走的是這種混合注意的路線
[00:20:44] 對其實千萬他也走的是這一條
[00:20:47] 現在主要是在逃入這種混合注意的這種路線
[00:20:51] 然後像DFCA的話
[00:20:52] 可能他們主要就是喜歡走這個
[00:20:55] 吸輸出列的這個路線
[00:20:57] 他們那個Kimi sparser騰訊
[00:20:58] 然後包括他們之前發的那個
[00:21:01] Late-to-sparser騰訊都是這種走的吸輸的路線
[00:21:05] 然後他們覺得可能吸輸是一種
[00:21:08] 更好的方式來降低這種DKoting的這個Cost
[00:21:14] 像DFCA的sparser騰訊的話
[00:21:19] 他應該是沒有負了騰訊的
[00:21:22] 對所以他應該是每一層都是DFCA sparser騰訊
[00:21:26] 對但是他每一層的話
[00:21:29] 他都要把所有的KiriCatch把它全部存下了
[00:21:33] 對然後他只能就是從一個TrapPoint
[00:21:37] 然後來經過一些真鈕
[00:21:40] 然後得到他的那個叫做Index的一個
[00:21:44] 東西來選那些Tobok的一個Token
[00:21:48] 最就是DepsyK的騰訊
[00:21:50] 然後像混合注意的最條路線的話
[00:21:54] 他還是有一些全局注意力的
[00:21:57] 然後他的那些比較快速的那些層
[00:22:00] 是一些線性注意力層
[00:22:02] 然後這個好處的話
[00:22:03] 就是說他可以使很多的KiriCatch
[00:22:06] 對然後混合注意的話
[00:22:09] 他就是不僅他能減少KiriCatch
[00:22:13] 他能減少很多KiriCatch
[00:22:15] 因為他絕大都是層都是這種
[00:22:17] Late-to-sparser的這種層
[00:22:19] 然後他同時也能提高這個Decoyant的效率
[00:22:23] 然後因為他減少了KiriCatch的Size
[00:22:26] 所以他做Decoyant的時候
[00:22:28] 可能就可以去用一些
[00:22:31] 更大的這個BatchSize
[00:22:33] 因為之前可能放不下
[00:22:35] 然後現在KiriCatch被減少了很多
[00:22:38] 然後這個時候可能就可以加到這個Batch的
[00:22:41] 這個Size
[00:22:43] 像DepsySparzel的Tension的話
[00:22:45] 他是沒有減少KiriCatch的作用的
[00:22:49] 但是他可以通過Sparzel的這個
[00:22:53] 機會來減少每一個頭肯深層的那個花費
[00:22:58] 對
[00:22:59] 還有一個講minimax
[00:23:01] 他們最新也做了一個算法的選擇
[00:23:03] 對吧
[00:23:04] 對像minimax的話
[00:23:06] 他上一版是Late-to-Tension
[00:23:10] 他應該算是這種混合
[00:23:13] 這種混合現行和平方
[00:23:16] 助理了一個先取
[00:23:18] 因為他年初發的那個M1的那個版本的話
[00:23:23] 是一個非常大規模的一個混合助理的一個時間
[00:23:28] 對
[00:23:29] 然後他們前幾天發了一個叫做M2的一個模型
[00:23:33] 然後這個模型
[00:23:34] 他現在就變成了一個Fort Tension
[00:23:37] 對
[00:23:38] 他既不是這種混合助理
[00:23:40] 他也不用Sparzel Tension
[00:23:42] 他就乾脆把他退回了那個Fort Tension
[00:23:47] 對
[00:23:48] 這是為什麼呀
[00:23:50] 我覺得是
[00:23:52] 我覺得他們的這個負責團
[00:23:55] 對他們非常的
[00:23:57] open然後他們分享了很多這種經驗
[00:24:00] 然後我覺得
[00:24:01] 所以今天都是很寶貴的
[00:24:03] 就是比方說
[00:24:05] 我知道他們說
[00:24:06] 就是他們第一版的
[00:24:09] 他們第一版的話
[00:24:10] 他們監控了一些指標
[00:24:12] 就是發現他們用到的那個Lighting
[00:24:15] Tension的那個模塊
[00:24:16] 在這些指標上面表現得都很好
[00:24:18] 然後這個Lighting的Tension
[00:24:20] 他又效率更高一點
[00:24:21] 所以他們最後就上這個Lighting的Tension了
[00:24:24] 對
[00:24:25] 然後後面他們發現就是
[00:24:27] 如果他們在一些比方說
[00:24:29] 那種叫做
[00:24:31] multi-hop reasoning
[00:24:33] 就是多跳的這個
[00:24:35] rhythm
[00:24:36] 你上面這種task的話呢
[00:24:38] 他發現這個調點會非常的大
[00:24:40] 對
[00:24:41] 然後這個的話就當初
[00:24:43] 用那個方的話呢
[00:24:44] 就是
[00:24:46] 因為他們最開始沒有去檢測這種
[00:24:49] 多跳推力的這個能力
[00:24:51] 然後他們主要只看那些
[00:24:53] 比如說mmulti-hop
[00:24:54] 然後這類的這種能力
[00:24:57] 然後他們就選了一個非常
[00:25:01] 就
[00:25:02] 就我來說的話
[00:25:02] 我覺得那個Netany的Tension的話呢
[00:25:04] 他其實是一個
[00:25:06] 比較弱的一個先行注意力
[00:25:09] 因為他那個機制就感覺
[00:25:11] 最近兩年先行注意力
[00:25:13] 這個領域發展了很多嘛
[00:25:15] 然後他用到了那個
[00:25:16] 音量的Tension
[00:25:18] 就給人的感覺
[00:25:19] 就是像是兩年前的一個
[00:25:20] 音量的Tension
[00:25:21] 就那個技術的那個
[00:25:23] 還挺有在兩年前
[00:25:25] 對
[00:25:26] 可能就是還用可能就是
[00:25:27] 因為他們
[00:25:29] 第一版他們的那個
[00:25:31] 做評價的那個Pap-N-N-
[00:25:33] 不夠
[00:25:34] 想盡吧
[00:25:35] 然後他們就選了
[00:25:37] 這麼一套比較
[00:25:38] 比較
[00:25:40] 略顯哪一個方案
[00:25:42] 然後最近的話呢
[00:25:44] 他們可能是想
[00:25:47] 做了這種
[00:25:48] Agentic Toss
[00:25:49] 可能然後像做這種
[00:25:51] Coding吧
[00:25:52] 然後像多跳推理
[00:25:55] 這一個能力的話呢
[00:25:56] 就是會在這種場面
[00:25:58] 下面變得非常重要
[00:26:00] 然後他們就發現
[00:26:01] Lineany Tension
[00:26:02] 可能跟富爾 Tension
[00:26:04] 他之間的這個
[00:26:07] Poformance的這個差距還挺大的
[00:26:09] 然後他們就暫時退回了
[00:26:12] 這個全部都是
[00:26:14] Somage Tension的這個富爾 Tension的這個價格
[00:26:17] 對
[00:26:17] 但他們說
[00:26:18] 他們還在繼續探索
[00:26:20] 這種混合主義的價格
[00:26:23] 說不定他們下一版
[00:26:24] Amson又變成混合主義的價格
[00:26:27] 你怎麼看待就是
[00:26:28] 大家在這種算法
[00:26:30] 上的不同的選擇
[00:26:31] 或者是反覆
[00:26:32] 像歷史的話
[00:26:33] 就會螺旋上升嘛
[00:26:35] 就是一套技術方
[00:26:36] 就肯定是要經過很多
[00:26:38] 很多
[00:26:40] 驗證才能最後定下來的嘛
[00:26:42] 對
[00:26:42] 像
[00:26:44] 像Amson可能當時就是沒有驗證
[00:26:46] 比較充分嘛
[00:26:47] 所以當時就比較
[00:26:49] 草率的上了
[00:26:50] 然後後面發現
[00:26:51] 他在這種對跳推理上面
[00:26:52] 他效果不好
[00:26:53] 然後就暫時退回了
[00:26:55] 這個也是
[00:26:56] 很正常的嘛
[00:26:57] 對
[00:26:58] 歸虎公司現在
[00:27:00] 對於混合主義
[00:27:01] 以及制他們的探索方向是什麼樣的
[00:27:04] 各家同學說什麼不一樣
[00:27:06] 這個我感覺
[00:27:07] 不能講了
[00:27:08] 哦
[00:27:09] OPEN I什麼的可以講了
[00:27:11] OPEN I的話
[00:27:12] 我只能講有一些
[00:27:16] 就是有一些有paper的一些法案
[00:27:19] 就是沒有paper的法案
[00:27:20] 我是不會講的
[00:27:22] OPEN的話
[00:27:22] 他是
[00:27:24] 他是表示像GPD3的話
[00:27:26] 他在那個
[00:27:28] 他的那個Tagliaical Report裡面就講了
[00:27:31] 他會用到一個混合的
[00:27:33] 一個全局的一個注意力
[00:27:35] 和一個Local的一個SlayerMino的探索
[00:27:37] 這麼一個混合的一個方案
[00:27:39] 對
[00:27:40] 像這個的話
[00:27:41] 他是在GPD3的那個報告裡面
[00:27:44] 就已經明確地寫出來了
[00:27:45] 所以這個是可以講的
[00:27:46] 對
[00:27:47] 然後像他們最近的那個
[00:27:49] OSS的那個發出來的那個
[00:27:52] 開源模型嗎
[00:27:53] 他們也是用到這種華東
[00:27:55] 注意力的這一套方案
[00:27:57] 對
[00:27:58] 所以他們應該就一直在用
[00:28:00] 這一套華東注意力的方案吧
[00:28:02] 對
[00:28:03] 我們到後就講一下
[00:28:04] 因為你剛剛說
[00:28:05] 你那天這兩年
[00:28:06] 發展也有很多
[00:28:07] 你們給大家講一下
[00:28:08] 它的這個發展線索
[00:28:10] 好
[00:28:11] 像
[00:28:14] 你那樣探索的話
[00:28:15] 他最開始的話
[00:28:16] 我覺得就是非常的
[00:28:17] BORK
[00:28:18] 對
[00:28:19] 他就算他在短文
[00:28:20] 不能下面
[00:28:21] 他也BORK
[00:28:22] 然後
[00:28:24] 因為他那個最早的
[00:28:25] 你那樣探索是12年
[00:28:26] 12年發明的嗎
[00:28:28] 然後我覺得他可能
[00:28:29] 這中間的這幾年
[00:28:32] 就是在Nanguage Modeling
[00:28:33] 原建模上面
[00:28:34] 他都沒有
[00:28:35] 效果沒有跑到很好
[00:28:37] 然後每一個表有代表現場
[00:28:39] 公通就是Ran Live
[00:28:40] 然後他就是通過
[00:28:42] 加一個以往衰減的一個機制
[00:28:45] 然後就發現
[00:28:47] Ninja探索
[00:28:48] 他
[00:28:49] Skyal上去他
[00:28:50] 在原建模上面
[00:28:52] 還是可以取得一個
[00:28:55] 比較好的一個效果的
[00:28:56] 對
[00:28:57] 然後Ran Live的話
[00:28:59] 他就是往他加了一個
[00:29:02] 就是輸入無關的一個DK嗎
[00:29:05] 輸入無關的DK的話
[00:29:06] 就是說
[00:29:07] 他那個以往率
[00:29:08] 他是跟輸入沒有關係的
[00:29:11] 比方說他的以往率是
[00:29:13] 0.99
[00:29:15] 那樣子的話
[00:29:16] 他每過一個頭肯的話
[00:29:18] 他前面的那個
[00:29:20] Pedance Data
[00:29:20] 他就要沉上0.99
[00:29:23] 真的換了他就以往掉
[00:29:24] 他1%的這個東西了
[00:29:26] 然後他在把新的這個東西
[00:29:27] 把它寫進去
[00:29:29] 這就是一個叫做輸入無關的一個衰減
[00:29:32] 這就是Ran Live他用到的一個技術
[00:29:35] 然後這種輸入無關的這種
[00:29:39] 以往的話
[00:29:40] 他在之後應該是被
[00:29:43] 逐漸替換成了那種輸入相關的一個衰減
[00:29:47] 就比如說像我
[00:29:49] 之前的一個叫GateNinja探索
[00:29:51] 前面也提到了
[00:29:52] 就是加了一個門控的一個機制
[00:29:56] 然後像Mamba和MambaTune的話
[00:29:58] 他們其實也是跟現行注意力的話
[00:30:00] 是很有很多年系的
[00:30:03] 尤其是MambaTune
[00:30:04] MambaTune的話
[00:30:05] 讓他基本上就可以看成是
[00:30:07] 現行注意力
[00:30:09] 然後他加了一個衰減
[00:30:12] 但是這個衰減跟Ran Live非常像
[00:30:14] 然後他跟Ran Live去
[00:30:15] 比如說那個衰減
[00:30:17] 他是有輸入來決定的
[00:30:20] 就是每一個頭肯
[00:30:22] 他的衰減率
[00:30:23] 他就可能不一樣
[00:30:25] 對
[00:30:25] 就不要說他遇到有一些頭肯
[00:30:28] 他覺得這些前面的內容
[00:30:30] 沒有不要忘
[00:30:31] 他就把可以把那個衰減率視為一
[00:30:34] 這樣子的話
[00:30:35] 前面就根本不去做這種衰減
[00:30:39] 然後如果他遇到一些頭肯
[00:30:41] 讓他覺得前面這些信息已經沒有用了
[00:30:45] 那這樣子的話
[00:30:46] 他可以在那位置上
[00:30:48] 用一個
[00:30:49] 比如說讓他的衰減率等於
[00:30:51] 那個DKD0
[00:30:52] 這樣的話
[00:30:53] 前面的那個State就是被完全地
[00:30:55] 把它忘掉了
[00:30:57] 因為他成了一個0上去
[00:30:58] 所以他前面的State
[00:30:59] 就完全沒有了
[00:31:00] 對
[00:31:01] 像這種輸入相關的這種DK
[00:31:04] 他就是比較靈活
[00:31:06] 能夠通過這個數據
[00:31:08] 來動態的學
[00:31:09] 什麼時候該去遺忘
[00:31:11] 然後什麼時候該去記
[00:31:13] 這前面這個State
[00:31:16] 然後這是
[00:31:17] 第一個比較大的改進
[00:31:20] 就是把這個衰減從輸入無關
[00:31:23] 變成輸入相關
[00:31:24] 然後第二個改進的話
[00:31:26] 就是
[00:31:28] 德塔奈這一種路線
[00:31:29] 就是把他的更新的那個公式
[00:31:33] 從最開始那個簡單的那個
[00:31:36] 相機量探訊
[00:31:37] 他用的其實應該叫做一個叫做
[00:31:39] Haven rule
[00:31:40] 對
[00:31:41] 這個如果的話
[00:31:42] 他就是簡單的把Key和Value
[00:31:44] 他們的外籍
[00:31:46] 把它加到那個HavenState上面
[00:31:49] 對
[00:31:49] 他就是一個Haven的一個R rule
[00:31:52] 然後像德塔奈這一套模型
[00:31:55] 他用的是一個叫做DataRule的一個東西
[00:31:58] DataRule的東西的話
[00:31:59] 就是說每一步的時候
[00:32:02] 他先用這個Key去取出
[00:32:05] 那個Memory裡面
[00:32:07] 會返回一個值
[00:32:09] 這就是這個Key
[00:32:10] 他在這個Memory裡面
[00:32:12] 他本來對應的這個Value
[00:32:14] 我們管他叫做All the Value
[00:32:16] 然後這個Key
[00:32:18] 他又會有一個輸入的一個Value
[00:32:20] 我們把它叫做InputValue
[00:32:23] 對
[00:32:24] 然後
[00:32:25] 因為他是一個有一個觀點
[00:32:27] 記憶網絡的一個視角
[00:32:29] 然後每一個Key
[00:32:30] 我們想讓他只對應一個Value
[00:32:32] 然後模型也不知道
[00:32:34] 他應該是對應前面的Value
[00:32:36] 還是輸入的Value
[00:32:38] 真的後來我們有一個可以學習的一個
[00:32:41] 他叫做Bita
[00:32:42] BitaR我們可以看成
[00:32:43] 他是一個
[00:32:45] 值在你到一之間的一個
[00:32:48] 一個系數
[00:32:49] 用來決定
[00:32:50] 我們要用多少的前面的這個
[00:32:53] All the Value
[00:32:54] 然後要用多少這個輸入的Value
[00:32:57] 我們會做一個現性組合
[00:33:00] 通過這個系數
[00:33:01] 我們回來做一個現性組合
[00:33:03] 然後得到他最後的新的Value
[00:33:07] 然後把這個舊的Value和Key
[00:33:09] 他的外籍
[00:33:10] 把它從Memory裡面剪去
[00:33:12] 然後把這個新的Value和Key
[00:33:15] 他的外籍把它加到這個
[00:33:17] 觀點網絡裡面
[00:33:19] 對
[00:33:19] 這就是DataNet
[00:33:21] 他那個跟新公式的一個
[00:33:24] Hallel的一個Ido
[00:33:28] 然後相比於
[00:33:29] 你兩三成的話
[00:33:30] 他其實是有一個
[00:33:32] 剪法的一個操作在裡面的
[00:33:33] 他不但可以
[00:33:35] 就加法的話
[00:33:36] 可以把它想像成是
[00:33:38] 往這個記憶網絡裡面
[00:33:39] 去把它去寄東西
[00:33:42] 那麼剪法的話
[00:33:43] 就可以把它理解成
[00:33:44] 從這個記憶網絡裡面
[00:33:46] 把它刪除一些東西
[00:33:48] 對
[00:33:49] 像這個畫面就會比較
[00:33:51] 更加有針對性的來刪東西
[00:33:53] 像之前那個DK的話
[00:33:55] 可能就是很多維度一起在做DK
[00:33:57] 像現在的話
[00:33:58] 就是就是只取某一個分量
[00:34:01] 然後他有一些
[00:34:04] 非常有目標性的
[00:34:05] 這種刪東西的操作在裡面
[00:34:10] 對
[00:34:11] 所以這Data
[00:34:13] 如果也代表了
[00:34:14] 第二個改進的話
[00:34:15] 就應該是
[00:34:17] 尼諜爾探訊這個領域裡面
[00:34:19] 最近的第二個改進了嗎
[00:34:21] 就包括像DataNet
[00:34:24] Gate DataNet
[00:34:25] 像Rockle Cyber
[00:34:27] 他們都用到這個DataRoo
[00:34:29] 對
[00:34:30] 為什麼尼諜爾探訊
[00:34:31] 從一開始效果不好
[00:34:32] 到慢慢的一步不改進
[00:34:33] 大家就是相信它還是Promise
[00:34:36] 我覺得每一次大家關心尼諜爾探訊
[00:34:39] 那肯定是因為大家
[00:34:40] 碰到了一些ContextWord
[00:34:43] 對
[00:34:43] 像大家最開始的時候
[00:34:45] 去研究尼諜爾探訊
[00:34:47] 就是比方說
[00:34:48] 在2020年左右的時候
[00:34:50] 大家去研究尼諜爾探訊
[00:34:52] 是因為那個時候
[00:34:53] 大家遇到第一個
[00:34:55] ContextWord
[00:34:57] 就是遇到一個
[00:34:58] Context這種強
[00:35:00] 然後他就是撞到這個強
[00:35:03] 就是他如果想
[00:35:04] 繼續提高Context
[00:35:06] 那就是能找一些
[00:35:08] 複雜度小魚
[00:35:09] Surfmice小魚平方的一些
[00:35:12] 東西來了
[00:35:14] 因為當時像BIRT
[00:35:15] 在那個年代
[00:35:16] 他的訓練其實就是500以上
[00:35:19] 然後當時可能覺得2048
[00:35:22] 當時在當時的那個視角
[00:35:24] 可能8192就是一個
[00:35:26] 算一個長文本
[00:35:27] 因為那個地方就非常的慢
[00:35:30] 然後後面就隨著Flash Irtension
[00:35:33] 這個技術的單身
[00:35:35] 然後就打破了這一毒牆
[00:35:37] 然後現在看來
[00:35:39] 就是8192就是一個
[00:35:40] 非常短的一個文本了
[00:35:41] 就在這上面做訓練
[00:35:43] 是沒有一點這個壓力的
[00:35:44] 但是在之前
[00:35:46] 之前的話
[00:35:47] 他沒有Flash Irtension的時候
[00:35:49] 他是
[00:35:50] 計算的話
[00:35:51] 他需要把這個
[00:35:52] 平方的這個
[00:35:54] Tension舉證
[00:35:55] 要把他
[00:35:56] 先把他
[00:35:58] 就是要把他使
[00:36:00] 實力化在那個
[00:36:01] Global Memory上面
[00:36:03] 然後要把他
[00:36:04] 從Global Memory裡面
[00:36:05] 把它搬回那個
[00:36:07] Flash Memory裡面
[00:36:10] 這樣子的話
[00:36:11] 他那個
[00:36:12] Memory的那個
[00:36:13] 毒血
[00:36:14] 他整體的那開箱
[00:36:15] 是非常的大的
[00:36:16] 然後同時
[00:36:17] 因為這個
[00:36:18] Tension舉證
[00:36:19] 會被
[00:36:20] 實力化在那個
[00:36:21] Global Memory裡面
[00:36:22] 他可能就是會帶來一個
[00:36:24] Out of Memory的一個問題
[00:36:26] 這就是最開始
[00:36:27] 大家研究
[00:36:28] 你兩Tension這個模特點
[00:36:30] 然後
[00:36:32] 隨著Flash Irtension的話
[00:36:34] 大家就發現
[00:36:35] 這一毒牆
[00:36:36] 就其實已經
[00:36:37] 破了嗎
[00:36:38] 就是既然我們
[00:36:39] 能用這種
[00:36:40] Exit方式
[00:36:41] 來直接
[00:36:42] 算這個
[00:36:43] 那我們就
[00:36:44] 沒必要找一些
[00:36:45] 去逼近他嗎
[00:36:46] 對
[00:36:47] 所以大家就
[00:36:48] Ninliot Tension的演技
[00:36:49] 就開始
[00:36:50] 沒有那麼受關注了
[00:36:52] 然後
[00:36:53] 直到
[00:36:54] 最近
[00:36:55] 這邊要說
[00:36:57] 像這種
[00:36:58] 長文本的
[00:36:59] Decoding
[00:37:00] 又重新成了一個
[00:37:01] 需求量非常
[00:37:03] 大的一個東西
[00:37:04] 然後
[00:37:05] 就是這種
[00:37:06] Out of Memory
[00:37:07] 他上面摸得
[00:37:08] 他要塗很多很多
[00:37:09] 偷肯
[00:37:10] 然後要都是這種
[00:37:11] Decoding
[00:37:12] 然後這個花銷的話
[00:37:13] 就是
[00:37:14] 就會讓人們
[00:37:15] 又不由自主
[00:37:16] 又重新來審視
[00:37:18] 這一套的技術
[00:37:19] 然後這一套技術
[00:37:20] 他本身
[00:37:21] 在學界
[00:37:22] 又有了這麼久的發生嗎
[00:37:24] 尤其是在Flash Irtension之後
[00:37:26] 就學界
[00:37:28] 其實也一時到
[00:37:29] 就是說
[00:37:30] 如果像Ninliot Tension
[00:37:31] 這一套模型
[00:37:32] 如果想讓大家
[00:37:33] 被大家接受的話
[00:37:35] 那他
[00:37:36] 硬件上面的效率
[00:37:37] 是非常關鍵的
[00:37:38] 對
[00:37:39] 這也就是
[00:37:40] 為什麼我最開始的時候
[00:37:41] 就是一個叫做
[00:37:42] Flash Ninliot Tension
[00:37:43] 一個Pro站
[00:37:44] 就是
[00:37:45] 致力於
[00:37:46] 把這些
[00:37:47] 寗liot Tension的
[00:37:48] 這些變種
[00:37:49] 然後用Tryton
[00:37:50] 把它在
[00:37:51] 就寫了一個
[00:37:52] 酷
[00:37:53] 然後寫了很多
[00:37:54] 可能讓它能夠在
[00:37:55] 當代的硬件上面
[00:37:57] 主要是GPU上面
[00:37:58] 能夠來快速運行
[00:38:00] 所以它
[00:38:01] 核心
[00:38:02] 是效率更高
[00:38:03] 價格更低
[00:38:04] 就是每當SoftMise
[00:38:06] 它的這個
[00:38:07] 它的效率
[00:38:08] 變成一個平靜的時候
[00:38:09] 大家就會
[00:38:10] 回來看Ninliot Tension
[00:38:11] 大家就是這樣子
[00:38:12] 的一個歷史
[00:38:13] 現在是公式了嗎
[00:38:15] 我覺得Ninliot Tension
[00:38:18] 我現在
[00:38:19] 公式的設計
[00:38:20] 就是說
[00:38:21] 純Ninliot Tension
[00:38:22] 是不 work的
[00:38:23] 對
[00:38:24] 就它在
[00:38:25] 這種長文本
[00:38:26] 下面它是有一些
[00:38:27] 漫頭的一些缺陷的
[00:38:29] 對
[00:38:30] 然後
[00:38:31] 現在大家
[00:38:32] 一般都不會
[00:38:33] 去嘗試這種
[00:38:34] 純陷信的這種
[00:38:36] 這種模型
[00:38:38] 然後像一些
[00:38:40] 比較
[00:38:42] 責集中的一些
[00:38:43] 法案像這種
[00:38:44] 混合注意的話
[00:38:45] 它就是
[00:38:46] 還是有很多
[00:38:47] 很多的
[00:38:48] 陷信注意力層
[00:38:49] 但是它
[00:38:50] 還是有
[00:38:51] 一定
[00:38:52] 舒目的這種
[00:38:53] 全局注意力層
[00:38:55] 這樣子的話
[00:38:56] 它這個模型
[00:38:58] 它的下線
[00:38:59] 是有保證的
[00:39:00] 對它出力長文本
[00:39:01] 它也是有
[00:39:02] 有很多
[00:39:03] 全局注意力層
[00:39:04] 像
[00:39:05] 全線性的
[00:39:06] 這個網絡的話
[00:39:07] 它可能
[00:39:08] 從理論上面
[00:39:09] 它就沒有辦法
[00:39:10] 做
[00:39:11] 那種長文本
[00:39:12] Task
[00:39:13] 因為它的
[00:39:14] 阿恩的
[00:39:15] 狀態
[00:39:16] 是
[00:39:17] 很定的
[00:39:18] 然後隨著
[00:39:19] 那個
[00:39:20] 它那個
[00:39:21] 長度增加的話
[00:39:22] 那它早晚
[00:39:23] 會
[00:39:24] 存不下
[00:39:25] 會
[00:39:26] 會
[00:39:27] 損失
[00:39:28] 很多
[00:39:29] 那種
[00:39:30] 金度在你
[00:39:31] 然後
[00:39:32] 但是像
[00:39:34] 混合這個話
[00:39:35] 它有很多
[00:39:36] 那個
[00:39:37] 全局注意力在你
[00:39:38] 所以還是可以
[00:39:39] 通過
[00:39:40] 這些
[00:39:41] 全局注意力
[00:39:42] 這些長文本的
[00:39:43] Task
[00:39:44] 對
[00:39:45] 然後像
[00:39:46] 表哥像
[00:39:47] Kimi
[00:39:48] Nino這個
[00:39:49] Paper
[00:39:50] 然後像之前
[00:39:51] 前文山
[00:39:52] 那個
[00:39:53] Kun
[00:39:54] Kun 3
[00:39:55] Nex
[00:39:56] 它
[00:39:57] 就是
[00:39:58] 它
[00:39:59] 那個長文本
[00:40:00] 比如說
[00:40:01] 像
[00:40:03] Ruler
[00:40:04] 比如說像
[00:40:05] Task
[00:40:06] 那個
[00:40:07] 表現
[00:40:08] 沒有掉點
[00:40:09] 所以它在長文本
[00:40:10] 上面
[00:40:11] 還是有一些
[00:40:12] 有一定的能力的
[00:40:13] 然後
[00:40:14] 混合
[00:40:15] 注意力
[00:40:16] 就會
[00:40:17] 很多
[00:40:18] 很多地方的關注
[00:40:19] 但是
[00:40:20] 我也不知道它
[00:40:21] 共識
[00:40:22] 因為
[00:40:23] 不同的地方
[00:40:24] 還是在
[00:40:25] 嘗試
[00:40:26] 不同的方案
[00:40:27] 比如說
[00:40:28] 像
[00:40:29] 文裡面
[00:40:30] 你們提出的是
[00:40:31] 美三層的KDA
[00:40:32] 也就是
[00:40:33] Kimi Delta
[00:40:34] Tension
[00:40:35] 機制
[00:40:36] 插入一層
[00:40:37] 全注意力
[00:40:38] 機制
[00:40:39] Tension
[00:40:40] 這個比例是
[00:40:41] 怎麼確定的
[00:40:42] 這個比例重要嗎
[00:40:43] 我覺得
[00:40:44] 3比1
[00:40:45] 現在有快變成
[00:40:46] 一個公司了吧
[00:40:47] 對
[00:40:48] 像
[00:40:49] MiniMess
[00:40:50] 它之前
[00:40:51] 是一個
[00:40:52] 7比1的一個比例
[00:40:53] 對
[00:40:54] 然後
[00:40:55] 7比1的話
[00:40:56] 可能
[00:40:57] SuffMess
[00:40:58] 沒有那麼好
[00:40:59] 然後我記得
[00:41:00] 之前
[00:41:01] 就是
[00:41:02] 自己
[00:41:03] 他也發了一個
[00:41:04] paper
[00:41:05] 就是來研究
[00:41:06] 這個
[00:41:07] Hybert的架構
[00:41:08] 他需要
[00:41:09] 百分之多
[00:41:10] 吵了
[00:41:11] SuffMess
[00:41:12] Tension
[00:41:13] 然後他們的
[00:41:14] 簡論也是說
[00:41:15] 就他們做了
[00:41:16] Pretrent
[00:41:17] Fans Grash的一些
[00:41:18] 實驗
[00:41:19] 然後他們
[00:41:20] 就是
[00:41:21] 通過
[00:41:22] 改那個
[00:41:23] 不同的
[00:41:24] 粘量
[00:41:25] Tension
[00:41:26] 的這個
[00:41:27] 改那個
[00:41:28] 混合的比例
[00:41:29] 然後他們的
[00:41:30] 結論大概
[00:41:31] 就是說
[00:41:32] 3比1的
[00:41:33] 比例是
[00:41:34] 最好的
[00:41:35] 然後
[00:41:36] Gate Data
[00:41:37] 那這個
[00:41:38] 模塊
[00:41:39] 就是比
[00:41:40] 其他的那些
[00:41:41] 另外的那些
[00:41:42] 看著的要好的
[00:41:44] 所以3比1
[00:41:45] 然後
[00:41:46] 後面
[00:41:47] Next的話
[00:41:48] 他也是
[00:41:49] 用到3比1
[00:41:50] Gate Data
[00:41:51] 那這個方案
[00:41:52] 然後
[00:41:53] 這個方案
[00:41:54] 應該是
[00:41:55] 不同的場上
[00:41:56] 更好的
[00:41:57] 對
[00:41:58] 然後這個可能
[00:41:59] 是最開始
[00:42:00] MiniMax沒有
[00:42:01] 驗證重訟
[00:42:02] 就是
[00:42:03] 現在也說他們
[00:42:04] 可能最開始的
[00:42:05] 評測還是
[00:42:06] 有一些不足
[00:42:07] 所以他們
[00:42:08] 用到一個
[00:42:09] 更加
[00:42:10] Gate Data
[00:42:11] 的一個方案
[00:42:12] 就是
[00:42:13] 7比1
[00:42:14] 然後現在的話
[00:42:15] 基本上
[00:42:16] 就是
[00:42:17] 都回到3比1
[00:42:18] 上面來了
[00:42:19] 然後我覺得
[00:42:20] 3比1
[00:42:21] 應該
[00:42:22] 就是
[00:42:23] 在這個
[00:42:24] 不公實的
[00:42:25] 比例
[00:42:26] 來換這個
[00:42:27] 模型
[00:42:28] 是不是
[00:42:29] 你們在
[00:42:30] 算法設計的時候
[00:42:31] 平衡
[00:42:32] 能力和
[00:42:33] 其算效率
[00:42:34] 這兩者
[00:42:35] 是
[00:42:36] 它的核心
[00:42:37] 比如說
[00:42:38] 確實
[00:42:39] 我覺得
[00:42:40] 還是有一些
[00:42:41] trade of
[00:42:42] 像
[00:42:43] 全球助力的話
[00:42:44] 他如果
[00:42:45] 太少的話
[00:42:46] 我感覺
[00:42:47] 就
[00:42:49] Rezoning
[00:42:50] Task
[00:42:51] 然後
[00:42:52] 相當的
[00:42:53] Task
[00:42:54] 他可能
[00:42:55] 一些
[00:42:56] 說的
[00:42:57] Task
[00:42:58] 沒有什麼影響
[00:42:59] Bell
[00:43:00] MMU
[00:43:01] 但是
[00:43:02] 那些
[00:43:03] 場文本
[00:43:04] 和推理的Task
[00:43:05] 他應該是
[00:43:06] 能看到的
[00:43:07] 那個
[00:43:08] 卸相會
[00:43:09] 比較大的
[00:43:10] 但是
[00:43:11] 從另外一方面來說
[00:43:13] 也不是說
[00:43:14] Tanzen
[00:43:15] 成月
[00:43:16] 多月好嗎
[00:43:17] 因為
[00:43:18] 像大家
[00:43:19] 訓完之後
[00:43:20] 會發現
[00:43:21] 9大多
[00:43:22] 說
[00:43:23] 一些關鍵的
[00:43:24] 存在
[00:43:25] 他是
[00:43:26] 有用的
[00:43:27] 他不是
[00:43:28] 每一層的
[00:43:29] 他
[00:43:30] 本身
[00:43:31] 自己
[00:43:32] 他就是
[00:43:33] 有一個
[00:43:34] 榮譽
[00:43:35] 都在裡面的
[00:43:36] 這次
[00:43:37] 他就
[00:43:38] 給我們帶來
[00:43:39] 一些機會
[00:43:40] 這
[00:43:41] 表示
[00:43:42] 我們可以
[00:43:43] 把一些
[00:43:44] 把它
[00:43:45] 換成一些
[00:43:46] 現身
[00:43:47] 所以
[00:43:48] 混合的
[00:43:49] 價格
[00:43:50] 我覺得
[00:43:51] 他不一定
[00:43:52] 可能
[00:43:53] 是一個全面
[00:43:54] 更好的
[00:43:55] 一個T-DIE方案
[00:43:56] 然後
[00:43:57] 像
[00:43:58] Mini Max
[00:43:59] 他
[00:44:00] 之前
[00:44:01] 他們也
[00:44:02] 發了
[00:44:03] 他說
[00:44:04] 他們發現
[00:44:05] Hybrid
[00:44:07] 或
[00:44:08] 華創墜率
[00:44:09] 在那個
[00:44:10] 場文本的
[00:44:11] Motry Hop
[00:44:12] 多跳推理
[00:44:13] 會有缺陷
[00:44:14] 對
[00:44:15] 像這個
[00:44:16] 的話
[00:44:17] 我覺得
[00:44:18] 應該是
[00:44:19] 現在
[00:44:20] 唯一的一個問題
[00:44:22] 對
[00:44:23] 因為他
[00:44:24] 就我
[00:44:25] 所知的話
[00:44:26] 我覺得
[00:44:27] 他在
[00:44:28] 其他
[00:44:29] 上面
[00:44:30] 基本上
[00:44:31] 是不會
[00:44:32] 全部都是
[00:44:33] Sophie
[00:44:34] 麥色彈身要
[00:44:36] 差的
[00:44:37] 他
[00:44:38] 會在這個
[00:44:39] 這個
[00:44:40] 比較
[00:44:41] 好理解
[00:44:42] 像這種
[00:44:43] 多跳推理的話
[00:44:44] 他
[00:44:45] 就是
[00:44:46] 比較
[00:44:47] 吃
[00:44:48] 他的這個
[00:44:49] 陳述
[00:44:50] 對
[00:44:51] 這個
[00:44:52] 我覺得
[00:44:54] 我非常
[00:44:55] 吃
[00:44:56] 全球
[00:44:57] 注意力的
[00:44:58] 陳述的這個
[00:44:59] 任務
[00:45:00] 不是
[00:45:01] 很多
[00:45:02] 可能
[00:45:03] 就只有這種
[00:45:04] 多跳推理
[00:45:05] 然後這種
[00:45:06] 場文本
[00:45:07] 做Rizzer
[00:45:08] 會稍微吃一點
[00:45:10] 然後
[00:45:11] 其他
[00:45:12] 很多Tas
[00:45:13] 基本上
[00:45:14] 不吃的話
[00:45:15] 那他是
[00:45:16] 完全不會受影響的
[00:45:17] 我覺得
[00:45:18] 就是
[00:45:20] 就是我們去開發一些
[00:45:22] 硬件高效
[00:45:23] 但是他表達
[00:45:24] 你更好的一些
[00:45:25] R&D的話
[00:45:26] 他
[00:45:28] 這個
[00:45:29] Gap他是有可能
[00:45:30] 被
[00:45:31] 直接被
[00:45:32] 縮小
[00:45:33] 甚至
[00:45:34] 甚至會
[00:45:35] 晚超這個Gap的
[00:45:37] 對
[00:45:38] 這是比方說
[00:45:39] 像
[00:45:40] Kimi
[00:45:41] 他
[00:45:42] 最近這個
[00:45:43] 尼尼爾
[00:45:44] 就張雲
[00:45:45] 之前完
[00:45:46] 就是把這個
[00:45:47] 力度
[00:45:48] 服的這個
[00:45:49] DK
[00:45:50] 換成力度
[00:45:51] 系的DK之後
[00:45:52] 他在這些
[00:45:53] Motihol Rydin
[00:45:54] Coding
[00:45:55] 和Math
[00:45:56] 這些Tas
[00:45:57] 上面
[00:45:58] 他那個提升
[00:45:59] 還是比較可觀的
[00:46:00] 對
[00:46:01] 然後
[00:46:02] 就是說
[00:46:03] 這些Tas
[00:46:04] 就是
[00:46:05] Hyper可以做得
[00:46:06] 更好
[00:46:07] 然後
[00:46:08] 現在我覺得
[00:46:09] 混合
[00:46:10] 現身最厲害
[00:46:11] 只是
[00:46:12] 一個開始
[00:46:13] 對
[00:46:14] 然後我覺得
[00:46:15] 就是
[00:46:16] 很有可能做出
[00:46:17] 就是
[00:46:18] 更好的這種
[00:46:19] 注意機制的
[00:46:20] 就是
[00:46:21] 可以去
[00:46:22] 掉一下
[00:46:23] 那個
[00:46:24] 先進
[00:46:25] 所以
[00:46:26] 你的過程中
[00:46:27] 有
[00:46:28] 給Kimi
[00:46:30] 什麼
[00:46:31] 算法
[00:46:32] 對
[00:46:33] 我就是
[00:46:34] 就是
[00:46:35] 張雲
[00:46:36] 想完
[00:46:37] 那個
[00:46:38] 翻棍
[00:46:39] DK
[00:46:40] 然後
[00:46:41] 我就
[00:46:42] 幫他想
[00:46:44] 這個工作的
[00:46:45] 唯一的
[00:46:46] 貢獻
[00:46:47] 因為他
[00:46:48] 這個
[00:46:49] 都是
[00:46:50] 竟然
[00:46:51] 都是張雲
[00:46:52] 在
[00:46:53] Kimi
[00:46:54] 做了很多
[00:46:55] 很多
[00:46:56] oblacen
[00:46:57] 基本上
[00:46:58] 都他做的
[00:46:59] 所以
[00:47:00] 快點
[00:47:01] 基本上
[00:47:02] 都在
[00:47:03] 他了
[00:47:04] 你不在我這裡
[00:47:05] 然後
[00:47:06] 然後
[00:47:07] 像這個算法
[00:47:09] 的話
[00:47:10] 我覺得
[00:47:11] 其實
[00:47:12] 也是
[00:47:13] 能夠
[00:47:14] 把那個
[00:47:15] GTD
[00:47:16] 他那個
[00:47:17] 球膩的
[00:47:18] 那個算法
[00:47:19] 把它
[00:47:20] 減少一次
[00:47:21] 對
[00:47:22] 然後我看完
[00:47:23] 那個算法之後
[00:47:24] 我就發現
[00:47:25] 那我
[00:47:26] 就可以
[00:47:27] 把那個
[00:47:28] GTDD
[00:47:29] 他那個球膩
[00:47:30] 把它
[00:47:31] 減少一次
[00:47:32] 然後我又
[00:47:33] 緊接著
[00:47:34] 我又推了一個
[00:47:35] 能夠
[00:47:36] 試用於
[00:47:37] KDD的這個算法
[00:47:38] 然後我就
[00:47:39] 把這個算法
[00:47:40] 告訴張雲
[00:47:41] 然後張雲
[00:47:42] 然後就發現
[00:47:43] 這個算法
[00:47:44] 還是
[00:47:45] 他的這個
[00:47:46] Skateboard
[00:47:48] 比之前的那個
[00:47:49] 算法要好一點的
[00:47:50] 對
[00:47:51] 問一個
[00:47:52] 很Juner的問題
[00:47:53] 是一個研究員
[00:47:54] 想問你的
[00:47:55] Antention
[00:47:56] 到底應該怎麼設計
[00:47:57] 這個問題的話
[00:47:59] 我覺得
[00:48:00] 現在可能
[00:48:01] 就只有
[00:48:02] 兩條
[00:48:03] 不要主流的路線吧
[00:48:05] 一種就是
[00:48:07] Hyper線線嘛
[00:48:09] 然後一種
[00:48:10] 就是Spaast嘛
[00:48:11] 然後我覺得
[00:48:12] 這兩種
[00:48:13] 他其實
[00:48:14] 都是非常的
[00:48:15] Promising的
[00:48:16] 對
[00:48:18] 然後
[00:48:19] 另外可能有一些
[00:48:21] 比較
[00:48:22] 非主流的一些
[00:48:23] 彈省設計嘛
[00:48:24] 就比方說
[00:48:25] 我看上次
[00:48:26] Mata還放了一個論文
[00:48:28] 就是搞一個
[00:48:29] Santh方的彈省
[00:48:30] 就是
[00:48:31] 顯平方
[00:48:32] 複雜度
[00:48:33] 他還不夠
[00:48:34] 他還要搞一個
[00:48:35] Santh方的
[00:48:38] 然後像
[00:48:39] 有些地方
[00:48:40] 他有一些
[00:48:41] 比較有意思的
[00:48:42] 一些
[00:48:43] 平方複雜度的
[00:48:44] 一些彈省的變種嘛
[00:48:45] 比方說
[00:48:46] 拜登
[00:48:47] 他之前
[00:48:48] Data
[00:48:49] Former的
[00:48:50] 就像那就是
[00:48:51] 把Data
[00:48:52] 入的思想
[00:48:53] 把他引入到
[00:48:54] Somayser
[00:48:55] 彈省
[00:48:56] 能夠讓他表達
[00:48:57] 那更強
[00:48:58] 然後像這個
[00:48:59] 工作
[00:49:00] 我覺得
[00:49:01] 非常有意思
[00:49:02] 然後
[00:49:03] 改進
[00:49:04] 注意力的話
[00:49:05] 他要麼
[00:49:06] 就是
[00:49:07] 把Somayser
[00:49:08] 做得更好嘛
[00:49:09] 大家搞笑的
[00:49:10] Waterer
[00:49:11] 比方說
[00:49:12] Spasser彈省
[00:49:13] 或者是這種
[00:49:14] 混合現性的這種
[00:49:15] 的彈省嘛
[00:49:16] 然後這兩種
[00:49:17] 我覺得
[00:49:18] 他也是可以結合的嘛
[00:49:19] 對
[00:49:20] 有點
[00:49:21] 和各自的缺點嘛
[00:49:22] 像Spasser彈省的話
[00:49:23] 他
[00:49:24] 做的
[00:49:25] 育娶肉
[00:49:26] 育娶了一點嘛
[00:49:27] 但他缺點
[00:49:28] 就是說
[00:49:29] 他KV開始
[00:49:30] 他不能省
[00:49:31] 然後像
[00:49:32] 現性的話
[00:49:33] 他
[00:49:34] 可以省
[00:49:35] 很多KV開始
[00:49:36] 所以我之前
[00:49:37] 起來
[00:49:38] 這兩種方案
[00:49:39] 為什麼我們不能
[00:49:40] 把它結合到一起呢
[00:49:41] 就比方說
[00:49:42] 我們可以
[00:49:43] 讓Spasser彈省
[00:49:44] 去取代
[00:49:45] 這種混合
[00:49:46] 注意力
[00:49:47] 裡面的那個
[00:49:48] 全局的那個
[00:49:49] 注意力層
[00:49:50] 這樣的話
[00:49:51] 我們就不需要
[00:49:52] 有一個全局
[00:49:53] 注意力的那個
[00:49:54] 複雜度在了
[00:49:55] 但我們還是要
[00:49:56] 成那個KV開始
[00:49:57] 但身家
[00:49:58] 很多層的KV開始
[00:49:59] 就可以通過這個
[00:50:00] 現性注意力
[00:50:01] 把KV開始的
[00:50:02] Size
[00:50:03] 把它打下來
[00:50:04] 這樣子的話
[00:50:05] 我覺得
[00:50:06] 可能就是我
[00:50:07] 比較理想的一個
[00:50:09] 高校的一個
[00:50:10] 架構
[00:50:11] Internals
[00:50:12] 高校
[00:50:13] 也能花了
[00:50:14] 是這樣子的
[00:50:15] 所以
[00:50:16] Linus
[00:50:17] 成是
[00:50:18] 可能是
[00:50:19] 融合到
[00:50:20] 一個統一的
[00:50:21] 狂家裡面
[00:50:22] 對
[00:50:23] 因為我覺得
[00:50:24] 現性的彈省
[00:50:25] 和Spasser彈省
[00:50:26] 它其實
[00:50:27] 沒有什麼
[00:50:28] 競爭關係
[00:50:29] 我覺得現性的
[00:50:30] 競爭對手可能
[00:50:32] 更多的是
[00:50:33] Size
[00:50:34] Linus
[00:50:35] 成是
[00:50:36] Linus
[00:50:37] 提到的
[00:50:38] 全局
[00:50:39] 或Slighting Window
[00:50:40] Slighting Window的話
[00:50:42] 如果
[00:50:43] 讓這個現性
[00:50:45] 去取到Slighting Window
[00:50:46] 能夠讓它
[00:50:47] 更好的
[00:50:48] 可能到
[00:50:49] 那
[00:50:50] 非常不可能
[00:50:52] 所以你覺得
[00:50:53] Linus
[00:50:54] 成是
[00:50:55] 成是
[00:50:56] 能夠做更好的結合
[00:50:57] 現在有人在探索這件事嗎
[00:50:58] 公約見得
[00:50:59] 我還
[00:51:00] 結果
[00:51:01] 我應該
[00:51:02] 沒有看到
[00:51:03] 有人在同時
[00:51:05] 去結合Spasser彈省
[00:51:08] 和現性的彈省
[00:51:10] 但學界有一些
[00:51:12] 工作還是
[00:51:13] 有一些
[00:51:14] 這方面的探索的
[00:51:15] 就是
[00:51:16] 成用Spass
[00:51:17] 有些
[00:51:18] 成用
[00:51:19] 你量的探索
[00:51:20] 對
[00:51:21] 所以
[00:51:22] Depth
[00:51:23] 也向你選了
[00:51:24] Spasser
[00:51:25] Kimmy選了
[00:51:26] Linus
[00:51:27] 這其實
[00:51:28] 可能也是
[00:51:29] 階段性的
[00:51:30] 對吧
[00:51:31] 可能未來會
[00:51:32] 大家會
[00:51:33] 探索
[00:51:34] 現在
[00:51:35] 也不是
[00:51:36] 非常
[00:51:37] 寂寞的關係
[00:51:38] 我覺得
[00:51:39] 我很討厭
[00:51:40] Depth
[00:51:41] 常度上去之後
[00:51:42] 他的問題
[00:51:43] 就是說
[00:51:44] 他還是會
[00:51:45] 被這個
[00:51:46] 全局
[00:51:47] 他的效率
[00:51:48] 把它
[00:51:49] 綁得住
[00:51:50] 後面
[00:51:51] 那個
[00:51:52] 屏進
[00:51:53] 就主要在這個
[00:51:54] 全局
[00:51:55] 就在這個
[00:51:56] 效率上面
[00:51:57] 對
[00:51:58] 像
[00:51:59] 全部都用
[00:52:00] Spasser
[00:52:01] 他
[00:52:02] 在KV開始
[00:52:03] 的管理上面
[00:52:04] 對
[00:52:05] 因為他還是
[00:52:06] 不省KV開始
[00:52:07] 對
[00:52:08] 所以
[00:52:09] 的長度上去
[00:52:10] 就可能
[00:52:11] 要做很多
[00:52:12] 各種各樣的KV開始
[00:52:13] 壓索
[00:52:14] 壓著的工作
[00:52:15] 然後
[00:52:16] 兩者
[00:52:17] 還是有
[00:52:18] 各自的問題
[00:52:19] 他的解合
[00:52:20] 是
[00:52:21] 比如說
[00:52:22] 可能是
[00:52:23] 不同的
[00:52:24] 產用
[00:52:25] 不同的
[00:52:26] 天神
[00:52:27] 我覺得
[00:52:28] 這好進
[00:52:29] 的話
[00:52:30] 就是
[00:52:31] 他
[00:52:32] 換成
[00:52:33] Spasser
[00:52:34] 對
[00:52:35] 我覺得
[00:52:36] 你論上
[00:52:37] 只要Spasser
[00:52:38] 他能選
[00:52:39] 的準的話
[00:52:40] 他
[00:52:41] 是完全
[00:52:42] 可以
[00:52:43] 取代
[00:52:44] 付了
[00:52:45] 他
[00:52:46] 現在
[00:52:47] 問題
[00:52:48] 可能
[00:52:49] 是
[00:52:50] 選不準
[00:52:51] 就是
[00:52:52] 一個
[00:52:53] 很大的問題
[00:52:54] 對
[00:52:55] 然後
[00:52:56] 為什麼
[00:52:57] 可能
[00:52:58] 是
[00:52:59] 為什麼
[00:53:00] 他
[00:53:01] 已經可能
[00:53:02] 讓他
[00:53:03] 那個
[00:53:04] indexer
[00:53:05] 就是
[00:53:06] 選
[00:53:07] 偷
[00:53:08] 很
[00:53:09] 準
[00:53:10] 對
[00:53:11] 這也可能
[00:53:12] 是
[00:53:13] 一個原因
[00:53:14] 好
[00:53:15] 選
[00:53:09] 選
[00:53:10] 的
[00:53:11] 核心
[00:53:12] 在哪
[00:53:13] 我
[00:53:14] 這就是
[00:53:15] 學心難度
[00:53:16] 對
[00:53:17] 像Spasser
[00:53:18] 天神的話
[00:53:19] 如果你
[00:53:20] 從頭開始
[00:53:21] 訓練
[00:53:22] 他可能
[00:53:23] 那個
[00:53:24] 踢
[00:53:25] 都不太準
[00:53:26] 他
[00:53:27] 選不準
[00:53:28] 那個
[00:53:29] 這種
[00:53:30] 吸收
[00:53:31] 踢住的
[00:53:32] 問題
[00:53:33] 像Spasser
[00:53:34] 他
[00:53:35] 經常
[00:53:36] 就會有這種
[00:53:38] 問題
[00:53:39] 然後
[00:53:40] 像
[00:53:41] 真幼的方式
[00:53:42] 他
[00:53:43] 其實
[00:53:44] 就是
[00:53:45] 訓練
[00:53:46] 訓練
[00:53:47] 他
[00:53:48] 就是
[00:53:49] 全部
[00:53:50] 都是
[00:53:51] Summei
[00:53:52] 探生的
[00:53:53] 一個
[00:53:54] T-Shirt
[00:53:55] 來
[00:53:56] 真幼一個
[00:53:57] 他
[00:53:58] 聽見
[00:53:59] 這個工作
[00:54:00] 他
[00:54:01] 相比
[00:54:02] 年初的
[00:54:03] Mini Max M1
[00:54:04] 工作
[00:54:05] 他的
[00:54:06] 進步
[00:54:07] 在哪裡
[00:54:08] 他主要
[00:54:09] 就是
[00:54:10] 在於
[00:54:11] 現心
[00:54:12] 注意力
[00:54:13] 他那個
[00:54:14] 模塊
[00:54:15] 他
[00:54:16] 還是
[00:54:17] 會
[00:54:18] 好很多的
[00:54:19] 對
[00:54:20] 就像我之前說
[00:54:21] Lightenite
[00:54:22] 你有
[00:54:07] 探生
[00:54:23] 給人的感覺
[00:54:24] 就像
[00:54:25] 一個
[00:54:26] 兩年前的
[00:54:27] 沖沖
[00:54:29] 沖
[00:54:38] 沖
[00:54:39] 沖
[00:54:40] 沖
[00:54:42] 沖
[00:54:43] 沖
[00:54:57] 一个KDA嘛
[00:54:59] 然后让他的那个
[00:55:01] 模型的能力会更强吗
[00:55:03] 然后另外的话可能
[00:55:07] 他还有其他不同吧
[00:55:09] 就比方说
[00:55:11] Moe的话像
[00:55:13] Kimi他应该用的是翻轨Moe
[00:55:16] 然后Moe
[00:55:18] Moe我记得他那个Moe好像还比较
[00:55:23] 比较粗吧
[00:55:25] 用到这么翻轨的这个Moe
[00:55:27] 对
[00:55:29] 所以就是有很多种可能性
[00:55:33] 如何做一个公平的比较
[00:55:35] 比较一下临点称声和
[00:55:37] sliding window的称声
[00:55:39] sliding window的谈声和
[00:55:41] 临点谈声做公平的比较的话
[00:55:43] 我觉得可以有两种嘛
[00:55:45] 一种话就比方说
[00:55:47] 控制他的这个status
[00:55:49] 就是sliding window的话
[00:55:51] 他有Kiri开始吗
[00:55:53] 这个Kiri开始他是因为他是划床
[00:55:57] 所以他
[00:55:59] 他那个Kiri开始的上线
[00:56:01] 是被爆得住的
[00:56:03] 然后我们就可以把他这个Kiri开始的上线
[00:56:07] 他的这个size
[00:56:09] 当成sliding window
[00:56:12] tension的他一个
[00:56:14] states size
[00:56:16] 然后R&R的话呢
[00:56:17] 他有R&R那个
[00:56:19] states size嘛
[00:56:21] 那个庄太属
[00:56:23] 然后如果这两个东西
[00:56:25] 大概在一个Live我的话呢
[00:56:27] 我觉得就是一个公平的比较
[00:56:29] 对
[00:56:30] 因为像Decoding的时候呢
[00:56:32] 他sliding window和
[00:56:35] 因为Decoding的话他基本都是
[00:56:37] 一个Mamory bound的一个过程
[00:56:39] 所以只要他的states size差不多
[00:56:43] 那他Decoding的效率就不会差太多了
[00:56:47] 因为Mamory bound的话呢
[00:56:49] 他主要就是看他读多少states嘛
[00:56:52] 然后只要他们这states差不多大
[00:56:55] 那他们这个Decoding的效率基本上就会差不多大
[00:56:58] 因为Decoding还是主要是Mamory bound的对
[00:57:02] 说到算法的就是眼睛啊
[00:57:04] 他最早从transfermed到Moe
[00:57:07] 然后到现在大家探索理念称声或者states
[00:57:10] 还得这种见尽式的创新
[00:57:12] 你觉得他优化的最终的目标可能是什么
[00:57:16] 然后最终可能形成的一个算法的共识
[00:57:20] 会是什么样的
[00:57:21] 我觉得这些优化基本上都是
[00:57:24] 这体现在
[00:57:27] 这是给你相同的flop
[00:57:29] 你怎么去更好的利用这些flop
[00:57:31] 然后取得更低的损失寒送
[00:57:33] 对
[00:57:34] 像Moe这个技术就是
[00:57:37] 前两年可能比如说
[00:57:40] 你要三年的时候都在传GbG4Moe
[00:57:43] 但有很多地方不太敢跟的嘛
[00:57:46] 然后像现在的话Moe
[00:57:48] 基本上都是已经变成一个显学了嘛
[00:57:51] 就是每一家都会做这种犯规的Moe
[00:57:54] 对
[00:57:55] 这Moe的话它其实也是一种
[00:57:57] 它可以想象成就是
[00:57:59] 更加高效的一个i5分的一个Ti评
[00:58:02] 对就是它可以更好的去扩大那个i5分的这个参数量
[00:58:08] 然后同时呢它又保证它那个flop不变
[00:58:12] 这样子的话呢它付出相同的flop
[00:58:14] 它能在一区念里面取得的那个
[00:58:17] 训练的loss就会越低对吗
[00:58:20] 这就是一个点
[00:58:22] 然后
[00:58:24] 我觉得Moe它可能是近几年
[00:58:26] 就是突破最大的一个
[00:58:29] 在加个方面就是突破最大的一个方案嘛
[00:58:31] 对然后下一个突破点可能就在个tansher
[00:58:36] 因为
[00:58:38] transformer就两个模块嘛
[00:58:40] 一个i5分一个tansher
[00:58:43] 现在i5分基本上已经雕成了这种
[00:58:46] Fangran的Moe的这种形状嘛
[00:58:48] 对然后而tansher我觉得大家也是可以来雕一下的
[00:58:51] 就是Y-N-O-T
[00:58:52] 这样的话呢它
[00:58:55] 表达在长文本
[00:58:57] 下面的话呢它付出相同的flop
[00:58:59] 然后它可能取得那个loss也会更低嘛对
[00:59:02] 我觉得这两套思路都是要的
[00:59:05] 就是减少flop
[00:59:07] 然后能够让它像i5分的话呢
[00:59:10] 它减少flop它就可以去用
[00:59:12] 更大的这个参数量
[00:59:14] 更大规模的一个模型嘛对
[00:59:16] 就比方说你总参数量就可以对高了嘛
[00:59:19] 因为你这个i5分的这个算力减少了嘛
[00:59:21] 大家都知道就是在大规模训练下面
[00:59:24] i5分的那个计算是主导的嘛
[00:59:27] 对然后把它换成这种Fangran Moe的画能它
[00:59:30] 其实是能降低很多很多这种cost
[00:59:33] 对然后而tansher它skeleton
[00:59:36] 就主要不是参数量它skeleton
[00:59:39] 就是那个context的这个window size嘛对
[00:59:42] 然后如果这个tansher
[00:59:45] 它的这个flops就在长文本下面
[00:59:48] 能够能够把它打下来的话呢
[00:59:51] 那我们就是做那种长文本的这种
[00:59:55] 深层啊然后
[00:59:57] 比如说你有很多Agent
[00:59:59] 让它去处理很多很多workflow啊
[01:00:01] 然后为很多很多context给它做
[01:00:04] 这样的话它也会
[01:00:06] Buy-FateFound这个更大的这个contextwindow的对
[01:00:10] 如果把模型的价格比例比如说大脑的结构
[01:00:14] 你觉得Moe和tansher他们分别
[01:00:18] 代表的是大脑的什么组件啊
[01:00:20] 能这样去形象化的去理解吗
[01:00:22] 像弹势的话它应该就相当于walking memory嘛
[01:00:26] 对就是那种工作记忆
[01:00:28] 它就是然后像FFF那的话
[01:00:31] 就有点像那种我忘记人的大脑存那种消息的那种
[01:00:36] 我会去的
[01:00:38] 对可能就是海马提就是来存
[01:00:40] 就是存储这种信息的就是过去信息的
[01:00:43] 对像FFF它基本上会被看成是一个
[01:00:47] 建制队的一个观念网络嘛
[01:00:50] 它可以记下很多很多这种lowlady
[01:00:53] 对就像这种我的lowlady主动会被它寄到这个FFFn里面
[01:00:56] 这就是一个就是一些我的lowlady会沉下来
[01:01:00] 然后Tansher的话就是比方说你在一个新的场景
[01:01:03] 然后你遇到新的这种喜来源
[01:01:05] 然后你会读到新的context
[01:01:08] 然后它会在这个contextwindow里面
[01:01:11] 就是动态的来做这个处理这些信息嘛
[01:01:15] 那就有点很像我们人的当中的工作记忆
[01:01:18] 那个walking memory对
[01:01:20] 它跟偏即使性一些对对对
[01:01:24] 当现在数据遇到数据强
[01:01:28] 数据平静比较明显的什么
[01:01:30] 是不是算法的创新片的更重要了
[01:01:32] 我觉得是的呀对就是
[01:01:34] 因为颜色有算数据里面去压缩更多的质呢
[01:01:38] 对我觉得之前的话
[01:01:42] 就比方说你数据一直能skill的话
[01:01:44] 你谈这个dataefficiency
[01:01:46] 就是没有什么特别大的用途嘛
[01:01:50] 因为就是大家别的原因加这个数据就行了
[01:01:54] 就让它模型继续skill up
[01:01:56] 然后继续加数据
[01:01:58] 所以大家都不需要去动算法了嘛
[01:02:00] 然后大家就只需要买卡这个型了对
[01:02:03] 然后现在如果有这种数据强呀
[01:02:06] 然后还有这种三立强的话来
[01:02:09] 那可能就到最终还是要回到这个算法
[01:02:14] 这种本质的东西上面拿来的
[01:02:15] 我觉得这些东西都是缺一不可的嘛
[01:02:17] 这比方说像data像这种算法
[01:02:20] 像这种算例
[01:02:21] 就是三就像有点像三屏马车嘛
[01:02:24] 就是来驱动整个人工质的发展嘛
[01:02:28] 对然后我记得之前像
[01:02:31] opai的CTO他也说
[01:02:34] 就可能在这个节点上面
[01:02:36] 算法的这个研究的重要性可能会被重新抬高嘛
[01:02:40] 对如果你记得记得那个采访的话
[01:02:42] 他应该是这么熟过的对
[01:02:44] 你觉得现在的加构
[01:02:47] 川斯坡们加构他的听话版是什么呀
[01:02:50] 他的听话版啊
[01:02:54] 我觉得呢还是先把
[01:02:57] Ephesion洗的问题解决掉吧对
[01:03:00] 因为现在还没有解决掉Ephesion洗的问题嘛
[01:03:02] 他的处理一个很长的一个
[01:03:05] 康泰斯坏的还是有一些举线性嘛
[01:03:10] 对所以大家会做很多上下文工程呀
[01:03:13] 做一些IG呀来
[01:03:15] 来通过一些其他的方式来来做这些问题嘛
[01:03:19] 但如果你这个康泰斯的问题
[01:03:21] 把它解决掉的话
[01:03:22] 那你IG这一套技术都不需要了嘛
[01:03:24] 你直接把它放到康泰斯里面做
[01:03:27] 英康泰斯阿埃及就行了
[01:03:29] 对然后我觉得
[01:03:31] 天花版的话
[01:03:32] 就先看看能不能
[01:03:34] 就是把全局这个注意力把它干掉吧
[01:03:38] 对所以第一点就是因为它确实它是阻止这个
[01:03:43] 康泰斯坏的机器
[01:03:45] 上去的一个主要的评计嘛
[01:03:47] 所以这个评计我觉得是早晚都要把它弄掉的
[01:03:52] 这个是第一点
[01:03:54] 然后调言话呢可能就是
[01:03:57] 抗听牛能力嘛对
[01:03:59] 然后像现在这种传送的架构
[01:04:01] 还是没法做抗听牛能力的嘛
[01:04:03] 对然后之后抗听牛能力让
[01:04:06] 哎呀自己学习啊这种甚至
[01:04:11] 甚至大家不都想把
[01:04:13] pre training这个地方变成直接从
[01:04:16] AR开始让这个模型直接从您开始学
[01:04:19] 不给它为这种pre-send data嘛
[01:04:21] 对像这种新的饭式可能就是
[01:04:23] 之后的这种探索嘛对
[01:04:26] 一个研究员问你
[01:04:28] 如何把临你的天神的
[01:04:29] transformer skill up继续扩展
[01:04:32] 我真的skill up
[01:04:35] 就skill up应该是没有什么
[01:04:37] 特别大的问题吧对
[01:04:39] 然后我觉得可能还有一点的话
[01:04:41] 就说
[01:04:42] 嗯
[01:04:43] 就像说
[01:04:44] 嗯就是说那些
[01:04:46] 配套的这种infra
[01:04:48] 设施
[01:04:49] 还是需要继续打的对
[01:04:52] 像flash拎掉的天神
[01:04:54] 只是提供了一些
[01:04:56] trotin的一些可能
[01:04:57] 基本上就是可以凑合用
[01:04:59] 但是它的那个
[01:05:01] 效率肯定不是最有的
[01:05:03] 因为它是刷灯写的嘛对
[01:05:05] 嗯
[01:05:06] 所以如果
[01:05:07] 有制项投入
[01:05:10] 这个领域的
[01:05:11] 比较说一些公司啊
[01:05:13] 或者可以花一些级密去
[01:05:15] 优化这些可能对
[01:05:17] 这个是对继续skill up上去要好处吗
[01:05:21] 然后像infra
[01:05:22] 那边的那种
[01:05:24] 我觉得现在像infra
[01:05:26] 那边的支持
[01:05:27] 已经在逐渐变多了
[01:05:29] 然后就比方说
[01:05:30] 像
[01:05:31] 半年前
[01:05:33] 我参加
[01:05:34] minimized它
[01:05:35] 它有一个那个
[01:05:36] 原则讨论样的东西嘛
[01:05:38] 然后当时主持人是
[01:05:40] 金险老师
[01:05:41] 金险老师问我
[01:05:42] 这个领域
[01:05:43] 它除了的平静是什么
[01:05:44] 我当时说是
[01:05:46] infra的那个配套
[01:05:47] 没有跟唱对
[01:05:49] 然后当时金险老师
[01:05:51] 还觉得挺意外的
[01:05:52] 因为我会回答一些
[01:05:53] 别的东西
[01:05:54] 对然后
[01:05:55] 其实是像
[01:05:56] 就这样子吗
[01:05:57] 我觉得
[01:05:58] 算法层面可以
[01:05:59] 就是比方说
[01:06:00] 像今两年的这个发展
[01:06:02] 就已经可以
[01:06:03] 去大规模的来试了嘛
[01:06:05] 比方说
[01:06:06] Nex和Kimi Ni
[01:06:08] 这些都是
[01:06:11] 可以大规模直接试的了
[01:06:13] 然后
[01:06:14] Deploy的这种平静的话
[01:06:15] 可能就更多就是在这种
[01:06:17] 配套设施
[01:06:18] 因为这两家发这两个
[01:06:22] 模型
[01:06:23] 那开源社区
[01:06:24] 就是支持力度也挺大的嘛
[01:06:26] 像之前
[01:06:28] 比如说SJNN
[01:06:29] 他的不支持这种
[01:06:30] Hyper mode做infraints嘛
[01:06:32] 然后
[01:06:33] 现在就是称这个机会
[01:06:34] 就可以把
[01:06:36] 前文3Nex
[01:06:37] 然后向Mini.3.1
[01:06:39] 这些模型
[01:06:40] 就是加一些
[01:06:41] 这种
[01:06:42] 推理的这种
[01:06:43] 推理引擎的这种
[01:06:45] support嘛
[01:06:46] 我觉得
[01:06:47] 这是一个
[01:06:48] 真相的一个
[01:06:49] 有一个
[01:06:50] 就是真相的一个
[01:06:51] 你于真相发展的一个过程嘛
[01:06:54] 就像这些做寂寞的场上
[01:06:56] 他们去做一些
[01:06:58] 比较promising的结果
[01:06:59] 然后把他
[01:07:01] 把这些开源
[01:07:02] 为他把它发出来
[01:07:03] 然后那些
[01:07:05] 就是做推理引擎的人
[01:07:07] 就会有很多动力来
[01:07:08] 想办法来支持这些东西嘛
[01:07:10] 然后
[01:07:12] 然后当这些
[01:07:13] infraints配套
[01:07:14] 更好的时候
[01:07:15] 然后比如说
[01:07:16] 别的公司可能
[01:07:17] 就是
[01:07:18] 觉得像你两贪是
[01:07:20] 他的infra的那个
[01:07:22] 生态太差了
[01:07:23] 就是可能就算
[01:07:25] 做出来
[01:07:26] 这个生态不好可能
[01:07:28] 他实际上
[01:07:29] deprole
[01:07:30] 他的成为也很高嘛
[01:07:31] 对
[01:07:32] 但现在
[01:07:33] 如果只要这个生态
[01:07:34] 做起来了
[01:07:35] 然后
[01:07:36] 就会
[01:07:38] 有一个
[01:07:39] 真相形怀的
[01:07:40] 一个作用嘛
[01:07:41] 我觉得
[01:07:42] 你觉得现在中国的
[01:07:43] 算法创新相对于
[01:07:44] 跟过来说
[01:07:45] 是差不多
[01:07:46] 更强还是落后的
[01:07:48] 我觉得
[01:07:49] 国内算法创新
[01:07:50] 肯定
[01:07:51] 是更强的
[01:07:53] 对
[01:07:54] 主要是
[01:07:56] 英特朗普
[01:07:57] 加购的话
[01:07:58] 那肯定是国内
[01:07:59] 更强的
[01:08:00] 我觉得这也是有一些
[01:08:02] 有一些生态
[01:08:03] 地位
[01:08:04] 不同吧
[01:08:05] 就比方说
[01:08:06] 国内没有那么多卡
[01:08:07] 然后他们
[01:08:08] 其实对这个
[01:08:09] 一笔神奇的要求
[01:08:10] 是更高的嘛
[01:08:11] 所以他们
[01:08:12] 更有动力来
[01:08:13] 尝试这些
[01:08:14] 更高效的一些
[01:08:15] 你俩太深这样的变种
[01:08:17] 然后像
[01:08:18] 规矩有些公司
[01:08:19] 基本上就是卡太多了
[01:08:20] 他们就乃得高
[01:08:21] 对
[01:08:22] 反正三件马车
[01:08:23] 有一定要跑得快一点
[01:08:25] 对对对对
[01:08:26] 他们有那个算力
[01:08:27] 那也能抽和跑吗
[01:08:29] 对
[01:08:30] 那脑子长
[01:08:31] 怎么样
[01:08:32] 无所谓
[01:08:33] 反正我先
[01:08:34] 算了一对传奇
[01:08:36] 然后我觉得
[01:08:37] 规矩这边
[01:08:38] 感觉
[01:08:39] 美国的公司会
[01:08:40] 更注重优化一点
[01:08:41] 对就像
[01:08:42] optimization
[01:08:43] 对对
[01:08:44] 对就比方说优化器
[01:08:45] 对
[01:08:46] 像国内公司
[01:08:47] 也感觉
[01:08:48] 在逐渐
[01:08:49] 在用嘛
[01:08:50] 就比方说
[01:08:51] 像KIMIT
[01:08:52] 就是这只老吃
[01:08:53] 没有这个螃蟹的一个
[01:08:55] 地方
[01:08:56] 对
[01:08:57] 然后给我个感觉
[01:08:58] 就是美国他们
[01:08:59] 对优化器的投入
[01:09:01] 明显是比国内
[01:09:02] 对优化器的投入
[01:09:03] 要大一些的
[01:09:04] 对
[01:09:06] KIMIT的
[01:09:07] 林念天圣的效果
[01:09:08] 跟去年
[01:09:09] Dipsick
[01:09:10] Spassit天圣的效果
[01:09:11] 比那个更强呀
[01:09:12] 我觉得
[01:09:13] 就是
[01:09:14] 效果对比的话
[01:09:15] 需要有个地方
[01:09:17] 来做一个Apple
[01:09:18] Triple的一个比较嘛
[01:09:19] 对
[01:09:20] 因为这个东西
[01:09:21] Traykey
[01:09:22] 就是不太好比嘛
[01:09:24] 我觉得不同的地方
[01:09:25] 迅出来
[01:09:26] 不同的
[01:09:27] 它可能就是完全不能比了
[01:09:28] 因为它那个
[01:09:29] 迅恋价购啊
[01:09:30] 那个Data
[01:09:32] Risepia
[01:09:33] 那个优化方案
[01:09:34] 完全都不一样的
[01:09:35] 它就没有
[01:09:36] 一个AppleTriple的一个比较嘛
[01:09:37] 对
[01:09:38] 像KIMIT
[01:09:40] 你得到它最近这个
[01:09:41] Report
[01:09:42] 它还有一点
[01:09:43] 就是说它有一个AppleTriple的一个
[01:09:44] 跟
[01:09:45] 附而贪生的一个比较
[01:09:48] 对
[01:09:49] 它是一个AppleTriple的比较的
[01:09:50] 但它没有AppleTriple
[01:09:52] 去Sparker贪生的一个比较
[01:09:54] 要是有一个地方
[01:09:55] 能做此善
[01:09:56] 来AppleTriple的比较
[01:09:57] 让大家能更好的知道
[01:09:59] 就更好了
[01:10:00] 对
[01:10:01] 但现在因为没有人
[01:10:02] 再做一个AppleTriple的比较嘛
[01:10:04] 所以这个问题
[01:10:05] 我也不知道
[01:10:06] 哪个会更好
[01:10:07] 对
[01:10:08] 我觉得很有意思
[01:10:09] 还比较是Fortness
[01:10:11] 它在看好就写了
[01:10:12] 说它们是
[01:10:13] 第一个验证了
[01:10:14] 就新的超越Fortness
[01:10:16] 的
[01:10:17] 混合
[01:10:18] 林念丞生加购
[01:10:19] 可能还是自然有限吧
[01:10:22] 如果就那么多卡的话
[01:10:24] 可能先投入
[01:10:25] 一个路线去验证吗
[01:10:27] 对
[01:10:28] 然后如果验证出来
[01:10:29] 再去投入另外一个路线
[01:10:31] 看看有没有可能
[01:10:32] 比方说
[01:10:33] 把全球处理力再把它替换掉嘛
[01:10:35] 对
[01:10:36] 对
[01:10:37] 可能就是没有这么多卡
[01:10:38] 来同时来跑一些
[01:10:41] 不同的方案的对比嘛
[01:10:43] 然后像归谷的话
[01:10:46] 就很多东西都避允了嘛
[01:10:48] 所以你也不知道它们有没有跑一下
[01:10:50] 或跳逢的笔角吗
[01:10:51] 对
[01:10:52] 你看那个
[01:10:53] Kimi林念的论文
[01:10:54] 你觉得还有哪些是
[01:10:56] 大家关注点
[01:10:57] 前面说就是这个
[01:10:59] 现行注意的某块嘛
[01:11:00] 对
[01:11:01] 然后还有可能就全集
[01:11:03] 追逆它的那个
[01:11:05] 用肉跑
[01:11:06] 还是用肉跑的一个笔角吧
[01:11:08] 对
[01:11:09] 像Kimi它选的
[01:11:11] 是用肉跑
[01:11:12] 然后像
[01:11:14] 千万三NICE的话
[01:11:16] 它是选到是一个
[01:11:17] 爬手肉跑
[01:11:19] 它就是
[01:11:20] 25%是肉跑
[01:11:22] 70%是肉跑
[01:11:24] 对
[01:11:25] 我觉得在这种混合注意力
[01:11:27] 你们大家都在砍肉跑吧
[01:11:29] 但是看大家砍多少嘛
[01:11:31] 像
[01:11:32] 千万三NICE感的看了
[01:11:33] 百分之75%
[01:11:34] 然后像Kimi看了百分之百
[01:11:37] 然后像这种长度外推
[01:11:40] 这种
[01:11:42] 就感觉像看起来的话
[01:11:44] 就是
[01:11:45] 肉跑在这种
[01:11:47] 这种
[01:11:48] Hip-Hip加个里面可能会被
[01:11:50] 比较阻碍
[01:11:51] 这种长度外推
[01:11:53] 然后这个地方其实
[01:11:54] 也没有共识嘛
[01:11:55] 就是说大家也不知道是用
[01:11:57] 有些还是用肉跑
[01:11:59] 有些还是用肉跑
[01:12:00] 我觉得这个地方还是没有共识的
[01:12:02] 然后有些地方还用
[01:12:03] 帕术肉跑
[01:12:04] 对
[01:12:06] 提一个
[01:12:07] 就是提外话
[01:12:08] 你有关注最近
[01:12:09] DPC它的那个新的工作
[01:12:11] 就是
[01:12:12] 这个它发一个
[01:12:13] OCA的paper
[01:12:15] 就知道大家就是说
[01:12:18] 其实
[01:12:19] 数据撞墙的时候
[01:12:22] 还是有很多那种
[01:12:24] 这书籍啊
[01:12:25] PDF里面有
[01:12:27] 有大量的数据的
[01:12:28] 所以他们做这个OCA
[01:12:29] 可以帮他们
[01:12:30] 更好的洗一些
[01:12:31] 对他出来
[01:12:32] 然后来做PRECHRIN
[01:12:34] 然后另外的话
[01:12:35] 他们说是由OCA来
[01:12:37] 做这种
[01:12:38] Contest Compression
[01:12:41] 这一点的话呢
[01:12:43] 我觉得
[01:12:45] 是一个有意思的
[01:12:47] 一个劳动
[01:12:48] 对
[01:12:49] 但
[01:12:50] 我不确定这个方案怎么样
[01:12:52] 对
[01:12:53] 千万的工作你有
[01:12:55] 采遇到
[01:12:56] 就千万三千万
[01:12:58] 好 我就基本上那次
[01:13:00] 就是我会
[01:13:01] 他们要是碰到什么问题啊
[01:13:03] 我就可以帮忙打一下
[01:13:05] 对
[01:13:06] 就是不采遇他们
[01:13:08] 去模型什么的
[01:13:09] 就是如果他们有一些
[01:13:11] 学术上的讨论的话
[01:13:12] 我是
[01:13:13] 會跟他們討論的
[01:13:15] 對我跟前問三
[01:13:17] 那是訓練的那幾個同學都還挺熟的
[01:13:20] 對
[01:13:21] 那個名字才有沒有
[01:13:22] 沒有
[01:13:23] 所以你才是什麼
[01:13:25] 如果他們應該不會用這個訪案
[01:13:29] 我會覺得這個訪案在開道場
[01:13:32] OK
[01:13:35] 我覺得你又是很好玩
[01:13:37] 你總是在又把這個架構玩一下
[01:13:39] 或者丟一下這種詞
[01:13:41] 這個是一種研究原始性的文化
[01:13:46] 我覺得丟這個字好像還挺常見的
[01:13:49] 就是
[01:13:51] 有種雕花等那種字炒的那種說法吧
[01:13:56] 就是要是另一個說不要時尚雕花
[01:13:59] 現在沒辦法
[01:14:00] 算了也不夠數據也有限了
[01:14:02] 所以只能丟
[01:14:05] 但是對
[01:14:06] 但是我覺得雕架構還是挺有用的
[01:14:08] 像Dipsy卡Moei那個雕出來之後
[01:14:11] 大家都已經成為一個公式了
[01:14:14] 基本上的一個公式了
[01:14:16] 就很容易會用Dipsy卡Moei的方案
[01:14:19] 如果在他之前
[01:14:22] 然後他在做那個
[01:14:24] 那可能大家也會說可能在丟Moei嗎
[01:14:27] 對
[01:14:28] 然後感覺丟一形變成一個常見的形容詞
[01:14:31] 我覺得他不是一個點一次
[01:14:34] 我覺得他是一個就是把一個模塊
[01:14:36] 打磨到
[01:14:38] 更好
[01:14:39] 對
[01:14:40] 如果數據的Skilling能夠非常突出的話
[01:14:43] 其實沒有必要掉
[01:14:44] 就對數據就好了
[01:14:46] 當數據還很少的時候
[01:14:47] 他比如說機器人領域現在就是數據
[01:14:49] 就沒什麼數據
[01:14:50] 只要加數據他就能夠顯著的效果提升
[01:14:54] 那就沒有必要去做模型
[01:14:56] 先算法的創新
[01:14:57] 對
[01:14:58] 這是一點
[01:14:59] 所以
[01:15:00] Lobody's Tang
[01:15:01] 最應該做的應該還是先把數據這個問題
[01:15:04] 把它搞定吧
[01:15:05] 對
[01:15:06] 然後數據搞定之後
[01:15:08] 再回來看這種
[01:15:09] Efficiency的這種問題
[01:15:11] 也不吃了嗎
[01:15:12] 對
[01:15:13] 你做AI的Researching
[01:15:15] 新你的是什麼
[01:15:16] 好
[01:15:17] 真正
[01:15:18] 你是怎麼介入AI最好業的
[01:15:20] AI的好業化就是
[01:15:22] 本科的時候就對Moei新的那樣
[01:15:26] 低伏能量挺有興趣的
[01:15:28] 然後
[01:15:29] 然後當時Master在上核大
[01:15:32] 念LP嘛
[01:15:34] 那個時候就已經進入AI了
[01:15:36] 然後
[01:15:37] 23年
[01:15:39] 22年就是
[01:15:40] Charge PT這一波
[01:15:41] 就是納斯兩位是模特
[01:15:43] Foome開來嘛
[01:15:45] 那就
[01:15:46] 做LP的人
[01:15:47] 就
[01:15:48] 基本上都來做納斯兩位是模特的嘛
[01:15:50] 對
[01:15:51] 然後做AI
[01:15:53] 我覺得像
[01:15:54] 更有意思一點
[01:15:56] 就是比之前會更有意思一點
[01:15:58] 因為之前可能大家還是在
[01:16:01] 分Task來做嘛
[01:16:03] 然後現在可能都
[01:16:05] 就是比較Ulyphile
[01:16:07] 然後可能就是
[01:16:09] 會比較Foome
[01:16:10] 更加同用的一些問題來了
[01:16:12] 對
[01:16:13] 就不需要去操心
[01:16:15] 某些特定的Task嘛
[01:16:17] 因為你只要穿一個
[01:16:18] 很好好的機膜
[01:16:19] 然後
[01:16:21] 然後你對不同的Task都可以用嘛
[01:16:23] 你無非是PoseTrain的時候
[01:16:25] 你要注意的地方不一樣嘛
[01:16:27] 然後現在
[01:16:28] 就是
[01:16:29] 感覺就是自己做的東西的話
[01:16:31] 可能就是能
[01:16:32] 看到更多的影響力嘛
[01:16:34] 然後
[01:16:35] 這一點感覺還是挺
[01:16:36] 就是看到自己的
[01:16:38] 開發的東西被
[01:16:39] 大家用還是挺開心的嘛
[01:16:41] 你過程中有遇到過什麼樣的
[01:16:43] 措著沒有
[01:16:44] 然後你覺得我
[01:16:46] 讀PSD好像
[01:16:48] 這些工作都還挺順的
[01:16:50] 沒啥樣的
[01:16:51] 對
[01:16:52] 然後感覺
[01:16:53] 就是這些工作都還挺年慣的嘛
[01:16:56] 然後
[01:16:57] 然後挺順的
[01:16:58] 然後我覺得還是
[01:17:00] 因為可能是
[01:17:01] 讀PSD之前
[01:17:03] 就是花了半年的時間
[01:17:05] 來調眼這些東西
[01:17:07] 然後可能對這些
[01:17:09] 這個領域的理解會
[01:17:10] 生很多
[01:17:11] 然後就生跟這個領域來做呢
[01:17:14] 其實問題也不是很多吧
[01:17:16] 因為
[01:17:17] 因為這個領域非常熟
[01:17:18] 然後碰到什麼問題
[01:17:19] 大家也知道怎麼去接
[01:17:21] 對
[01:17:22] 讀PSD前花半年去調眼
[01:17:24] 這個是
[01:17:25] 做的是你什麼樣的工作
[01:17:26] 是入學之前的半年是不是
[01:17:28] 就是之前
[01:17:29] 申請完半年嘛
[01:17:31] 就申請完之後
[01:17:32] 有半年可以自由的時光
[01:17:35] 然後
[01:17:36] 當時就基本上就是在
[01:17:38] 調眼這種
[01:17:39] 架構的這種
[01:17:40] paper
[01:17:41] 然後當時
[01:17:42] 讀了很多
[01:17:43] 比較老的paper
[01:17:44] 就比方說
[01:17:45] 像德塔納特
[01:17:46] 他最早是
[01:17:47] 2021年
[01:17:49] 就是那個
[01:17:51] SDM支付
[01:17:53] 那個paper
[01:17:55] 然後
[01:17:56] 當時我就在這個工作
[01:17:57] 有印象嘛
[01:17:58] 然後
[01:17:59] 後面的話
[01:18:00] 就
[01:18:01] 後面
[01:18:02] 就那年年底嘛
[01:18:03] 就做完那個
[01:18:04] Gating Linear
[01:18:05] 探索
[01:18:06] 然後發現
[01:18:07] 這個領域的話
[01:18:08] 大家會對那個
[01:18:09] 音康泰式
[01:18:10] Require
[01:18:11] 就是從那個
[01:18:12] 前面的文章裡面
[01:18:13] 去做一個日趨
[01:18:14] 這個
[01:18:15] Task會感興趣
[01:18:17] 然後這個
[01:18:18] 就讓我一下子
[01:18:19] 年想到了
[01:18:20] 2011年那一片工作了
[01:18:22] 對
[01:18:23] 因為之前的
[01:18:24] 整個領域
[01:18:25] 它把握得非常的
[01:18:27] 通緝嘛
[01:18:28] 所以我知道就是
[01:18:29] 如果你
[01:18:30] 因為大家
[01:18:31] 其他人關心這個問題的話
[01:18:33] 我應該從什麼角度去切入
[01:18:35] 然後
[01:18:36] 然後
[01:18:37] 我也知道
[01:18:38] 他前面工作有什麼問題嘛
[01:18:39] 比方說
[01:18:40] 2021年德塔納特的話
[01:18:42] 他是
[01:18:43] 沒有
[01:18:44] Hardware Efficiency的一個保證嘛
[01:18:46] 對
[01:18:47] 然後我就覺得
[01:18:48] 然後我後面就覺得
[01:18:49] 就教你這個工作之後
[01:18:51] 做德塔納的話
[01:18:52] 我就知道
[01:18:53] 那是一個很好的一個模型
[01:18:55] 然後他的缺點就是
[01:18:57] 現在大家還不能
[01:18:59] 大規模用起來
[01:19:00] 然後如果我能開發
[01:19:02] 出一個算法
[01:19:03] 能把它SKILL UP
[01:19:05] 那就是一個
[01:19:06] 非常有意義的工作
[01:19:07] 對
[01:19:08] 然後我大概就是
[01:19:09] 這一套邏輯念嘛
[01:19:11] 然後
[01:19:12] 就可能也是運氣化吧
[01:19:13] 就3號
[01:19:14] 正好能
[01:19:15] 推出一個
[01:19:16] 能夠把它SKILL UP的一個算法
[01:19:18] 對
[01:19:19] 然後後面的話
[01:19:20] 就是
[01:19:21] 後面的話
[01:19:22] 可能就是
[01:19:23] 像Gate Data的話
[01:19:24] 就是以這個工作做
[01:19:25] 因為當時發現
[01:19:27] 他還是在很多
[01:19:29] 他上面
[01:19:30] 是打不過這個MEMBA2的
[01:19:32] 對
[01:19:33] 然後我當時就覺得
[01:19:34] 打不過就加入嘛
[01:19:35] 那我就把MEMBA2的
[01:19:37] 他那個Gate Team
[01:19:38] 把它拿過來
[01:19:39] 然後
[01:19:40] 把他都在家回來
[01:19:42] 對
[01:19:43] 這樣子的話
[01:19:44] 就把他A加B
[01:19:45] 變成一個
[01:19:46] AT DataNet
[01:19:47] 對
[01:19:48] 我之後感覺我做的東西
[01:19:49] 就是
[01:19:50] 就會看這個領域
[01:19:52] 他需要什麼樣的工作嘛
[01:19:54] 然後
[01:19:55] 那些做什麼樣的東西
[01:19:56] 會帶來更多的
[01:19:58] 這種領域的影響力嘛
[01:20:00] 然後還有業界的影響力嘛
[01:20:02] 對
[01:20:03] 然後
[01:20:04] 就是如果當你很清楚
[01:20:05] 你要做什麼的時候
[01:20:07] 你其實是不會遇到什麼
[01:20:08] 錯誠的
[01:20:09] 對
[01:20:10] 就是接受
[01:20:11] 太快的那種
[01:20:12] Channel你其實我覺得都是
[01:20:14] 有辦法把它
[01:20:16] 搞定的
[01:20:17] 對
[01:20:18] 跟那種Channel你不知道
[01:20:19] 做什麼東西
[01:20:20] 你不知道
[01:20:21] 做什麼東西是有用的
[01:20:22] 我覺得這個才是最大的Channel
[01:20:23] 那你和核心是從歷史中學習的很多
[01:20:26] 對吧
[01:20:27] 我覺得我還是挺喜歡看
[01:20:29] 最早的那些Paper
[01:20:32] 我覺得那些Paper
[01:20:33] 現在都挺好的
[01:20:34] 然後
[01:20:35] 我管這個叫做
[01:20:36] 考古
[01:20:37] 對
[01:20:38] 因為我就喜歡
[01:20:39] 考那些古代的Paper
[01:20:40] 就古代的話
[01:20:42] 可能要你要一年
[01:20:43] 也算古代嘛
[01:20:44] 因為
[01:20:45] 現在
[01:20:46] 一年前的Paper
[01:20:47] 叫老Paper
[01:20:48] 五年前的Paper
[01:20:49] 那可以叫做古代的Paper
[01:20:51] 對
[01:20:52] 那半年你讀的最老的Paper
[01:20:53] 到什麼時候
[01:20:55] 最近半年嗎
[01:20:56] 就是你掉眼的那半年
[01:20:58] 那半年
[01:21:00] 可能就是
[01:21:01] 唐讀到不要說
[01:21:03] 2011幾年的文章吧
[01:21:06] 讀了多年
[01:21:07] 你會賺嗎
[01:21:08] 我覺得
[01:21:10] 我可以說這個你沒什麼
[01:21:11] 我基本上都讀過一篇
[01:21:13] 這個做到的人
[01:21:14] 很少是吧
[01:21:16] 對
[01:21:17] 我覺得不同的人
[01:21:18] 有不同的
[01:21:19] 瑞瑟是Felosophy
[01:21:20] 我覺得
[01:21:21] 就一定要吧
[01:21:22] 這個你與裡面
[01:21:23] 值得看的文章
[01:21:24] 全部都看一遍
[01:21:25] 對
[01:21:26] 你為什麼在AI的
[01:21:28] 眾多領域分支裡面
[01:21:29] 你喜歡的是加購
[01:21:31] 因為我比較喜歡做算法嘛
[01:21:33] 然後
[01:21:34] 當時覺得
[01:21:36] 那什麼的
[01:21:37] 然後就可以看出
[01:21:38] 哪些東西
[01:21:39] 是值得做的嘛
[01:21:40] 對
[01:21:41] 然後
[01:21:43] 就想做一些
[01:21:44] 比較通用的
[01:21:45] 然後整體
[01:21:47] 都是對這種
[01:21:48] 那種人們的
[01:21:49] 有用的一些
[01:21:50] 我可能
[01:21:51] 然後請問一下自己
[01:21:52] 興趣
[01:21:53] 然後發現
[01:21:54] 然後
[01:21:55] 還正如最開始說到嘛
[01:21:56] 就是像
[01:21:57] 黑地如色尺
[01:21:58] 他們有很多寶客
[01:21:59] 然後
[01:22:00] 主要是自己喜歡做算法嘛
[01:22:02] 然後
[01:22:03] 就發現這個領域
[01:22:04] 很深刻我來做
[01:22:05] 對
[01:22:06] 你說寫什麼
[01:22:07] 很好
[01:22:08] 應該還挺好的
[01:22:09] 為什麼
[01:22:10] 因為你這篇文
[01:22:11] 裡面
[01:22:12] 這麼多公式
[01:22:13] 都是很難
[01:22:14] 都是一些舉證的一些
[01:22:16] 舉證的一些懲罰
[01:22:18] 這個東西
[01:22:19] 然後
[01:22:20] 像
[01:22:22] 現信注意
[01:22:24] 由安的形式的話
[01:22:26] 它會
[01:22:27] 有一些
[01:22:28] reconference嘛
[01:22:29] 會有一些現信轉移方程
[01:22:31] 那些公式嘛
[01:22:32] 然後
[01:22:33] 兵型的話
[01:22:34] 可能就是
[01:22:35] 兵型確實
[01:22:36] 數據會比較多一點
[01:22:38] 對
[01:22:39] 那個都比較
[01:22:41] 所以它這個論文
[01:22:42] 顯著比加論文的
[01:22:43] 稅公式是要更多的
[01:22:44] 因為
[01:22:46] 我覺得現信最厲害的主要
[01:22:48] 就是一個玩舉證變化的一個東西嘛
[01:22:50] 它可以把一個
[01:22:51] 平方的東西
[01:22:52] 變成一個現信的
[01:22:53] 然後它又
[01:22:55] 它就是玩這種舉證變化
[01:22:57] 對
[01:22:58] 然後它從
[01:22:59] reconference
[01:23:00] 把它變成創可
[01:23:01] 他們都等價的嘛
[01:23:02] 但他們都設立到
[01:23:03] 很多這種舉證變化嘛
[01:23:05] 所以它
[01:23:07] 數學多一點
[01:23:08] 我覺得很正常吧
[01:23:09] 你剛才提到
[01:23:10] 你讀過之前
[01:23:12] 半年做了
[01:23:13] 很多算法的考古
[01:23:14] 我們給大家講講
[01:23:15] 就是算法是
[01:23:16] 怎麼一步步
[01:23:17] 眼睛到今天的這段算法歷史
[01:23:19] 那我從
[01:23:20] Transformer開始
[01:23:22] 想
[01:23:23] 就轉former
[01:23:24] 換它
[01:23:25] 可能就
[01:23:26] 三個主要模塊吧
[01:23:28] 它
[01:23:29] 也就是
[01:23:30] 3D機制
[01:23:31] 然後另外一個是
[01:23:33] 位置變嘛
[01:23:34] 然後
[01:23:35] 最後就是
[01:23:36] i5分
[01:23:37] 然後
[01:23:39] 最開始那幾年
[01:23:40] 我感覺可能
[01:23:41] 加個RESER是
[01:23:42] 非常多吧
[01:23:43] 然後有一些
[01:23:45] 加個的改進
[01:23:46] 也確實被
[01:23:47] 用到的今天
[01:23:48] 比方說
[01:23:49] 然後就
[01:23:50] 像
[01:23:51] 編碼嘛
[01:23:52] 比如說像Rope
[01:23:53] 它
[01:23:54] 最開始的話
[01:23:55] Transformer的話
[01:23:56] 它是
[01:23:57] 絕對位置編碼
[01:23:58] 然後
[01:24:00] 像今天進行
[01:24:01] 都改成這種
[01:24:02] 相對位置編碼了嘛
[01:24:03] 然後
[01:24:05] 像
[01:24:06] M1的話
[01:24:08] 可能
[01:24:09] 也是
[01:24:10] 從
[01:24:11] 2021年
[01:24:12] 左右就開始
[01:24:13] 發展了
[01:24:14] 然後可能中間
[01:24:16] 有段時間
[01:24:17] 大家可能就不怎麼
[01:24:18] 信按模移嘛
[01:24:19] 然後後面
[01:24:20] 又發現
[01:24:21] 比如說像
[01:24:22] DFC
[01:24:23] 一個
[01:24:24] 把M1做通了
[01:24:25] 然後大家又回來重新
[01:24:26] 做M1
[01:24:27] 然後現在M1
[01:24:28] 應該就是
[01:24:29] 大家都會用的東西嘛
[01:24:31] 然後像
[01:24:33] 單身的話
[01:24:34] 單身
[01:24:35] 就這種變種可能
[01:24:36] 就更多了
[01:24:37] 像
[01:24:38] 前一回說到
[01:24:39] 想要
[01:24:40] 你要0.5
[01:24:41] 可能
[01:24:42] 單身的變種
[01:24:43] 就非常非常多了
[01:24:44] 其實也主要就是
[01:24:46] 兩手變種嘛
[01:24:47] 地都就先
[01:24:48] 信注意你
[01:24:49] 然後
[01:24:50] 第二的話就是
[01:24:51] 吸塑塑力
[01:24:52] 他們
[01:24:54] 先用塑力的話
[01:24:55] 他們就會搞
[01:24:56] 很多那種
[01:24:57] 可能買紫
[01:24:59] 去
[01:25:00] Prosymade
[01:25:01] Service
[01:25:02] 然後在今天來看
[01:25:03] 我覺得
[01:25:04] 這是一個非常
[01:25:05] 錯誤的方向
[01:25:06] 不知道
[01:25:07] 就不應該
[01:25:08] 去用可能
[01:25:09] Message
[01:25:10] 去估計
[01:25:11] 這些
[01:25:12] Service的單身嘛
[01:25:13] 然後有一些好工作的話
[01:25:15] 可能就會
[01:25:17] 因為沒有followup
[01:25:19] 然後被
[01:25:20] 埋沒在
[01:25:22] 文獻海裡面呢
[01:25:24] 比方說
[01:25:25] 像
[01:25:26] Data
[01:25:27] 那這個工作
[01:25:28] 我前面也說
[01:25:29] 他稍微1點就有了嘛
[01:25:31] 然後
[01:25:32] 對他入那個東西
[01:25:33] 可能
[01:25:34] 後面幾年
[01:25:35] Taggy Seriously
[01:25:37] 就是沒有什麼
[01:25:38] Full-up work
[01:25:40] 然後
[01:25:41] 從時間內
[01:25:42] 也見上來說
[01:25:43] 也話來說
[01:25:44] 可能
[01:25:45] 有一些技術
[01:25:46] 比如說像
[01:25:47] 這種
[01:25:48] 系列度的這種
[01:25:49] 疑問
[01:25:50] 可能
[01:25:51] 很多年前就有了
[01:25:52] 比方說
[01:25:53] 像這個
[01:25:54] 系列度的這種DK
[01:25:56] 至少
[01:25:58] P
[01:25:59] 2.2.2.2.3
[01:26:00] 他可能就有一篇工作了
[01:26:01] 然後
[01:26:02] 最早的話
[01:26:03] 我可以考古的
[01:26:04] 2016年
[01:26:06] 然後
[01:26:07] 但後面的話
[01:26:08] 比如說
[01:26:09] 2023年
[01:26:10] 他反而用的是一個
[01:26:11] 遺忘
[01:26:12] 速率
[01:26:13] 跟
[01:26:14] 速率度的一個DK
[01:26:16] 所以我覺得可能
[01:26:18] 就是之前的技術
[01:26:19] 不能
[01:26:20] 更好的
[01:26:21] 傳承下來
[01:26:23] 然後
[01:26:24] 然後我又比較喜歡
[01:26:25] 把所有的
[01:26:26] 之前所有的技術
[01:26:27] 全部重新審視
[01:26:29] 然後
[01:26:30] 學一下
[01:26:31] 我覺得
[01:26:32] 最美的
[01:26:33] 這篇說
[01:26:34] 他如果這個技術
[01:26:35] 又可以
[01:26:36] 重
[01:26:37] 縣光芒
[01:26:38] 對
[01:26:39] 但如果
[01:26:40] 如果沒有我來
[01:26:42] Full-Out文化
[01:26:43] 可能就不好說了
[01:26:44] 可能
[01:26:45] 可能這一套
[01:26:46] 技術路線可能
[01:26:47] 就會
[01:26:48] 演藏在
[01:26:49] 文縣海裡面
[01:26:50] 對
[01:26:51] 然後
[01:26:52] Fast-Out-Tension的話
[01:26:55] 他們這早可能
[01:26:57] 就做一些
[01:26:58] Static的
[01:26:59] 一些Fast-Out-Tension
[01:27:00] None Formal
[01:27:01] 那種
[01:27:03] Big Bird
[01:27:04] 他們會有
[01:27:05] 各種各樣的
[01:27:06] Fast-Out-Tension
[01:27:07] 然後好像
[01:27:08] 後面就
[01:27:09] 主見
[01:27:10] 收捏到
[01:27:11] 用Slayer Meadow了
[01:27:12] 然後
[01:27:13] 可能
[01:27:14] 近幾年的話
[01:27:15] 他會有一些
[01:27:16] 不一樣的東西
[01:27:18] 就是
[01:27:19] 早幾年
[01:27:20] 比較少
[01:27:21] 最近有
[01:27:22] 表示
[01:27:23] 像
[01:27:24] 動態係數
[01:27:25] 像
[01:27:26] Kimmy Mobile
[01:27:27] 然後
[01:27:28] DF-C
[01:27:29] 跟著這種
[01:27:30] 動態係數
[01:27:31] 對
[01:27:32] 然後
[01:27:33] 總統來說
[01:27:34] 就是
[01:27:35] 我感覺
[01:27:36] 整體還是
[01:27:38] 上海
[01:27:39] 在不斷演進
[01:27:40] 然後
[01:27:41] 可能他
[01:27:42] 整個發展
[01:27:43] 就是需要
[01:27:44] 有一些技術
[01:27:45] 可能需要
[01:27:46] Rething
[01:27:47] 幾次
[01:27:48] 對
[01:27:49] 然後
[01:27:50] 很多上上的話
[01:27:51] 感人這個發展
[01:27:52] 還是會有一些
[01:27:53] 有點諾選
[01:27:55] 上升的味道在裡面
[01:27:56] 對
[01:27:57] 其實歷史中
[01:27:58] 已經有很多的工具
[01:27:59] 但是今天
[01:28:00] 我們需要拿哪些工具來
[01:28:01] 應用
[01:28:02] 推動今天的
[01:28:03] 算法眼睛
[01:28:04] 其實是一個
[01:28:05] 很關鍵的事
[01:28:06] 對
[01:28:07] 我覺得
[01:28:08] 其實
[01:28:09] 很多
[01:28:10] 歷史的算法
[01:28:11] 他其實
[01:28:12] 很先進的
[01:28:13] 但是可能
[01:28:14] 當時
[01:28:15] 同好
[01:28:16] 沒有意識到這個工作的
[01:28:18] 價值
[01:28:19] 可能
[01:28:20] 那個工具就不會買摸了
[01:28:22] 然後
[01:28:23] 也有可能
[01:28:24] 就是那個工作
[01:28:25] 他的配套
[01:28:26] 比如說
[01:28:27] 那些帶馬
[01:28:28] 然後其他人想
[01:28:29] 服樂也沒法服樂
[01:28:30] 對
[01:28:31] 所以
[01:28:32] 所以
[01:28:33] 總的來說
[01:28:34] 就是
[01:28:35] 我覺得
[01:28:36] 如果今天做工作
[01:28:37] 可能的話
[01:28:38] 就是
[01:28:39] 像我
[01:28:40] 我就會
[01:28:41] 把這種
[01:28:42] 帶馬把它
[01:28:43] 做得好
[01:28:44] 讓大家好用
[01:28:45] 所以這套技術
[01:28:46] 肯定能
[01:28:47] 把它
[01:28:48] 讓他
[01:28:49] 流傳下去
[01:28:50] 然後
[01:28:51] 別人的工作
[01:28:52] 我會找一些
[01:28:53] 我覺得
[01:28:54] MexSense的一些工作
[01:28:55] 然後讓他
[01:28:56] 盡可能
[01:28:58] 都看看
[01:28:59] 他這個
[01:29:00] 錢裡有多大吧
[01:29:01] 對
[01:29:02] 然後
[01:29:03] 又說回
[01:29:04] 就比如說
[01:29:05] 加工裡面的算法的話
[01:29:07] 因為他
[01:29:08] 算法的
[01:29:09] 這種
[01:29:10] 也太多了
[01:29:11] 是加工的話
[01:29:12] 肯定還是
[01:29:13] 需要很多算力
[01:29:15] 來試
[01:29:16] 然後
[01:29:17] 有很多算法
[01:29:18] 可能
[01:29:19] 只在小規模
[01:29:20] 下面有握
[01:29:21] 可能到大規模
[01:29:22] 就不握可能
[01:29:23] 這個是非常
[01:29:24] 非常簡單
[01:29:25] 然後
[01:29:26] 可能
[01:29:27] 對
[01:29:28] 可能
[01:29:29] 比如說
[01:29:30] 像今年
[01:29:31] 國內公司又對開源
[01:29:33] 重新有心情
[01:29:35] 可能
[01:29:36] 今年可能會大家見到
[01:29:37] 加個的這種開源工作
[01:29:38] 會更多一點
[01:29:39] 然後
[01:29:40] 變化可能
[01:29:41] 會
[01:29:42] 想要確認
[01:29:43] 說可能
[01:29:44] 大家會覺得
[01:29:45] 加個的變化
[01:29:46] 比去年會多很多吧
[01:29:47] 你的
[01:29:48] Delta Role
[01:29:49] 是什麼
[01:29:50] 給大家的靈感
[01:29:51] 就是
[01:29:52] 2021年嘛
[01:29:53] 那工作
[01:29:54] 是他們提出來的嘛
[01:29:56] 對
[01:29:57] 然後
[01:29:58] 我就想一個
[01:29:59] 冰型算法嘛
[01:30:00] 然後
[01:30:01] 其實
[01:30:02] 就停類似於
[01:30:03] FlySher TanShan
[01:30:05] 至於SurfmySher TanShan
[01:30:07] 對
[01:30:08] 就是一個算法
[01:30:09] 能夠讓他
[01:30:10] 硬件
[01:30:11] 靠小團實現
[01:30:12] 對
[01:30:13] 就是如果
[01:30:14] 沒有FlySher TanShan
[01:30:15] SurfmySher TanShan
[01:30:16] 也走不到今天嘛
[01:30:17] 然後
[01:30:18] 像
[01:30:19] 沒有那個
[01:30:20] 冰型算法的話
[01:30:21] 的話
[01:30:22] 大概是一個
[01:30:23] 這樣子的關係
[01:30:24] 對
[01:30:25] 然後我做
[01:30:26] Risters可能
[01:30:27] 就是比較喜歡
[01:30:28] 從
[01:30:29] 實際上的這種
[01:30:31] 硬件的這種
[01:30:33] 清合力來研究的
[01:30:35] 就是看
[01:30:36] 因為我看
[01:30:37] 又算法
[01:30:38] 有沒有潛力
[01:30:39] 我會來分析這個算法
[01:30:40] 它的這個
[01:30:41] 冰型潛力
[01:30:42] 有多大
[01:30:43] 然後
[01:30:44] 它的Skaterability
[01:30:45] 會有多大
[01:30:46] 對
[01:30:47] 然後我會在
[01:30:48] 歷史的
[01:30:49] 文獻
[01:30:50] 裡面找出一些
[01:30:52] Machine Learning算法
[01:30:54] MakeSense
[01:30:55] 然後
[01:30:56] 同時
[01:30:57] 我又能想辦法
[01:30:58] 把它冰型的一些算法來玩
[01:30:59] 對
[01:31:00] 這是我的
[01:31:01] 做Risters的
[01:31:02] 思路
[01:31:03] 我覺得總的來說
[01:31:04] 就是
[01:31:05] 就是
[01:31:06] Machine Learning
[01:31:07] 算法
[01:31:08] MakeSense
[01:31:09] 然後它這個算法
[01:31:10] 它又可以
[01:31:11] 有冰型的算法
[01:31:12] 只是這樣
[01:31:13] 的算法才能
[01:31:14] 才能被
[01:31:15] 然後在
[01:31:16] 這個年代
[01:31:17] 被用到
[01:31:18] 因為
[01:31:19] 就在
[01:31:20] Machine Learning
[01:31:21] 的年代
[01:31:22] 就是Skaterability
[01:31:23] 左右
[01:31:24] 肯定需要
[01:31:25] 能夠
[01:31:26] Skaterable的算法
[01:31:27] 對
[01:31:28] 然後如果一個算法
[01:31:29] 它
[01:31:30] 就跟MakeSense
[01:31:31] 像德
[01:31:32] 它如果這個算法
[01:31:33] 我覺得它們
[01:31:34] 這個算法
[01:31:35] MakeSense
[01:31:36] 然後同時
[01:31:37] 用了Skaterability
[01:31:38] 就不要好了話
[01:31:39] 那
[01:31:40] 就完全
[01:31:41] 有可能
[01:31:42] 就是在
[01:31:43] 今天的
[01:31:44] 這個時代
[01:31:45] 上面
[01:31:46] 是會有
[01:31:47] 前文字
[01:31:48] 和Kimining
[01:31:49] 就已經讓我們
[01:31:50] 大家的一些
[01:31:51] 星期
[01:31:52] 像
[01:31:53] 就做個新加工的
[01:31:54] 原來
[01:31:55] 我前幾天
[01:31:56] 做了一個
[01:31:57] 論文的不可
[01:31:58] 就提到
[01:31:59] 川斯科目
[01:32:00] 是
[01:32:01] 這一代硬件的
[01:32:02] 天選加工
[01:32:03] 對
[01:32:04] 川斯科目它
[01:32:05] 肯定是天選嘛
[01:32:06] 比如說
[01:32:07] 川斯科目
[01:32:08] 就是
[01:32:09] 為了讓它
[01:32:10] 硬件快
[01:32:11] 設計出來
[01:32:12] 像
[01:32:13] FF
[01:32:14] 肯定不用說
[01:32:15] 都是
[01:32:16] 肯定
[01:32:17] 快嘛
[01:32:19] 然後
[01:32:20] 像
[01:32:21] Euthansion
[01:32:22] 它
[01:32:23] 其實就是
[01:32:24] 之前
[01:32:25] Euthansion之前
[01:32:26] ISTM
[01:32:27] 製作安排
[01:32:28] 這種
[01:32:29] 並行得
[01:32:30] 快
[01:32:31] 來做的
[01:32:32] 然後
[01:32:33] 像ISTM
[01:32:34] 肯定硬件加速
[01:32:35] 就
[01:32:36] 更難搞嘛
[01:32:37] 然後
[01:32:38] 像
[01:32:39] Euthansion
[01:32:40] 它
[01:32:41] 就算
[01:32:42] 是
[01:32:43] 平方的
[01:32:44] 複雜度
[01:32:45] 它就可以通過
[01:32:46] 舉證懲罰
[01:32:47] 然後
[01:32:48] 來
[01:32:49] 算到那個
[01:32:51] Opo
[01:32:52] 所以它的硬件
[01:32:54] 卿和
[01:32:55] 是比
[01:32:56] 安要好
[01:32:57] 很多的
[01:32:58] 所以
[01:32:59] 大家會
[01:33:00] 寧願
[01:33:01] 去用
[01:33:02] 理論複雜度
[01:33:03] 更高的
[01:33:04] Transformer
[01:33:05] 也不會
[01:33:06] 來用這個
[01:33:07] 理論複雜度
[01:33:08] 更定的
[01:33:09] ISTM
[01:33:10] 因為
[01:33:11] 它們這個
[01:33:12] 硬件卿
[01:33:13] 表現
[01:33:14] 又
[01:33:15] 更好的一些
[01:33:17] 算法
[01:33:18] 因為
[01:33:19] 川普
[01:33:20] 他不但
[01:33:22] 硬件
[01:33:23] 跟卿和
[01:33:24] 確實
[01:33:25] 也解決了一些
[01:33:26] 長程一來的關係的問題
[01:33:27] 所以
[01:33:28] 他才會
[01:33:29] 流行的
[01:33:30] 這麼開
[01:33:31] 像今天的
[01:33:32] 化粒量
[01:33:33] 他身
[01:33:34] 又重新跟上舞臺
[01:33:35] 肯定
[01:33:36] 也離不開
[01:33:37] 這一系列的發展
[01:33:38] 就
[01:33:39] 比如說
[01:33:40] 像那些
[01:33:41] 就是
[01:33:42] 把他分成創
[01:33:43] 那些
[01:33:44] 冰型的
[01:33:45] 算法
[01:33:46] 然後
[01:33:47] 他
[01:33:48] 更強的那些
[01:33:50] 設計
[01:33:51] 能讓他
[01:33:52] 從
[01:33:53] Motion Learning
[01:33:54] Performance
[01:33:55] 這個角度
[01:33:56] 更加沒
[01:33:57] 甚少
[01:33:58] 這些
[01:33:59] 這些
[01:34:00] 才是
[01:34:01] 能
[01:34:02] 推動
[01:34:03] 他發展的
[01:34:04] 圓動力
[01:34:05] 所以我還是
[01:34:06] 非常主張
[01:34:07] 就是
[01:34:08] 來
[01:34:09] 做一些
[01:34:10] 非常
[01:34:11] 很principal
[01:34:12] 很有
[01:34:13] 就是
[01:34:14] 他會
[01:34:15] Mathematically
[01:34:16] 一個Round Day
[01:34:17] 就會
[01:34:18] 就從事
[01:34:19] 上
[01:34:20] 他是Mexons的
[01:34:21] 就比如說
[01:34:22] 他從事
[01:34:24] Mexons
[01:34:26] 因為
[01:34:27] 我覺得
[01:34:28] 心還是
[01:34:29] 肯定要
[01:34:30] 結合
[01:34:31] 當前硬件的
[01:34:32] 就會有些人說
[01:34:33] 那
[01:34:34] 我設計
[01:34:35] 一個算法
[01:34:36] 他足夠好
[01:34:37] 那
[01:34:38] 公司
[01:34:39] 來幫我
[01:34:40] 那
[01:34:41] 這算法
[01:34:42] 你是
[01:34:43] 精子做的
[01:34:44] 還是
[01:34:45] 因子做的
[01:34:46] 硬件公司
[01:34:47] 天天幫你
[01:34:48] 那是不可能的
[01:34:49] 那你肯定
[01:34:50] 你要
[01:34:51] 讓你的
[01:34:52] 上法
[01:34:53] 先去
[01:34:54] 滿足一些
[01:34:55] 非常
[01:34:56] 通用的原則
[01:34:57] 像
[01:34:58] Hardwell
[01:34:59] 他有一些
[01:35:00] Prince of
[01:35:02] MemoryHardrock
[01:35:04] 這種東西
[01:35:05] 然後
[01:35:06] 像
[01:35:07] 舉證懲罰
[01:35:08] 更有
[01:35:09] 基本上
[01:35:10] 都是會
[01:35:11] 遵徹
[01:35:12] 這種原則
[01:35:13] 就有一些
[01:35:14] Ulyss
[01:35:15] 的一些
[01:35:16] Prince of
[01:35:17] MemoryHardrock
[01:35:18] 你設計算法
[01:35:19] 你可能
[01:35:20] 每必要
[01:35:21] 去專門
[01:35:22] 譬如
[01:35:23] 針對Hardwell
[01:35:24] Hardwell
[01:35:25] 但
[01:35:26] 做算法
[01:35:27] 製造
[01:35:28] 要去
[01:35:29] 滿足
[01:35:30] 這些
[01:35:32] 硬件
[01:35:33] 比較
[01:35:34] 通用的
[01:35:35] 這種Prince of
[01:35:37] MemoryHardrock
[01:35:38] 沒有什麼
[01:35:40] 實際價值的
[01:35:41] 就存
[01:35:42] 自於自樂
[01:35:43] 對
[01:35:44] KIMI
[01:35:45] 連你的這個
[01:35:46] 為硬件
[01:35:47] 有做什麼樣的
[01:35:48] 有畫面
[01:35:49] KIMI
[01:35:50] 我覺得
[01:35:51] 他
[01:35:52] 的那個
[01:35:53] 算法
[01:35:54] 還是
[01:35:55] 硬件
[01:35:56] 傾合的
[01:35:57] 然後
[01:35:58] 可能的話
[01:35:59] 他
[01:36:00] 現在
[01:36:01] 應該還是
[01:36:02] 張玉寫的
[01:36:03] 那個
[01:36:04] Tryton的算法
[01:36:05] 對
[01:36:06] 就
[01:36:07] 可能
[01:36:08] 有畫
[01:36:09] 他是一個
[01:36:10] 非常
[01:36:11] 好時的
[01:36:12] 一個公眾
[01:36:13] 就
[01:36:14] 非常的需要時間
[01:36:16] 要慢慢
[01:36:17] 摸
[01:36:18] 就要老師
[01:36:20] 可能有畫
[01:36:21] 慢慢來打摸
[01:36:22] 我覺得
[01:36:23] IQA的
[01:36:24] 跌在
[01:36:25] 跌在
[01:36:26] 的時候
[01:36:27] 大家用
[01:36:28] Tryton寫一下
[01:36:29] 錯課
[01:36:30] 就行了
[01:36:31] 然後
[01:36:32] 如果
[01:36:33] 他
[01:36:34] 驗出來
[01:36:35] 有用的話
[01:36:36] 對
[01:36:37] 就我
[01:36:38] 所知
[01:36:39] 他們現在應該
[01:36:40] 還在用
[01:36:41] Tryton課
[01:36:42] 我再去
[01:36:43] 吧
[01:36:44] 我不知道他們
[01:36:45] 會不會
[01:36:46] 找人
[01:36:47] 來寫
[01:36:48] 擴大課
[01:36:49] 從
[01:36:50] 1.7顏
[01:36:52] 合的角度
[01:36:53] 你覺得
[01:36:54] 下一代
[01:36:55] 得算
[01:36:56] 法會
[01:36:57] 怎麼
[01:36:58] 眼鏡
[01:36:59] 現在
[01:37:00] 我覺得
[01:37:01] 音劍
[01:37:02] 和他
[01:37:03] 跟
[01:37:04] Transformer
[01:37:05] 對於
[01:37:06] Altality
[01:37:07] 來說是
[01:37:08] 有些
[01:37:09] 不好的
[01:37:10] 因素在裡面
[01:37:11] 因為現在
[01:37:12] 要加個
[01:37:13] 音劍
[01:37:14] 大家會發現
[01:37:15] 它就是
[01:37:16] 為了去
[01:37:17] 有畫
[01:37:18] 舉證成
[01:37:19] 然後讓它
[01:37:20] 舉證成
[01:37:21] 越快越好
[01:37:22] 因為
[01:37:23] Transformer
[01:37:24] 你們有
[01:37:25] 大量的舉證成
[01:37:26] 它就像
[01:37:28] 音劍
[01:37:30] 上搞一些
[01:37:31] 快速的舉證成
[01:37:32] 的東西
[01:37:33] 這邊
[01:37:34] 像最近
[01:37:35] Black Wheel
[01:37:36] 上面還有一些
[01:37:37] 專門針對這種舉證成
[01:37:38] 還有一些
[01:37:39] 單獨的那種
[01:37:41] 那種
[01:37:42] Layson
[01:37:43] 單獨的那種
[01:37:44] Memory
[01:37:45] 都是來
[01:37:46] 有畫舉證成的
[01:37:47] 所以可能
[01:37:48] 大家會看到
[01:37:49] FlyShot10身
[01:37:50] 會
[01:37:51] 越來越快
[01:37:52] FA4
[01:37:53] 它會
[01:37:54] 在 Black Wheel
[01:37:55] 上面會越來越快
[01:37:56] 對
[01:37:57] 然後
[01:37:58] 我覺得
[01:38:00] 既然
[01:38:01] 這個音劍
[01:38:02] 從我
[01:38:04] 一握舞的
[01:38:05] 從
[01:38:06] 社稷算法的走來看
[01:38:08] 你就必須要設計一些
[01:38:09] 能有舉證成法的算法
[01:38:11] 要不然
[01:38:12] 你這個音劍
[01:38:13] 效率肯定是
[01:38:14] 更不上
[01:38:15] 像
[01:38:16] 尼諾探生
[01:38:17] 它創作算法有個好處
[01:38:19] 就是它基本上
[01:38:20] 都是一些舉證成
[01:38:22] 當然它
[01:38:23] 還會有一些其他的
[01:38:24] 偶威害的
[01:38:25] 這個的話
[01:38:26] 可能就是
[01:38:28] 得
[01:38:29] 克肥下
[01:38:31] 它可能
[01:38:32] 比如說在
[01:38:33] Training 的時候
[01:38:34] 可能還是
[01:38:35] Brew
[01:38:36] FlyShot10身4
[01:38:37] 這種
[01:38:38] 在 Black Wheel 上面
[01:38:39] 高效
[01:38:40] 但
[01:38:41] 其實也無所謂
[01:38:42] 就可能地方也不
[01:38:43] 訓練效率
[01:38:44] 它是
[01:38:45] Cle那種
[01:38:46] Inference效率
[01:38:47] 所以
[01:38:48] 只要訓練的時候
[01:38:50] 就是
[01:38:51] 能以
[01:38:52] Reasonable 速度來訓
[01:38:54] Reasonable 速度來
[01:38:56] Pre-Fail
[01:38:57] Decoding 快的話
[01:38:58] 這種下個
[01:38:59] 其實也是有市場的
[01:39:00] 然後
[01:39:02] 另外的話
[01:39:03] 這邊就說
[01:39:04] 像
[01:39:05] 翻櫃M11
[01:39:07] 然後
[01:39:08] Spot10身這種
[01:39:10] 醬Vlog
[01:39:11] 又能用舉證成法
[01:39:13] 它們都是
[01:39:14] 屬於這種類型
[01:39:16] 它們肯定還是要用舉證成法了
[01:39:18] 然後
[01:39:19] 想辦法
[01:39:20] Flover
[01:39:21] 打下去
[01:39:22] 然後同為一些
[01:39:23] 算法的創新
[01:39:24] 來
[01:39:25] Flover
[01:39:26] 打下去
[01:39:27] 然後同時
[01:39:28] 保證
[01:39:29] 它們沒有
[01:39:30] 大家的舉證成
[01:39:31] 這一代用算法裡面
[01:39:32] 基本上都舉證成
[01:39:33] 那基本上
[01:39:34] 這個算法
[01:39:35] 它的不要也停
[01:39:36] 相當的
[01:39:37] 還是挺
[01:39:38] 挺好
[01:39:39] 優化的嘛
[01:39:40] 因為
[01:39:41] 因為我覺得這個
[01:39:42] 硬件就是
[01:39:43] 笨整
[01:39:44] 往舉證成法
[01:39:45] 越來越快的方向再走了
[01:39:47] 就甚至
[01:39:48] 說像
[01:39:50] Face
[01:39:51] 就是舉證成
[01:39:52] 太快了
[01:39:53] 導致
[01:39:54] 它那個
[01:39:55] SonylS
[01:39:56] 那個
[01:39:57] 某塊的變成一個平靜
[01:39:59] 然後
[01:40:00] 然後它們
[01:40:01] 就用一些
[01:40:02] Face的話
[01:40:03] 它就用一些
[01:40:04] Approxy-made方法
[01:40:06] 來做那個
[01:40:07] EXP
[01:40:08] 對
[01:40:09] 這個也挺
[01:40:11] 挺好笑的
[01:40:12] 就是
[01:40:13] 舉證成
[01:40:14] 太快了
[01:40:15] 那我們
[01:40:16] 現在還是要去
[01:40:17] 進行去
[01:40:18] 利用
[01:40:19] 舉證成
[01:40:20] 快速的這個
[01:40:21] 性質
[01:40:22] 然後自己在一些身份
[01:40:23] 我覺得
[01:40:24] SpaS
[01:40:25] TanShan
[01:40:26] 那個就已經用到了這種性質
[01:40:28] 我覺得
[01:40:29] DivShan它是一個非常
[01:40:30] 非常注重這種
[01:40:32] 硬件和算法
[01:40:34] 寫同設計的
[01:40:36] 一個公司
[01:40:37] 像DivShan
[01:40:38] SpaShan
[01:40:39] 它會有一個
[01:40:40] 那個
[01:40:41] 叫IndyShan
[01:40:42] 它就是
[01:40:43] FP8
[01:40:44] 來
[01:40:45] 做這個
[01:40:46] 算這個
[01:40:47] TanShan
[01:40:48] 因為它
[01:40:49] 不需要
[01:40:50] SuffMax
[01:40:51] 它只需要
[01:40:52] SanLogit
[01:40:53] 然後來做Tob
[01:40:54] 可以來選Score嗎
[01:40:55] 所以首先它
[01:40:56] 就是FP8
[01:40:57] 然後它
[01:40:58] 又可以
[01:41:00] 把那個
[01:41:01] 昂貴的
[01:41:02] Esponential
[01:41:03] 的那個
[01:41:04] 操作
[01:41:05] 把它去掉
[01:41:06] 這樣的話
[01:41:07] 它就是基本上
[01:41:08] 就是一大堆
[01:41:09] 舉證成
[01:41:10] 然後它們
[01:41:11] 所以說它
[01:41:12] 它
[01:41:13] 它
[01:41:14] 它
[01:41:15] 它
[01:41:16] 它
[01:41:17] 它
[01:41:18] 它
[01:41:19] 它
[01:41:20] 它
[01:41:21] 它
[01:41:22] 它
[01:41:23] 它
[01:41:24] 沈沈
[01:41:26] 对
[01:41:27] 相对来说Dipsick和Kimme哪个在印建庆盒上做得很好
[01:41:32] 听起来是Dipsick
[01:41:33] Dipsick
[01:41:34] Dipsick
[01:41:35] Absolutely
[01:41:36] OK
[01:41:37] 因为没有把这个做一个重要的友好目标对吧
[01:41:40] 不太确定
[01:41:41] 我觉得Kimme肯定还是SKR的这种印建上面的东西
[01:41:47] 但是没有Dipsick那么追求
[01:41:50] 对Dipsick我觉得他们非常追求之中
[01:41:54] 比如说这个Sanford的Mundler在P8上面跑了这种之类的
[01:41:59] 我觉得他们Infer 应该在他们Sanford
[01:42:02] 迭代的过程中应该换一拳会比较高一点
[01:42:05] 我觉得这个都是就是
[01:42:07] 英公司而已了吧
[01:42:09] 就有些公司他Infer的换一拳会高一些
[01:42:12] 就有些公司他算法的换一拳会高一些
[01:42:15] 感觉Sanford就今天会搞一些让Infer不闯的东西出来
[01:42:19] 对
[01:42:21] 我觉得如果年轻的研究者想要进入朱利基之或者架构
[01:42:25] 算法这些领域的话对他没有什么建议
[01:42:28] 他应该是从哪些地方还是入手
[01:42:30] 现在的话
[01:42:31] 现在的话找个公司去试戏
[01:42:35] OK 就是跟上来
[01:42:37] 因为我觉得做架构必须要算你
[01:42:41] 没有算你就没法做架构
[01:42:43] 所以我觉得还是先找个Lib去试戏吧
[01:42:51] 好了 今天的节目就是这样
[01:42:53] 这里是商业访谈路
[01:42:55] 是一档由语言及世界工作室出品的深度访谈节目
[01:42:58] 你可以到公众号关注我们的工作室
[01:43:00] 或取更多的信息
[01:43:02] 我们的公众号是语言及世界
[01:43:05] Language is world
[01:43:07] 我们希望和你一起从这里探索新的世界
[01:43:20] 我们的公众号是一档由语言及世界
[01:43:23] Language is world
[01:43:25] 我们的公众号是一档由语言及世界
[01:43:27] Language is world
[01:43:30] 我们的公众号是一档由语言及世界
[01:43:32] Language is world
[01:43:35] 我们的公众号是一档由语言及世界
[01:43:38] Language is world
[01:43:40] 我们的公众号是一档由语言及世界
[01:43:42] Language is world
[01:43:44] 我们的公众号是一档由语言及世界
[01:43:47] Language is world