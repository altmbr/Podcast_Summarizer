# Nvidia's Jensen Huang on AI & the Next Frontier of Growth

**Podcast:** Training Data
**Date:** 2025-10-19
**Video ID:** m1wfJOqDUv4
**Video URL:** https://www.youtube.com/watch?v=m1wfJOqDUv4

---

[00:00:00] Good morning, everyone. My name is Constantine Bueller and I'm a partner at Sequoia Capital
[00:00:11] SPEAKER_01: focused on AI investing. Both in video and Citadel Securities actually have a lot in common.
[00:00:19] They're both exceptional businesses. They really well run. Really brilliant leaders. They
[00:00:28] SPEAKER_01: are exceptionally well run. They were both powered by the computing revolution and both
[00:00:34] are leaders in their respective industries with technology. They also have another lesser
[00:00:40] SPEAKER_01: known fact in both cases their first outside investor, Lysicoia Capital.
[00:00:45] So one million dollars they risked one million dollars in the video. In 1993 you were worth
[00:00:55] SPEAKER_01: it. One solid million dollars went way out on the limb. It was a little more in Citadel Securities.
[00:01:05] SPEAKER_01: So when we were asked to speak about AI at this conference it was incredibly clear who the
[00:01:14] SPEAKER_01: best person in the world to speak would be. It's the man who has built the entire infrastructure
[00:01:20] SPEAKER_01: for the AI revolution upon which all of this AI rests and the man who built the most valuable
[00:01:27] SPEAKER_01: company in the world. Please join me in welcoming Jensen Huang.
[00:01:31] SPEAKER_01: It's nice to wake up this way.
[00:01:39] SPEAKER_00: You've been working for hours.
[00:01:43] SPEAKER_01: Yes, I have.
[00:01:45] SPEAKER_01: So Jensen we have a room full of institutional investors who are some of the best in the world.
[00:01:52] SPEAKER_01: They manage many trillions of AUM and they are constantly looking for edge. You are someone
[00:01:59] SPEAKER_01: who always has edge. In every one of our conversations you have compelling insights about what the
[00:02:04] SPEAKER_01: future is going to look like. In the next 60 minutes we have an ambitious agenda to cover
[00:02:10] the stories of the edge from the very beginning of the video all the way through its rise to
[00:02:15] SPEAKER_01: the center of the AI revolution and then we will spend the majority of the time on what's
[00:02:19] SPEAKER_01: next for a video in AI.
[00:02:21] SPEAKER_01: Okay.
[00:02:22] SPEAKER_01: So let's start at the very beginning. It's 1993, you're 30 years old. What's the insight
[00:02:27] SPEAKER_01: that gave you the edge to start a video?
[00:02:30] We were going through basically the PC revolution and the revolution of CPUs. It was the era
[00:02:38] SPEAKER_00: of Moore's Law. It was the time when integrated microprocessors, Intel, Moore's Law, the scaling,
[00:02:49] SPEAKER_00: the scaling laws of transistors. That was the buzz and that was nearly all of the investment
[00:02:56] SPEAKER_00: dollars in Silicon Valley and in the computer industry.
[00:03:01] SPEAKER_00: We observed something a little different. We said there are many problems that one of
[00:03:07] SPEAKER_00: the benefits of the CPUs is the general purpose. But the fundamental problem of general
[00:03:12] SPEAKER_00: purpose technologies is that they tend not to be very good, extremely good at very hard
[00:03:17] SPEAKER_00: problems. And so we congestured two things. One, we observed that there are problems that
[00:03:26] SPEAKER_00: we could solve with an accelerator that is more domain specific, more targeted and those
[00:03:35] problems could be interesting to solve. And we observed that general purpose technologies,
[00:03:41] SPEAKER_00: the shrinking of these transistors would eventually reach the limits. The idea that you could
[00:03:47] SPEAKER_00: keep reducing the size of transistors and scaling it using this technique, it's a set of
[00:03:53] heuristics called denards scaling. And denards scaling and meet and con way came up with
[00:04:00] SPEAKER_00: what are really the fundamental principles behind Moore's Law. And if you go back to those,
[00:04:05] SPEAKER_00: you'll discover that there will be a limit to how far you can shrink transistors. And
[00:04:10] SPEAKER_00: at some day, you'll get diminishing returns. And there are large computing problems. We believe
[00:04:15] SPEAKER_00: that the computing problems that we could solve are nearly infinite in scale. And so one
[00:04:20] SPEAKER_00: of these days, a new type of computing approach would emerge. And we focused our company on
[00:04:28] SPEAKER_00: augmenting, supplementing general purpose computing with this technology called accelerator
[00:04:32] SPEAKER_00: computing. And that was really the observation. And you said something earlier about how
[00:04:41] SPEAKER_00: Nvidia is always ahead of the curve. Oftentimes, if you reason about things from first principles,
[00:04:50] SPEAKER_00: what's working today incredibly well. If you could reason about it from first principles
[00:04:56] SPEAKER_00: and ask yourself on what foundation is that first principle built on top. And how would
[00:05:01] SPEAKER_00: that change over time? It allows you to hopefully see around corners.
[00:05:08] So when you built the graphics accelerator, you were early to the party, but then hundreds
[00:05:14] SPEAKER_01: of other competitors sprung up. You eventually won in that market. In the early 2000s, you said,
[00:05:21] hey, this technology might be able to generalize itself. You're talking about the generalization
[00:05:25] SPEAKER_01: of a CPU. Perhaps the GPU could also be generalized for more processing. Let's talk about CUDA.
[00:05:32] SPEAKER_01: How did that come about? Where did you get that insight? The story goes that it's from researchers.
[00:05:38] SPEAKER_01: How did you read their work and conclude that the GPU could be a general computer device?
[00:05:43] SPEAKER_01: Well, the first of all, the reason why Nvidia was hard to build was because we had to invent
[00:05:52] a new technology and invent a market. At the time, in 1993, in order to create a new computing
[00:06:02] SPEAKER_00: platform, you need a large market. Silicon graphics, which doing 3D graphics at the time,
[00:06:08] SPEAKER_00: the markets are too small to enable a new computing platform. If we wanted to create a new
[00:06:14] SPEAKER_00: computing architecture, we need a large market and that large market didn't exist because
[00:06:18] SPEAKER_00: the architecture didn't exist. You got the chicken and the egg problem. What Nvidia became
[00:06:23] SPEAKER_00: good at, the modern 3D graphics video game market, we contributed tremendously to. So, Sequoia
[00:06:32] SPEAKER_00: capital is a big issue at the time with Nvidia's funding principles that we had to go invent
[00:06:37] SPEAKER_00: the technology and the market simultaneously. The odds of that happening is approximately
[00:06:43] SPEAKER_00: 0%. I still remember when I pitched the story and I said, in Don Valentine, at the time,
[00:06:50] SPEAKER_00: you said, what's your app? Where's the killer app? I said, oh, yeah, there's this company
[00:06:55] SPEAKER_00: called Electronic Arts. I didn't realize Don had just invested in Electronic Arts. Electronic
[00:07:02] SPEAKER_00: Arts, we're going to help them create 3D graphics games and we're going to create this market.
[00:07:07] SPEAKER_00: He goes, you know, Jensen, I want you to know that we invested in Electronic Arts and
[00:07:11] SPEAKER_00: their CTOs 14 years old and it's driven to work and you're telling me that's your killer
[00:07:15] SPEAKER_00: app. Anyhow, we created the modern 3D graphics gaming ecosystem and as you know, it's one
[00:07:25] SPEAKER_00: of the largest entertainment industries in the world. The fundamental problem of 3D graphics
[00:07:30] SPEAKER_00: is basically simulating reality. If you go back to first principles, what it's doing is
[00:07:36] SPEAKER_00: trying to recreate reality. The fundamental, the mathematics of reproducing photo-realistic
[00:07:46] SPEAKER_00: images and dynamic worlds is fundamentally physics simulation. So, linear algebra is obviously
[00:07:53] SPEAKER_00: very important to it and we realize that concept. The question is, how do you bring something
[00:08:02] SPEAKER_00: general purpose into something very specialized? That's the great invention of our company.
[00:08:09] SPEAKER_00: We invented the technology, we invented the market and we invented the pathways for
[00:08:15] SPEAKER_00: us to systematically grow from a very vertically focused industry to eventually become more
[00:08:21] SPEAKER_00: and more general purpose. That hardly ever happens and that pathway was hard to do but I don't
[00:08:29] SPEAKER_00: want to take up the rest of the time explaining it but I think the KUTA's invention is part
[00:08:36] SPEAKER_00: invention of the technology which is observation of how we can generalize our GPUs. But a lot
[00:08:45] SPEAKER_00: of it is about the invention of new products, how to take it to market, invention of new
[00:08:50] SPEAKER_00: strategies, how to get the market to adopt it and invention, inventing essentially ecosystems
[00:08:58] that ultimately creates the flywheel that makes a computing platform happen. So, we invented
[00:09:03] SPEAKER_00: all of those things, they're all brand new and if you go back and you take a step back and
[00:09:08] SPEAKER_00: you ask yourself, aside from ARM and SITEM, X86, what is another computing platform that
[00:09:13] SPEAKER_00: exists in the world that almost everybody uses? It doesn't exist and so inventing a new
[00:09:18] SPEAKER_00: computing platform rarely happens in our case at all it's almost 30 years.
[00:09:25] So you were able to take this very specialized, extremely high-performance acceleration device
[00:09:32] SPEAKER_01: and generalize it so that researchers and academics around the world would be able to run
[00:09:38] their processing much faster. The Moore's law limitations that they were up against all
[00:09:43] SPEAKER_01: of a sudden were relaxed dramatically. Now let's jump forward to the early 2010s. At the
[00:09:50] SPEAKER_01: time, deep learning was kind of an academic backwater. The idea of neural networks had
[00:09:56] SPEAKER_01: gone through a winter phase then in 2012 there was a breakthrough with Alex Nebden computer
[00:10:02] SPEAKER_01: vision and that was all accelerated on Nvidia GPUs. Was that the moment that you realized
[00:10:09] SPEAKER_01: this AI revolution was becoming real and if so how did you capitalize on it was the
[00:10:15] SPEAKER_01: edge to make Nvidia the center of this revolution?
[00:10:18] SPEAKER_01: Two serendipitous moments and then one which is just a great again first principle observation
[00:10:29] SPEAKER_00: about deep learning. The serendipity started with I was trying to solve computer vision and
[00:10:38] SPEAKER_00: we wanted to solve computer vision for a lot of different reasons. We wanted to solve
[00:10:43] SPEAKER_00: computer vision. Computer vision was really brittle, really hard to generalize, a collection
[00:10:53] SPEAKER_00: of a whole bunch of tricks and I really hated how the industry was evolving and really
[00:11:00] SPEAKER_00: quite frustrated with the progress. Meanwhile, one of our major strategies of democratizing
[00:11:10] SPEAKER_00: the architecture is to get scientists in higher education to use our platform, use CUDA.
[00:11:19] SPEAKER_00: So I started with seismic processing, molecular dynamics, particle physics, quantum chemistry.
[00:11:26] SPEAKER_00: I took Nvidia CUDA everywhere and there was a strategy at the company called CUDA
[00:11:33] SPEAKER_00: everywhere that meant Jensen schlepping CUDA all over the world. So I went to universities
[00:11:40] SPEAKER_00: everywhere and we meet with researchers and that initiative of getting CUDA into higher
[00:11:48] SPEAKER_00: education and researchers everywhere caused some researchers to reach out to us in 2012,
[00:11:55] SPEAKER_00: 2011. And Jeff Hinton was trying to solve computer vision and Andrew A. was trying to
[00:12:02] SPEAKER_00: solve computer vision and Yanle Kuhn was trying to solve computer vision because there
[00:12:08] SPEAKER_00: was a contest coming up called ImageNet that faith is in charge of and I was trying to
[00:12:13] SPEAKER_00: solve computer vision. And so when you're naturally trying to solve a problem and then all
[00:12:18] SPEAKER_00: of these interesting people are solving similar problems to the attract your attention.
[00:12:22] SPEAKER_00: So that's serendipity. The thing that was great observation is that we could create a new
[00:12:29] SPEAKER_00: type of solver for them that's called CUDA and N kind of like the sequel of in storage
[00:12:37] SPEAKER_00: computing we invented CUDA and N which is in network computing if you will. And that
[00:12:45] SPEAKER_00: way of doing computation, this library called CUDA and N made it possible for all of them
[00:12:50] SPEAKER_00: to use CUDA successfully. But the thing that was I saw the same results as everybody else.
[00:12:59] SPEAKER_00: Everybody saw the big jump in computer vision effectiveness. But where we took it further
[00:13:05] SPEAKER_00: was we reasoned about so this is so good at computer vision and why and what else could
[00:13:12] SPEAKER_00: it be good at. And the ability for deep neural networks to be extremely deep meaning because
[00:13:21] SPEAKER_00: each layer is trained independently of the others. And you could propagate from a loss function
[00:13:29] SPEAKER_00: all the way back to its input. You could learn almost any function and we came to the conclusion
[00:13:35] SPEAKER_00: this is a universal function approximator. And if we can then add to a state which is
[00:13:42] SPEAKER_00: CNN was kind of a two dimensional multi dimensional pattern recognizer. And then RNNs gave you
[00:13:53] SPEAKER_00: a state machine within it. And LSTM gives you an even better state machine and then transformers
[00:13:58] SPEAKER_00: give you the ultimate state machine. And so the idea that we would have a universal function
[00:14:05] SPEAKER_00: approximator that can learn almost any function. Well the question is what problem can it solve.
[00:14:12] Now you invert the question and we came to the conclusion most of the problems we wanted to solve
[00:14:17] SPEAKER_00: could have a deep learning component to it. And so we decided how would we reason about where deep learning
[00:14:25] SPEAKER_00: could be 10 years from now 20 years from now we broke down the computation problem and we came to the
[00:14:30] SPEAKER_00: conclusion that every single chip, every single system, every software, every single layer of the
[00:14:35] SPEAKER_00: computing stack could be reinvented. And that decision to go after it was probably one of the
[00:14:42] SPEAKER_00: better decisions in history. I was doing a research at the time at Stanford and the major
[00:14:49] SPEAKER_01: constraint was always the compute. We had limited clusters in order to run these algorithms and
[00:14:57] SPEAKER_01: Nvidia came in and not only relaxed that compute but made it possible with the CUDA infrastructure.
[00:15:04] SPEAKER_01: That is largely your history. You make more and more compute possible. In 2016 you very famously created the
[00:15:11] SPEAKER_01: world's first AI factory, the DGX-1. You actually hand delivered it to Elon Musk at OpenAI at
[00:15:21] SPEAKER_01: the time. I built this brand new computer and it doesn't look like anything the world has ever
[00:15:26] SPEAKER_00: seen before. It doesn't work like anything the world has ever seen before. And I remember announcing it at
[00:15:31] SPEAKER_00: GTC and literally the audience was just like this. Nobody knew what I was talking about. That was a joke.
[00:15:41] And so with the same amount of applause. And so I announced this thing and I remember going,
[00:15:46] SPEAKER_00: uh-huh. And literally on that GTC I was invited Elon to talk about the two of us were working on self-driving cars.
[00:15:54] SPEAKER_00: And so he came on stage and he says, Jensen, what's the computer? And I said, DGX-1. And I built it for this reason.
[00:16:01] SPEAKER_00: And he goes, I could use one. And I finally got a PO. And then he goes, he goes, yeah, I have this nonprofit.
[00:16:10] SPEAKER_00: And I said, oh. Oh. You know, when you build something brand new, the last thing you want to hear is your first
[00:16:19] SPEAKER_00: customers a nonprofit. And so anyhow, anyhow, I delivered the door dash computer guy and I door dashed this
[00:16:29] SPEAKER_00: computer up to San Francisco and the company was open AI.
[00:16:35] It is a very profitable nonprofit or revenue scale nonprofit.
[00:16:40] SPEAKER_01: Oh, we've been working together for a whole time. Every model has been built on Nvidia since, yeah.
[00:16:46] And this thing is huge. Physically, when Jensen is talking about a computer, we're talking about a massive device.
[00:16:51] SPEAKER_01: And videos GPUs, when they say, our GPUs, people imagine a little GPUs. Our GPUs, one GPU, is now RackScale. It's two times,
[00:17:01] 120,000 watts, about $3 million. That's a GPU. We also sell smaller GPUs, the ones that Jeff Hinton used.
[00:17:10] SPEAKER_00: That's like $1,000, $500 that plugs into your PC. And you could use it for video games or AI and things like that.
[00:17:18] SPEAKER_00: But we also have bigger GPUs. And then one gigawatt AI factory GPU is about $50 billion.
[00:17:27] So tell us about these AI factories. Because you might have the small one, might be the AI blender.
[00:17:32] SPEAKER_01: But then you have the really big one, these AI factories that you went all in in 2016 and started to say the world is going to need AI factories.
[00:17:41] SPEAKER_01: How did you get that edge, that conviction?
[00:17:43] SPEAKER_01: And then you just got to reason about it.
[00:17:44] SPEAKER_01: Exactly. You got a reason about it. So we built the first one, DGX1. It was the most expensive computer the world's ever seen.
[00:17:51] SPEAKER_00: 300,000 dollars per node. And it wasn't that successful. And so I came to the conclusion, we didn't make it big enough.
[00:18:01] SPEAKER_00: And so we made a bigger one. And the second one became super successful. And now the question then becomes, how large do you make it?
[00:18:11] SPEAKER_00: And how hard do you drive computation? The reason why things are moving so fast is,
[00:18:16] SPEAKER_00: is Envityus product cycles and the way we innovate, the way we design, we're not designing a chip.
[00:18:22] SPEAKER_00: We're designing an entire infrastructure all one time. We're the only company in the world today that you can give a building, some power, and a blank sheet of paper, and we can create everything within it.
[00:18:32] SPEAKER_00: All of the networking, all the switches, all the CPUs, all the GPUs, all of the technology within that entire factory we can build.
[00:18:40] And it all runs the same software stack from Envityus. And because we can integrate like that, we can also move extremely fast.
[00:18:51] SPEAKER_00: So I could redesign the next years, the redesign the next years, and every single year they're all software compatible.
[00:18:57] SPEAKER_00: The benefit of software compatibility is velocity. The reason why the PC was able to move so fast was because they were all Windows compatible.
[00:19:07] SPEAKER_00: And therefore, by definition, if you're compliant with the stack, you could build chips as fast as you like.
[00:19:13] SPEAKER_00: And so we're now building AI factories as fast as we like, at the limits of what's physically possible.
[00:19:20] And because we're innovating at such incredible scale, and we're co-designing, meaning we're changing algorithms, we're changing software, we're changing networking and CPUs, and GPUs all the same time,
[00:19:32] we break out of Moore's Law's limits, which is, as you know, slowing down.
[00:19:38] And so, generationally, we introduce performance levels by about 10 times.
[00:19:43] I mean, it's an incredible level of performance that we give to the market every single year.
[00:19:48] SPEAKER_00: The reason why we do that is we believe that just around the horizon is a problem that is so large you need a larger computer, faster computer,
[00:20:01] SPEAKER_00: on the one hand, on the other hand, when we increase performance at the same power, we're decreasing your cost.
[00:20:08] SPEAKER_00: And so, we're driving costs down incredibly fast, which allows customers to do bigger things, which allows them to generate more revenues from the same factory.
[00:20:21] SPEAKER_00: And so, Nvidia's adoption today is because we are both the highest performance, we're the highest scale, and so if you want giant systems, you could do so, and we're the lowest cost.
[00:20:35] Our performance is so high, you know, for example, if your data center is 1 gigawatt, you're not going to get more than that, you're 1 gigawatt, and so if our per per watt, our energy performance per unit of energy used,
[00:20:50] SPEAKER_00: is 3 times, your company can generate 3 times more revenues in that factory, that's why I call it a factory.
[00:20:58] SPEAKER_00: It's not a data center, it's a factory, they're making money from it.
[00:21:01] And so, these AI factories, once they keep driving the scale up, they want to keep driving the revenues up, they want to keep driving the throughput up.
[00:21:09] SPEAKER_00: And so, that's the reason why we're innovating so fast, and it's hard to keep up with us, and it also explains why we're successful.
[00:21:18] SPEAKER_00: Jensen, you have shifted from a component to a whole platform, that's the AI factory concept, for an investor audience, can you break down what goes into the platform, and then also start to talk about what's next for what the platform looks like?
[00:21:35] Well, the CPUs, GPUs, network processors, there are 3 types of switches, there's a scale up switch that turns one rack into a whole computer, we invent at RackScale Computing, it's called scaling up.
[00:21:55] SPEAKER_00: You scale it out by taking a whole lot of these racks and connecting them together, that switch and that networking has a bunch of software on it, software on top of all this stuff, and then you create one giant system, the size of this building, and this building would probably be about 100 megawatts, a gigawatt is a few thousand acres, and then you connect all these data centers together with even networking so that all the data centers can think together.
[00:22:23] SPEAKER_00: So that's what we built together, that's what we built today.
[00:22:28] There are several reasons why infrastructure is being built so fast, and there's some questions that floating around about the bubble and comparing it to the year 2000, and so just the compare it.
[00:22:45] SPEAKER_00: During the time of 2000 internet companies, there were hospital.com, there was pets.com, most of the internet companies were not profitable, and the size of the whole internet industry was about $20,30 billion if you recall.
[00:23:01] And today, the first thing you need to observe is that AI isn't just about the brand new companies, open AI and anthropocon, others.
[00:23:12] SPEAKER_00: AI is transforming the way that hyperscalers do work, like for example, search is now powered by AI.
[00:23:20] So, you can see how you see ads and news and stories are now movies generated by AI.
[00:23:30] SPEAKER_00: User generated content. So basically, Google's business, Amazon's business, Meta's business, hundreds of billions of dollars of revenues are all powered by AI now.
[00:23:40] SPEAKER_00: So, in the absence of open AI and anthropoc, this entire hyperscale industry is being powered by AI.
[00:23:46] SPEAKER_00: And so, the first thing to observe is that whole thing needs to go from classical CPUs with classical machine learning to now deep learning with AI.
[00:23:56] SPEAKER_00: So, that transition alone is hundreds of billions of dollars, doesn't make sense?
[00:24:01] SPEAKER_00: Absolutely.
[00:24:02] SPEAKER_01: And so, that's one. The second thing is that we now have this new market. This new market is called AI, and it's got a new industry, and they produce AI.
[00:24:14] And so, open AI is the anthropocs, the XAI's, the Gemini from Google, of course, and Meta is going to be an AI maker.
[00:24:23] SPEAKER_00: And so, this entire layer of AI model makers is also building AI factors.
[00:24:31] And these AI's are going to power the next generation of new opportunities. And this is where the Harvey's, the open evidence, the cursors, I mean, you see all of these AI native companies.
[00:24:46] SPEAKER_00: And they're going to be connected to AI models, and they're going to go after for the very first time in history, an industry that never was addressable.
[00:24:59] And that's the labor industry. And as digital labor, digital, called, called, agentic AI is going to supplement and augment the enterprise market.
[00:25:11] SPEAKER_00: So, for example, in video already today, we use 100% of our software engineers, 100% of our chip designers. Every single engineer today is augmented by cursor.
[00:25:19] SPEAKER_00: We use cursor, originally, inside our company. And so, we now have AI's for all of our engineers.
[00:25:25] Productivity gains, the work that we do, and so much better. You also see that there's a new industry showing up, it's called physical AI.
[00:25:34] SPEAKER_00: So, you have enterprise AI, you have physical AI, our augmenting labor. And so, for example, a robot taxi is essentially a digital chauffeur.
[00:25:49] And we're now going to have AI's that are going to be embodying, going to embed into anything that moves. And so, in the case of robot taxi, it's a steering wheel and wheels.
[00:26:01] SPEAKER_00: But you're going to pick in place arms, you're going to have one arm, two arms, you got three legs, all kinds of different embodiments.
[00:26:10] SPEAKER_00: And so, these two industries represent about $100 trillion of the world's economy. And for the very first time, we have technology that's going to be able to augment that.
[00:26:20] And so, that's the reason why people are so excited about the next wave of AI.
[00:26:26] So, let's talk for a moment on the previous wave, because you mentioned how AI has already been offering an ROI.
[00:26:33] And for the investor audience, I think the meta example is a great case study, because in Q4 2022, Apple basically removed attribution data from meta.
[00:26:44] SPEAKER_01: And you all saw hundreds of billions of dollars of market cap decline. And the meta team said, how are we going to fix this? They fixed that with AI powered by NVIDIA GPUs.
[00:26:55] SPEAKER_01: And they got their attribution back up to where it was, and that has recovered many hundreds of billions. It's over a trillion higher than it was at its low.
[00:27:04] SPEAKER_01: And that is all ROI that was powered really by your GPUs.
[00:27:08] SPEAKER_01: What meta was classically, not just meta, but one of the most complicated systems, software systems, is called a recommender system.
[00:27:19] SPEAKER_00: And there's a couple of basic technologies. One of them is called collaborative filtering, which is based on what I'm doing.
[00:27:27] SPEAKER_00: And looking at what everybody else is doing, if we have similar patterns, it would recommend maybe the same movie to me, the same next item in your grocery list, a book to me, a video to me, so on, so forth.
[00:27:40] SPEAKER_00: And then the other thing is called a content filtering, just based on who I am and my preferences. And based on what that book actually is, you might be able to recommend that book to me.
[00:27:53] SPEAKER_00: And so that recommender system is the largest software ecosystem in the world, and that ecosystem is moving very significantly, very quickly to AI. And so you're going to need a mountain of GPUs.
[00:28:05] SPEAKER_00: And those systems were made famous by the Netflix challenge a couple decades ago. Now Netflix, their recommendations are all powered by AI. And Amazon, as you said, when you go and purchase something, a significant number is by a recommendation system.
[00:28:20] SPEAKER_01: Moving search to AI. Moving search to AI. All of this is being powered now.
[00:28:24] SPEAKER_01: To talk to AI, right? Google shorts AI. I mean, without it. And now all of the personalized ads are going to AI. So just amount of AI is just incredible. And that has nothing to do. Notice I've just described a whole bunch of classical use cases.
[00:28:41] SPEAKER_01: Well, quantitative trading is going to move to AI. What used to be human engineered feature extraction is going to move towards AI.
[00:28:51] And I think that's actually an area that Citadel Security is pioneered for the past 20 some years. So that's the classical AI.
[00:28:59] SPEAKER_00: Citadel is a great customer. Thank you.
[00:29:02] So that is a classical example. And for the investor audience talking about AI or I, it's already there in the form of trillions of market cap. Let's talk about what's next for spend.
[00:29:14] SPEAKER_01: So 2025 estimates can be as high as $500 billion of AI investment in the ground. Where do we go from here? Does this become a multi trillion dollar a year investment category?
[00:29:28] Yes, the manufacturing the foundry part of AI, if you will, is the model makers. They're kind of like think of them like wafer makers. The applications of that. And one way of thinking about AI is the large language models.
[00:29:44] SPEAKER_00: That's the operating system, if you will, of the modern computer. And you build applications on top of these AI models. Not just one AI model, but a system of AI models.
[00:29:57] SPEAKER_00: And so applications have, you know, it's going to have a collection of different AIs that it connects together. And so the question is what's the application space on top?
[00:30:08] SPEAKER_00: The most sensible way of thinking about the application space on top, aside from all of the, whatever applications we have are going to be improved by AI that we've been talking about.
[00:30:18] SPEAKER_00: So the simple metaphor is just digital humans. And so a digital software engineer, right? AI coding. It's going to be a couple of trillion dollar market opportunity probably.
[00:30:33] SPEAKER_00: So AI digital nurses, AI countants, AI lawyers, AI, right? So there's AI marketers. So that we call all of that a gentick AI. And that technology is evolving very nicely. And so for the very first time technology is no longer just a tool used by accountants, tools used by software engineers.
[00:30:56] SPEAKER_00: And I wouldn't be surprised if you license them and you hire some. And so depending on the quality and depending on the deep expertise. And so future workforces in enterprise will be a combination of humans and digital humans. And some of them will be open AI based and some of it would be a Harvey based or open evidence or cursor or, you know,
[00:31:24] SPEAKER_00: rep, rep, let or, you know, loveable or some of it will be third party and some of them you'll home grow. And so we home grow a lot of our own a eyes because we have a lot of proprietary knowledge and data that we want to protect and we have we have skills in developing those a eyes over time.
[00:31:42] SPEAKER_00: And so we have more and more people be able to cultivate their own digital a eyes because it's just easier easier to do so. And so enterprise and agent to AI.
[00:31:54] SPEAKER_00: Obviously augmenting the labor force is trillions of dollars of opportunity. And what's unique about about AI also versus previous software is that AI needs to think.
[00:32:11] SPEAKER_00: And so you can't pre compile it, put it into a binary, download it and use it. It's got a process all the time. And the reason why it has the process is it has to take your context.
[00:32:22] SPEAKER_00: It has to think about what you wanted to do and then produce an output. And so it's thinking and thinking and generating. It needs a machine. It needs computers to do that. And that's the reason why AI factories exist. And so these AI factories will be in the cloud.
[00:32:38] SPEAKER_00: And so they will be beyond pram. They'll not be all over the world. And I, you know, it's part of the AI infrastructure, if you will. But there's going to be a whole lot of thinking to produce these we call tokens.
[00:32:50] SPEAKER_00: But basically intelligence. And so that's the cognitive AI, the digital workforce, if you will. And then the second one is robotics.
[00:33:00] You know, for the very first time. So let me give you a thought experiment. You know, why robotics is so close. As you know, as you know, you can now prompt an AI and it could generate, you know, prompt, gents and picking up a bottle, opening it and taking a sip.
[00:33:21] Okay, and it would generate the video of me right opening up a bottle, taking a sip. Well, if it can generate all that, why can't it maneuver a robot to do that?
[00:33:33] And so, you know, your thought experiment would suggest that, you know, that's probably very likely. Now, if you could design a digital chauffeur that could drive a car.
[00:33:45] Why can't you have a robot, a physical robot drive a car? And so, so if a physical robot, if you can embody a physical robot to even drive a car, why can't you embody a pick and, you know, pick and place arm or any type of robotics?
[00:34:00] SPEAKER_00: And so notice we have the ability to embody almost anything. We could pick up a pick up knives and forks and it becomes an extension of our body and somehow we articulated.
[00:34:12] SPEAKER_00: We could pick up a baseball bat and use it as an extension of our body. And so we embody these physical extensions. Future future AIs will be able to embody, you know, and manipulate a car, robotic arms, human robot, surgical robot, you know, so on and so forth.
[00:34:35] SPEAKER_00: So, I think these two markets are within reach of AI. And then lastly, if I just give you one example, you know, whenever you see the observation of one thing, the rest of is just engineering, right?
[00:34:50] SPEAKER_00: And so, and so we now we've now seen the evidence of one excellent thing, which is a, a digital and AI software coder, which is the reason why we use it so much.
[00:35:03] SPEAKER_00: If you can have an AI software coder, why can't you have that AI software coder also write software to be a marketing campaign, write software to, you know, help you solve any accounting, you know, whatever, whatever you want to do.
[00:35:19] SPEAKER_00: And so, so almost the existence of that says the rest of it is engineering. And then, and then we now have robot axes, you know, it's a embodied robot that controls the steering wheel and wheels.
[00:35:34] SPEAKER_00: And why, that exists, why can't you generalize that? And so the rest of it is just engineering. And so I think it's, that's a good way to reason from first principles.
[00:35:45] How likely it is we're going to be able to have this technology proliferate across industries and society. And then the next thing that you have to reason about is, okay, so how do you scale this out?
[00:35:55] SPEAKER_00: How do you deliver this intelligence to all of these different applications? Well, you need AI factories. And so.
[00:36:02] So let's talk a little more about robotics. You have an exceptional robotics team, one of your executives who runs robotics here today.
[00:36:11] SPEAKER_01: In a previous conversation, you shared some insight about how robotics might play out. You know, is it going to be a single humanoid project, is it going to be open source projects, how are those open source projects going to tie back?
[00:36:23] SPEAKER_01: How do you think robotics will actually manifest in the physical world and on what timeline?
[00:36:30] Well, robotaxies are here now. And their, their ability to generalize from city to city to city is really, really getting fast. And, and the reason for that is because the same fundamental technology, we went through the same journey.
[00:36:45] SPEAKER_00: And, and for all the quantitative trading, the algorithmic trading people in the room, you went from human engineered features, machine learning, to using more, more deep learning and, you know, embedding certain modalities and multi-modality models to, to now, largely end to end.
[00:37:11] And the reason why, the reason why, and, and its multi-modal. In this journey, we, we became more, more generalizable. And, and the, the AI model that you use for self-driving car, and the AI model that you use for a human robot is highly similar.
[00:37:32] SPEAKER_01: And, it's just in two different embodiments. And, the reason why I know that for sure is because I can drive a car. And, I can manipulate my body. It's the same intelligence. And, so, and I could pick up fork and knife and somehow, you know, pretend like I'm a surgeon, you know, doing surgery on a, on a steak, you know.
[00:37:56] And, so, so you could notice it's the same AI in different embodiments. And, so that's, that's where AI is going. Robotics is going towards a general, more, more generalizable, AIs that are, that are multi embodiment. It's multi-modality, it's multi embodiment. And, in order, in order to create this future, you need three things. You need the AI factory I was talking about, where you have to train the models.
[00:38:23] And, you need a place where the AI could learn how to be an AI without, without having to come into the world right away. So, you could try trillions of different iterations inside of virtual world.
[00:38:36] SPEAKER_00: Well, that virtual world resembles a video game. And, so, the AI is basically playing a game inside a virtual world, like a video game character, and it obeys the laws of physics.
[00:38:49] And, and when it's done, learning how to be a great video game player, because the sim to real gap is extremely low, because the simulator is really, really good. We call it omniverse. That omniverse computer, then the robot can come out of that virtual world. And, this world become one more version of the virtual world is played in. And, it comes into the physical world.
[00:39:16] SPEAKER_00: It comes into the physical world. It needs a computer as well. So, you need three computers. You need the AI computer, training computer. You need the simulation, the lab, the virtual world computer. And, you need a computer where the robot actually operates the brain. And, so, Nvidia offers all three of those computers. And, we work with just about every robotics company, self-driving car company, you know, robotics of different embodiments. And, this is likely going to be one of the largest markets of all.
[00:39:43] SPEAKER_00: So, Nvidia touches just about everything in technology now. And, as you've said in the past, you start with zero billion dollar markets and help to turn them into trillion dollar markets. Robotics is one of the next frontier markets. Are there any other next frontier markets that you're particularly excited about? You mentioned healthcare a moment ago. Is that one you're passionate about? Are there others that the investors in the room should be on the lookout for?
[00:40:07] Well, the technology needed for healthcare is really complicated. And, we're making fast progress. If you can understand the meaning of words, sequences of characters, you might understand the meaning of structures like the virtual world.
[00:40:27] And, you know, when you look at the reason why we're able to generate video is because we understand the virtual world to generate an image, a representation of the world. And, so, if you can generate video, it must be because you understand the world.
[00:40:41] SPEAKER_00: If you can generate, if you can understand worlds, is it possible that you understand proteins and chemicals that have structure? And, the answer is yes. And, so, we're increasingly getting closer to closer to understand the meaning of proteins, alpha-fold and others.
[00:40:55] SPEAKER_00: We're able to understand the meaning of cells. And, we recently did a partnership with ARC. And, Evo 2 comes very, is one of the first examples of a large language model that a foundation model for cell representation.
[00:41:11] SPEAKER_00: So, you can now talk to it and say, I want you to generate other cells of these properties. And, or, you could talk to a cell.
[00:41:23] SPEAKER_00: And, what are your properties? And, what can you bind to, and what can you, your metabolism, what can you activate with? And, so, you could talk to a cell like you could talk to chatbot.
[00:41:39] SPEAKER_00: And, so, understanding the meaning of proteins, you know, anyways, there's a lot of progress there.
[00:41:47] I mean, the list goes on. I mean, I'm excited about the work that we're doing to bring AI into telecommunications. 5G and 6G will be revolutionized by AI.
[00:42:03] SPEAKER_01: I'm excited about the collaboration we have with quantum computers so that we can pull in the quantum computer schedule by about a decade by creating quantum GPU hybrid computing systems.
[00:42:18] SPEAKER_00: Where we do the error correction, we control the quantum computer, we do the post processing. And, so, so, we have a new, new architecture called CUDA-Q, which extends CUDA to quantum. And, that's getting incredible adoption.
[00:42:32] SPEAKER_00: And, so, there's a whole bunch of problems we can now solve that we're hard to solve before.
[00:42:38] Let's talk a little bit about sovereign AI. We just had Mario Draghi on the stage, he was talking about the importance of new investments in technology for the European Union, including obviously AI at a large scale.
[00:42:51] SPEAKER_01: This revolution is materially different and that governments are highly involved, both in potentially regulating, but also in purchasing AI factories.
[00:43:02] SPEAKER_01: Can you tell us, what do you think is the way forward? Both for sovereign AI, how come countries have their own AI systems, and also for import exports, how we, as the United States, should be interfacing with the rest of the world with AI?
[00:43:19] Well, no country can afford to outsource all of their, their nation's data so that, and import your own intelligence back to yourself.
[00:43:31] SPEAKER_00: And, I just think, on first principle, this is not sensible. And, however, nobody needs to only build everything themselves. You could buy, you could import, but you shouldn't give up on the production of your own national,
[00:43:47] SPEAKER_00: intellectual intelligence. And, and so, I, I think the, today the technology is rather hard, but it's getting easier and easier very, very quickly. And there's an enormous amount of open source capability.
[00:44:02] SPEAKER_00: And so, so I would, I wouldn't give up on, on building your own sovereign AI. I wouldn't give up on, on taking the data that you have and creating your own national intelligence from it.
[00:44:14] SPEAKER_00: And, and, and now countries all over the world are, are, are doing so. And so I think sovereign AI is likely, every country is likely to, import some, buy some, and also build some.
[00:44:27] SPEAKER_00: And, and, there's a lot of capabilities to do in that. And, and so we're, we're seeing just a lot of momentum around sovereign AI. The UK is doing it. You know, I was, I was in France, we support a company called Mistral in UK.
[00:44:41] SPEAKER_00: There's a company called Nscale. There's a company called Nebius. And, and, and Italy, there's, there's several companies in Spain, there's several companies, Germany, there's several companies.
[00:44:53] SPEAKER_00: And, and so there's companies all over the world in, in, in, in Japan, there's companies in, you know, in Korea, there's companies. And so they're sovereign AI is cropping up all over the world.
[00:45:03] SPEAKER_00: So, one country that's come up a lot is China. What's the right thing for the United States in terms of exports to China, of AI factories?
[00:45:13] SPEAKER_01: Well, AI is a new technology and we have to think about before we, you know, we have to be thoughtful about about ultimately how to regulate it.
[00:45:23] United States, of course, wants, wants to win the AI race. And I think, I think, the policymakers all want to do the right thing and they want America to, to win.
[00:45:34] SPEAKER_00: However, it's important to be mindful that what is, what harms China could oftentimes also harm America and even worse.
[00:45:48] SPEAKER_00: And so, before, before we leap towards policies that are hurtful to other people, take us that back and, and, and maybe reflect on what are the policies that are helpful to America.
[00:46:01] SPEAKER_00: And, and it, it probably is the case that you have to go back to first principles again. In the case of AI, what's most important about AI and any computing, any software industry, the developers are vitally important, as you know.
[00:46:19] SPEAKER_00: And so, winning developers is what creates the future platform. And we want the world to be built on American technology, you know, and, and, and, and videos of proud American company and, and we want, we want of course, we, we hope that we could create American technology that the world's built upon.
[00:46:37] Well, a lot of the AI researchers are in China. You know, China has about 50% of the world's AI researchers, incredible schools, incredible focus in AI, lots of passion around AI.
[00:46:49] SPEAKER_00: And I think it's a mistake to not have those researchers build AI on American technology. On first principles, I think that's a mistake.
[00:47:00] And so the question is, how, how do you balance winning, staying ahead, on the other hand, ensuring that the world builds on American tech stack. That's the balance.
[00:47:13] And in order to balance, you have to have nuance. And it's probably not, you know, all or nothing. And so nuance, a nuance strategy that, that, that changes that, that, you know, is, is,
[00:47:28] SPEAKER_00: is changing over time has a, you know, that allows the United States to stay ahead while we continue to win researchers around the world. It's probably the right balance. And that, that's what I would advocate.
[00:47:43] SPEAKER_00: At the moment, we are 100% out of China. And so China is 0% of, we went from 95% market share to 0%.
[00:47:55] So I can't imagine any policy maker thinking that that's a good idea. That whatever policy we implemented caused, one, it caused America to lose one of the largest markets in the world to 0%.
[00:48:09] But anyhow, in all of our forecast, if there are any shareholders out there, all of our forecast, we're, we're assuming zero, zero for China.
[00:48:17] SPEAKER_00: If anything happens in China, which I hope it will, it'll be a bonus. But it's a large market, China is the second largest computer market in the world.
[00:48:26] It is a vibrant ecosystem. I think it's a mistake for United States to not participate. And, and, and so hopefully we'll, we'll continue to, to explain and inform and, and, hold out hope for, for a change in policy.
[00:48:44] Jensen, you were at our offices recently for an AI conference we were holding. And you had some really brilliant insights into the future of AI security and the importance of it.
[00:48:56] SPEAKER_01: It's somewhat related. There are nation state actors that might interfere with AI. There are individual users that might use AI incorrectly. What do you think the future of AI security looks like?
[00:49:10] SPEAKER_01: Well, AI security in the future is going to look a little bit like cyber security. It's going to, it's going to require that, that we all work as a community. You probably know this.
[00:49:23] SPEAKER_00: All of your cyber security, your, your chief security officers. We're all one large community. And when, when somebody finds a, you know, some intrusion, we share with everybody.
[00:49:38] SPEAKER_00: Whenever we find a vulnerability, we share with everybody. And so, it's very likely that the future of AI security will be like cyber security.
[00:49:48] Second, if the marginal cost of intelligence, the marginal cost of AI goes to zero. If the marginal cost of AI goes to zero, then why would the marginal cost of security focused AI go also to zero?
[00:50:03] SPEAKER_00: And so, that's, that's very clear. It's very likely that every AI will be surrounded by a whole bunch of cyber security AI, so watching it.
[00:50:11] SPEAKER_01: And so, we're going to have lots and lots of AI protectors. Thousands of them. Millions of them. Inside the company, outside the company. And so, that's, that's kind of the future of the idea that, that an AI has to itself be good is good.
[00:50:31] SPEAKER_00: But, I don't think we should rely on it. And so, just like the idea that a piece of software should be properly functioning, we, we like. But the idea that, you could have bugs or, you know, it could be a virus or whatever it is, it could be an intruder, we have to assume.
[00:50:48] SPEAKER_00: And so, we're going to, we're going to make AI advance as safely as possible.
[00:50:54] And the world is also going to surround AI with a lot of security AI.
[00:50:57] SPEAKER_00: You shared that really the dynamics of the physical world are decoupled in this digital world, where in the physical world you might have one security person to 100 normal people.
[00:51:09] SPEAKER_01: It could be inverted in an AI world.
[00:51:12] SPEAKER_01: Yeah. You also shared this idea that I found my.
[00:51:15] For example, like cyber security. Yeah. Yeah. We have a lot more cyber security agents than we have people working in the company on cyber security.
[00:51:23] SPEAKER_00: You also shared this idea that in the future we're not just going to have rendered computation, but everything is going to be generated.
[00:51:32] Can you unpack what that prediction is and what that means for Nvidia?
[00:51:37] SPEAKER_01: Well, the, the, the best example of that, a couple of examples. Let's say, perplexity.
[00:51:44] Everything that you see on perplexity when you ask a question is completely generated. 100%.
[00:51:49] SPEAKER_00: 100% of everything you see is generated. And yet in the past, before perplexity, you would type in something and it would give you a list and you would go and click on it.
[00:52:00] SPEAKER_00: And all of that content was written by somebody or generated by somebody a priori.
[00:52:09] So search is storage based computing. It's retrieval based computing. It's retrieving information for you to consume yourself.
[00:52:19] SPEAKER_00: Perplexity or AI is generating. It goes and studies. It goes and reads all the content and it generates a for you.
[00:52:27] SPEAKER_00: Okay, so perplexity is a great example of the classical computer approach. We go retrieve a file and read it to a generative approach perplexity, which is AI based.
[00:52:40] Another one, which is, I look at the videos that we see today. You know, Sora is, of course, nano banana, of course.
[00:52:49] SPEAKER_00: You know, all of those pixels are generated. It's conditioned and prompted by you. You know, you might give it an initial seed of something and say, you know, I would like you to generate a video of Constantine and Jensen having a fireside chat.
[00:53:05] SPEAKER_00: And then you would prompt it and say, this fireside chat is going to talk, they're going to talk about, you know, crazy stuff for those online. This is real.
[00:53:14] SPEAKER_00: And then, and then Sora would generate it. And so every single pixel, every single motion, every single word is generated. So, so the way of computation in the future will likely be generative.
[00:53:29] SPEAKER_00: And then let me just give you one final idea. 100% of what you and I just went through is generated.
[00:53:40] Every question you asked me, I didn't run back to my office and retrieve something and bring it to you. Is this what you meant Constantine? And then you read it aloud for everybody to hear.
[00:53:50] SPEAKER_01: That's yesterday's computer. Today's computer is we just, we're just interacting. And so we are generating everything in real time based on the context that is happening right here, based on the audience, based on what's happening around the world.
[00:54:05] SPEAKER_00: And so we're generating everything in real time. That's the future computer. You know, your future computer is, is a CEO in front of you. It's an artist. It's a, you know, it's a poet. It's a storyteller. And you collaborate with it to create unique content for yourself. And so the future of computation is 100% generative. And behind it, you need an AI factory, which is the reason why I'm 100% certain we're at the beginning of this journey.
[00:54:34] SPEAKER_00: And, and, you know, we're a few hundred billion dollars, a few extremely small. We're only a few hundred billion dollars of infrastructure built.
[00:54:46] SPEAKER_00: For what likely will be trillions of dollars of infrastructure built each year. And so that's the easiest way to think through it.
[00:54:56] And that computing paradigm is so much more like the human mind.
[00:55:00] SPEAKER_01: Yeah, it's thinking. Yeah, it's so.
[00:55:03] SPEAKER_00: If you're up for it, how about we generate a few lightning round answers?
[00:55:07] SPEAKER_01: Okay. Okay.
[00:55:09] SPEAKER_01: Just in the last few minutes together. I'm sure fried chicken is the answer.
[00:55:14] I don't know what the question is for that one. So let's jump in. What's one KPI that Wall Street underweights?
[00:55:27] In the future of AI factories, your throughput per unit energy governs the revenues of your customers.
[00:55:35] It's not just about selecting a better chip. It's about deciding what your revenues are going to be.
[00:55:41] And in fact, if you go back and look at all the CSPs, the ones that chose right, solid revenue growth.
[00:55:52] And the ones that slow down subsequently chose right.
[00:55:57] And so you could see it playing out and people are starting to understand it.
[00:56:03] SPEAKER_00: Your throughput, token, it's called tokens, token generation rate per unit energy of your factory is your revenues.
[00:56:15] The most underrated piece of NVIDIA's platform.
[00:56:22] Most people talk about CUDA and CUDA is very important.
[00:56:26] SPEAKER_00: But there's a suite of libraries that sit on top of CUDA. And I mentioned one earlier today.
[00:56:31] SPEAKER_00: It's called CUDA and N. It is probably one of the most important libraries ever created in history of humanity.
[00:56:38] SPEAKER_00: The previous one was called SQL as QL. And this one, CUDA and N. There's a few others.
[00:56:46] SPEAKER_00: CUDA, CUDA, which is going to be used for semiconductor manufacturing, lithography.
[00:56:52] SPEAKER_00: We have about 350 of these libraries. And these libraries, that is NVIDIA's treasure trove.
[00:56:59] What's one technology that you think is wildly undervalued. And one that you think might be overvalued.
[00:57:06] Undervalued. Undervalued. Undervalued.
[00:57:14] SPEAKER_00: Wow. I think that the virtual world for physical AI to learn to be a good physical AI, we call it omniverse, is hard to understand.
[00:57:34] SPEAKER_00: But it is deeply undervalued. And not because people use it. They just don't know they need it yet.
[00:57:43] SPEAKER_00: But now omniverse is sweeping across the robotics industry. And everybody now gets it.
[00:57:50] SPEAKER_00: Once you start building robots, you'll start to realize how visionary it was that we started working on omniverse almost a decade ago.
[00:57:57] SPEAKER_00: And so omniverse is really important.
[00:58:03] What's the book that most shaped your business and leadership philosophy?
[00:58:11] SPEAKER_00: One of my favorite books was your everybody's first calculus book. When you realize a math was in motion.
[00:58:22] SPEAKER_00: That was a good book. All of Clay's books, Christian sense books, and he's past, but good friend. All of his books were great.
[00:58:32] Al Ray's positioning book, really good book, if you haven't had a chance. Of course, you know, Sapiens is always good.
[00:58:41] SPEAKER_00: But those are good ones. You know, Jeffries book on crossing a chasm, that's a good book. But all of Christian sense books, read them all.
[00:58:50] SPEAKER_00: Favorite comfort food? There you go, fried chicken. There you go. Okay, we got it in. All right.
[00:58:59] SPEAKER_01: And then last question, if you were a CIO in the audience with $10 billion to allocate toward AI in the coming years, what would you invest it into?
[00:59:10] SPEAKER_01: I would write away experiment with building your own AI. I mean, I just, you know, the fact of the matter is we take pride in unboarding employees.
[00:59:23] SPEAKER_00: And how the method by which you do so, the culture by which you bring them into, the philosophies of your company, the operating methods, the practices, that makes your company what it is.
[00:59:35] SPEAKER_00: The collection of data and knowledge that you've embodied over time, that you make accessible to them. And so that is what defines a company in the past.
[00:59:46] A company of the future includes that of course, but you need to do that for AI. You need to onboard digital. You need to onboard AI employees. There's methodology for onboarding AI employees for we call them fine tuning, but basically teaching them, you know, the, the, the, the culture of the knowledge of the skills of evaluation methods.
[01:00:10] SPEAKER_00: And so the entire flywheel of your agentic and employee is something that you need to go and learn how to do. I tell my CIO, our company's IT department, they're going to be the HR department of agentic AI in the future.
[01:00:23] SPEAKER_00: They're going to be the HR department of digital employees of the future. And those digital employees are going to work with our, of course, biological ones. And that's going to be the shape of our company in the future. And so if you get a chance to do that, I would do that right away.
[01:00:39] Thank you, Jensen. Well, we heard an incredible story. Really, the story of NVIDIA is one of exceptional generalization from an accelerated graphics processor to the technology that powers all of AI in the world today from a component and the world's first GPU to all of the components in a platform in the world's AI factory.
[01:01:00] SPEAKER_01: We talked about how services are the baseline for this new revolution and how robotics are in all of our future. We covered foreign policy. We even touched on fried chicken. You did it all, Jensen. Thank you so much.
[01:01:14] SPEAKER_01: Thank you.
[01:01:15] SPEAKER_00: Good job.
[01:01:16] Thank you.