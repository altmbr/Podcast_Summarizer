[00:00:00] 2025 has been another remarkable year in AI. This week on No Priors, we're sharing our
[00:00:09] favorite moments from the podcast from the year so far. We've talked to visionary leaders
[00:00:14] at RV, opening eye, glean, a bridge, and more. We also talked to legends of science, like
[00:00:18] Dr. Fei-Fei Li and New Barra Fan. But first, let's start with the moment that captures
[00:00:22] the magic of leaning into new capabilities at the right time. RV CEO Winston Weinberg discovered
[00:00:27] an extraordinary opportunity hidden in plain sight. Gabe and I actually had met a couple years
[00:00:33] before, and I definitely didn't know anything about the startup world and didn't have a plan
[00:00:37] of doing a startup. And what had happened was he showed me GP3, which at the time was public,
[00:00:44] and I was, first of all, just incredibly surprised that no one was talking about GP3 and no one
[00:00:49] was using it in any way, shape or form. And he showed me that, and I showed him kind of my legal
[00:00:55] workflows. And we started the kind of aha moment was we went on our slash legal advice,
[00:01:03] which is basically a subreddit where people ask a bunch of legal questions. And almost every
[00:01:09] single answer is, so who do I sue? Almost every single time. And we took about 100 landlord
[00:01:15] tenant questions. And we came up with kind of some chain of thought problems. And this is before,
[00:01:20] you know, anyone was talking about chain of thought or anything like that. And we applied it to
[00:01:24] those landlord tenant questions and we gave it to three landlord tenant attorneys. And we just
[00:01:29] said, nothing about AI, we just said, here's a question that a potential client asked. And here is
[00:01:35] an answer. Would you send this answer without any edits to that client? Would you be fine with that?
[00:01:40] You know, is that ethical? Is it a good enough answer to send? And 86 out of 100 was yes.
[00:01:47] And actually, we called email the general counsel of open AI. And we sent him these results.
[00:01:53] And his response basically was, oh, I had no idea the models were this good at legal.
[00:01:58] And we met with the the C suite of open AI a couple weeks after now from legal reasoning to
[00:02:03] spatial intelligence, the legendary doctor, Fei Fei Lee opened our eyes to an entirely different
[00:02:08] dimension of AI capability. I think from a neural and cognitive science point of view that
[00:02:15] spatial intelligence is a really hard problem that evolution has to solve for animals.
[00:02:21] And what's really interesting is I think animals have solved it to an extent but not fully
[00:02:27] solved it. It's one of the hardest problem because what is the problem animal has to solve?
[00:02:34] Animals have to evolve the capability of collecting lights in something which we call eyes mostly.
[00:02:45] And then with that collection of eyes, it has to reconstruct a 3D world in their mind somehow,
[00:02:54] so that they can navigate and they can do things. And of course, they can interact for humans
[00:03:00] where the most capable animal in terms of manipulation, we can do a lot of things.
[00:03:06] And all this spatial intelligence, to me, that's just rooted in our intelligence.
[00:03:14] What is interesting is it's not a fully solved problem, even animals. We, for example,
[00:03:22] for humans, right? And if I ask you to close your eyes right now and draw out or build a 3D model
[00:03:31] of the environment around you, it's not that easy. We don't have that much capability to generate
[00:03:39] extremely complicated 3D model till we get trained. There are some of us, whether they're
[00:03:45] architects or designers or just people with a lot of training and a lot of talent. And that's
[00:03:54] that's a hard thing to do. And imagine you do it at your fingertip much more easily and allow
[00:04:01] much more fluid interactivity and editability. That would just be a whole different world for
[00:04:11] people, no pun intended. Data is the beast feeding the AI train. And thus,
[00:04:15] Merkwar CEO, Brendan Fudy, is working with major AI labs on how to build what's next. He gives
[00:04:21] a clear prediction about what's coming for the workforce. I think displacement in a lot of roles
[00:04:28] is going to happen very quickly and it's going to be very painful and a large political problem.
[00:04:37] Like I think we're going to have a big populist movement around this and all the displacement
[00:04:42] that's going to happen. But one of the most important problems in the economy is figuring out
[00:04:47] how to respond to that, right? Like how do we figure out what everyone who's working in customer
[00:04:53] support or recruiting should be doing in a few years? How do we reallocate wealth once we have,
[00:05:00] once we approach super intelligence, especially if the value and gains of that are more of the
[00:05:07] power law distribution. And so I spend a lot of time thinking about like how that's going to play
[00:05:12] out. And I think it's really at the heart of. What do you think happens eventually?
[00:05:16] X percent of people get displaced from my color work. What do you think they do?
[00:05:20] I think there's going to be a lot more of the physical world. I think that there's also going
[00:05:26] to be a lot that of like niche. What does the physical world mean? Well, it could be everything
[00:05:33] ranging from people that are creating robotics data to people that are waiters at restaurants or
[00:05:40] are just like therapists because people want like human interaction. Like whatever that looks
[00:05:47] like, I think all of I think that automation in the physical world is going to happen a lot slower
[00:05:55] than what's happening in the digital world just because of so many of the like self-reinforcing
[00:06:02] gains and a lot of yeah self-improvement that can that can happen in in the virtual world
[00:06:09] but not the school. Which brings us to one of the biggest questions of our time. How do we navigate
[00:06:13] the geopolitical implications of superintelligence? Dan Hendricks, the director of the center for
[00:06:18] AI safety has an answer. Let's think of what happened in nuclear strategy. Basically a lot of
[00:06:25] a lot of states deterred each other from doing a first strike because they could then retaliate.
[00:06:30] So they had a shared vulnerability. So they were we're not going to do this really aggressive action
[00:06:35] of trying to make a bid to wipe you out because that will end up causing us to be damaged. And we
[00:06:41] have a somewhat similar situation later on when AI is more salient when it is viewed as pivotal
[00:06:48] to the future of a nation. When people are on the verge of making a superintelligence more when
[00:06:54] they can say automate pretty much all AI research, I think states would try to deter each other
[00:07:00] from trying to leverage that to develop it into something like a super weapon that would allow
[00:07:07] other countries to be crushed or use those AIs to do some really rapid automated AI research
[00:07:14] and development loop that could have a bootstrap from its current levels to something that's a super
[00:07:20] intelligent, vastly more capable than any other system out there. I think that later on it becomes
[00:07:25] so destabilizing that China just says we're going to do something preemptive like do a cyber attack
[00:07:31] on your data center. And the US might do that to China. In Russia, coming out of Ukraine, we'll
[00:07:38] reassess the situation, get situationally where I think, what's going on with the US and China?
[00:07:44] Oh my goodness, they're so ahead on AI. AI is looking like a big deal. Let's say it's later in
[00:07:49] the year when a big chunk of software engineering is starting to be impacted by AI. Oh wow,
[00:07:54] this is looking pretty relevant. Hey, if you try and use this to crush us, we will prevent that by
[00:07:59] doing a cyber attack on you. And we will keep tabs on your projects because it's pretty easy for
[00:08:04] them to do that espionage. New bar of fan has been thinking about how biotech gets built and how to
[00:08:08] change the game for three decades. His breakthroughs have impacted global health. He's the founder and
[00:08:13] CEO of flagship pioneering and the co-founder of Moderna. He wants to make entrepreneurship a scientific
[00:08:18] effort, not a random one. Anything's AI can help. The motivation for flagship stems from what I was
[00:08:26] doing before, which was that I started a company in 1987 when 24-year-old immigrants didn't
[00:08:32] start companies in this country. But instead, it was kind of like former Merck senior executives
[00:08:37] or IBM senior executives were the only ones who were entrusted with the massive amounts of
[00:08:42] venture capital, namely two, three million dollars per round used to go into venture capital.
[00:08:47] So this was very early days. And I had the kind of chance opportunity to start a company right out
[00:08:53] of my graduate school and ended up raising quite a bit of venture money and eventually kind of
[00:09:00] went down a path of entrepreneurship. Along the way, one of the things that interested me was
[00:09:05] why it is that kind of the entrepreneurial process was supposed to be random improvisational,
[00:09:13] kind of idiosyncratic, almost emotional, gamey. All of those things I kind of thought was a bit of
[00:09:20] put off when it comes to actually doing things in a serious professional way. And I kind of used to go
[00:09:27] around in the very early 90s saying why isn't entrepreneurship a profession? And if it was going to
[00:09:32] be a profession, how could it be a profession? What do you mean by gamey? Because it's like supposed
[00:09:37] to fail most of the time and once in a while you win and then you celebrate the win. And what I mean
[00:09:42] is like it's random. But not only random, but there's like winners and losers and keeping score. I
[00:09:48] don't know, it's maybe the wrong word, but I just mean like people even caught gamification in the
[00:09:54] software space. There is a version of this like I don't mind being playful because if you're overly
[00:10:00] serious, sometimes you miss things, but it can't just all be played. We take hard earned money.
[00:10:06] We deploy it to do things that are damn near impossible. Once in a while we reduce them to practice
[00:10:11] so they become not only possible, but valuable. And yet people treat it like, oh well, you know,
[00:10:16] it didn't work. There's 20 different things we tried. One of them worked. That I don't know,
[00:10:20] as an engineer by background as a scientist, I just thought that what we do, especially listening
[00:10:26] in healthcare, especially in climate, especially in kind of like agriculture, food security,
[00:10:31] you can't think of this as, you know, like shots on goal and this night. You've got to kind of say,
[00:10:36] hey, we can get better at this. Reasoning is the biggest paradigm shift in AI architecture
[00:10:40] since the transformer. Brandon McKenzie and Eric Mitchell from OpenAI explained a crucial insight
[00:10:45] about racing models. I can give maybe very concrete cases for like the visual
[00:10:50] reasoning side of things. There's a lot of cases where in back to also the model being able to estimate
[00:10:56] its own uncertainty, you'll give it some kind of question about an image and the model will
[00:11:01] very transparently tell you what it should have thought like. I don't know. I can't really see
[00:11:04] the thing you're talking about very well or like it almost knows like that its vision is not very good.
[00:11:09] And what it's kind of magical is like when you give it access to a tool, it's like, okay, well,
[00:11:14] I got to figure something out. Let's see if I can manipulate the image or crop around here
[00:11:19] or something like this. And what that means is that it's much more productive use of tokens
[00:11:25] as it's doing that. And so your test time scaling slope goes from something like this to something
[00:11:29] much deeper. And we've seen exactly that like the test time scaling slopes for without tool use
[00:11:36] and with tool use for visual reasoning specifically are very noticeably different.
[00:11:40] Yeah, I also say like for like writing code for something like
[00:11:45] there are a lot of things that an LLM could try to figure out on its own but would require a lot of
[00:11:53] attempts and self verification that you could write a very simple program to do in like a
[00:11:59] verifiable and much faster way. So you know, I do some research on this company and like use this
[00:12:08] type of you know, valuation model to tell me like, you know, what the valuation should be like
[00:12:14] you could have the model like try to crank through that and like fit those coefficients or whatever
[00:12:20] in its context or you could literally just have it like write the code to just do it the right way
[00:12:26] and just know what the actual answer is. And so yeah, I think like part of this is you can just
[00:12:32] allocate compute a lot more efficiently because you can defer stuff that the model doesn't have
[00:12:37] comparative advantage to doing to a tool that is like really well suited doing anything.
[00:12:41] Sometimes the most profound moments in AI development aren't the grand theoretical breakthroughs.
[00:12:45] They're based on taste, data generation and grinding work. The visceral experience of watching
[00:12:50] something you hoped would work actually come to life. Esa Falford from opening my captures that
[00:12:54] moment perfectly. Here she's describing the training that went into deep research.
[00:12:58] It really was one of those things where we thought that you know, training on browsing tasks
[00:13:03] would work. You know, felt like we had good conviction in it, but actually the first time you
[00:13:09] train a model on a new dataset using this algorithm and seeing it actually working and playing with
[00:13:14] the model was pretty incredible even though we thought it would work. So honestly just that it worked
[00:13:23] so well was pretty surprising. Even though we thought it would if that makes sense.
[00:13:27] Yeah. Yeah. It's the visceral experience of like, oh, the path is paved.
[00:13:31] But strawberries or whatever. Exactly. But then sometimes some of the things that it fails out
[00:13:35] also surprising, like sometimes it will make a mistake where it will do such smart things and then
[00:13:40] make a mistake where I'm just thinking, why are you doing that? Stop. So I think there's
[00:13:45] definitely a lot of room for improvement, but yeah, we've been impressed with the model so far.
[00:13:49] One of the biggest surprises of AI and a core principle for us here at conviction is how it can
[00:13:55] make bad markets suddenly good ones. The right technology can meet the right moment in unexpected
[00:14:00] ways. Arvin Jane built Glean and what everyone said was a graveyard market enterprise search.
[00:14:05] It was like a graveyard like, you know, of all these companies that tried to solve the problem and
[00:14:09] it didn't. Part of it was just that I think search is a hard problem with an enterprise like
[00:14:14] even getting access to all the data that you want to search. It was such a big problem. In the
[00:14:19] pre-sass world, there was no way to sort of go into those data centers, figure out where the
[00:14:24] servers were, where the source systems were, try to connect with information in them. It was a big
[00:14:29] challenge. The SaaS actually solved that issue. So like search products like most of them, most of
[00:14:34] the companies started in the pre-sass world. They failed because you could just put built a turn to
[00:14:38] product, but SaaS actually allowed you to actually build something, you know, which is my insight
[00:14:44] was that like look, you know, the enterprise world has changed. We have these SaaS systems now
[00:14:49] and SaaS systems don't have versions like everybody, all customers have the same version,
[00:14:54] you know, they are open, they're interoperable. You can actually hit them with APIs and get all the
[00:14:59] content. I felt that the biggest problem was actually solved, which was that I could actually
[00:15:03] easily go and bring all the enterprise information and data in one place and build this unified
[00:15:09] search system on top. So that was actually big unlock. And by the way, the origins of Glean is,
[00:15:13] so at rubric, you know, we had this problem, like, you know, we grew fast. We had a lot of
[00:15:17] information across 300 different SaaS systems and nobody could find anything in the company.
[00:15:21] And people were complaining about in our calls surveys and I was, you know, I always run IT
[00:15:26] in my startups and so there's a complaint that came to me like I had to solve it. So I tried to buy
[00:15:31] a search product and I realized there's nothing to buy. I mean, that's that's really the origins of
[00:15:35] how Glean got started as a company. And so that was like, you know, one big issue, like, you know,
[00:15:40] so SaaS made it easy for to actually connect, you know, your enterprise data and knowledge to
[00:15:45] a search system. So that actually made it possible for us to for the very first time build a turnkey
[00:15:50] product. But there are a lot of other advances as well. You know, one is, you know, like, look,
[00:15:54] you know, businesses have so much information and data. One interesting, you know,
[00:15:57] facts are one of our largest customers. They have more than one billion documents inside their
[00:16:02] company. Now here, there's, you know, when a lot and I know when we were working on search
[00:16:06] at Google, you know, in 2004, the entire internet was actually one billion documents. You know,
[00:16:11] there's a massive explosion of content like inside businesses. So you have to build scalable
[00:16:16] systems and you couldn't build like a system like that before in the pre-cloud era.
[00:16:21] Perhaps no story captures the human impact of this AI moment and its potential,
[00:16:25] better than what's happening in healthcare. Here's Shiv Rao, CEO and founder of a bridge.
[00:16:30] It's pretty heroic in general for a doctor to give you feedback like, hey, this sucked and you
[00:16:34] got to do better. Like, you didn't recognize the way I said this medication or I'm a gastroenterologist
[00:16:40] and I would never, you know, sequence my problems in my assessment and plan section of my note
[00:16:45] this way. It doesn't serve me well and makes me look like terrible as a doctor or whatever. We
[00:16:49] get that feedback. We love it. It's oxygen. But then we also get the feedback that's like, hey,
[00:16:53] this is amazing and I'm not going to retire anymore. And I've got like years, decades left in my
[00:16:58] career now thanks to this technology. But in this channel love stories, all of that feedback,
[00:17:03] a positive feedback, we just get it like programmatically funneled. So any one of our people inside
[00:17:08] of the company can always go into that channel and it's like purpose, you know, it's like fulfillment
[00:17:13] immediately. Like you immediately understand why we're all working so hard and why it makes sense
[00:17:18] because like being on this very telephone pole like journey these last couple years is obviously
[00:17:24] like it's news for so many of us and we're all kind of building new muscles, but it's a lot of
[00:17:29] pressure. But this is my favorite bit of feedback. So this love story comes from a doctor at Tanner
[00:17:34] Health, which is a rural health system. And she wrote to us, she wrote, I was sitting at dinner
[00:17:39] last week and my son asked me, mommy, why aren't you working right now? I literally took my phone
[00:17:44] out and explained to him that it bridges a new tool that lets mommy come home early and eat dinner
[00:17:49] with her family. I started to tear up and looked over at my husband who then said, mommy's going to
[00:17:54] be able to eat dinner with us every night now. And we get feedback like that every day, you know,
[00:17:59] and so like there's dopamine hits, you know, in hypergrowth. And like those are awesome. But I
[00:18:05] think that they get us through like sprints. But I think it's the oxytocin hits like this. It's
[00:18:10] the purpose. It's the fulfillment. It's like that's, I think, what I think we're really after
[00:18:14] in this company. And so like everybody's mission driven out out there, but I think this mission,
[00:18:19] like it hits me at least a little bit different. These conversations remind us that we're
[00:18:23] living through a hinge moment in history. Stay tuned as we have more conversations with the
[00:18:28] builders and thinkers leading the way for the rest of the year. If you like what we're doing, leave
[00:18:31] us a review on Apple podcasts or Spotify, comment on YouTube or let us know who we should have with
[00:18:36] guest. Thanks for listening.