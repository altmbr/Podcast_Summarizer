# Sequoia Partner, David Cahn on Who Wins in AI, Defence & The New $0â€“$100M Playbook

**Podcast:** 20VC
**Date:** 2025-10-27
**Video ID:** aGoUu5VLvLY
**Video URL:** https://www.youtube.com/watch?v=aGoUu5VLvLY

---

[00:00:00] I do think we're in an eye bubble. You can see the fragility. Everybody can see the fragility.
[00:00:03] The thing that I think is more interesting is who's going to survive the bubble?
[00:00:07] Consumers of compute benefit from a bubble. Because if we overproduce compute,
[00:00:12] prices go down, your cogs goes down, and your gross margin goes up.
[00:00:15] The lesson that punches you in the stomach, adventure, is you can't make a company succeed.
[00:00:20] How would you respond to Sequoia were asleep at the wheel when it came to defense,
[00:00:24] not being in-house in an entril to clear market leaders in the category?
[00:00:29] I would see. Ready to go?
[00:00:44] David, I love your writing. Our episode last year was one of the most downloaded shows.
[00:00:51] I have like the cmr of matter. Tell me that it is the single show that he is forwarded to more people
[00:00:57] and sites more often than any other, not to make you nervous. We'll set the pressure for this
[00:01:03] episode. Thank you so much for joining me again, dude. Thanks for having me, Harry. You're always
[00:01:07] a very kind. Now, the year of the data center sounds wonderful. We had an amazing discussion last
[00:01:15] year. What did you predict last year, David, that happened and we are seeing an action now?
[00:01:21] I think there's really, so we talked about last year this concept of steel servers and power.
[00:01:26] I think if you remember rewind to summer of 2024, the big conversation at that time was compute
[00:01:32] models and data. That's what everybody was talking about. I had this view that everyone was
[00:01:36] underestimating the physicality of these data centers. I'm on the front lines. I'm talking to
[00:01:40] people every day. You talk to people. They're flying electricians to Texas and they're trying to buy
[00:01:46] out generator capacity and generators are sold out until 2030. How do you get in line? How do you
[00:01:52] do that? I sort of had this sense that people were thinking very abstractly in a bit's perspective
[00:01:58] about AI, but they should be thinking in an atom's perspective about AI. I think that prediction
[00:02:03] came true in two ways. The best trade of 2025 was the AI power trade. A lot of Wall Street people
[00:02:10] made a lot of money betting on the fact that power was going to be the constraint. We were going
[00:02:14] to move away. You hear Sam Altman now talking about gigawatts every day. He's not talking about
[00:02:18] dollars anymore. We're moving away from dollars and we're moving toward gigawatts and I think
[00:02:22] that transition has fully happened in the last year. The second way I think it was right. It's
[00:02:27] funny now. A year and a half later, you see this on the cover of the economist and the cover of
[00:02:31] the Wall Street Journal and the cover of the Atlantic. The mainstream media has now really picked
[00:02:35] up on this narrative of the physicality of AI is what translates to GDP. GDP is an imperfect
[00:02:41] metric and it generally captures physical things more than virtual things. GDP now is picking up
[00:02:47] all of this construction boom that's happening, all the steel that's getting created,
[00:02:50] all of the physical stuff that's happening in the AI data centers and you're seeing these stories,
[00:02:55] which I think are true, which is AI is now one of the biggest contributors to GDP growth in the
[00:02:59] United States and so I think that's the second way in which that prediction has played out.
[00:03:03] Does its contribution, you know me, I just go rogue and go off grip, but it's much more fun.
[00:03:07] Does its contribution to GDP growth go contra your $600 billion question in terms of where
[00:03:14] the revenue will come from? Well, the $600 billion question and maybe just to remind folks what
[00:03:19] that is. I mean, it's basically a very simple equation that says, if we invest, and this was 2024,
[00:03:25] when I wrote this, if you invest $150 billion in video chips, that's about $300 billion of data
[00:03:30] center investments and to pay that back, the person using the compute needs to earn a 50%
[00:03:34] gross margin. So there's about $600 billion of revenue that needs to get generated. If you redo
[00:03:38] that analysis in the summer of 2025, it's about $840 billion. So it's grown, but it hasn't grown
[00:03:44] dramatically. And so the question behind the question was, is the customer's customer healthy?
[00:03:50] We know that the customer is healthy. We know that people are buying all these data centers. We
[00:03:53] know that people are building these data centers. We know that those stocks have all gone up.
[00:03:57] We can see that, but is the customer's customer healthy? Is there actually an end user for this
[00:04:01] compute? I don't think that's been answered. And I think that the question last year, which was
[00:04:06] the valid question was, if everyone's spending all this money, it hasn't showed up yet because
[00:04:10] people haven't put the shovel in the ground yet. I literally wrote a piece last summer called
[00:04:14] AI a shovel ready. You know, the shovel is going to start hitting the ground. And so now the shovel
[00:04:18] is hitting the ground. We're mid construction on a lot of these projects. One of the predictions
[00:04:22] I made last year, in addition to saying it was going to be the year the data center in 2025,
[00:04:27] I said, hey, we're going to have these construction delays. We're going to have issues now in building out
[00:04:31] these data centers. And the information has done a very good job of reporting on this, but I think
[00:04:36] we're at the beginning now of seeing some of that play out as well. Are we going to see a mass
[00:04:40] proliferation of delays on data center construction, do you think? I think we're going to see variability.
[00:04:45] One thing I'm always interested in as an investor is like there's winners and there's losers and
[00:04:48] there's variability. And I'm very skeptical when anyone tells me like everybody is going to win
[00:04:54] or everybody is going to lose or everyone is going to do anything. Like there's always variability.
[00:04:58] Imagine a race. You have a track race like there's somebody in the front and there's somebody behind
[00:05:02] and someone's faster than the other person. And so I think with data center construction,
[00:05:06] one of my core perspectives that I've been developing over the last 18 months of writing about this
[00:05:11] is that construction itself is going to be a mode. The ability to build things is hard. And I think
[00:05:16] we underestimate that. And I think we continue to underestimate that because we sort of say, oh,
[00:05:20] well, it's fine. Like everyone's going to do it. The timeline is two years. Okay. But like there's
[00:05:25] a lot of complexity that goes into that. And by the way, the complexity compounds when everybody is
[00:05:29] doing the exact same thing at the exact same time and everyone is trying to buy from the same vendors.
[00:05:34] And I've written a lot about the AI supply chain for that reason because you really need to care
[00:05:38] about not only, okay, meta and Google are both building a data center, but who's the guy that
[00:05:42] they're calling and who's the guy that he's calling and you got to follow it all the way down the
[00:05:45] supply chain to get to the core of really what's going on. There's so many things I want to unpack
[00:05:49] within those. I do want to go to what did you not predict or foresee that did play out that you
[00:05:55] was surprised by. I think there were two big misses last year. I think the first big miss was
[00:06:01] these like big talent acquisitions. I mean, I think that if you would ask me the probability a
[00:06:06] year ago that, you know, if you're a 25 year old recent grad from elite university who is perceived
[00:06:12] to be an AI expert, you can get a $50 million dollar pay package right now. And if you are a
[00:06:16] brand name that everyone recognizes your name, you can get a billion dollar pay package right now
[00:06:21] for a single individual. I totally did not see that coming and I think that you asked me a year
[00:06:25] ago to predict that I would have said you were crazy. So sometimes I do think the beauty of AI is
[00:06:30] like reality is stranger than fiction and a lot of crazy things happen. The second thing I just
[00:06:34] left before we move to the second. Do you think those scaled pay packages are justified?
[00:06:39] I think there's symbolic of this sort of desperation in the ecosystem where it's like we need to
[00:06:43] eke out progress. We need to prove that all these investments are worth it. And I think there's
[00:06:47] this logic that gets really abused in the venture world and in the tech world, which is like,
[00:06:52] hey, if I increase the probability of making a trillion dollars by 1%, that's worth ton of money,
[00:06:58] right? That's worth 10 billion dollars. And sure, that's true, but it's very easy to overestimate
[00:07:02] the 1%. Is it 1%? Is it a 100th of 1%? Is it a 1000th of 1%? Is it a 10,000th of 1%? Our brains
[00:07:08] were very bad at reasoning about that scale of number. And so I think to the extent that you believe
[00:07:14] that hiring this very impressive researcher increases the probability you win by 1%. I totally
[00:07:19] can see why you will justify a billion dollar pay package for an individual. That said, I think we
[00:07:23] are psychologically biased to overestimate what that percent contribution is. And it may be the case
[00:07:28] that there's these broader macro variables, which we'll talk about. I'm sure later in this
[00:07:32] discussion, there's these broader macro variables that are actually driving progress in AI that are
[00:07:37] that are not a single individual can change. I'm very upset looking at these pay packages that my
[00:07:42] mother didn't push me towards a more engineering happy. Does everyone feel that way? I think that's
[00:07:47] probably the universal reaction to seeing these packages. Hi, I'm my mom. You should have done
[00:07:51] bad parenting. You encouraged me to do English, really? Come on. Warren piece doesn't quite make
[00:08:00] it. Does it when you're getting paid 3.5 billion by Zuck? What was the second? I think the second
[00:08:05] one, one thing we talked about on the podcast last year, I predicted that Meadow was going to do
[00:08:10] really well. And I think that prediction was clearly false in a 12 month time horizon.
[00:08:15] I thought that the vertical integration that Meadow had was going to be an advantage. And I think
[00:08:19] that Meadow, you know, these 100 million packages are coming and large part from Meadow because
[00:08:24] they haven't performed as well as they thought they were going to. The reason I thought Meadow would
[00:08:28] do well is that it was vertically integrated and found to run. And I sort of continue to believe
[00:08:32] that in the fullness of time, it is possible. And I think the dramatic actions that Zuck is taking
[00:08:37] represent this, it is possible that I will be proven right in a longer time horizon,
[00:08:40] which is to say that Zuck's going to fix the problem. It's amazing what founders can do. He's
[00:08:44] so focused on this. He's spending all of his time on it. But I think if you look back a year ago
[00:08:49] at the prediction that Meadow would do well, I think you would say wrong. Have you changed
[00:08:53] from a buy to a salon matter? I think the dramatic action that Zuck's taking represents just how
[00:08:59] deeply invested in this he is. And I think it also shows us what founder CEOs can do and why founder
[00:09:04] CEOs are different than non-founder CEOs. I mean, there's all these studies of like if you just
[00:09:09] invest in the basket of founder CEOs, you will outperform the basket of non-founder CEOs. And I
[00:09:13] think what Zuck is doing represents that. And so I remain optimistic about Meadow long term.
[00:09:18] You said about the vertical integration that being part of like your thesis. I totally
[00:09:22] agree with you and was probably shaped by hearing you to be quite honest, David. You said to me,
[00:09:27] data center and model teams need to be coupled kind of going to the vertical integration elements.
[00:09:32] Do you stand by that? How do you think about that when hearing that today? And does open AI
[00:09:38] and anthropic not having that vertical integration challenge that? Well, I think the simple version
[00:09:44] would be open AI and anthropic are now steel servers and power companies. And that's like a big change
[00:09:49] that's happened in the last 12 months. And so I actually think we're at, you know, in many ways open
[00:09:54] AI and anthropic are becoming more and more vertically integrated every day. You're seeing a lot of
[00:09:58] announcements around them developing their own chips. They're working, you know, every day you
[00:10:02] hear Sam Alman talking about gigawatts of power and procuring his own power. And so I think you
[00:10:06] will continue to see the big labs moving vertically down the supply chain. And that's been one of
[00:10:11] the biggest trends in the last 12 months. Do you think we'll continue to see that? We saw pool
[00:10:16] site recently announced a two gigawatt data center that they're building out in conjunction with
[00:10:21] core weave. Do we think all model providers will need to be vertically integrated in this way?
[00:10:26] I think that competitive pressures will push all of the model providers to spend more time on
[00:10:30] this and to have teams focused on this. So I think the answer is yes. I do think that this is a trend
[00:10:35] that is going to be durable. When we think about where we are today, everyone says bubble.
[00:10:41] You've heard it. I've heard it. It's on my take talk. Do you think we're in an AI bubble?
[00:10:47] I do think we're in an AI bubble. I also think to your point a year ago when we had our last
[00:10:51] conversation, it was a very contrarian thing to believe that we're in an AI bubble. Today it's a
[00:10:55] very consensus thing to believe we're in an AI bubble. I mean, Sam Altman, Vinod Kosla,
[00:11:00] Jeff Bezos, some of the biggest AI bulls have now come out and basically said, hey, we're in a bubble
[00:11:05] of some sort of the other and each has their own perspective on exactly how that's going to manifest.
[00:11:10] So I think right now the bubble conversation has sort of reached kind of full consensus.
[00:11:15] The thing that I think is more interesting is who's going to survive the bubble? What's going to
[00:11:20] come next? And so I think there's two components to that. Number one, who are the winners and who are
[00:11:23] the losers? If you remember from the .com, a lot of companies from the 90s still did well. Amazon
[00:11:28] still became an amazing company after the .com bubble. So I think there's an opportunity for winners
[00:11:33] to continue to do well after the bubble. And I think the second thing that's really interesting
[00:11:37] is just timelines, right? Like a lot of us, you know, I've always said like my core belief is
[00:11:42] that in 50 years, when you and I are 80 years old, AI is going to have completely changed the
[00:11:46] world. It's going to dramatically reshape everything about society. And so if you take that time
[00:11:51] horizon and you say, okay, AI is this tremendous, tremendous technology innovation. It's the most
[00:11:56] important thing that's going to happen in our lifetimes. Probably it's going to be among the most
[00:11:59] important thing that's ever happened in human history and in the history of this planet, right? So
[00:12:03] it is this amazing thing. And yet the market is implying some probability that all of this is going
[00:12:08] to happen in such a short time horizon with a very specific chipset and all of this stuff. And so I
[00:12:13] think unpacking the tension between AI as a long-term winning trend and a long-term generational change
[00:12:20] and a short-term market cycle that will incinerate capital, I think that's the second kind of
[00:12:24] area that I think is really interesting. How do you balance that being invested, David?
[00:12:29] Like, you play the game on the field to build early quotes, but then also the awareness of the
[00:12:34] long-term impact that will come over multi decades. I think it's tricky. And I think the one benefit I
[00:12:40] have is I've been investing in AI for about eight years. And so I've been able to, you know, for
[00:12:45] me, this is not like, hey, this is like a 12-month thing where you're like running and have this
[00:12:49] FOMO to get into AI. I started investing in AI in weight and biases series A when everyone said
[00:12:54] deep learning was going to be tiny. It was a year after the transformer paper came out and ever
[00:12:58] said deep learning is a tiny market. Why would you invest in this company? And of course,
[00:13:01] they had a really nice exit to Corey recently. I invested in Runway ML when stable diffusion
[00:13:07] hadn't even been born yet. And everyone was saying, oh, Transformers is the only way. And of course
[00:13:11] stable diffusion introduced a new model architecture. And I invested in a hugging phase, which I
[00:13:16] still remember the first meeting I ever had with Klem. It was the, you know, he had launched this
[00:13:19] Transformers library. It's funny now Transformers on the tip of everyone's tongue. But that time,
[00:13:23] NLP, it was NLP, by the way, it wasn't AI at that time. And he had this amazing transformer
[00:13:28] library. And for folks who are steeped in AI, it was a successor to Bert and this old school of
[00:13:33] NLP models. So I just say that to say that, I think when you take a long enough time for
[00:13:37] eyes in an AI, over the last eight years, you have more opportunity to find investment opportunities.
[00:13:42] It's not about finding 10 investment opportunities, at least for me. I don't need to find 10
[00:13:46] investment opportunities this year. I'd like to find one or two investment opportunities year that
[00:13:49] I really love. This year, I've invested in Clay, which I think is an amazing application layer
[00:13:53] company we can talk about. I invested in Juice Box, which is building an AI recruiter that has
[00:13:57] tremendous love. And so I think you can find exceptional AI companies that I believe will do
[00:14:02] really well over the long time horizon and will continue to succeed for decades and decades to come.
[00:14:07] And one thing I ask myself before I make every investment is, is this company going to succeed
[00:14:12] in spite of market volatility. If you're the only way that your company is going to succeed,
[00:14:15] is that it can raise infinite capital and a cheap capital market. That's very difficult.
[00:14:19] If you have real customer love and you've built something that people absolutely need,
[00:14:23] you're going to be able to navigate through any market environment. And by the way, we've kind of
[00:14:27] seen that now with all of these 2021 companies navigating that environment. Some of them came out
[00:14:32] really strong on the other side. Look at Databricks, 60 billion, now 100 billion valuation.
[00:14:35] So you can come out the other side of market cycles if you have compelling product market fit,
[00:14:40] a great team, a great founder. So David, when we play out your question there of the winners
[00:14:44] and the losers, just so I understand that. Who do you think the winners and the losers will be
[00:14:48] when we look back on this last 12 to 18 months? I've had a very simple framework for this. It's
[00:14:54] actually, I think probably the first thing I ever published an AI and AI's $200 million
[00:14:58] question way back when in 2023. And the framework is this, consumers of compute benefit from a bubble.
[00:15:06] Because if we overproduce compute, prices go down, your cogs goes down and your gross margin goes
[00:15:11] up. So I've had the view that you want to invest in consumers of compute, producers of compute,
[00:15:16] imagine you're producing any commodity asset. If other people produce a lot of that commodity asset,
[00:15:21] it doesn't matter. It has nothing to do with you. You might be running the best operation possible.
[00:15:25] You might be an amazing business person. But if everybody else starts producing the same commodity
[00:15:29] asset, prices go down. And so it's very hard to control your destiny in commodity businesses.
[00:15:34] By the way, this is why commodity businesses tend to trade cyclically and tend to trade at
[00:15:37] lower multiples, the non-commodity businesses. So I think if you're a producer of compute,
[00:15:42] you're fundamentally in a commodity business, just like an oil company is in a commodity business.
[00:15:46] And that is going to trade a different way. And that is going to have more
[00:15:50] cyclicality than if you're in a non-commodity business, consuming the commodity, consuming the
[00:15:55] energy and producing intelligence on top of that. And so I think if you're consuming this raw
[00:16:00] resource, which is power, and you're producing intelligence and doing something that people love
[00:16:04] with that intelligence, those are the businesses that are going to do well on the other side of
[00:16:08] this market cycle. All three of the best businesses, not commodity businesses, in the form of Google,
[00:16:14] cloud, AWS, and Azure. I love this question. So let's talk about it. I think it's really interesting.
[00:16:20] One thing I've written a lot about, and you and I have talked about this, is like game theory
[00:16:24] and these big companies. And one of my core beliefs, or one of the things that I think is underestimated
[00:16:29] in the market, is that we're living in an anomalous monopoly era. And it's funny because there's
[00:16:33] so many comparisons to industrial revolution. And in some ways, we're living in this new gilded age.
[00:16:38] And we have these seven companies, and they represent 40% of the SMB 500, which is just mind-blowing.
[00:16:43] And have these amazing monopolistic businesses. And these businesses are cash cows. And I think
[00:16:49] people extrapolate from that. And they say, oh, all businesses are monopolistic. I think people
[00:16:53] have a mental model that implies too much monopoly and not enough commodity. And what I think
[00:17:00] people underestimate about the big tech companies, is that when the big tech companies were founded,
[00:17:04] when Google was founded, nobody thought it was going to be a monopoly. Think about YouTube
[00:17:08] selling for a billion dollars. I mean, that would be crazy if you had known how big all of this
[00:17:12] was going to be. So nobody knew that Google was going to be monopolistic. And you can build monopolies
[00:17:17] when they're hiding in plain sight. Nobody can see them. And so you build this monopoly and you
[00:17:21] don't have that much competition. AWS is the same. You mentioned AWS. Nobody knew that the cloud
[00:17:26] was going to be this tremendous opportunity when AWS started doing this. And to their credit,
[00:17:30] that's why they have the biggest market share in the cloud business. And that's been very
[00:17:33] durable for them. And so I think when nobody sees the monopoly, you can build a monopoly,
[00:17:38] and then you can extract margins on the other side. But AI is so different. Everybody knows that
[00:17:43] AI is going to be big. Like this is, I think, the irony of the AI is that everybody knows AI is
[00:17:48] going to be massive. But if everybody knows something's going to be massive, then everybody builds
[00:17:52] companies. And if everyone builds companies, those tremendous competition. And so I think the
[00:17:56] difference between the AI era and the big tech era, and it makes sense why everyone is over
[00:18:00] indexing or overtraining on the big tech era, because that's the era we live in. But the difference
[00:18:05] is that these monopolies are not hiding in plain sight. We all now know that if you build an amazing
[00:18:10] tech company, it can be worth a trillion dollars. In 2000, if you told people that they can have a
[00:18:14] trillion dollar tech company, they would have laughed you out the room. And so I think the market
[00:18:18] environment in which these companies are getting built is dramatically different. And monopoly profits
[00:18:22] are unlikely to exist. And by the way, final point on this, that's good for us. That's like good
[00:18:27] for everybody. Like we shouldn't want monopolies to exist. Monopolis are bad for the consumer. The
[00:18:32] consumer wants to get things for free. And the consumer wants to get things for the cost of capital.
[00:18:37] And I think that to the extent that there are not monopolies in AI, that's much better
[00:18:41] for how AI is going to evolve in a healthy way than if it evolved in a sort of a monopolistic
[00:18:45] direction. You said about kind of consumers of compute will win. I like that. But respectfully,
[00:18:51] it feels relatively accepted in ventricle systems for sure, in a way that your bets before
[00:18:57] weren't, weights and biases weren't, runway wasn't hugging face was kind of a kind of weird
[00:19:01] community play at a point. What do you think is obvious to you that is not obvious to the rest
[00:19:07] of the community today? When I first started saying this 18 months ago, it was definitely not
[00:19:12] consensus. And so one thing that is tricky in the business of ideas is that as soon as the idea
[00:19:16] becomes accepted, it was always obvious. But in the moment where you propose a contrarian idea,
[00:19:21] you know, everyone kind of criticized it. So I do think it's been interesting to see the change.
[00:19:26] And then by the way, the people who had the wrong opinion very quickly changed their opinion
[00:19:30] such that they weren't actually wrong. And so anyways, I think the idea game is a tricky one.
[00:19:36] I think that, and the second thing I would say to that is while people say they believe this,
[00:19:41] and you and I talked about this on the podcast last year, you probably remember this,
[00:19:44] everyone says they believe this. And then you look at these pitch book charts where it's like,
[00:19:47] where's the dollars going? And I think probably 80% plus of the dollars in AI are still going to
[00:19:51] producers of compute, not consumers of compute. So I do think you're right that it's an accepted
[00:19:56] narrative. But the producers of compute consumes so much more capital than consumers of compute,
[00:20:01] that if you are in a capital deployment strategy and you're trying to deploy as much capital as
[00:20:05] possible, you have to invest in the producers of compute. And I think that's one of the dangerous
[00:20:10] things in investing, which is that you have this, there's this almost like incentive to invest in
[00:20:14] people who consume more capital because they're calling you every day. And the people who can,
[00:20:18] don't consume capital, don't want to raise capital. And I think some of the best investments are
[00:20:22] those companies that don't want to raise capital. When Sequoia invested in Zoom, they didn't want
[00:20:25] to raise capital, right? They were profitable. They were doing really well. Those are the businesses
[00:20:29] that I think as an investor, you really have to focus your time on. I spoke to Sonia on your team
[00:20:33] beforehand. And she gave me a fantastic question. She said, if this is a game theoretic bubble,
[00:20:38] is there a coordinating mechanism for the spending to stop and the bubble to pop?
[00:20:44] You know, I love game theory. So I mean, my basic framework on AI, and this is actually kind of how
[00:20:48] I write all these pieces, is there's like 10 players around this big chess board. And they're
[00:20:53] extremely powerful. And each of their moves affects the other people's moves. So it's kind of
[00:20:57] recursive. And so you sort of have to think, first order, second order, third order, how does my
[00:21:02] move affect other people's moves? And these are very sophisticated players doing this. And so one,
[00:21:07] the simple answer to your question is, it's not coordinated. That's the beauty of the invisible
[00:21:11] hand. That's the beauty of people's incentives. These are big companies that are acting out these
[00:21:15] incentives. And so I think it told the incentives change. The behavior is not going to change. And so
[00:21:21] there is no coordinating mechanism. I do think that's one of the, it's always the surprising
[00:21:25] fact of capitalism. Like everyone wants to believe that everything is kind of coordinated.
[00:21:28] It's easier for our brains to grok. Everything being coordinated. But I actually think it's
[00:21:31] it's pretty uncordinated and incentive driven. If we think about you said earlier, it is definitely
[00:21:36] a bubble. And we're seeing this consensus across the different visionaries in our ecosystem.
[00:21:41] If it's a bubble, does it pop or does it deflate? And how do you expect that to play out?
[00:21:47] So I'm a student of Nassim Taleb. And I will lean on Nassim Taleb's sort of, he's a hedge fund
[00:21:53] investor and philosopher and he's written, uh, food by randomness,
[00:21:56] anti-fragile black swan. I think these are books that a lot of folks will be familiar with and
[00:22:00] really influential books in the investing world. And his philosophy and he says this in
[00:22:05] anti-fragile is, you know, it's really hard to know if a building is going to fall down, but you
[00:22:10] can see when it's wobbly. And so you can't really predict when the wobbly building falls, but you
[00:22:14] can notice the fragility. And so I think my perspective on AI right now is, uh, you can see the
[00:22:20] fragility. Everybody can see the fragility. Can I ask you, what specifically is you say you
[00:22:24] can see the fragility? Well, the circular deals, I think the circular deals dynamic is probably,
[00:22:30] when I think about why did, why did this AI bubble narrative go from contrarian a year ago to
[00:22:34] consensus today? I think the main thing driving the consensus is these circular deals and the
[00:22:40] big tech company dynamics. Let me, let me unpack that. A year ago, hyper scalers were holding up
[00:22:45] the AI ecosystem and everybody felt very comfortable with that because everyone knew that these
[00:22:49] were very robust businesses. Microsoft and Amazon specifically were driving the vast majority
[00:22:54] of the AI CapEx growth. And they were explicitly saying, hey, we're going to buy out your generator
[00:23:00] capacity for five years. We're going to sign a 20 year lease on this data center and we'll back
[00:23:04] it up with our credit. So they were basically putting themselves in front of all the risk. And the
[00:23:08] way I thought about it a year ago and wrote about it a year ago is like they're almost grabbing the
[00:23:12] demand hot potato and saying like, it's ours. Don't worry about it. We got this covered. A year
[00:23:17] later, Microsoft and Amazon have really stepped back. And this started, and again, the information
[00:23:22] has done a really nice job reporting on this, this started in the beginning of the year. There was this
[00:23:26] big public announcement or leak or whatever you want to call it where Microsoft walked away from
[00:23:32] two data centers. And it sent a message to the market like, hey, we're not stepping up. We're
[00:23:35] not going to take all the risk on everybody else's behalf. We're not going to be this risk
[00:23:39] absorber in the ecosystem anymore. And then what happened later this year is Oracle obviously
[00:23:45] stepped up and took on a huge amount of the compute demand. And Corrieve has really stepped up and
[00:23:49] taken on a huge amount of the compute demand. And so you have this shift from Microsoft and Amazon
[00:23:54] to Oracle and Corrieve. And then the second order effect of that is that Oracle and Corrieve are
[00:23:59] a lot smaller than Microsoft and Amazon. They simply can't absorb as much risk as Microsoft and
[00:24:03] Amazon could. And so the chip companies are now stepping up and saying, okay, we'll absorb
[00:24:07] some of the risk. We'll put in the capital to finance this build out where the demand on the other
[00:24:11] side is not so clear. Because of course, the chip companies also get to book this as revenue. So
[00:24:17] their cost of capital is very low. One might even say their cost of capital is negative in some of
[00:24:21] these deals. And so it's the cheapest capital available. And so moving from expensive capital from
[00:24:30] these big tech companies to cheaper capital from the chip companies themselves who get to benefit
[00:24:36] from circularity. I think that's probably been the biggest change in the last 12 months in AI.
[00:24:41] And I think that's a lot of people have observed. It's fairly obvious. And so that I think
[00:24:46] has changed a lot of people's minds. Do you think these deals are priming the pump, so to speak?
[00:24:51] I think all of these deals now are priming the pump. I mean, you basically announce the deal.
[00:24:55] They're 10 or 20% funded. And then you have to go raise capital to fund the rest of it. And so,
[00:25:00] you know, everyone announces these deals in gigawatts, not dollars anymore. And I think most people don't
[00:25:05] know how many dollars a gigawatt is. And so the rough math is, you know, a gigawatt is 40 billion
[00:25:10] dollars to build out. Jensen says it's 50 or 60 if you use the next generation viewer rubin chips.
[00:25:15] So let's say it's somewhere between 40 and 60 billion. So 800 gigawatts of power build out,
[00:25:20] which is what people are talking about now, that's eight. That would be AI's 8 trillion dollar question.
[00:25:25] And then 250 gigawatts of power is AI's 20 trillion dollar question. So we've totally up the
[00:25:31] ante. And the magnitude is just, it's just much, much bigger. But of course, that's not funded.
[00:25:37] And so I think the funding for these deals is going to be an important thing that has to play out.
[00:25:40] How do you read them? When I hear you speak now, I feel very concerned. Like, I think,
[00:25:45] is there even the capital supply in the world for these? You know, we've heard about some
[00:25:50] Altman and the trillion dollars that he needs and requiring the same energy as Japan. And you're
[00:25:56] actually looking at that game. Well, not even the sovereigns have enough money for that, actually.
[00:25:59] Well, we're living through this amazing moment. And I do think it's precarious. We're living
[00:26:02] through this amazing moment where like the entire capital market is just AI, right? I mean, 40%
[00:26:07] of the SMP 500 is these big tech companies. They're all basically trading on AI.
[00:26:12] Private capital is all targeted AI. And so I do think the world's capital machine
[00:26:16] is directed in a single direction. I think the risk is that it's all focused on a very
[00:26:20] constrained period of time. I actually think in the fullness of time, it's not that risky. Like,
[00:26:24] these things are going to play out. We're going to get these amazing, AI is going to be amazing.
[00:26:27] We're going to get these huge technological breakthroughs. Tremendous revenue is going to get
[00:26:30] created. It's going to be a big driver of the economy. The problem is, and the simple way to
[00:26:35] think about it is, it's all B100s and H100s. And what if it actually takes three years? And it's
[00:26:41] the Rubenships that get us there, where it's the Feynman ships that get us there, which is the
[00:26:45] 2028 chip, right? So I think again, it comes back to where we started, which is the physicality of AI.
[00:26:51] You can't just say like, oh, I'm upgrading my chip. Great. It's not my fingers. I've upgraded my
[00:26:55] chip. No, you have a giant warehouse sitting with these chips. And they might be legacy chips.
[00:27:00] And maybe it's going to take us 10 years to get there instead of two years to get there.
[00:27:04] And I think that is kind of the risk that the financial ecosystem is taking on. Whereas as an AI
[00:27:09] investor and an AI believer, I'm like, we actually just need to spread that risk over a longer period
[00:27:13] of time and a greater number of bets. Oracle is one of the biggest players that we've seen
[00:27:19] into the market, as you mentioned there. When you look at that like debt equity ratio,
[00:27:23] traditionally considered very, very high, do you not think they're out over their skis?
[00:27:28] I think that one narrative that I have been thinking about a lot is this narrative that I think
[00:27:33] a lot of the media has also been painting of like, hey, debt is going to unwind the AI bubble,
[00:27:37] which is to say a lot of these AI investments are debt-funded. And the problem with credit is
[00:27:42] that credit unwinds. And then when you have a credit unwind a lot about things happen. Actually,
[00:27:46] that's not the way it's going to play out, which is maybe surprising. I think that the reason people
[00:27:50] are so anchored to this sort of debt narrative is that 2008 was a debt credit unwind and people
[00:27:56] understand how messy credit unwinds are. I actually think what's interesting about this AI buildout
[00:28:01] is that for the most part, and let's put Oracle aside, which maybe has some debt, but for the most
[00:28:05] part, the AI buildout today has been equity-funded and cash-funded. And so I think it's actually,
[00:28:12] every bubble looks different and every unwind looks different. And I think we always sort of over
[00:28:16] anchor on the lessons that passed. What I think is going to be interesting, if to the extent that
[00:28:20] the bubble unwinds at some point, it's going to be an equity unwind. And what that looks like is
[00:28:24] 40% of the SME 500 is basically a bet on AI. And so to the extent that the bet unwinds stock
[00:28:29] prices go down. And what's different this time, again, versus 2008 is more Americans, you know,
[00:28:36] greater percentage of Americans net worth is equities than I think ever before in history. And so
[00:28:41] people are going to feel this in the form of their equity portfolio going down more likely
[00:28:46] than some credit unwind where the banks get affected and all of that stuff.
[00:28:49] Are you as concerned as I am by the concentration of value in Mag 7? It's not, and again,
[00:28:53] if I'm pushing you on company specifics, dude, I mean, I know, really, I'm not a journalist in
[00:28:57] any way. I have zero desire to get a clickbait answer. But like, I look at the concentration of
[00:29:03] value in Mag 7 as a class or cohort, and I am worried. Yeah, I was sitting down yesterday with
[00:29:12] Sandy Noren, who's the author of this book, The Engines That Move Markets, which is one of the
[00:29:15] all-time great tech investing books. And we were talking about AI and we were talking about
[00:29:19] markets. And he sort of made this comparison to Japan in the 90s, where basically if you didn't
[00:29:24] invest, if your portfolio was not levered to Japan in the 90s, then you were like the best
[00:29:27] performing fund in the 90s. And that it was, I think he said, and this was like really surprised me,
[00:29:32] he said that Japan was basically 43% of the equity market and the US was 41%. And so it was really,
[00:29:38] really a huge percentage of the market, right? And that really unwound. And so I think you have a
[00:29:42] similar dynamic here, where the Mag 7 are just the humongous portion of the market. Now these
[00:29:47] companies are great. They have cash machines, like they're going to do fine. But I do think we
[00:29:51] should be concerned that these companies represent such a huge fraction of the market and that any
[00:29:56] change in the AI narrative really affects them. I want to discuss, you mentioned earlier in the
[00:29:59] conversation, and we mentioned the concentration value of Mag 7. A lot of that's predicated around
[00:30:04] the belief that it will impact GDP, GDP meaningfully. And we touched on it earlier.
[00:30:11] Massa said that he thinks it will see 5% GDP impact. How do you think bottom
[00:30:18] respond to the magnitude of which we will see AI impact GDP and productivity levels?
[00:30:24] So I think Massa makes an interesting point here. And I actually agree with him,
[00:30:26] fundamentally, that AI is going to affect 5% of GDP. Probably where I disagree with Massa. So I
[00:30:32] think he used the $9 trillion thing that's a number he used. It's going to disrupt $9 trillion
[00:30:36] of GDP. And then he says his next assumption is it's going to be a 50% profit margin. And then
[00:30:42] it's going to be $4 trillion of economic profit. And I think, so I agree with him, it's going to
[00:30:47] affect 5% of GDP maybe more in the fullness of time. But I think this comes back to the point we're
[00:30:51] discussing earlier where people overestimate the monopolistic nature of businesses and that we're
[00:30:57] living in this sort of unique, gilded age monopolistic era and that that is not the steady state of
[00:31:02] business. And I was reading, I found this McKinsey report recently, which said that if you look
[00:31:07] at total global GDP, 1% of global GDP is economic profit above the cost of capital,
[00:31:14] which I think is surprising. And I think again confirms this intuition that I think some people
[00:31:19] that that I think is important, which is for the most part GDP accrues to the regular people,
[00:31:24] working people who get wages and salaries. And it is very hard to sustain an economic profit
[00:31:30] above your cost of capital. And again, to moralize for a second, that's a good thing. I do think
[00:31:36] that's really good. And I hope that the economic benefits of AI accrue to everybody and not just a
[00:31:41] few companies. In terms of overestimations, I was just chatting with Rory O'Droskyl from
[00:31:46] scale and Jason Lemkin here, we have our weekly show. And they actually said that the biggest
[00:31:51] problem with today is we're seeing this overestimation of demand. They were specifically talking about
[00:31:56] legal, where every law firm is looking for an AI provider today because they've been told,
[00:32:03] look for an AI provider, that will not be the case next year and the year after. And so it is a
[00:32:09] atypical market cycle, where 100% of market is looking for a new provider or a provider,
[00:32:16] where normally it would only have been 5%. Do you think that's a fair description?
[00:32:21] I think that we are, I think there's a number of things that are being overestimated. I think
[00:32:24] the most important one is the timeline. And I mean, you've probably seen a lot of commentary now
[00:32:30] in the last few days about this like AGI timeline getting pushed out. And this is like I've been
[00:32:36] talking about for the last like four months. And because a lot of the leading indicators were there
[00:32:40] in June, July, but this did change over the summer. So it makes sense why everyone's talking about
[00:32:43] this right now, which is in June or July, Andre Karpathy at Y commenter said, hey, we're in for
[00:32:49] the decade of agents as opposed to AGI in 2027. And a few weeks ago Richard Sutton was on the
[00:32:55] Dwarkech podcast and basically explained why and Dwarkech I think has been doing a good job of
[00:32:59] fleshing out why the current technology paradigm is not enough potentially to get us to AGI.
[00:33:06] And so, and then Sam Altman came out, I think also in June or July and said, hey, it's going to be
[00:33:09] a more gentle singularity. I've actually been surprised by how, you know, gradual the change has
[00:33:14] been as opposed to being sort of this crazy change. And so for me, there's this contrast between what I
[00:33:19] think of as like the lunchroom conversation at these big labs, like you have these 25-year-olds
[00:33:22] sitting around lunch being like, AGI is 100 days away. No, it's 200 days away. No, it's 300 days
[00:33:27] away. And like the highest status person is the person who says it's 100 days away because they're
[00:33:30] the most aggressive. And then you contrast that against like the truth thought leaders and God
[00:33:36] fathers of AI, the people who really invented this category, people like Richard Sutton, people like
[00:33:40] Andre Karpathy, people like Ilya Satskever who said in December that pre-training is dead. And
[00:33:45] those people think, hey, the timelines actually like 20 years, 30 years, et cetera. And so,
[00:33:50] I think that contrast is probably the biggest thing that's being underestimated. And I think the
[00:33:54] irony of that is that it's actually the people who are the forward-thinking leaders who sort of
[00:33:59] let us down this path. Like the path we're on was invented by these people who are raising the
[00:34:04] most concern or saying the timeline is longest. And it's the people who've been in AI the shortest
[00:34:08] who I think are saying like, hey, it's going to come tomorrow. And I think they're sort of this
[00:34:11] experience curve of these things are just hard and they take time. And by the way, just to say this
[00:34:17] because it's so important, it's like, if this happens, it's a cataclysmic event in the history
[00:34:21] of our species. So it doesn't really matter if it happens in 200 days or 50 years. What matters
[00:34:25] is that it does happen. I almost feel apologizing because you're so small and intellectual and then
[00:34:29] I'm like, yeah, well, venture baby. But like, Kingmaking is a real thing. Making one person the
[00:34:36] anointed winner with a large amount of capital distribution and brand. Allah Harvey is a very
[00:34:43] real dynamic that we're seeing play out. How do you balance that? The importance of Kingmaking
[00:34:49] today with the long cycles, the decade plus that we're talking about there?
[00:34:55] I don't believe in Kingmaking and that's maybe a controversial thing to say. I think one of the
[00:34:59] lessons, you know, you'd think like, oh, Sequoia, she built a Kingmake companies and like, that's
[00:35:03] so great. And that would be, by the way, if that was true, it would be really economically valuable
[00:35:07] for our LPs if that was true. And I don't think that we think that's the case. And I think if
[00:35:11] anything, some of the hardest learned lessons in this business are like, you think that your capital
[00:35:15] is going to change the business. It's not. It's not. Fundamentally, the founder has to be amazing.
[00:35:20] The idea has to be amazing. Product market fit is to be amazing. Maybe we can help them navigate
[00:35:25] a few difficult decisions along the way. And we like to think of ourselves as company builders.
[00:35:29] But I think the lesson that punches you in the stomach, adventure, is you can't make a company
[00:35:34] succeed. The company has to already be successful. And then I think the second or effect of that
[00:35:38] is like, you should be humble because the company succeeded, not because of you, the company succeeded
[00:35:42] because of the founder. And maybe you helped a little bit, but you can't make company succeed
[00:35:46] as a vendor capitalist. And I think that ego gets in the way where people think they can and I
[00:35:51] just don't think they can. So you don't think in a market like profound that Sequoia and the subsequent
[00:35:57] quick ground has helped them significantly get great talent, get great customers, and get
[00:36:04] subsequent funding, which is then widen the mode between them and the plethora of other people.
[00:36:10] I'm sorry, I love you, but I respectfully disagree. I think that there are flywheel dynamics
[00:36:15] for sure, in venture. And so I'm not saying that having, I think having screw in your cap table
[00:36:20] makes your company more successful. So I'm not saying that having a brand name great DC who's
[00:36:24] going to work really hard on your cap table doesn't change the probabilities. I just think it
[00:36:28] changes the probabilities less and meaningfully than people think on average. And I think that,
[00:36:33] you know, you use profound as an example because I was in the pitch when they came to the I see
[00:36:37] the business was ripping. It was an amazing business. They had tons of customers lining up at
[00:36:41] their door to buy the product. And so yeah, we're lucky to be in business with them and we're grateful
[00:36:44] to be in business with them. And I hope that we can shape the journey in some way. And if there's
[00:36:49] five engineers that joined and Sequoia helped, you know, help having Sequoia involved help them join
[00:36:54] phenomenal. And by the way, I think that's the number one way that companies do benefit for
[00:36:57] having me screwing on the cap table is that is talent and recruiting. And we can talk more
[00:37:00] about that. And I'm fascinated by recruiting and recruiting dynamics. So I do think Sequoia helps
[00:37:05] with that. It especially helps with folks who are more dramatic where I think the brand name really
[00:37:09] helps. That said, I just resist the idea that like, oh, you know, I think this is just something
[00:37:15] that you learn the hard way in this business. Like, oh, I'm going to put 20 million in this business.
[00:37:18] Now it's the Sequoia company in this space. And suddenly it's going to succeed. Like, no,
[00:37:21] I don't it doesn't work that way. We've learned that the hard way. And I think we in our investment
[00:37:24] committee conversations, we really resist that because I think that is how you make mistakes in venture.
[00:37:29] So funny. I remember when I interviewed Doug and he was like, people think that like,
[00:37:33] because with Sequoia, everyone just comes and says, uh, hey, you are his my ideal. You must have it
[00:37:39] to take it. And he's like, I wish I would love that. It's not how it works. Like, I have to fight
[00:37:44] and fight and fight. And I'm like, you're biceps of bulging, Doug. I totally believe that you have
[00:37:51] to fight for the fairery deal. It's all good. Um, you mentioned a couple of companies that you
[00:37:55] work with. The common critique posed to consumers of compute is margins, margin structure, unhealthy
[00:38:03] margins. Do margins matter today in this entry point of AI or not? I think they matter. And the
[00:38:12] companies I've invested in typically have reasonably high margins. Um, that's that I think they
[00:38:16] matter. They are, they're a directional indicator of how much product you built on top of the
[00:38:21] foundation models. They are not absolutely important. I, you know, I remember investing in
[00:38:26] a company many years ago that had a 30% gross margin. And now it has a 70% gross margin. And so
[00:38:30] gross margins go up over time. I think one thing as an investor that I guess you viscerally experience
[00:38:35] is that plenty of companies that get critiqued having low gross margins end up being super healthy
[00:38:39] businesses in the long run. You know, Snowflake was one of the big indites on Snowflake in the
[00:38:43] early days was that it had a low gross margin. Obviously, it's a, it's a very good business. So I
[00:38:47] think if you have a real product that delivers a lot of value and there's reason to why as you get
[00:38:53] bigger, the cost is going to go down and an AI does such an obvious reason, which is the cost
[00:38:57] that compute just keeps coming down every year. So the trend line is very clear. I think you
[00:39:01] can build a healthy business. And so I would even go to the extreme and I haven't invested any of
[00:39:05] these companies, but I would go to the extreme to say that even some of these companies is zero
[00:39:08] percent gross margins. I can imagine how they're going to work. Now the companies I've invested in
[00:39:12] typically have higher gross margins than that. And I think that's an indicative of the amount
[00:39:17] of product that they've built. At the end of the day, our job is to invest in companies that
[00:39:20] become really successful, not to be like super smart about analyzing them. And so I think sometimes
[00:39:24] the instinct to criticize a gross margin can get in the way of money making. And you mentioned
[00:39:30] Doug. I sort of, the thing I've learned from Doug or the thing I admire most about Doug is like
[00:39:33] the job is to make money at the end of the day for LPs for founders for everybody. We all,
[00:39:38] that's the business that we're in. And so I try to keep that as the as the goal at the end of the day.
[00:39:44] I have something called WDDD, which is what would Doug do, which is like in a tough situation,
[00:39:51] I'm like, hmm, WDDD. We mentioned, oh, margins is one. Gross rates is another. The companies that
[00:40:00] just growing so much faster than we've ever seen before. I had him on the show from GC. He said,
[00:40:08] travel, travel, double, double. I say, go, like, you know, come back when you got something better.
[00:40:12] Brian Kim said recently, it calls a lot of Ferrari. If two million in air are like in a 10 days,
[00:40:18] like, come on. How do you feel about this gross rate on steroids requirement from VCs? And how
[00:40:24] do you feel? Is triple triple double double dead? I think of it, I think of it as the zero to 100
[00:40:29] club. So I think it's a variation on this, which is the best AI companies right now are going zero
[00:40:34] to 100 million of revenue very quickly. And I don't think you have to be at 100 million
[00:40:38] revenue to be clear. But I think that as an investor, you want to believe the company is going to be
[00:40:42] one of those companies. And I think companies that are on that trajectory or have crossed that
[00:40:45] trajectory are companies like Harvey and open evidence and, uh, and I think, and Clay and juice
[00:40:50] box. And I think these are companies that are kind of on this trajectory of growing really, really fast.
[00:40:55] The reason why it's important is because to your point on how there's so much demand right now
[00:40:59] for AI, the best companies, it is the best indicator we have that you built something really useful.
[00:41:04] People are, and we've talked about this actually a number of times in our partner meetings at
[00:41:07] Sequoia. You know, you sort of look back at the internet. There weren't that many people on the
[00:41:10] internet. And so these companies could only grow so fast. Right now, everybody's on the internet
[00:41:14] and everybody wants to buy AI. So if you have something really good, it's going to get adopted really
[00:41:18] fast. And so I do think to the point of playing the game on the ground and adapting to what you see
[00:41:23] in the market, the biggest thing that we've seen in the market is that these companies growing zero
[00:41:26] to 100 are the companies that have smashing product market fit. And so I'm happy to invest in a
[00:41:31] with two million in a row that is smashing product market fit. But I would tell you is the
[00:41:34] companies with smashing product market fit are growing faster right now. And by the way,
[00:41:38] they don't always have to grow faster. Like the goal is to invest in something that in 20 years is
[00:41:42] this amazing public company with billions of dollars of revenue. And that is still the first order
[00:41:45] thing. But I think you, you know, don't fight the tape like you can't ignore the traction on the
[00:41:50] ground. I always say I don't care how long you take to get a million in revenue. But I can't
[00:41:55] desperately about how long it takes for you to go from one to 50. There's a lot of data that
[00:42:02] indicates that that is a very good leading indicator for its worth. The data I've looked at
[00:42:05] suggests that that is a historically good algorithm. You know, one of yours is Uipoth. And he's
[00:42:11] a dear friend of mine, Daniel. And I mean, it took nine years to get to 550K of error.
[00:42:18] I'd wish I'd invested in him in the first few years. I got to work on the investment when it
[00:42:22] was later stage. But I mean, obviously, amazing story. And I think one that should inspire people.
[00:42:26] One thing I try to talk about with founders also is like, I want to inspire founders that it
[00:42:30] can take a long time because Silicon Valley sometimes has this such a short term time horizon. And
[00:42:35] I look at juice box, you know, this company started three years ago. The founders, the CEO is 25.
[00:42:40] The CEO is 22. Sorry, he's now 25. The CEO is 22. He had dropped, you know, finished Harvard in
[00:42:45] three years. The CTO dropped out of Dartmouth. He was 19. They took them three years. They were
[00:42:49] always focused on recruiting. They had initial music app in college and they evolved that into
[00:42:53] the recruiting market. And they spent three years figuring out what the product should be. And now,
[00:42:57] of course, it's growing really fast and they're really good founders. And one thing I've learned
[00:43:01] and this incentivizes me to invest in companies like this is people like David and Ashan, the juice
[00:43:05] box founders who've sort of been through the founder journey, they've been through the pain,
[00:43:08] they understand how hard product market fit is. I think in the fullness of time, they are better
[00:43:12] founders for it. And those those scar tissue, even though they're really painful, I do think they
[00:43:17] paid dividends long-term. And I think for founders who are listening, who are like in year one
[00:43:21] and things are hard, you know, that's that's you know, I know that's painful. And there's nothing
[00:43:26] that I can say that's going to make that less painful. But I think there is like we would love
[00:43:29] to invest in you two years as you figure it out. And and we're super patient. And the most,
[00:43:34] there's this false narrative, I think, that like all the good companies, they, you know, they
[00:43:39] raise the seed and then they raise the A and then they raise the B and it's all in 12 months.
[00:43:42] That's not really how most of these companies work. Clay spent many years. It's funny because we've
[00:43:46] been talking about juice box and clay. Clay spent many years in the wilderness figuring out what
[00:43:50] their product was going to be. Sequoia invested at the series A in I think 2019. The company spent
[00:43:56] three or four years in the wilderness really figuring it out. I look at Korea and I think the man
[00:43:59] is like enlightened from this experience. Like a super painful experience. Varune ended up joining
[00:44:05] as a later co-founder, amazing combination. So the company completely changed from the series A
[00:44:10] and then I led Sequoia's investment at, you know, a little north of a billion,
[00:44:14] which are doubling down in the growth stage of the company. And obviously the company now has
[00:44:19] has continued to rip and has done has done really well. So I think the the default narrative of like,
[00:44:24] oh, I'm going to start the company and then 12 months later I'm going to be successful. At least in
[00:44:27] the case of two of the investments that I'm most excited about, that was definitely not what happened.
[00:44:31] The reason you come on the show is because I stalk the shit out of you. I spoke to Varune and
[00:44:35] David from juice, but I've ruined from clay and David from juice box before. But I told you,
[00:44:39] I didn't have one person not respond to my calls or messages about you, which is like very, very
[00:44:46] radical. Like that's testament to you. You mentioned they're like, oh, for founders who, you know,
[00:44:51] it's hard and, you know, we don't want to present this false picture of being easy. Completely true,
[00:44:55] but we are seeing these very quick, successive rounds, you know, if we look at, say, a rillet or a
[00:45:00] profound or do you worry about them? I remember Pat Grady once saying to me that his biggest challenge
[00:45:09] is that when he does the deal, everyone else wants to put in money at double or travel the price,
[00:45:14] and that really stuck with me. Do you worry about these very quick, successive rounds?
[00:45:18] I think we try to find the right balance. And to be honest, this is a conversation I have with
[00:45:22] a lot of founders, right? So this is like a very active conversation. We're all having these
[00:45:25] conversations all the time. And we're obviously in a market where capital is very abundant and very
[00:45:29] available. And so I see the argument for why people want to take the capital. I think one lesson
[00:45:35] we've learned is more capital does not make a company more successful. Capital is a fuel, but
[00:45:39] capital does not create the engine. And so I think this is a tension. I think this will always be
[00:45:44] a tension. And this is definitely a tension for companies right now where we learned this the
[00:45:48] hard way in 2021, getting over capitalized has downsides. I think it leads to the biggest downside
[00:45:53] in my opinion is that it leads to this sort of internal perception of like we're winners. We're so
[00:45:58] successful. We're so great. And the only thing that makes you a winner is having tremendous product
[00:46:02] market fit and having customers who love you. And so I think that's a tension. Some founders and
[00:46:08] I've seen some founders do a great job of this that I've worked with. They they they really act
[00:46:12] like they'd money is not in the make account. And they really behave diligently and the team size
[00:46:16] doesn't grow too fast and all of this stuff. But I think that is the exception, not the rule. And I
[00:46:21] think it's actually the not the founders that are you should be most worried about. But it's the
[00:46:25] engineer who joins the company the day after the billion dollar fundraise with very little revenue.
[00:46:30] That dynamic is tricky and I admire the founders are now beginning it. I don't think there's an easy
[00:46:35] answer. I wish there was. I don't think there's an easy yes, no answer. But I think it's a tension
[00:46:39] we should be talking about. And as company builders, it's something that we need to we really need
[00:46:43] to think about. Speaking of Pat, quite a lot. Paul guy is like an advert for Pat. He taught me something
[00:46:48] that was really interesting, which was two questions, which are framework for amazing insight
[00:46:54] from founders. And he said number one is like what does everyone think they know that actually
[00:47:01] they get wrong? If we apply that to AI and what we see today, what does everyone think they know
[00:47:08] that they actually are getting wrong? I guess I would say and this is a really hard one lesson.
[00:47:12] And it's only I've learned from a lot of my mentors in this industry because I think one of
[00:47:16] the things I really try to do is learn from people who've been doing this for longer or smarter,
[00:47:19] who are more thoughtful. And one lesson that I've learned in this business is that anything
[00:47:25] multiplied by zero is zero. And I think that's one of the really tricky things in investing,
[00:47:30] which is just to say that market volatility doesn't matter in the long run if you have a great
[00:47:34] business. But if you overextend yourself and then some crash happens and you go bankrupt,
[00:47:40] you're bankrupt, right? Like there's no way out of that. And so, and I think there's sort of
[00:47:45] this sense, I heard this phrase recently, momentum has its own reality. And I think there's this
[00:47:49] sense of everyone is living in this like reality distortion field of momentum. And I think of
[00:47:55] it almost like this boomerang, the slingshot, you like pull the slingshot back and then, you know,
[00:48:00] you release the thing and then it sort of it has its own momentum after that. And then that's
[00:48:05] sort of a fundamental law of physics, things in motion, same motion, things that rest, stay at rest.
[00:48:09] And so, I think the thing that kind of people think, I feel so confident in is this like reality
[00:48:14] distortion field that comes from momentum. And when that reality distortion field goes away,
[00:48:20] you need to survive that. And I think one thing that I hope that I can be to my founders is a
[00:48:25] partner and they'll listen to me 10% of the time and that's fine. But a partner in, you know,
[00:48:31] making sure that we survive those moments and navigate those moments well and position ourselves
[00:48:35] well against that. And I think that the most prudent of investors or like the most sober of
[00:48:39] investors can actually be really helpful. Your job as a founder is to be maximally aggressive.
[00:48:43] And you should do that. And then the the investor should hopefully be giving some advice,
[00:48:49] helping think these things through, giving some perspectives, you know, understanding a broader
[00:48:54] time horizon perspective and a broader data set of companies. And then you sort of navigate to
[00:48:58] the right to the right end destination. So I think I don't think people are thinking about this
[00:49:04] sort of concept of anything once we're about zero is zero because the time horizon is so compressed
[00:49:09] into this shorter period of time. And that's just something that I think a lot about.
[00:49:13] The final one that Pat taught me and then we'll move to talent, which I do want to touch on
[00:49:17] for a quick part. But he said the other one. Yeah, it's a very, very, very good guy.
[00:49:21] What is no one thinking about that everyone should be thinking about? So like for me, one I think
[00:49:26] says, don't actually say no one is thinking that if you foie gras engineers in terms of the capital
[00:49:32] that you are stuffing down any of the multi-billion dollar, you may not get an
[00:49:37] equivalent level of productivity as when they didn't have multiple billions of dollars.
[00:49:42] Given nerd billions of dollars, nerd buys five cars in a boat,
[00:49:47] nerd not so productive. I'm sorry to be so blunt and dire. But it's the same with companies.
[00:49:53] I think that companies underestimate 23 year olds and 24 year olds. I think this is something
[00:49:58] that people really, really underestimate. And I think this is more true than ever right now in AI.
[00:50:02] Like I, you know, I recently, like I meet probably 200 or 300 young recent college grads every year.
[00:50:08] And the reason I meet them is I want to recruit them into my companies. A lot of them are founders.
[00:50:12] This is the population that I learned the most from because I know that my blind spot is going to
[00:50:16] be that somebody started using Chad GVT when they were 18 and I didn't. And so they're going to
[00:50:21] have a different perspective. And that's the perspective I need most in my life. In any case, I introduce
[00:50:25] some of these people to companies and the companies like, well, what's their skill set? Like why should I hire them?
[00:50:30] And, you know, I guess I think this is something that people are not thinking enough about in AI right
[00:50:35] now, which is Chad GV's been around for five years. Nobody has more than five years of experience in AI.
[00:50:40] The playing field is super level. And I think in a changing and dynamic market environment,
[00:50:46] dynamism and slope and ability to learn are more valuable than ever. And so the thing that inspires me
[00:50:52] and the thing I spent a lot of time thinking about is, you know, in a juice box, for example,
[00:50:56] how can we get the very best 23 year olds in the world working at this company? And that's a big
[00:51:00] part of my job. And I spent a huge amount of time on that, a huge amount of time on their one day
[00:51:03] a week right now at juice box just working on this. So how do we get the best people in the world inside
[00:51:08] of these companies? And I think maybe 10 years ago in the era of software, you know, a senior
[00:51:14] software engineer, a staff software engineer, they had more experience than an L3. And, you know,
[00:51:19] architecture was hard writing code is hard and they were much better. And so maybe it made sense
[00:51:23] there was this old playbook for startups of like, oh, you hire the staff software engineer who
[00:51:26] kind of knows what they're doing and you don't have to train people. I think that the new playbook
[00:51:30] for these AI startups is actually going to be much more about hired AI generalist, this 23, 24,
[00:51:35] 25 year old who's really native in AI, really passionate about it. And I think those are the
[00:51:40] sort of the front lines that are going to make great companies. Totally agree and understand that.
[00:51:44] Do you worry about emotional maturity a little bit? And I don't mean that patronizingly, but Jesus,
[00:51:49] I mean, I'm 29 now, but when I was 22, 23, I did some things that I would not do now.
[00:51:56] I think that hiring always has trade-offs. I think one thing I believe more generally speaking,
[00:52:00] because it's worth saying is, I really believe in trade-offs. I think everybody wants the free
[00:52:05] launch thing. When you don't know the trade you're making, then the negative is hiding from you.
[00:52:09] There's no such thing as a trade without negatives. There's no such thing as a decision where it's
[00:52:12] all positive in our negatives. So I always talk, and I talk about this a lot at Sequoia actually.
[00:52:16] It's like hidden risk versus visible risk. And so when you hire a 23 year old, there's a very
[00:52:21] visible risk. They're emotionally mature. They don't have any work experience. It's very obvious
[00:52:24] the negatives that you're taking. When you hire someone who's more experienced, it's like less obvious,
[00:52:28] the risk that you're taking. It seems to be lower risk. And every decision is a risk, right? And
[00:52:33] so maybe the risk that you're taking is that they're not going to work as hard. Maybe the risk
[00:52:36] that you're taking is that they're less AI native. You know, there's always a risk. And I think
[00:52:39] people have this tendency to favor the hidden risk by the prices of hidden risk. You don't perceive
[00:52:44] it as a risk, but it is a risk. And so people prefer hidden risk over visible risk. And I prefer
[00:52:50] visible risk. I want to know exactly what risk I'm taking. And then by the way, I'm a huge risk
[00:52:53] taker. I started investing eight years ago, right? Like I love risk. So I think it's important to
[00:52:58] calibrate that like I love risk taking. But I want to take visible risks. I know the risk I'm
[00:53:02] taking. And I think herd behavior and consensus mentality is about hidden risks. The risk is just
[00:53:08] beneath the surface. And you're not paying attention to it. Or I want to take risks that I can see.
[00:53:13] And I think there's a lot of areas. The point I'm trying to make is in the hiring dynamic,
[00:53:18] when you hire a 23 year old, it's like super obvious why you shouldn't hire them. And yet sometimes
[00:53:23] that's okay because the reason you should hire them makes up for that more. Completely agree
[00:53:28] from the employer side. On the flip side, when you think about like advice to them, if you were
[00:53:34] advising your younger sibling on choosing their first job, I saw on LinkedIn, you said,
[00:53:40] follow the smallest people a year ahead of you. That moniker of advice may not be
[00:53:44] relevant anymore. What advice would you give to them? Well, this is like the biggest learning
[00:53:48] because I've met with 200 or 300 young people a year. I have a very big data set. And I think
[00:53:52] I've probably spent more time than anybody got to call you on this specific thing. And my biggest
[00:53:57] lesson is that the way that young people choose their career is this what I call the medic algorithm.
[00:54:02] And the medic algorithm is, yeah, what did the people one year ahead of me in school that I
[00:54:06] thought were the best? What did they go do? And it's a recursive algorithm, right? So it's like
[00:54:10] what are the people your head of me do? But those people chose based on the people your head of them
[00:54:14] did and those people chose based on the your head of them did. Now one reaction to that would be
[00:54:18] negative of like, oh, that's so mimetic. They should think for themselves. I actually don't have
[00:54:21] that perspective. I'm fine with it. I think it's like a reasonably good algorithm. When I graduated
[00:54:25] from college, Poundtier was the hottest company to go work for. All the really smart people went
[00:54:28] to go to work for Poundtier, going to work for Poundtier would have been a great life decision
[00:54:31] at that stage. Before that, in the early 2010s, Google and the big tech companies were the hot
[00:54:37] place to go work. And I think those companies were all 10Xs over the over the 2010s. Some of them
[00:54:42] even I think 25Xs. So that it was a good decision to go work at Google in 2010. And so I don't think
[00:54:48] the medic algorithm is inherently broken. And I respect it. And I think that people to your point
[00:54:53] of maturity, people are going to go through maturity curve. They're not going to use this algorithm
[00:54:57] when they're 30s. They're going to evolve. They're going to change. And so I sort of have this
[00:55:00] respect for it. That said, I do think that recursive algorithms break down in the face of dramatic
[00:55:05] new data. And the dramatic new data is the AI cataclysm. AI has totally changed how the world is
[00:55:10] going to work. And it should change your forecasts on the future. And so the recursive algorithm of
[00:55:15] like, what are the guy you're above me and the person you're above me do is actually breaking
[00:55:18] because those people didn't have this information. They didn't know that AI was going to change
[00:55:21] the world. They didn't understand Jenny. And so I think the advice that I try to give young people
[00:55:26] is just factor that into your algorithm. Like, you do you. It's like join the company that you want to
[00:55:30] join. Go to the place that's going to make you the happiest. But you know, factor that in. And then
[00:55:35] it's worth at least giving a shout out to this group of people that I called builders in this
[00:55:39] sub-sac post that I did, which is builders are people, most people, 90 plus percent of people,
[00:55:44] their question they're asking when they're choosing a job is like, what can I get from this job?
[00:55:48] What is it going to enable me to do? Who am I going to surround myself with? How am I going to
[00:55:51] become better? It's very, it's a very like, what do I get out of it? I think there's like a 10%
[00:55:56] group of people. Maybe it's 5%, maybe it's 1%, I don't know exactly what the percentage is. But
[00:56:00] there's a group of people that they're asking the question, what can I contribute? And by the way,
[00:56:03] if you contribute a lot, you generally get to extract a lot. And so I think contribution,
[00:56:07] this is again, the beautiful thing about capitalism is like when you contribute a lot, I do think
[00:56:11] that you get rewarded for that. And so those are the people driving Silicon Valley. Like, those are
[00:56:16] when you enter a company and you're like, why is this company succeeding? It's those type of people.
[00:56:20] And those are the type of people who like, they go from one great start to another great start,
[00:56:22] to another great start up. And so anyways, I distinction between these two groups of people,
[00:56:27] both valid, no problem with either of them. Like, you got to respect careers, a very personal
[00:56:31] decision. And so anyways, depending on what you're, what you're trying to solve for,
[00:56:36] what's going to grow my career? Option one, where can I contribute the most and therefore extract
[00:56:40] the most? Option two, I think there's a bunch of great opportunities ahead of you. And just factor
[00:56:47] in the AI variable. I think one thing that just frustrates me on this topic is like the
[00:56:51] mimeticism that continues despite market changes in the UK. And what do I mean by that?
[00:56:57] Goldman Sachs investment banking consulting is still whatever people tell you. If you go and
[00:57:03] speak at universities, which I do once or twice a week now, everyone still wants to be an
[00:57:09] investment banker. And so when you were talking, I was thinking, what does it take to break the
[00:57:13] mimetic chain? And maybe it's AI and the proliferation of AI and popular culture and media and
[00:57:20] that I think it's changing. I agree with you. Like, it's changing too slowly. And that's why I'm
[00:57:24] having these conversations. I'm trying to help. And I'm sure you are as well. And in these talks
[00:57:28] that you're doing, I think that one positive that I would say is I've seen a material change
[00:57:35] in the last 12 months. We're just sort of interesting because it's not like I'm not seeing the
[00:57:37] last 24 months. I'm not seeing the last 36 months. Like, it took two years after a chat Gbt for
[00:57:41] this to really start flowing through. But I would say it is, and by the way, a lot of people I'm
[00:57:46] talking to are currently investment bankers who want to get into AI companies. So it is sort of
[00:57:49] funny that way. I think there's more and more of these high-performing people want to be inside
[00:57:53] of AI companies. And that's why I think it's sort of a, it is a two-way match. Like, these companies
[00:57:58] need these people more than ever. But I think these young people can benefit more than ever from
[00:58:01] being an AI company. And again, maybe to make the value prop clear for like the young person,
[00:58:05] right? Like, the value prop is, hey, maybe 10 years ago, if you joined a startup and people didn't
[00:58:10] join startups that often 10 years ago, maybe 10 years ago, if you joined a startup, like there's
[00:58:13] this whole experience curve, you're the junior engineer, there's a lot of people who are smarter than
[00:58:17] you. And you're going to have to learn. And it's like going to take five or 10 years to become
[00:58:20] a really meaningful contributor. That's not really true anymore, right? You're sort of entering it
[00:58:25] like much more parity with everybody else. And so I think there's good reason why people are making
[00:58:28] this change. Dude, I'm throwing in a curveball here. But I was told that you're the man who does
[00:58:34] defense at Sequoia. And I, you know, you know, I say this with love, but I'm going in hard
[00:58:41] ball on this one. How would you respond to Sequoia were asleep at the wheel when it came to
[00:58:47] defense, not being in housing in Andral, the two clear market leaders in the category?
[00:58:52] I would say, and I think this ties into our conversation so far, that defense is the next AI.
[00:58:56] I'm like, that's how I started getting involved in AI. I think that defense is, if the transformer
[00:59:02] moment was sort of the starting gun in AI, I think that the chat GPT moment hasn't happened yet.
[00:59:08] So I do think, look, there's no way around it. Sequoia was late to defense. But I think
[00:59:12] Sequoia is working really hard to catch up. And that's part of business. You don't always get
[00:59:15] things right, but you keep trying. And I think we have that ethos and we have that humility.
[00:59:19] What do you think defense is a nice AI? Sorry. So I think that, you know, it's funny because I
[00:59:24] started investing as we were talking about a year after the transformer paper in 2018. And,
[00:59:30] you know, I think that it's sort of, defense reminds me in some ways of like a few years after
[00:59:34] the transformer paper, which is to say, people who are really paying attention, understand that
[00:59:38] that defense is going to change. And the transformer moment was the Ukraine war. It was a very,
[00:59:43] you know, before that, you had to be a visionary and to Palmer's credit and Peter Tills credit
[00:59:48] and people like this, like they were visionaries. Before the transformer paper, you're a visionary.
[00:59:52] And, you know, Ilya Andrekarpathy, these people are visionaries. After the transformer paper,
[00:59:56] you're an early adopter, right? And I think our job as investors is to be early adopters for the
[01:00:00] most part, especially in the growth business, to be early adopters. And so you see the change that
[01:00:04] happened in Ukraine. And I think it was very obvious that like, you know, warfare, you see these
[01:00:10] pictures, these tanks, you know, and these like long chains of tanks from Russia. And it's like,
[01:00:14] wow, like defense technology is 50 years old. And technology has moved so much in 50 years. And
[01:00:20] yet like the way that we do war just hasn't changed. And that's because, you know, we've been in this
[01:00:24] period of golden era for the world of dramatic peace and prosperity and all this stuff. And so,
[01:00:30] anyways, I think that the transformer paper moment was the, um, was the Ukraine war. And
[01:00:35] then I think that China GVT moment hasn't happened yet. And so I think the defense is actually,
[01:00:40] you know, there's underwrite in some ways. They're like underestimated in some ways. And
[01:00:44] that's why I started getting interested in defense, uh, two years ago.
[01:00:48] When you look forward to the world of AI, you've assumed that everyone will be improved with AI.
[01:00:52] We'll use it hundreds of times a day. And it'll be a part of everything that we do and think and,
[01:00:57] and say in many respects, taking that view on defense and assumes this continuing conflict
[01:01:05] increases, not even decreases from where we are today. That would go against
[01:01:10] the human cycles. There are periods of intense conflict periods of not. But suggesting that defense
[01:01:16] like AI would suggest that that is the case. How do you feel about that? Yeah. So I think, by the
[01:01:22] I share a little bit of how I got interested in defense. And, um, you and I know each other now. So
[01:01:26] like, before I got an AI, I was reading all this stuff and I'm trying to learn from people. And
[01:01:29] I think my sort of investing style is like, you spend two years learning about the thing.
[01:01:33] And then you kind of start investing in the thing. And so I sort of take my time to sort of build
[01:01:37] a foundation. And my foundation defense is like reading Napoleon and Churchill and like all of the,
[01:01:42] you know, the history of war, you know, the history of wars, history of defense, like geopolitics,
[01:01:45] really like getting educated. And I probably spent two years really educating myself and meeting
[01:01:50] founders and you learn a lot from founders on this space before I got involved. And the thing
[01:01:54] that I learned, and I think the thing that a lot of people who are deeper in the space than I am
[01:01:57] already understand, is deterrence is the first thing. You know, you only go to war because you
[01:02:06] have to. The whole point of defense is to prevent wars. And geopolitics is a real thing. And there's
[01:02:12] like real competition between nation states. And that's always that will continue. And so,
[01:02:18] as the world gets reshaped and we are living through a reshaping of the world order, I think that's
[01:02:21] something that a lot of people have seen have written about. There's a lot of variables about that
[01:02:25] that we can unpack. I think Radalio's principles of the changing world order is a really good book
[01:02:30] on this topic. So the world order is sort of fundamentally changing. And that leads to this
[01:02:35] interesting opportunity where we have to sort of catch up. There's a 50 years of catch up that
[01:02:40] has to happen. That's what I see the current defense moment. And this is why I say we're, you know,
[01:02:44] two years after the transfer paper, we're not even at the charity moment yet is we're like 1%
[01:02:49] there on catching up. Like we're actually so, so early in this defense cycle because, you know,
[01:02:55] now we have a few dozen companies, maybe a hundred companies that have sort of new innovations.
[01:02:58] They're not integrated into the four structure meaningfully yet. There's so much more that has to
[01:03:03] happen. And I think that we have our, you know, the clear market leader now in the United States
[01:03:07] with Andrew L. And I think there's more companies internationally that are going to do really well
[01:03:11] as well. And, but I think that we're like, you know, we've sort of crossed the chasm of like,
[01:03:17] this is a thing that matters. We've crossed the chasm of the government knows this matters. We've
[01:03:21] crossed the chasm of you talked to me when Washington DC, they now understand Palantir and Andrew
[01:03:26] all they know those businesses. But in terms of the four structure changing, in terms of the way
[01:03:30] that we actually protect ourselves changing, in terms of US deterrence changing, I don't think it's
[01:03:34] changed that meaningfully. And I think after the chat GBT moment, what's going to happen is that,
[01:03:39] you know, pre-chat GBT, if you're paying attention, you noticed. After chat GBT, everyone knew
[01:03:44] this was important. Every American, every single person. And I do think we're going to get to
[01:03:47] place in defense where everybody knows that this is really, really important and that we need these
[01:03:51] companies to succeed. Do you not worry about the concentration of buyers in that world? Again,
[01:03:56] when you compare it to defense, you have every business in the world, every consumer in the world.
[01:04:01] What I really don't like with defense is actually what Brian Singerman told me about what makes
[01:04:05] Andrew also special, which is the complementary skill set of the founding team, you know,
[01:04:10] whether it be GTM into like defense and government, whether it be product, whether it be intense
[01:04:16] ops with, you know, less CEO, Brian Schimpff. And I just don't like the concentration of buyers
[01:04:22] and the selling to governments and the lack of incentive for them. Do you not worry about that?
[01:04:28] I think I definitely think about that. And I guess my framework, and this is the thesis that I've
[01:04:32] been investing behind now for the last couple of years is my framework is there are going to be
[01:04:36] fewer companies that succeed in defense for this reason. Defense is consolidated for good reason.
[01:04:40] There's a single customer. And so you need to serve that customer really well. And I think that
[01:04:45] what makes a great defense company is to be a national champion. Fundamentally, what makes a great
[01:04:49] defense company is to understand the customer and to be able to serve the customer and to be able
[01:04:53] to drive what is fundamentally a nationwide transformation that needs to happen. We are going to be
[01:04:57] talking about digital transformation. This is a digital transformation for the defense space. That's
[01:05:01] what it is. It's funny that it's a very old phrase, right? But defense actually hasn't gone through
[01:05:06] it yet. You know, it reminds me of whizz where like whizz really benefited from the rise of cloud.
[01:05:10] And you would have said, what do you mean? Like cloud was already a thing in 2017. But of course,
[01:05:14] these things take time. And so I think we're finally going through the digital transformation
[01:05:19] for defense. And I think there's going to be a few concentrated winners in each country. And we'll
[01:05:24] have a venture funded equity funded sort of R&D companies that come out and they'll get consolidated
[01:05:30] into the national champions. And in my view, Andrew is clearly the national champion in the US
[01:05:34] and credit to that team, just really phenomenal company, phenomenal visionaries. So the other two
[01:05:41] national champions that I've invested in. One is a company called Kela, which we think is going
[01:05:44] to be a national champion. It's based in Israel. The thesis is that Israel has the best people in the
[01:05:48] world for this. And they can help defend the United States and they can help defend Europe.
[01:05:52] And the second company in Europe is a company called Stark, which Sequoia is now invested in over two
[01:05:57] rounds. And that we believe can be the European national champion. And both companies have done,
[01:06:02] have done really well. But they're earlier. I speak to Alon at Kela, I think. Alon and Hamutal.
[01:06:08] Phenomenal people. Hamutal is the GM for Palantir Israel. To our conversation on talent,
[01:06:13] they've like, you know, they've become a massive talent consolidator in Israel. I think the two
[01:06:17] big talent consolidators right now in Israel are Kela and Descartes. I get in trouble for this.
[01:06:21] I don't think defense is a category. And you're like, what? A category is enough that can support
[01:06:27] an ecosystem with its breath and depth. I don't think defense is. I think there is your
[01:06:33] Andrews and maybe two to three more in the US. And I think there's, you know, Kela and Helsing
[01:06:39] and Stark. But I don't think it's like Sass where there is 30 to 50, FinTech where there is 20 to 30.
[01:06:47] Do you agree with me? I do agree with you. I mean, my objective, I've probably invested in a
[01:06:52] dozen AI companies in my career. I hope to invest in 20 more. My objective is not to invest in 20 more
[01:06:57] defense companies throughout my career. I think it's going to be a very small handful of companies.
[01:07:01] Maybe we'll do one every couple of years. But it's, there's, you have to go after the right
[01:07:06] opportunities. You have to build them the right way. And the winners are going to keep scaling.
[01:07:11] I think so many of the dollars going into its day will be lost. I see so many like, you know,
[01:07:16] McKinsey consultants who are now VCs being like, oh, my cost per kill. And I'm like, you have no
[01:07:21] freaking idea what you're talking about. Yeah, we don't think that way. I think we think in terms of
[01:07:26] defending the country in terms of having people feel safe and in terms of deterrence. So I agree with
[01:07:30] you. I don't, I don't love that type of language. Dude, I want to do a quick fire round. So I say
[01:07:35] a short statement. You give me your immediate thoughts. Does that sound okay? Perfect. So what
[01:07:40] have you changed your mind on in the last 12 months? We talked about this a bit last time, but I can
[01:07:44] close the loop for people, which is I finally decided to learn how to drive. And I got my driver's
[01:07:48] license in January, which I think is funny because it's kind of like capitulating right before the
[01:07:53] trade is in the money. I was waiting for self-driving cars for all these years. And then I finally got
[01:07:58] a license. And now, of course, self-driving cars are on the streets every day. So come on.
[01:08:04] We were equal on one thing, which is we good. Why do I encourage you?
[01:08:09] Go out and learn. It's a good experience. Well, a lot told me that I had to because I was having
[01:08:14] a baby. And I think that was pretty reasonable to help my wife and drive around the, drive around
[01:08:19] the baby. Oh, tell me, how has being a fall that changed you? You know, people, a lot of people
[01:08:24] say this and it's true. It just focuses your priority. It's so important. I think it, it makes you
[01:08:30] less abstract. Like you can think about things and abstract. Your child does not abstract. Like
[01:08:33] your child has needs in them right now. And so I think there's something that in terms of just
[01:08:38] bringing you into the present is really valuable about that. What would be your biggest advice to
[01:08:42] me on partner selection? I mean, so many people told me you had a great, wonderful marriage.
[01:08:50] And they wished to emulate it. And I was like, wow, fuck, I can't. I would say I guess
[01:08:56] pick right. I mean, my wife is smarter than me and better than me always. If your wife is
[01:09:02] smarter than you, David, I'm worried for the conversations you have at dinner.
[01:09:09] I think that, look, one thing that I've really shaped me over last few years, especially like
[01:09:13] after getting married and having a kid is, you know, people talk about the importance of shared
[01:09:17] values. And I think there's like every year that becomes more clear to me, though, that is true.
[01:09:22] And I think I met my wife very young. I didn't understand that fully when we first met,
[01:09:26] and I'm very grateful for that every day. Tell me, dude, what's your biggest mess? And what did
[01:09:31] you not see that you should have seen with the benefit of hindsight? One big financial
[01:09:35] in this is DataDog. And I worked on this before joining Sequoia. But I remember, you know,
[01:09:40] DataDog was this amazing company. The numbers were incredible. It was profitable. It was one of
[01:09:44] these businesses where you just, like, you're mouthwater's looking at the financials of that business.
[01:09:49] And I remember we lost, and we lost a dragineer. And I never confirmed this with dragineer,
[01:09:55] but the story behind it really stuck with me, which was that dragineer had this list of 20 companies.
[01:10:00] And they only worked on this 20 companies. And they had been spending years and years in years
[01:10:04] quoting DataDog. And like, this was their number one priority, and they knew it was their
[01:10:07] more party for a few years. And this was probably six years ago now that this happened. But
[01:10:12] it's a principle that has really shaped how I pursue new investments, which is,
[01:10:17] if it's not, I want to really focus my time. And I've actually adapted this to,
[01:10:21] if it's not one of the top five opportunities, that's really I really want to be spending 80%
[01:10:25] of my time. And then I want to spend the next 20% of my time on the next 15. And then after that,
[01:10:30] just like really try to focus your time. And so that actually shaped who I became as an investor.
[01:10:35] And I learned a lot from that. Penultimate one, dude. What one technology do you think is wildly
[01:10:42] undervalued? And why? I think that people are underestimating voice as an interface for AI.
[01:10:50] And, you know, we just, I think today, right before this podcast, we announced our
[01:10:55] investment in a company called Sesame, which is an AI voice company and AI conversation company.
[01:11:00] I got to work on this with Rolloff. The founder is the CEO of former CEO of Oculus. And it's
[01:11:06] Rolloff, Mark Andreessen, and Santo, who's the founder of Spark on the board. So it's a really good
[01:11:10] company. And the company you might have seen there launch a few months ago. They launched this
[01:11:15] product, this AI voice product that you can talk to you. Got a million users in a few weeks,
[01:11:19] five million minutes, like just tremendous product market fit. I always had this view that,
[01:11:23] you know, we're not always going to be like staring at our phones. And that's not like the terminal
[01:11:27] interface to technology and to AI. And I think that, you know, I always had this view, but you tried
[01:11:33] all the AI voice products and they all kind of sucked like they were not good. They were born to
[01:11:37] talk to. They don't remember anything about you. You couldn't interrupt them. You couldn't
[01:11:41] really have a dynamic conversation. Your brain just said like, this is a robot. And when Sesame
[01:11:47] came out, it was just a radically better experience within 10 minutes of seeing this technology. I
[01:11:52] knew that we were going to invest and we ultimately did. And so I think the idea that we're going
[01:11:56] to be sitting here in 10 years talking to AI, having a relationship with our AI, I think that's
[01:12:02] very likely. And I think it's, it's a little bit sci-fi right now. And I think it's going to get
[01:12:06] less so in the coming years. Well, listen, Sam Altman has opened the door to Erotica. So I mean,
[01:12:11] you never know what's coming. We're not going to end on that because that would be a weird ending
[01:12:16] to end on. The thing I want to end on, I like positivity. I fucking hate the Doomsday scenarios.
[01:12:24] What are you most excited for when you look forward 10 years? What is like, this is what gets me
[01:12:30] out of bed? I think this is a good place to end the conversation because my answer is AI. It's
[01:12:35] sort of funny because we've been talking this whole time about the ups and downs of AI and the risks
[01:12:38] and the challenges and all the complexity. But at the end of the day, AI is the most important
[01:12:43] story of our lifetime. It's going to complete, transform the world. It's going to be, you know,
[01:12:48] is this event that is sort of a once in human history kind of event? And I think it's going to be
[01:12:52] a really, really epic ride to be on. And I'm excited to be on it with you and with everybody else,
[01:12:56] because I think we're all going to, it's going to change our lives a lot. Do you know what's going
[01:13:00] to happen, David? I'm going to come to the valley. And if it's okay with you, you're going to tape
[01:13:04] me on a drive. Okay, great. We're going to get a photo for Ronald. I'm both of us in a car driving.
[01:13:12] I like it. Beautiful. Dude, you're a star. Thank you so much for joining me, man.
[01:13:16] Thanks for having me, Harry. This is very fun.