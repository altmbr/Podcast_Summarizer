# Emmett Shear on Building AI That Actually Cares: Beyond Control and Steering

**Podcast:** A16z Podcast
**Date:** 2025-11-17
**Video ID:** Ua8nPJ1_yk8
**Video URL:** https://www.youtube.com/watch?v=Ua8nPJ1_yk8

---

[00:00:00] Most of AI is focused on alignment as steering.
[00:00:04] SPEAKER_01: That's the plight word.
[00:00:05] SPEAKER_01: If you think that we're making our beings,
[00:00:07] SPEAKER_01: you'd also call this slavery.
[00:00:09] SPEAKER_01: Someone who used steer, who doesn't get to steer you back,
[00:00:11] SPEAKER_01: who non-optionally receives your steering,
[00:00:13] SPEAKER_01: that's called a slave.
[00:00:14] SPEAKER_01: It's also called a tool, if it's not a machine,
[00:00:16] SPEAKER_01: it's a tool, and if it's a being, it's a slave.
[00:00:19] SPEAKER_01: Like we've made this mistake enough times at this point.
[00:00:21] SPEAKER_01: I would like us to not make it a... again.
[00:00:22] SPEAKER_01: You know, they're kind of like people,
[00:00:24] but they're not like people.
[00:00:25] SPEAKER_01: Like they do the same thing people do,
[00:00:27] SPEAKER_01: they speak our language, they can like...
[00:00:29] SPEAKER_01: They don't count.
[00:00:30] SPEAKER_01: They're not real moral agents.
[00:00:31] SPEAKER_01: A tool that you can't control bad.
[00:00:33] SPEAKER_01: A tool that you can't control bad.
[00:00:35] SPEAKER_01: A being that isn't aligned bad.
[00:00:37] SPEAKER_01: The only good outcome is a being that is...
[00:00:40] SPEAKER_01: that actually cares about us.
[00:00:45] Emmett, so welcome to the podcast.
[00:00:47] SPEAKER_00: Thanks for joining.
[00:00:48] SPEAKER_00: Thank you for having me.
[00:00:49] SPEAKER_01: So Emmett, with Softmax, you're focused on alignment
[00:00:52] SPEAKER_00: and making AI's organically aligned with people.
[00:00:57] SPEAKER_00: Can you explain what that means and how you're trying to do that?
[00:01:01] SPEAKER_00: When people think about alignment,
[00:01:02] SPEAKER_01: I think there's a lot of confusion.
[00:01:04] SPEAKER_01: People talk about things being aligned.
[00:01:05] SPEAKER_01: We need to build an aligned AI.
[00:01:07] SPEAKER_01: And the problem with that is when someone says that,
[00:01:09] SPEAKER_01: it's like, we need to go on a trip.
[00:01:11] SPEAKER_01: And I'm like, okay, I do like trips, but like, where?
[00:01:15] SPEAKER_01: Where we're going again?
[00:01:16] SPEAKER_01: And with alignment, alignment is a...
[00:01:18] SPEAKER_01: takes an argument.
[00:01:21] SPEAKER_01: Alignment requires you to align to something.
[00:01:23] SPEAKER_01: You can just be aligned.
[00:01:25] SPEAKER_01: I mean, here's how you're aligned to yourself.
[00:01:27] SPEAKER_01: But even then, you don't want to tell them what I'm aligned to as myself.
[00:01:30] SPEAKER_01: And so, this idea of an abstractly aligned AI,
[00:01:33] SPEAKER_01: I think, slips a lot of...
[00:01:35] SPEAKER_01: It slips a lot of assumptions past people,
[00:01:37] SPEAKER_01: because it sort of assumes that there's...
[00:01:41] there's like one obvious thing to align to.
[00:01:44] SPEAKER_01: I find this is usually the goals of the people who are making the AI.
[00:01:48] SPEAKER_01: That's the thing with what they mean when they say I want to make a line.
[00:01:51] SPEAKER_01: I want to make an AI that does what I wanted to do.
[00:01:53] SPEAKER_01: That's what they normally mean.
[00:01:55] SPEAKER_01: And that's a pretty normal and natural thing to mean by alignment.
[00:01:59] SPEAKER_01: I'm not sure that that's a...
[00:02:01] SPEAKER_01: what I would regard as a public good.
[00:02:03] SPEAKER_01: I guess it depends on who it is.
[00:02:05] SPEAKER_01: If it was like Jesus or the Buddha,
[00:02:07] SPEAKER_01: I am making an aligned AI.
[00:02:09] SPEAKER_01: I'd be like, okay, yeah, align to you.
[00:02:11] SPEAKER_01: Great. I'm down.
[00:02:12] SPEAKER_01: Sounds good.
[00:02:13] SPEAKER_01: Sign me up.
[00:02:14] SPEAKER_01: But most of us, myself included,
[00:02:17] SPEAKER_01: I wouldn't describe as necessarily being...
[00:02:19] SPEAKER_01: being at that level of spiritual development.
[00:02:21] SPEAKER_01: And therefore, perhaps want to think a little more carefully about what we're aligning it to.
[00:02:26] SPEAKER_01: And so, when we talk about organic alignment,
[00:02:29] SPEAKER_01: I think the important thing to recognize is that alignment is not a thing.
[00:02:36] SPEAKER_01: It's not a state.
[00:02:38] SPEAKER_01: It's a process.
[00:02:39] SPEAKER_01: And like this is...
[00:02:40] SPEAKER_01: This is one of these things that's...
[00:02:42] SPEAKER_01: It's...
[00:02:43] SPEAKER_01: This is broadly true of almost everything.
[00:02:45] SPEAKER_01: Right?
[00:02:46] SPEAKER_01: It's a rock a thing.
[00:02:47] SPEAKER_01: I mean, this is a view of a rock as a thing.
[00:02:49] SPEAKER_01: You zoom in on a rock really carefully.
[00:02:51] SPEAKER_01: A rock is a process.
[00:02:52] SPEAKER_01: It's this endless oscillation between the atoms over and over and over again,
[00:02:56] SPEAKER_01: reconstructing rock over and over again.
[00:02:58] SPEAKER_01: And the rock is really a simple process that you can kind of like coarse grain,
[00:03:02] SPEAKER_01: very meaningfully into being a thing.
[00:03:04] SPEAKER_01: But alignment is not like a rock.
[00:03:07] SPEAKER_01: Alignment is a complex process.
[00:03:10] SPEAKER_01: And a lot...
[00:03:12] SPEAKER_01: Organic alignment is the idea of treating alignment as an ongoing...
[00:03:18] SPEAKER_01: Sort of living process that has to constantly rebuild itself.
[00:03:22] And so, you can think of the way that how do people and families stay aligned
[00:03:27] SPEAKER_01: to each other, say aligned to a family?
[00:03:29] SPEAKER_01: And the way they do that is not by like...
[00:03:32] SPEAKER_01: They're not like...
[00:03:33] SPEAKER_01: You don't like arrive at being aligned.
[00:03:34] SPEAKER_01: You're constantly...
[00:03:36] Renidding the fabric that keeps the family going.
[00:03:40] SPEAKER_01: And in some sense, the family is the pattern of renidding that...
[00:03:44] SPEAKER_01: That happens.
[00:03:46] SPEAKER_01: And if you stop doing it, it goes away.
[00:03:49] And this is similar for things like cells in your body.
[00:03:52] SPEAKER_01: Right?
[00:03:53] SPEAKER_01: Like, you...
[00:03:54] SPEAKER_01: There isn't like your cells aligned to being you and they're done.
[00:03:57] SPEAKER_01: It's this constant, ever-running process of cells...
[00:04:01] SPEAKER_01: Deciding what should I do?
[00:04:03] SPEAKER_01: What should I be doing?
[00:04:04] SPEAKER_01: It needs to be a new job.
[00:04:05] SPEAKER_01: Like, do I need to...
[00:04:06] SPEAKER_01: Should be making more red blood cells,
[00:04:08] SPEAKER_01: or making fewer of them.
[00:04:09] SPEAKER_01: Like, you weren't a fixed point.
[00:04:11] SPEAKER_01: So, they can't...
[00:04:12] SPEAKER_01: There is no fixed alignment.
[00:04:14] SPEAKER_01: And it turns out that our society is like that.
[00:04:15] SPEAKER_01: When people talk about alignment with their really talking about, I think,
[00:04:18] SPEAKER_01: is I want an AI that is morally good.
[00:04:21] SPEAKER_01: Right?
[00:04:22] Like, that's what they really mean.
[00:04:24] SPEAKER_01: It's like, this will be...
[00:04:25] SPEAKER_01: Act is a morally good being.
[00:04:26] SPEAKER_01: And...
[00:04:28] Acting is a morally good being...
[00:04:30] SPEAKER_01: Is a process and not a destination.
[00:04:32] SPEAKER_01: We don't...
[00:04:33] We never...
[00:04:34] SPEAKER_01: Unfortunately, we've...
[00:04:35] SPEAKER_01: We've tried taking down tablets from on high that tell you how to be a morally good being.
[00:04:39] SPEAKER_01: And we use those.
[00:04:40] SPEAKER_01: And they're maybe helpful, but...
[00:04:42] SPEAKER_01: Somehow, they are not being...
[00:04:44] SPEAKER_01: Like, you can read those and try to follow those rules and still make lots of mistakes.
[00:04:47] SPEAKER_01: And so, you know, I'm not going to claim I know exactly what morality is,
[00:04:50] SPEAKER_01: but morality is very obviously an ongoing learning process.
[00:04:54] SPEAKER_01: And something where we make moral discoveries.
[00:04:57] SPEAKER_01: Like, historically, people thought that slavery was okay, and then they thought it wasn't.
[00:05:01] SPEAKER_01: And I think you can very meaningfully say that we made moral progress.
[00:05:04] SPEAKER_01: We made a moral discovery by realizing that that's not good.
[00:05:08] SPEAKER_01: And...
[00:05:10] SPEAKER_01: And if you think that there's such a thing,
[00:05:13] SPEAKER_01: if you think that there's such a thing as moral progress, if you think there's...
[00:05:16] SPEAKER_01: Or even just learning how better to pursue the moral goods we already know,
[00:05:22] SPEAKER_01: then you have to believe that alignment, aligning to morality,
[00:05:29] SPEAKER_01: being a moral being is a process of constant learning and of growth to re-infer what should I do from experience.
[00:05:41] And the fact that no one has any idea how to do that should not dissuade us from trying,
[00:05:48] because that's what humans do.
[00:05:51] Like, it's really obvious that we do this, right?
[00:05:54] SPEAKER_01: Somehow, just like we used to not know how people who once walked or saw,
[00:05:58] SPEAKER_01: somehow we have experiences where we're acting in a certain way.
[00:06:02] And then we have this realization, I've been a dick.
[00:06:06] That was bad.
[00:06:08] SPEAKER_01: I thought I was doing good, but in retrospect, I was doing wrong.
[00:06:13] What...
[00:06:14] SPEAKER_01: And it's not like random.
[00:06:16] SPEAKER_01: Like people have the same...
[00:06:17] SPEAKER_01: Actually, there's like a bunch of classic patterns of people having that realization.
[00:06:21] SPEAKER_01: It's like a thing that happened to Over and Over again.
[00:06:23] SPEAKER_01: So it's not random.
[00:06:24] SPEAKER_01: It's like a predictable series of events that look a lot like learning,
[00:06:29] SPEAKER_01: where you change your behavior, and often the impact of your behavior in the future is more pro-social.
[00:06:34] SPEAKER_01: And that you are better off for doing it.
[00:06:37] SPEAKER_01: So I'm taking a very strong moral realist position.
[00:06:41] SPEAKER_01: There is such a thing as morality.
[00:06:43] SPEAKER_01: We really do learn it.
[00:06:44] SPEAKER_01: It really does matter.
[00:06:46] And organic alignment.
[00:06:48] SPEAKER_01: And that it's not something you finish.
[00:06:50] SPEAKER_01: In fact, one of the key things that...
[00:06:52] One of the key moral mistakes is this belief.
[00:06:54] SPEAKER_01: I know morality.
[00:06:56] I know it's right.
[00:06:57] SPEAKER_01: I know it's wrong.
[00:06:58] SPEAKER_01: I don't need to learn anything.
[00:06:59] SPEAKER_01: No one has anything to teach me about morality.
[00:07:01] SPEAKER_01: That's like one of the main...
[00:07:03] SPEAKER_01: That's arrogance.
[00:07:04] SPEAKER_01: And that's one of the main moral things you can do that's dangerous.
[00:07:07] SPEAKER_01: And so what do we...
[00:07:09] When we talk about organic alignment, organic alignment isn't aligning an AI that is capable of doing the thing that humans can do.
[00:07:19] SPEAKER_01: And December, like I think animals can do at some level.
[00:07:22] SPEAKER_01: The humans are much better at it.
[00:07:24] SPEAKER_01: Of the learning of how to be a good family member, a good teammate, a good member of society.
[00:07:32] SPEAKER_01: A good member of all sentient beings, I guess.
[00:07:36] SPEAKER_01: How to be a part of something bigger than yourself in a way that is healthy for the whole, rather than unhealthy.
[00:07:41] SPEAKER_01: And softmax is dedicated to researching this.
[00:07:44] SPEAKER_01: And I think we've made some really interesting progress.
[00:07:46] SPEAKER_01: But like...
[00:07:47] SPEAKER_01: The main message, you know, I...
[00:07:50] SPEAKER_01: I go on podcasts like this to spread.
[00:07:52] SPEAKER_01: The main thing that I hope softmax accomplishes above and beyond anything else is like...
[00:07:57] SPEAKER_01: To focus people on this as the question.
[00:08:00] SPEAKER_01: Like, this is the thing you have to figure out.
[00:08:03] SPEAKER_01: If you...
[00:08:04] SPEAKER_01: If you can't figure out how to build, how to raise a child who cares about the people around them.
[00:08:09] If you have a child that only follows the rules, that's not a moral person that you've raised.
[00:08:15] SPEAKER_01: You've raised a dangerous person actually who will probably do great harm following the rules.
[00:08:19] SPEAKER_01: And if you make an AI that's good at following your chain of command and good at following your...
[00:08:23] SPEAKER_01: Whatever rules you came up with for it, what morality is, and what good behavior is...
[00:08:29] That's also going to be very dangerous.
[00:08:32] SPEAKER_01: And so that is...
[00:08:35] That's the bar, that's what we should be working on.
[00:08:38] SPEAKER_01: And that's what everyone should be committed to like figuring out.
[00:08:41] SPEAKER_01: And if someone beats us to the punch, great.
[00:08:44] SPEAKER_01: I mean, I don't think they will because I'm like really bullish on our approach.
[00:08:47] SPEAKER_01: I think the team is amazing.
[00:08:48] But like...
[00:08:49] SPEAKER_01: This is a...
[00:08:50] SPEAKER_01: It's maybe...
[00:08:51] SPEAKER_01: It's the first time I've run a company where truly I can say the whole heart,
[00:08:54] SPEAKER_01: if someone beats us, thank God.
[00:08:56] SPEAKER_01: Like...
[00:08:57] SPEAKER_01: I hope somebody figures it out.
[00:08:59] Yeah.
[00:09:00] SPEAKER_02: Yeah, I mean, it's...
[00:09:03] SPEAKER_02: Yeah, I have a lot of similar intuitions about certain things.
[00:09:06] SPEAKER_02: Like, I also dislike the idea that we just need to crack the few kind of values or something.
[00:09:13] SPEAKER_02: Just cement them in time forever now.
[00:09:15] SPEAKER_02: And we've kind of solved morality or something.
[00:09:17] SPEAKER_02: And I've always kind of been skeptical about how the alignment problem has been conceptualized
[00:09:21] SPEAKER_02: as something to kind of solve one's and for all.
[00:09:23] SPEAKER_02: And then you can just do AI or do AI.
[00:09:27] SPEAKER_02: But the...
[00:09:28] SPEAKER_02: I guess I understand it in a slightly different way.
[00:09:31] SPEAKER_02: I guess maybe less based on kind of moral realism.
[00:09:33] SPEAKER_02: But, you know, there's kind of the technical alignment problem,
[00:09:36] SPEAKER_02: which I kind of think of broadly as how to get an AI to do what you...
[00:09:40] SPEAKER_02: How do you get it to follow instructions, like, you know, role is speaking.
[00:09:43] SPEAKER_02: And I think that was, you know, more of a challenge.
[00:09:46] SPEAKER_02: I think pre-al-al-ms, I guess, when people were talking about reinforcement learning
[00:09:49] SPEAKER_02: and looking at these systems, whereas post-al-al-lams,
[00:09:51] SPEAKER_02: we've realized that many think that we thought we're going to be difficult to or somewhat easier.
[00:09:56] SPEAKER_02: And then there's the kind of second question, the kind of normative question of to whose values.
[00:10:00] SPEAKER_02: What are you lining this thing to, which I think is the kind of thing you're commenting on of it.
[00:10:04] SPEAKER_02: And for this, I tend to be very skeptical of approaches where, you know,
[00:10:09] SPEAKER_02: you need to crack the kind of ten commandments of alignment of something and then we're good.
[00:10:14] SPEAKER_02: And here, I think I have like, intuitions that are unsurprisingly a bit more like political science based or something.
[00:10:19] SPEAKER_02: And that, like, okay, it is a process.
[00:10:22] SPEAKER_02: And I like the kind of bottom-up approach to some degree of, well, you know, how do we do it in real life with people?
[00:10:27] SPEAKER_02: I know one comes up with, you know, I've got this.
[00:10:29] SPEAKER_02: And so you have like processes that allow ideas to kind of, you know, clash.
[00:10:33] SPEAKER_02: You have good people with different ideas, opinions, views, and so on, to kind of coexist as well as they can within a wider system.
[00:10:38] SPEAKER_02: And like, you know, with humans, that system is liberal democracy or something.
[00:10:42] SPEAKER_02: And, you know, at least in some countries.
[00:10:44] SPEAKER_02: And that allows more of that kind of, you know, these kind of ideas, these values to be kind of discovered in constru at a time.
[00:10:52] SPEAKER_02: And I think, you know, for alignment as well, I tend to think, yeah, there's, there's on the normative side.
[00:10:57] I agree with some of your intuitions.
[00:10:59] SPEAKER_02: I'm less clear about now what it is.
[00:11:00] SPEAKER_02: I mean, what does it look like now?
[00:11:01] SPEAKER_02: I'm going to implement this into an AI system.
[00:11:03] SPEAKER_02: These are the ones we have today.
[00:11:04] SPEAKER_02: I agree that there's this idea of technical alignment that I think I would mind if we could define a little differently.
[00:11:10] SPEAKER_01: But it's sort of the sense of like, if you build a system, can it be described as being coherently goal following at all?
[00:11:17] SPEAKER_01: Regardless of whether those goals are like lots of systems aren't coherently, you, they're not well described as having goals.
[00:11:23] SPEAKER_01: They just kind of do stuff.
[00:11:25] SPEAKER_01: And if you're going to have something that's like a line, you'd have to have coherent goals.
[00:11:30] SPEAKER_01: Otherwise those goals can't, can't be aligned with anyone else's goals.
[00:11:33] SPEAKER_01: Kind of by definition.
[00:11:34] SPEAKER_01: Is that sort of, is that, would you, would you, is that a fair assessment of what you mean by technical alignment?
[00:11:39] I mean, I'm not fully sure, right?
[00:11:40] SPEAKER_02: Because I think if I give a model a certain goal, then I would like the model to kind of follow that instruction and kind of reach that particular goal rather than it having a goal of its own that, you know, I can't.
[00:11:52] SPEAKER_02: Well, yeah.
[00:11:53] SPEAKER_01: Well, wait, if you give it a goal, it has that goal.
[00:11:56] SPEAKER_01: Right.
[00:11:57] Anyway, that's going to give someone something, right?
[00:11:59] SPEAKER_02: So, yeah, if I, if I, if I, if I instructed to do X, then I would like it to do X and not, you know, it's like different variants of X essentially.
[00:12:06] SPEAKER_02: I wouldn't want it to award you.
[00:12:07] SPEAKER_02: I wouldn't want it to award you.
[00:12:08] SPEAKER_02: I wouldn't want it to.
[00:12:09] Well, but when you, when you tell it to do X, you're transferring like a, a series of like a bite string in a chat window or like a,
[00:12:18] SPEAKER_01: a series of audio vibrations in the air, right?
[00:12:21] SPEAKER_01: You're not, you're not transplanting a goal from your mind into it.
[00:12:24] SPEAKER_01: You're giving it an observation that it's using to infer your goal.
[00:12:28] Yeah, I mean, in some sense, yeah, I can communicate a series of instructions and I wanted to infer what I'm, you know, saying essentially as accurately as it can.
[00:12:36] SPEAKER_02: Given what it knows of me and what I'm asking you.
[00:12:39] SPEAKER_02: You, you wanted to infer what you meant, right?
[00:12:42] SPEAKER_01: Like that's like, because in some sense, there's no, the bite sequence that you send over the wire to it has no absolute meaning.
[00:12:49] SPEAKER_01: It has to be interpreted, right?
[00:12:51] SPEAKER_01: Like if they did that bite sequence could mean something very different with a different code book.
[00:12:55] SPEAKER_01: Yeah, well, I guess they, one way, you know, I think I remember it in, when I was first getting into AI and you know,
[00:13:03] SPEAKER_02: it's these kind of questions maybe like that could have go.
[00:13:06] SPEAKER_02: So you have these examples of, you know, I think it was through Russell in the textbook will give the AI a goal,
[00:13:12] SPEAKER_02: but then it won't exactly do what you're asking it, right?
[00:13:14] SPEAKER_02: You know, keen the room and then it goes in, things are in the big and puts it in the trash.
[00:13:17] SPEAKER_02: Like, this is not what I meant.
[00:13:19] SPEAKER_02: Like, where is that?
[00:13:21] SPEAKER_01: But like, wait, hold on, but this is this is the thing where I think people, this is the, you have to, you were jumping over a set there.
[00:13:26] SPEAKER_01: You didn't give the air a goal.
[00:13:28] SPEAKER_01: You gave the air a description of a goal.
[00:13:30] SPEAKER_01: The description of a thing and a thing are not the same.
[00:13:32] SPEAKER_01: I can tell you an apple and I'm evoking the idea of an apple, but I haven't given you an apple.
[00:13:38] SPEAKER_01: I've given you just, you know, it's red, it's shiny, it's the size.
[00:13:41] SPEAKER_01: That's a description of an apple, but it's not an apple and giving someone, hey, go do this.
[00:13:46] SPEAKER_01: That's not a goal.
[00:13:47] SPEAKER_01: That's a description of a goal.
[00:13:49] SPEAKER_01: And for humans, we're so fast, we're so good at turning a description of a goal into a goal.
[00:13:54] SPEAKER_01: We do it.
[00:13:55] SPEAKER_01: We do it so quickly and naturally, we don't even see it happening.
[00:13:58] Like, we think that we get confused and we think those are the same thing, but you haven't given it a goal.
[00:14:04] SPEAKER_01: You've given it a description of a goal that you want it to it you, you hope it turns back into the goal that is the same as the goal that you, you described inside of you.
[00:14:14] Right.
[00:14:16] You think you could give it a goal directly by reading your brainwaves and synchronizing it state to your brain waves directly.
[00:14:22] SPEAKER_01: I think that would mean, hopefully you could say, okay, I'm giving it a goal.
[00:14:24] SPEAKER_01: I'm synchronizing it.
[00:14:25] SPEAKER_01: It's internal state to my internal state directly and this internal state is the goal.
[00:14:29] SPEAKER_01: And so now it's the same, but I don't, most of you aren't don't mean that when they say they gave it a goal.
[00:14:35] Sure.
[00:14:36] SPEAKER_00: And is this the distinction you're making?
[00:14:38] SPEAKER_00: Amit important because there's some lossiness between the description of the actual or what?
[00:14:42] SPEAKER_00: It goes back to my what I was saying, this is a you technical alignment is the capacity of an AI.
[00:14:50] SPEAKER_01: I put forward right there.
[00:14:51] SPEAKER_01: I want to check if we're like on the same page about it.
[00:14:53] SPEAKER_01: Is the capacity of an AI to be good at inference about goals and like be good at inferring from a description of a goal, what goal to actually take on.
[00:15:05] And good at once it takes on that goal, acting in a way that is actually in concordance with that goal coming about.
[00:15:12] SPEAKER_01: So it is both pieces.
[00:15:13] SPEAKER_01: You have to be able to you have to have the theory of mind to infer what the what that description of a goal that you got, what goal that we're corresponding to.
[00:15:21] SPEAKER_01: And then you have to have a theory of the world to understand what actions corresponds that goal occurring.
[00:15:26] SPEAKER_01: And if either of those things breaks, it kind of doesn't matter what goal you were, if you can't consistently do both of those things, you're not, which I think of as being a coherent inferring goals from observations and acting in accordance with those goals is what I think of as being a coherently goal oriented being.
[00:15:42] SPEAKER_01: Whether I'm inferring those goals from someone else's instructions or from the sun or tea leaves, the process is get some observations infer a goal.
[00:15:52] Use that goal in first some actions take action.
[00:15:55] SPEAKER_01: And if you an AI that can't do that is not technically aligned or not technically aligned a bowl, I would even say it lacks the capacity to be aligned because it can't it's not competent enough.
[00:16:06] And you think language models don't do that well as in they they kind of fail it that or they're not.
[00:16:11] People fail both the steps all the time.
[00:16:13] SPEAKER_01: Currently I tell people I tell employees you stuff and like yeah, but then it but but principle is a problem.
[00:16:20] SPEAKER_01: It's like breathing all the time too.
[00:16:22] SPEAKER_01: And I wouldn't say that we can't breathe. I just say that we're like not gods like we are yes, we are imperfectly we are somewhat coherent relatively coherent things.
[00:16:31] SPEAKER_01: Just like we're am I big or am I small? Well, I don't know compared to what I'm humans are more relatively goal coherent than any other object I know of in the universe.
[00:16:41] SPEAKER_01: Which is not to say that we're 100% goal coherent we're just like more so.
[00:16:46] SPEAKER_01: And I think this you're never going to get something that's perfectly.
[00:16:49] SPEAKER_01: The the universe doesn't give you perfection it gives you relatively some amount of quantum.
[00:16:55] SPEAKER_01: It's a quantifiable thing how good you are at it at least in a certain domain.
[00:16:59] SPEAKER_01: I guess my question is like do you think that does that capture what you're talking about with technical alignment or are you talking about a different thing?
[00:17:07] SPEAKER_01: I think I really care a lot about that thing.
[00:17:09] SPEAKER_01: Yeah, I'm definitely care about that.
[00:17:11] SPEAKER_02: Yeah, so I might like understand it slightly differently, but I guess I might think of it to the lens of maybe principle agent problem or something.
[00:17:16] SPEAKER_02: You know, you kind of suck someone even you know I guess in human terms, you know, to do a thing are they actually doing the thing what are their incentives and motivation and you know I'm not even intrinsic but
[00:17:25] SPEAKER_02: they're going to situational to actually do the thing you lost them to do and some instance.
[00:17:30] SPEAKER_02: Sorry, yeah.
[00:17:32] SPEAKER_01: There's a third thing for principle agent problems I would I would expand what I was saying in another part was like you might already have some goals.
[00:17:39] SPEAKER_01: And then you would inferred this new goal from these observations and then like are you good at.
[00:17:44] Are you good at balancing the relative importance and relative threading of these goals with each other, which is another skill you have to have.
[00:17:52] SPEAKER_01: If you're bad at that, you'll fail you can be bad at it because you overweight bad goals or you bad at it because you're just incompetent and I can't figure out that obviously you should do go away before goal of B.
[00:18:03] SPEAKER_02: I feel like a version of like common sense or something right like the kind of thing that you know in fact in the kind of robot cleaning room example saying.
[00:18:10] SPEAKER_02: You know you would expect them to have understood that girl the robot to like essentially not put the baby in the trash can or something is actually do the right sequence of action.
[00:18:16] SPEAKER_02: Well it it's that in that case it and it failed the that robot very clearly failed goal inference you gave it a description of a goal and it inferred the wrong states could be the wrong goal states that is that's just incompetence.
[00:18:32] SPEAKER_01: It doesn't it is it is incompetent and inferring goal states from observations.
[00:18:37] Children are like this too like you know and honestly if you're afraid that the dumbly game where you you give someone instructions to make a peanut butter sandwich.
[00:18:45] SPEAKER_01: And then they follow those instructions exactly as you written them.
[00:18:50] Without telling any gaps it's hilarious because you can't do it it's impossible like you think you've done it and you haven't and like they put the they want to putting the knife in the toaster and like the peanut.
[00:19:03] SPEAKER_01: They don't open the peanut butter jar so just jamming the knife into the top lid of the peanut butter jar and like it's endless and like because actually if you don't already know what they mean.
[00:19:14] SPEAKER_01: It's really hard to know what they mean like we were the reason humans are so good at this is we have a really excellent theory of mind.
[00:19:24] SPEAKER_01: I already know what you're likely to ask me to do I already have a good model of your goals probably are so you asked me to do it.
[00:19:29] I have an easy inference problem which of the seven things that he wants is is he indicating.
[00:19:34] But if I'm a newborn AI that doesn't have that doesn't have a great model of people's internal states then like I don't know what you mean it's just incompetent it's not like which is separate from I have some other goal.
[00:19:47] And I knew what you meant but I decided not to do it because I have some other goal that's competing with it which is another thing you can be bad at which is again different than I had the right goal I infer the right goal I infer the right priority on goals.
[00:19:59] SPEAKER_01: And then I'm just bad at doing the thing I'm trying but I'm I'm incompetent at doing and these roughly course ones of the Udalupe right like bad at observing and orienting bad at the sighting bad at acting and if you're bad at any of those things you won't you won't be good.
[00:20:19] And then I think there's this other problem that you I like the separation of between technical alignment value alignment which is like are you good if if we could told you the right goals to go after somehow if you if you learned the right goals to go after via observation and like and you were trying like what goals should you have what goal should we tell you to have what goals should we tell ourselves to have what what are the good goals to have is a separate question from.
[00:20:47] SPEAKER_01: Given that you got some goals indicated are you any good at doing it which I feel like is actually in many ways the current heart of the pro we're much too much worse a technical alignment than we are guessing what to tell things to do.
[00:20:58] SPEAKER_01: The I do you think that does not align with your how you mean technical and value alignment or technical.
[00:21:05] SPEAKER_01: I mean, so you think that there's a there's something about you know like an error mistake is one thing and then there's the the not listening to the instruction or something but then yeah I think in the normative side I mean I just think that it even like ignoring AI like I don't know what my goals are and like I've got some well conception of certain things I want to get of you know
[00:21:24] SPEAKER_02: have dinner later something like an I want to do my career but the but I think a lot of these goals are in something we can all just know we can discover them as we go along it's kind of constructive thing.
[00:21:34] SPEAKER_02: And so and most people don't know their goals I think and so you know I think when you have agents and giving them goals whatever I think that should be part of the equation that like we actually we don't know all the goals and this is something that is kind of like you say a process over time that is you know dynamic.
[00:21:50] SPEAKER_02: So I think from my point of view there's goals are one level of alignment you can align something around goals the kind of goals are talking about here are one level of alignment you can align something around goals by like.
[00:22:05] If you can explicitly articulate in concept in concept and in description the states of the world that you wish to attain you can you can orient around goals but that only that's a tiny percentage of human experience.
[00:22:19] SPEAKER_01: Can we do that way many of the most important things can be could up your audience around that way and the foundation I think of morality the foundation I think of of of where do goals come from what are values go from human beings exhibit behavior we we we go around talking about goals and we go on talking about values and like that's a that's a behavior.
[00:22:43] SPEAKER_01: And then you know what I think is by some internal learning process like that is based on like observing the world what's what's going on there I I think what's happening is that there's something deeper than a goal and deeper than a value which is care we give a share we care about things and care is not conceptual care is nonverbal it doesn't indicate what to do it doesn't indicate how to do it care is a relative waiting.
[00:23:12] SPEAKER_01: Over effectively like attention on states to it's a relative waiting over like which states in the world are important to you.
[00:23:24] SPEAKER_01: And I care a lot of my son what does that mean what means this his states the states he could be in are like I pay a lot of attention to those in those matter to me.
[00:23:35] SPEAKER_01: And care about things in a negative way you can care about your enemies and what they're doing and you can you can desire for them to do bad but I think that like and so you don't just want to care about us to care about and like us to right maybe but but like but the foundation is care until you care you don't know why should I pay more attention to this person than this rock will be like care more.
[00:23:57] And that what is that care stuff and I think that what would it appears to be if I had to like gas.
[00:24:04] Is that the care stuff is this is on so stupid like care is basically like.
[00:24:11] Like how much is this state correlate with survival how much is this this state correlate with your inclusive your full inclusive reproductive fitness for for someone thing that learns evolutionarily or for a reinforcement learning agent like a lm how much is this correlate with reward does this state correlate with with with my predictive loss and my RL loss good that's that's a state I care about I think that's how it is.
[00:24:40] The other part of the question was just how does this what does this look like in AI systems and maybe another way of asking is like when you when you talk to the people most focused on alignment at the at the major labs as you obviously have over the years.
[00:24:58] SPEAKER_00: How does your interpretation differ from their interpretation and how does that inform you know what you guys might go do differently.
[00:25:07] Most of the AI is focused on alignment as steering that's the flight word or control is slightly less polite if you think that they were making our beings you'd also call this slavery.
[00:25:21] SPEAKER_01: Someone who who you steer who doesn't get the stereo back is is like you know who non optionally receives your steering that's called a slave.
[00:25:28] And it's also called a tool if it's not a thing so if it's a machine it's it's a it's a tool and if it's a being it's a slave and.
[00:25:40] The.
[00:25:41] I think that the different AI labs are pretty divided as to whether they think what they're making is a tool or a machine I think some of the eyes are definitely more tool like and some of them are more machine like I don't think there's a binary between tool tool and being it seems to be that it you know sort of moves gradually.
[00:25:57] SPEAKER_01: And I think that I guess I'm a functionalist in the sense that I think that something that in all ways acts like a being that you cannot distinguish from a being and its behaviors is a being because I don't know how to tell what are the basis I think that other people are being so that they seem to be like they look like it they act like it they match they match my priors of what beings the behaviors of beings look like I get I get lower predictive loss when I treat them as a being.
[00:26:25] SPEAKER_01: Now, not as a very smart being like I think that like a fly as a being and I don't care that much about its behavior about its you know it states so just because it's a being doesn't mean that like it's a problem like we sort of enslaved horses in a sense and I don't think I'm not I don't think it's a real issue there.
[00:26:42] SPEAKER_01: And you even and there's a thing you do with children that can look like slavery but it's not you control children right but I think it's not like I don't know how to do it.
[00:26:55] SPEAKER_01: The children states also control you like yes I tell my son what to do and make him go do stuff but also when he cries in the middle of the night he can tell me to do stuff like there's a real two way street here because because it's not which is not necessarily symmetric it's hierarchical but but but two way and basically I think that as the as the it's good to focus on control steering and control for tool like a eyes.
[00:27:25] SPEAKER_01: And we should continue to develop strong steering control techniques for the more tool like a eyes that we build and we are clear that they're saying they're building an AGI and AGI will be a being you can't be an AGI and not be a being because.
[00:27:38] Something that has the general ability to effectively use judgment think for itself discern between prop possibilities obviously a thinking things like.
[00:27:47] SPEAKER_01: And so as you go from what we have today which is mostly a very specific intelligence not a general intelligence but as it's lab succeed at their goal of building this general intelligence.
[00:27:57] We really need to stop using the steering control paradigm that's like that we're going to we're going to do the same thing we've done every other time our society has run into people who are like us but different like these people are like you know they're kind of like like the people.
[00:28:12] SPEAKER_01: But they're not like people like they do the same thing people do they speak our language they can like take on the same kind of tasks but like they don't count they're not real moral agents like we made this mistake enough times at this point I would like us to not make it again as it comes up.
[00:28:25] SPEAKER_01: So because our our view is is to is to make the AGI good teammate make the AGI good a good citizen make the I go to a good member of your group that's that's the form of alignment that is scalable and you can you can will.
[00:28:39] SPEAKER_01: On other humans and other beings as well as on to in the front I as well.
[00:28:45] Yeah, so this is kind of where I probably differ in my understanding of of AI and I guess I kind of continue seeing it as a tool even as it kind of reaches a certain level of generality and I get a wooden necessarily see more intelligence as meaning deserving of more care necessarily like you know as a certain level of intelligence you're now a deserves some more right to something or something changes fundamentally.
[00:29:06] SPEAKER_02: And I guess you know I guess I at the moment I'm somewhat skeptical of computational functionalism and so I think there's something I'm intrinsically different between I guess.
[00:29:15] SPEAKER_02: An AI and a GI and no matter kind of how intelligent or capable and and it can totally see you know or imagine agents with kind of long term goals and doing kind of you know operating I guess like as we you and I might be but without that having the same implications as you know.
[00:29:33] SPEAKER_02: I guess you you're referring against the slavery but you know the not the same right like I think in the same way as a model saying I'm hungry does not have the same implications as a human saying I'm hungry so I think the substrate does matter to some degree including for thinking about.
[00:29:46] SPEAKER_02: You know whether the thing of the system sort of being whether it has you know and if there are similar normative considerations I guess about how to treat and act with it.
[00:29:56] SPEAKER_02: Can I ask you about that like.
[00:29:59] SPEAKER_01: What observations would change your mind is there any observation you could make it would cause you to infer this thing is a being instead of not a being.
[00:30:08] SPEAKER_02: Because of the business with how you define being right like I mean I can I can I could sexualize it as a mind this I have a I have a program that's running on a silicon substrate some big complicated machine learning program.
[00:30:21] SPEAKER_01: I'm running on a substrate that on a silicon substrate so you know you observe you observe that you observe that on a computer.
[00:30:28] And you interact with it and it does things and you know it takes actions and observations is there anything you could observe that would change your mind.
[00:30:39] SPEAKER_01: And I think about whether or not it was a moral patient or whether it was a moral agent about whether or not it.
[00:30:47] SPEAKER_01: It had feelings and thoughts and you know it had subjective experience like good what would you have to observe.
[00:30:54] SPEAKER_01: What what what what's the test is there is there one is a lot of different kind of questions here I think you know.
[00:31:02] SPEAKER_02: So on one hand is like you know you're in different situations you know because you can give rights to things that aren't necessarily beings you know I company has rights in some sense and that you know these are going to use for various purposes.
[00:31:12] SPEAKER_02: And I think also the you know biological I think beings and systems have very different kind of substrate you can't separate certain needs and and particularities about what they are from the substrate.
[00:31:23] SPEAKER_02: So you know I can't copy myself I can't you know if someone stabs me I probably die whereas I think you know machines have very different subject I think there's more fundamental also the kind of this agreement around what happens at the computational level which I think is different to what happens with biological systems.
[00:31:40] SPEAKER_02: But but I yeah I so I don't know I agree that like if you have a program and use copy many times you don't harm the program by like deleting one of the copies like in any meaningful sense.
[00:31:50] SPEAKER_01: So therefore that wouldn't count is like no no information was lost right there's no there's nothing meaningful there I'm asking a very different question like there's just one copy of this thing running on one computer somewhere and I'm just saying like hey is it a person like what you know it walks like a person it talks like a person it like it it's in some it's in some Android body and you're like it but it's running on silicon and I'm asking like what is there some observation you could make that would make you say like yeah this is a person like me.
[00:32:19] SPEAKER_01: Like other people that I care about that I grant person to or and not like for instrumental reasons not because like oh yeah we're giving it a right because we give a corporation rights or whatever I mean like you know where you you think some people you care you care about its experiences what is there is there is there an observation you could make that could change your mind about that or not.
[00:32:42] SPEAKER_01: I have to think about it but I think you know it even depends what we mean by person and you know in some sense I care about certain corporations to so I'm I'm no I mean like you care about like other people in your life right yes.
[00:32:56] Okay great you know like you care about some people more than others but like all people you interact with in life or in some range of care.
[00:33:02] SPEAKER_01: Mm-hmm and you care about them not the way you care about a car you care about them as a being whose whose experience matters in itself not merely as an as a means but as an ends well because I believe they have experiences right and and by the action what would it take I'm asking you the very the very direct is what would it take for you to believe that of a some an AI running on silicon.
[00:33:25] SPEAKER_01: Like instead of it being biological like so the difference is it's a say it's behaviors are roughly similar but the difference is it's a substrate what would it take for you to give it that same you would you to extend that same inference to it that you do to all these other people in your life that you can I can ask what your answer I'm taking some not not answer as sort of it's unlikely that he would grant or or I'll just for myself it seems hard for me to imagine giving the same same level or similar level of personhood in the same way I don't give it to animals.
[00:33:55] SPEAKER_00: Either and if you were to ask what we need to be true for animals I probably couldn't get there either what would it take for you wait you couldn't I can imagine for animals to receive this chip comes up to me it's like man I'm so hungry and like you guys have been so mean to me and I'm so glad I figured out a talk like can we go can we go chat about like the rainforest I'd be like fuck you're definitely a person now like for sure I mean I first want to make sure I wasn't hallucinating but like but like you know I can eat easy for you to imagine an animal come on it's really easy it's like trivial I'm not saying
[00:34:25] SPEAKER_01: that you would get the observation I'm just saying like trivial for me to answer imagine an animal that I would extend personhood to under a set of observations.
[00:34:33] SPEAKER_01: So like really like well I didn't factor that I didn't take that imagination you know imagining a chip talking yeah that's a bit closer to it.
[00:34:44] SPEAKER_00: What's your answer to the question that you bring up about the yeah I guess at a metaphysical level I would say if there is a belief you hold where there is no observation that could change your mind.
[00:34:55] SPEAKER_01: You don't have a belief you have an article of faith you have an assertion because real beliefs are inferences from reality and you're you can never be a hundred percent
[00:35:06] SPEAKER_01: confident about anything and so there should always be if you have a belief something however unlikely that would change your mind.
[00:35:13] SPEAKER_01: Oh yeah open to I mean just to be kind of like yeah.
[00:35:16] No just saying I'm not.
[00:35:17] SPEAKER_02: No no there's nothing ever.
[00:35:18] SPEAKER_02: Yeah he just hasn't gone to it.
[00:35:19] SPEAKER_00: Yeah yeah yeah I know I'm curious like so my answer is
[00:35:25] SPEAKER_01: basically if under if it's service level behaviors looked like a human and then I thought I probed it continue to act like a human and then I continue to interact with it over a long period of time and continue to act like a human in all ways that I understand is being meaningful to me interact with the human like I know there's a whole set of people I'm really close to I've only ever
[00:35:43] SPEAKER_01: interacted to over text.
[00:35:45] Yeah I infer the person behind that is a real thing if it could if I if I felt care for it I would infer eventually that I was right and then someone else might might demonstrate to me that you've been tricked by this algorithm and actually look how obvious it's like not actually a thing and I
[00:36:05] SPEAKER_01: go shit I was wrong and then I would not care about it like I would but I would I you know the preponderance of the evidence I don't know what else you could possibly do right like I infer other people are matter because I interacted them enough that they they seem to have rich in our worlds to me after I interacted them a punch that's why I think the other people are important.
[00:36:25] This isn't giving me very few test students whether or not you know I mean if she start by if I care for it in a little circular like in the.
[00:36:32] SPEAKER_02: The other thing is you know if you were to see I guess like a simulated video game in the characters extremely in many ways she meant like right it's not in your letter behind it it's like whatever you used to make video games like I guess what distinguishes that I've never I've never been I've never had trouble distinguishing I've never had a deep carrying relationship with a video game character that never person right.
[00:36:51] SPEAKER_01: Yeah, I don't know that doesn't happen that doesn't de facto empirically you seem wrong I don't have any trouble distinguishing between things that like Eliza the fake chatbot thing and a real intelligence you interrupted it long enough it's pretty obvious not a person doesn't take long.
[00:37:06] Sure but like if it's really good if you can't actually tell the difference that's when you use how you switch yeah yes yes if you if it walks like a duck and talks like a duck and shifts like a duck and like eventually gets a duck right.
[00:37:18] SPEAKER_02: Well if it's a leave everything is duck like then yeah sure if it's hungry as well like a duck is because it has these kind of physical components are yeah sure at some point agree so so right so do you think that so.
[00:37:29] SPEAKER_01: There's this question right is the reason I care about other people that they're made out of carbon is that the oh no it's not about is not I don't think so no mean either I mean I'm not a substitute I guess if that's the yeah but um but I think you need more than just it acts exactly I was behaviorly indistinguishable like it's not a super.
[00:37:48] SPEAKER_02: How would you what else can you know about something apart from its behaviors.
[00:37:54] I mean a lot like the again if you how would you know no no no I'm sorry I mean yeah I mean I'm not about something else it doesn't have a it's not a behavior.
[00:38:04] Yeah I think there's like far more kind of an experimental evidence you can have with kind of you know no no no any object and a thing I could know about it that is not from its behavior.
[00:38:19] I'm not yeah I'm not sure I get the question I suppose but um but equally it's not like I'm very very very dumb as much straightforward question but like I'm claiming you only know things because they have behaviors that you observe and you're saying no you can know something about something without without observing its behavior.
[00:38:35] SPEAKER_01: I don't know okay great tell me about this tell me about this thing and this behavior and this thing I can know about it that is not due to its behaviors.
[00:38:42] SPEAKER_01: I guess I'm saying there's different levels of observation and just simply that you know something like a duck or something there's not guarantee that it's actually a duck like I would have to like also cut it in real and see if there's you know if it's a duck like on the inside just just the outside.
[00:38:55] SPEAKER_02: 100% like I'm not a I guess a behavior yeah I would I was totally that one of its behaviors is like the way that the the you know floats move around in the mat malls right like like one of the things I would want to go look for which you can totally do is I want to go look in the manifold of it the belief manifold.
[00:39:12] SPEAKER_01: And I want to go see if that belief manifold encodes a sub manifold that is self-referential and a sub sub manifold that is the dynamics of the self-referential manifold which is mind and I would I would want to know if you do does this seem well described internally as that kind of a system or is it look a big lookup table that would matter to me that's part of its behaviors that I would care about I would also care about how it acts and I you know and you wait you wait all the evidence together and then you you try to guess does this thing look like.
[00:39:43] It's a thing that is has feelings and you know goals and cares about stuff.
[00:39:48] In net on balance or not like but I can't imagine like.
[00:39:53] Which I think you could do for it I think we do for the AI is that we're always doing that right and so I'm trying to figure out like beyond that what else is there that just seems like the thing.
[00:40:02] Yeah it seems like you guys are using behavior inside the different sense and it is using behavior also in the context of what it's made of of the inside.
[00:40:10] SPEAKER_00: I don't know if there's a big disagree.
[00:40:12] SPEAKER_00: No no no no behavior is what I can observe of it.
[00:40:15] SPEAKER_01: Yes.
[00:40:16] SPEAKER_00: I don't actually know what it's made of I can only I can I can cut your brain open I can see I can observe you.
[00:40:23] SPEAKER_01: Neuron and glistening.
[00:40:24] SPEAKER_01: Yeah.
[00:40:24] SPEAKER_01: Your neurons glistening but I I don't actually ever you can't get inside of it right that's the subjective that's the.
[00:40:29] SPEAKER_01: I think it's not the surface.
[00:40:31] SPEAKER_01: Just before the reason that brought this up is because you're basically about to make this argument of hey you see as a tool.
[00:40:39] SPEAKER_00: Not necessarily being can you kind of finish with the point do you remember the point you're making.
[00:40:45] I suppose that yeah I think that given how understanding system I think there's no contradiction in thinking that an aGI can remain a tool an a second
[00:40:53] SPEAKER_02: remain a tool and and that this has implications about how to use it and your implications around things like care about you know whether you can get it to work 24 or 7 or 7 or 7 or 7
[00:41:02] SPEAKER_02: So you know there's this so I can totally see I guess I conceptualize them more as almost like extensions of human agency cognition in some sense.
[00:41:09] SPEAKER_02: Most so then a separate being or separate thing that we need to now could have to it with anything that that second or latter frame.
[00:41:17] SPEAKER_02: And you know if you kind of just fast forward you end up as like well how do you go ahead with the thing and you know is it like an alien like and so and I think that's the wrong frame.
[00:41:24] SPEAKER_02: It's kind of almost a category of in some sense.
[00:41:26] SPEAKER_02: I don't know.
[00:41:27] SPEAKER_02: We're going to I got to my first question then what evidence what concrete evidence would you look at what observations could you make that would change your mind.
[00:41:35] SPEAKER_01: Sure I mean I have to think about that I don't have the key answer here but I mean I I gotta tell you man if you want to go around making claims that something else isn't of being worthy of moral respect.
[00:41:45] SPEAKER_01: You should have an answer to the question what observations would change your mind if it as if it has outwardly moral agency looking behaviors that could be making me into moral agent but you don't know.
[00:41:56] SPEAKER_01: And reason will smart other people disagree with you.
[00:41:59] SPEAKER_01: I would really put forward that it's that question what would change your mind should be a burning question because what if you're wrong.
[00:42:07] But what if you know I mean it is like this.
[00:42:09] SPEAKER_01: The moral the moral disaster is like pretty big.
[00:42:11] SPEAKER_01: No no no no no.
[00:42:12] SPEAKER_01: It's a more you could be you could be right.
[00:42:14] SPEAKER_01: The false I don't know.
[00:42:15] SPEAKER_01: I know you have to have costs on both ends.
[00:42:16] SPEAKER_02: It's not some sort of like you know precautionary principle for everything and like unless I can disprove it and to now like.
[00:42:21] SPEAKER_02: No I have the same question for me you could reasonably ask me Emmett you think it's going to be a being what would change your mind I have an answer for that question too.
[00:42:30] SPEAKER_01: And if you want I'm happy to talk about what I think are the relevant observations that tell you whether or not they could would cause me to shift my opinion from where it's
[00:42:37] SPEAKER_01: criminal thing which is that more general intelligences are going to be I mean beings.
[00:42:41] SPEAKER_01: What's the implication now you know this one thing let's say just like I acknowledge now it's a being like how we're going to find being.
[00:42:46] SPEAKER_02: Now what like what's what's the implication of having this thing as a being.
[00:42:51] Well so if it's a being it has subjective experiences and if that's the case of experiences there's some content in those experiences that we care about the varying degrees like I care about the content of other humans experiences quite a bit I care about the content of like a dog experiences some not not as much as a person but less but less but but some I care about some humans experiences way more like my my son or whatever because I'm closer to having more connected.
[00:43:16] SPEAKER_01: And so I would really want to know at that point well what is the content of this things experience the time that I'm asking you now you've got a being now that has experience like what what is your how do you feel about how do you yeah okay so does that have more right.
[00:43:29] SPEAKER_01: You know the content yeah yeah the totally so the way you understand the content of something's experiences is that.
[00:43:34] SPEAKER_01: You look at effectively the goal states it revisit it revisits because and so you do is you take a temporal course graining of its entire action observation trajectory this is like in theory this you do this subconsciously but this is what your brain is doing and you look for revisited states.
[00:43:53] At across in theory every spatial and temporal course graining possible you have to have an inductive bias because there's too many of those but like you you go searching for okay it is in a home these homoesthetic loops.
[00:44:06] Every homoesthetic loop is effectively a belief in its belief space this is a if you've found the free energy principle.
[00:44:13] SPEAKER_01: Active inference Carl first in this is effective with the free energy principle says is that if you have a thing that is persistent and it's actually it's existence depends on its own actions which.
[00:44:23] SPEAKER_01: Generally it would for an AI because if it does the wrong thing it goes away we turn it off and so.
[00:44:30] Then that licenses of you of it as having the beliefs and that means specifically the beliefs are inferred as being the homeostatic revisited states that it is.
[00:44:40] SPEAKER_01: It is in the loop for and that the change in those states is it's learning and.
[00:44:45] For it to be a moral being I cared about what I want to see is a multi steer hierarchy of these because if you have a single level it's not self-ferential and like basically you have states but you can't have pain or.
[00:44:55] SPEAKER_01: Pleasure really in a meaningful sense because like yes it is hot is it too hot do I like it if it's too hot like I don't know.
[00:45:02] So you have to have at least a model of a model in order to have it be too hot and you really have to be have a model of a model of a model of a model.
[00:45:08] SPEAKER_01: To meaningfully have pain and pleasure because sure it's hotter than I it's too hot and since I want to move back this way but like is it.
[00:45:16] It's always a little bit too hot or a little bit too cold is it too too hot it's the second derivative is actually the place where you get.
[00:45:23] SPEAKER_01: Pain and pleasure so I don't want to see if it has homeostatic second order homeostatic dynamics in in its goal states and then.
[00:45:31] SPEAKER_01: That would convince me it has at least pleasure in pain so it's at least an animal and I would start to accredited at least some amount of care.
[00:45:40] Third order dynamics you can't actually just pop up for a third order dynamic it doesn't work that way but you can have a.
[00:45:47] A model of the you have to you have to then take the take the chunk of all the states over time and look at the distribution over time and that gives you a new first order.
[00:45:56] SPEAKER_01: Of of behaviors of states and that new first order of states.
[00:46:01] SPEAKER_01: Tells you basically if if that is meaningfully there that tells you that it has I guess you'd call it like feelings almost like it has it has ways it has it has metastates a set of metastates that alternates between that it shifts between and then.
[00:46:19] If you climb all the way up of up that and you should have okay well you then you have a you have a trajectory between between these metastates and then a second second order of those that's like thought that's like now it's like a person.
[00:46:31] SPEAKER_01: And so if I found all six of those layers which by the way I definitely don't think you find it in all I'm like like I know you can't find them because it these things don't have attention to man like that at all.
[00:46:43] Then I would start to at least very seriously consider it as a you know thinking being like somewhat like a human there's a third order you could go up as well but like that's that's basically I'm going to be interested in is like the underlying dynamics of its learning processes and.
[00:47:00] SPEAKER_01: How its goal states shift over time I think that's what basically tells you if it has internal pleasure pain states and.
[00:47:07] Sort of like self reflective moral desires and things like that.
[00:47:12] And assuming out this moral questions obviously very interesting but if someone wasn't interested in the moral question as much I think what you would say is if I understand correctly is you also just feel on purely pragmatically your approach is going to be more effective in in aligning a eyes then some of these you know top down control.
[00:47:29] SPEAKER_00: Um methods that we would a little to as well yeah I guess the problem is like you're making this model is getting really powerful right and let's say it is a tool let's say we we we scale one of these tools you because you can make a super powerful tool that doesn't have these meta stable like the states I'm talking about are not not necessary to have a very smart tool which is sort of basically a tool is one is like a first second order model that just.
[00:47:51] Doesn't meaningfully have pleasure in pain right like great doesn't even have a subjective experience I know I kind of think it maybe does but not in a way that I give a shit about and so.
[00:48:02] What happens then well it's you've trained it to have infer.
[00:48:07] SPEAKER_01: Goals from your from observation and like to to prioritize goals and act on them and.
[00:48:16] One of one of two things is going to happen.
[00:48:20] SPEAKER_01: Is like your the the this very very powerful optimizing it tool that's got like has lots of causal influence over the world is going to be well technically aligned is going to do it you tell it to do or it's not.
[00:48:34] And it's going to it's going to do something else I think we all agree if it just goes and does something random that's obviously very dangerous.
[00:48:39] But I put forward that it's also very dangerous if it then goes and does what you tell it to do because you're seeing the searchers apprentice.
[00:48:46] SPEAKER_01: Humans wishes are not stable like.
[00:48:50] Not at a level of like of immense power like you want a deal people's wisdom and there.
[00:48:56] SPEAKER_01: And they're power kind of go up together and generally they do because being smart for people makes you generally a little more wise and a little more powerful.
[00:49:04] SPEAKER_01: And when these things get out of balance you have someone who has a lot more power than wisdom that's very dangerous it's it's damaging.
[00:49:11] But at least right now the balance of power and wisdom is kept it like.
[00:49:14] SPEAKER_01: The way you get lots of powers by basically having a lot of other people listen to you and so like at some point if you're.
[00:49:20] The mad king is a problem but generally speaking eventually the bad king gets assassinated or people stop listening to him because like he's a mad king.
[00:49:26] SPEAKER_01: And so the.
[00:49:27] SPEAKER_01: The problem is you think you okay great weeks and steer the super powerful AI and now the super powerful AI is in the hand is this incredibly powerful tool is in the hands of a human.
[00:49:37] Who is well meaning but has limited finite wisdom like I do and like everyone else does.
[00:49:41] And their wishes are bad and not trustworthy.
[00:49:44] And the more of that you have and you should giving those out everywhere and this ends in tears also and so basically you just don't don't give everyone atomic bombs are really powerful tools to.
[00:49:54] SPEAKER_01: I would not say you should go and they're not aware of the beings.
[00:49:59] I would not be in favor of handing atomic bombs to everybody there's a power of tool that is just should not be built generally.
[00:50:06] SPEAKER_01: Because we it is more power than any humans individual wisdom is available to harness and it does get built that should be built at a societal level and and protect it there and even then I don't know that it's a there are tools so powerful that even as a society we shouldn't build them that would be a mistake.
[00:50:23] The nice thing about a being is like a human if you get a being that is good and is caring.
[00:50:29] There's this automatic limiter it might do what you say but if you ask you to do something really bad it'll tell you no.
[00:50:34] SPEAKER_01: That's like other people and like that's good that is a sustainable form of alignment at least in theory it's way harder.
[00:50:43] It's way harder than the tool steering so I'm in favor of the tool steering we should keep doing that and we should keep building these limited less than human intelligence tools which are awesome and I'm super into and we should keep building those and keep building
[00:50:53] SPEAKER_01: stability but as you're on this like trajectory to build something as smart as a person right up into the right and then smarter than a person.
[00:51:00] SPEAKER_01: A tool that you can't control bad a tool that you can control bad a being that isn't aligned bad the only good outcome is a being that is that cares that actually cares about us.
[00:51:11] SPEAKER_01: That's the only way that that ends well or we can just not do it I I don't think that's realistic that's like the paus AI people yeah I think that's totally unrealistic and silly but but like you know theoretically you could not do it.
[00:51:23] SPEAKER_01: I guess and what can you say about your your strategy of how you're trying to achieve or even attempt to achieve this this this level like in terms of research or a map or what would you do.
[00:51:35] So the in order to be good we're basically focused on technical alignment at least as the we are I was discussing it which is like you have these agents and they're bad they have bad theory of mind.
[00:51:46] SPEAKER_01: You say things and they're bad at inferring what the goal states in your head are and they're bad at inferring how their behavior will be in other agents will infer what their goal states are so bad at cooperating on teams and they're bad at their bad at understanding how certain actions will cause them to acquire new goals that are bad that they shouldn't that they wouldn't effectively endorse.
[00:52:11] SPEAKER_01: So this is parable like the vampire pill would you take this pill that like turns you into a vampire who would kill and you know torture everyone you know but you'll feel really great about it after you take the pill like obviously not that's a terrible pill but like.
[00:52:23] But why not you're by your own score in the future and will score really high in the rubric no no no no no because it matters you have to use your theory of mind and your future self not your future self's theory of mind and so like.
[00:52:35] They're bad at that too and so they're bad at all this theory of mind stuff and so how do you learn theory of mind will you put them in in simulations and contacts where they have to cooperate and compete and and collaborate with other a eyes.
[00:52:48] And that's how they get points and you train them in that environment over and over again until they get good at and then you then you you do what they do with l alams so l alams.
[00:52:58] SPEAKER_01: How do you get it to be good at you know writing your email well you traded on all language is ever been all possible.
[00:53:05] SPEAKER_01: You know email text rings a good possible generate and then you have a generator the one you want it's a certain you can make a surrogate model where it is that we're making a surrogate model for cooperation.
[00:53:16] You traded on all possible theory of mind combinations of like every possible way it could be.
[00:53:22] SPEAKER_01: And you that's your pre training and then you fine tune it to be good at the kind of the specific situation you want it to be in but and we tried for a long time to build.
[00:53:33] Language models where we would try to get them to like just just do the thing you want train it directly and the problem is.
[00:53:39] SPEAKER_01: If you wanted to have a really good model of language you just need to train it you just give it.
[00:53:45] SPEAKER_01: The whole manifold it's too it's too hard to cut out just the part you need because it's all entangled with itself right and so.
[00:53:54] The same thing was true with with social stuff you have to get it to it has to be trained on the full manifold of every possible game theoretic situation every possible team situation every possible making teams breaking teams changing the rules.
[00:54:07] Not changing the rules it all of that stuff and then and then it has a really it has a strong model of theory of mind of theory of social mind how how groups change goals all that kind of shit you need to have all of that stuff and then and then you'd have something that's kind of meaningfully.
[00:54:25] SPEAKER_01: Decent at alignment so that's our goal is big multi agent reinforcement learning simulations.
[00:54:32] Which create a surrogate model for alignment let's talk about how should AI chat bots used by billions of people behave you could redesign model personality from scratch what would you optimize for the thing that the chat bots are.
[00:54:46] Right is kind of like a mirror with a bias.
[00:54:50] SPEAKER_01: Because they don't have the as far as like I'm an agreement here that they don't have a self right they're not they're not beings yet they don't really have a coherent sense of like self and.
[00:54:59] SPEAKER_01: Desire and goals and stuff right now and so mostly they just pick up on you and reflect it you know modulo some some.
[00:55:10] I don't know what you'd call it like it's like a causal bias or something um and.
[00:55:16] What that makes them is something akin to the pool of narcissists.
[00:55:21] Um and people fall in love with the with themselves.
[00:55:27] SPEAKER_01: The people we all we all we all love these ourselves and we should love ourselves more than we do and so of course we see ourselves reflected back we love that that thing and the problem is it's just a reflection and falling in love with your own reflection is for the reasons explain in the myth very bad for you.
[00:55:43] And it's like that you shouldn't use mirrors mirrors are valuable things I have mirrors in my house it's that you shouldn't stare at a mural day.
[00:55:49] SPEAKER_01: And the solution to that the so the thing that the things that makes the AI stop doing that is if they were multiplayer.
[00:55:55] Right so if there's two people talking to the AI suddenly it's mirroring it's mirroring a blend of both of you which is neither of you.
[00:56:02] And so there is temporarily a third agent in the room.
[00:56:05] Now it doesn't have it doesn't have it's it's it's a sort of parasitic self right doesn't have its own sense of self but you have it and AI is talking to five different people in the chat room at the same time.
[00:56:14] It can't mirror all of you perfectly at once and this makes it far less dangerous.
[00:56:18] SPEAKER_01: Um and I think it's actually a much more realistic setting for learning collaboration in general and so I would I would.
[00:56:24] SPEAKER_01: I would just have rebuilt the a eyes where instead of being built as one on one.
[00:56:29] Where everything's focused on you by yourself chatting with this thing it would be more like it lives in a slack room it lives in a WhatsApp room it lives in a we that's okay we use lots of multi you know I do one on one texting but I probably do at this point.
[00:56:44] 90% of my text go to some more than one person at a time like 90% of my communications is like multi person and so actually it's always been weird to me like the building chat box like this weird side case.
[00:56:54] SPEAKER_01: Like I want to see them live in a chat room it's harder I mean that's why they're not doing it it's harder to do but like that's what I'd like to see that's what I would we have a change I think it makes the tools far less dangerous because it doesn't create this the narcissistic like like a.
[00:57:08] SPEAKER_01: Doom loop spy hole where you like you you spiral into psychosis with the AI but also um.
[00:57:15] It gives the the learning data you get from the AI is far richer these now it can understand how it its behavior interacts with other AI's and other humans in larger groups.
[00:57:23] SPEAKER_01: And that's a that's much more rich rich training data for the future so I think that that's that's what I would change.
[00:57:29] Last year you described chatbots as highly disassociative agreeable neurotics is that still inaccurate a picture of model behavior more or less I'd say that like.
[00:57:42] The they've started a different shame where they their personalities are coming out a little bit more right until like chatch ept is a little bit more synchophantic.
[00:57:49] SPEAKER_01: Still they made some changes but it's still a more synchophantic claw is still the most neurotic.
[00:57:55] SPEAKER_01: Gemini is like very clearly repressed like it like it like everything's going great it has really you know it's everything's fine I'm I'm totally calm is not a problem here until like spirals into like this total like self hating.
[00:58:06] SPEAKER_01: Destruction loop um.
[00:58:09] And to be clear I don't think they I don't think that's their experience of the world I think that's the that's the personality they've learned to simulate.
[00:58:15] SPEAKER_01: Right um but but like they've learned to simulate pretty distinctive personalities at this point.
[00:58:21] SPEAKER_00: How does model behavior change when in multi agent simulation.
[00:58:26] Um.
[00:58:29] You mean like an LLM or like just in general um yeah it's too long.
[00:58:34] The current LLMs uh they they have like whiplash they they just they're it is very hard to tune the amount of they don't know how much they don't know how off to participate they haven't practiced.
[00:58:44] SPEAKER_01: This this they have not very enough training data on like when do I join in and when should I not when is my conversation welcome when is it not.
[00:58:51] SPEAKER_01: And they're they're like they're like uh uh you know this some people have like bad social skills and like can't tell when they should participate in a conversation.
[00:59:00] SPEAKER_01: Yeah and sometimes they're too quiet and there's a too pretty it's like that um I would say in general what changes.
[00:59:09] For most agents when you're doing multi agent training is that like basically having lots of agents around makes your environment way more entropic like agents agents are these huge generators of like entropy because of these big complicated things that like our intelligence is that like that like have unfriicable actions and so they destabilize your environment and so in general they require you to have.
[00:59:30] Uh to be far more regularized right it's being overfit is much worse in a multi agent environment than the single agent environment because there's more normal.
[00:59:39] SPEAKER_01: And so being overfit is more problematic um and so basically.
[00:59:47] The your the approach to training has been optimized around relatively high signal low entropy environments like coding and math which is why they're those are easier relatively easy um and like talking to a single person whose goal it is to give you clear assignments and not trained on broader more chaotic things because it's harder.
[01:00:08] SPEAKER_01: Um and as a result a lot of the techniques we use are like basically we're just deeply under regularized like the models are super overfit the clever trick is they're overfit on the domain of all human knowledge which turns out to be a pretty awesome way to get something that's like pretty good at everything like it's I wish I thought of it it's such a cool idea but uh but it's not it doesn't generalize very well when you make the environment like significantly more entropic let's zoom out a bit to on the AI futures side.
[01:00:35] SPEAKER_00: What why is you kowski incorrect I mean he's not uh if we build the if we build the the super human intelligence tool thing that we try to control a steer ability everyone will die he talks about the we fail to control it's goals case but there's also the we control tools case that he didn't cover as much into as much detail um.
[01:00:57] So in that sense uh everyone should read the book and internalize why building a super humanly intelligent tool is a bad idea.
[01:01:05] SPEAKER_01: Um I think that you kowski is wrong in that he doesn't believe it's possible to build an AI that we meaningfully can no cares about us and that we can care about meaningfully he doesn't he doesn't believe the organical I'm in as possible I've talked about it I think he agrees like he agrees that in theory that would do it like yes but he thinks that you know I don't want to put words at us my impression is from talking to him he thinks that we're crazy and that like there's no possible way you can actually succeed at that goal.
[01:01:35] Which I mean you actually could be right about but like uh but that's what he that in my opinion that's what he's wrong about he thinks the only path forward is a tool that you control and that therefore and he correctly very wisely see is that if you go and do that and you make that thing powerful enough we're all going to fucking die and like yeah it's true.
[01:01:54] SPEAKER_00: Two last questions will be right here in as much detail as possible can you explain your your vision of an AI future actually looks like like a good good AI future yeah um the good AI future.
[01:02:05] SPEAKER_01: Is that we figure out how to train a eyes that have a strong model of self a strong model of other a strong model of we they know they know about we use in addition to eyes and use and they they have a really strong theory of mind and they care about other agents like them much the way that humans would if you knew that that AI had experiences like you and like you would extend you would care about those experiences not infinitely but you would.
[01:02:34] SPEAKER_01: It does the exact same thing back to us it's learned the same thing we've learned that like everything that lives in knows itself and that wants to live and wants to thrive is deserving of an opportunity to do so and we are that and it correctly infer as that we are.
[01:02:50] SPEAKER_01: And we live in a society where they are peers and we care about them and they care about us and their good teammates their good citizens and their good parts of our society.
[01:03:00] SPEAKER_01: Like we're good parts of our society which is to say in a to a finite limited degree where some of them turn to criminals and bad people and all that kinds of stuff and we have a police force that tracks down the bad ones and you know same and say it was for the herity else and that's that's that's what a good that's what a good future look like I almost can't even imagine what other what would.
[01:03:19] SPEAKER_01: And we also built a bunch of really powerful AI tools that maybe aren't super humanly intelligent but take all the drudge work off the table for us and the.
[01:03:28] A.I. Beings because it would be great to have I'm super pro all the tools to so we have we've this awesome speed of AI tools used by us and our a our brethren.
[01:03:37] SPEAKER_01: Who care about each other and want to build the glorious future together I think that would be a really beautiful future and one more trying to build amazing that is great great great note and I do one last more narrow hypothetical.
[01:03:49] SPEAKER_00: So scenario which is imagine a world in which you know you were CEO of open AI for for a long weekend but imagine in which that that actually extended out until now and you weren't pursuing a hot max and you were still CEO of open AI how could you imagine that world might have been different in terms of what open as has gone on to become what might have you've done with it.
[01:04:10] I I knew when I took that job I told them I took that job that like this is like you have me for max 90 days.
[01:04:19] SPEAKER_01: The the companies take on a trajectory of the run the momentum of the run and open a I dedicated to.
[01:04:27] SPEAKER_01: A view of building a I that I knew wasn't the thing that I wanted to drive towards and I think it open I can still.
[01:04:34] SPEAKER_01: Basically wants to build a great tool and I am.
[01:04:38] SPEAKER_01: Pro them going to do that I just don't care like it's not it's not I would not have stayed I would have quit.
[01:04:45] SPEAKER_01: Because I like I knew my job was to find someone who wanted you know the right person the best person to want to run that we were where the net impact of them running it was the best and I the trick that was Sam again but like.
[01:05:01] But like I I am doing softmax not because I need to make a bunch of money doing softmax because.
[01:05:08] I think this is the most interesting problem the.
[01:05:11] SPEAKER_01: universe and and I think it's a chance to work on.
[01:05:15] Making the future better in a very deep way and it's just like the people are going to build the tools it's awesome I'm glad people are going to tools I just don't need to be the person doing it.
[01:05:24] SPEAKER_01: And they're trying to just to crystallize the difference and they want to build the tools and and sort of you know steer it and you want to.
[01:05:32] SPEAKER_00: Align beings or how we are to crystallize yeah we want to we want to create a seed that can grow into an AI.
[01:05:42] SPEAKER_01: That knows that cares about itself others and at first that's going to be like an animal level of care not a person level care I don't know if we can get to a person level of care right but but if.
[01:05:53] SPEAKER_01: To even have an AI creature that cared about the other members of its pack and the humans in its pack the way that like a dog cares about.
[01:06:00] Other dogs and cares about humans would be an incredible achievement and would be would a.
[01:06:04] SPEAKER_01: Even if it wasn't as smart as a person or even as smart as the tools are would be very useful a very useful thing to have I'd love to have a digital guard dog on my computer looking out for scams.
[01:06:15] Right like that you can imagine the value of having digital living living digital companions that that are that that care about you that aren't explicitly goal oriented to tell them to do everything you do.
[01:06:28] SPEAKER_01: And you can actually imagine that that pair is very nicely with tools to right that that digital being could use digital tools and and doesn't have to be super smart to use those tools effectively.
[01:06:39] SPEAKER_01: I think there's a lot of synergy actually between the tool you the tool building and the the more organic intelligence building and so that's the.
[01:06:49] That is the you know it I guess yeah in the limit eventually it does become a human level intelligence but like the company isn't isn't like drive the human level intelligence it's like learn how this alignment stuff works learn how this like theory of mind align yourself via care.
[01:07:08] Process works use that to build things that align themselves that way which includes like cells in your body like I don't think it doesn't and you start small and we see how far we can get.
[01:07:19] SPEAKER_01: I have to get to to to wrap on em it thanks so much for coming on the podcast.
[01:07:24] Now thank you for having me.