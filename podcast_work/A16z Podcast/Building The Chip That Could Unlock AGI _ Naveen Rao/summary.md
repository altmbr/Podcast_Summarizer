# [Building The Chip That Could Unlock AGI | Naveen Rao](https://www.youtube.com/watch?v=wZ4DT20OHXE)

**Podcast:** A16z Podcast
**Date:** 2025-12-08
**Region:** Western
**Video ID:** wZ4DT20OHXE
**Video URL:** https://www.youtube.com/watch?v=wZ4DT20OHXE
**Transcript:** [View Transcript](./transcript.md)

---

# Podcast Summary: Naveen Rao on Unconventional AI and Analog Computing

## 1. Key Themes

### The Energy Crisis in AI Computing Will Force Architectural Innovation

The United States currently dedicates 4% of its energy grid to data centers, housing 50% of the world's data center capacity. This is already causing brownouts in the Southwest during summer. The trajectory is unsustainable: estimates suggest 400 gigawatts of additional capacity will be needed over the next decade to power AI demand, yet the US can only add about 4 gigawatts per year. "By some estimates, we need 400 gigawatts additional capacity over the next 10 years to power the demand for AI. Well, so a huge shortfall. And so we really just need to rethink this." [[00:12:12]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=12m2s) Naveen argues this creates an existential imperative for more efficient computing architectures, making analog approaches commercially viable for the first time in 80 years.

### Intelligence Requires Temporal Dynamics and Causality That Digital Systems Don't Naturally Capture

Naveen posits that current AI systems lack something fundamental: an innate understanding of causality rooted in temporal dynamics. "My intuition says that anything where the basis is dynamic, which has time and causality as part of it will be a better basis than something it's not. So we've largely tried to remove that." [[00:17:25]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=17m15s) He observes that children innately understand causality in ways current AI doesn't, and that biological neural networks implement intelligence through physics rather than numerical abstractions. Digital computers simulate time with numbers, but physical systems actually evolve through time—a critical distinction for achieving more complete intelligence.

### The Full Stack Engineer Has Been Redefined, Losing Critical Cross-Domain Insight

Naveen highlights a significant shift in what "full stack" means: "Now, full stack engineer means something different than it did back then. I think back then, it meant someone who understands potentially devices like Silicon, how to do logic design, computer architecture, low level software, maybe OS level software and then application. That was a full stack engineer." [[00:02:33]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=2m23s) Today it simply means knowing JavaScript and Python. This narrowing represents a loss of systems-level thinking that could hinder innovation, particularly when paradigm shifts require understanding across traditional boundaries between hardware and software.

## 2. Contrarian Perspectives

### Transformers Are Artifacts of GPU Architecture, Not Natural Laws of Intelligence

"Transformers are really, they're big innovation because they made the constructs of a GPU work extremely well. And it doesn't mean it's wrong, but I don't think there's nothing natural. There's no natural law about the parameter of a transformer." [[00:16:21]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=16m11s) Naveen argues transformers succeeded because they map efficiently to existing GPU hardware, not because they represent an optimal architecture for intelligence. This suggests the current AI paradigm is hardware-constrained rather than algorithmically optimal—a contrarian view that challenges the assumption that scaling transformers is the only path forward.

### Working at Big Companies Early in Career Limits Long-Term Adaptability

Counter to conventional wisdom about prestigious tech jobs, Naveen advises: "If you want to be prepared for change in the future, being really good at one thing is probably less valuable than being very good at, but slightly good at a lot of things." [[00:27:37]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=27m27s) He argues that big companies, through no fault of their own, narrowly specialize employees who "get hired to do a thing and you do that thing over and over again." In contrast, startups force breadth that pays "dividends later on" by enabling cross-domain thinking essential for navigating future disruptions.

### Analog Computing Failed Due to Manufacturing Limitations, Not Fundamental Flaws

"Analog computers were actually some of the first computers. And they worked really well. They were very efficient, but they couldn't be scaled up because of manufacturing variability." [[00:06:40]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=6m30s) This directly contradicts the implicit narrative that digital won because it was superior. Naveen argues we went digital because 1940s manufacturing couldn't reliably characterize vacuum tubes except as binary high/low states. With modern semiconductor manufacturing capabilities, the original advantages of analog—direct physics-based computation without lossy abstractions—become accessible again. This reframes 80 years of computing history as a temporary detour rather than inevitable progress.

## 3. Companies Identified

### TSMC
**Description:** Taiwan Semiconductor Manufacturing Company, the world's leading semiconductor foundry.

**Why Mentioned:** Identified as a critical manufacturing partner for scaling unconventional's analog computing chips. Manufacturing scalability is essential to addressing the global energy problem—the company needs someone who can "go build 10 million of these things."

**Quote:** "I think TSMC is absolutely gonna be a partner forward, you know, met with them recently. And you know, we wanna work closely with them to make sure we get what we need, get faster on time, to prototype and all of that." [[00:19:52]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=19m42s)

### Databricks
**Description:** Cloud-based data and AI company (where Naveen previously served as head of AI).

**Why Mentioned:** Part of Naveen's career background, representing his software/cloud computing expertise and experience at scale before founding Unconventional.

**Quote:** "Prior to that, Naveen was at Databricks as head of AI." [[00:01:03]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=53s)

### Intel
**Description:** Major semiconductor and technology company.

**Why Mentioned:** Acquired Naveen's previous company Nirvana, and where he worked as an executive. Referenced as an example of his commitment to hardware innovation—he was "one of the only execs would go to the lab" when chips came back from fabrication.

**Quote:** "When I was at Intel, like, I was one of the only execs would go to the lab and force ship would come back and I'm like, I want to see what happens." [[00:21:42]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=21m32s)

### Google
**Description:** Technology giant with significant AI investments.

**Why Mentioned:** Referenced as having everything internally (TPUs) and working on "lower risk, but continual improvements for their hardware," representing the incremental optimization approach of established players versus Unconventional's radical rethinking.

**Quote:** "Google kind of has everything internally. And I think they're working on sort of lower risk, but you know, continual improvements for their hardware with TPUs." [[00:20:11]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=20m1s)

### NVIDIA
**Description:** Leading GPU manufacturer dominating AI compute.

**Why Mentioned:** Built "the platform that everyone programs on today" but represents the digital computing paradigm Unconventional aims to complement or challenge. Naveen sees potential for both competition and collaboration.

**Quote:** "And video, of course, you know, they've built the platform that everyone programs on today. So, is it, are we gonna be at odds with Nvidia going for? I don't know, we'll see what the world looks like, but I mean, we are trying to build a better substrate than Matrix multiply. There could be a world where we collaborate on such solutions." [[00:20:37]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=20m27s)

## 4. People Identified

### Alex Honnold
**Description:** Professional rock climber famous for free soloing El Capitan.

**Why Mentioned:** Used as an example of extraordinary precision achieved by biological neural networks. Honnold's ability to climb without ropes where "if he slips, like just, at least off kind of millimeter in some places, he dies" demonstrates how noisy neural systems can achieve extreme accuracy through integrating diverse, variable inputs.

**Quote:** "Alexander O'Connor, Al Capitan. He's just thinking about the precision that's required. It still scares me every time I see it. And if he slips, like just, at least off kind of millimeter in some places, he dies. And that's true for like every top level athlete." [[00:14:04]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=13m54s)

### Steph Curry
**Description:** NBA player known for exceptional shooting accuracy.

**Why Mentioned:** Exemplifies how biological neural networks achieve precision despite constant variability in inputs. "Steph Curry, when he shoots a ball, is never going to shoot under ideal circumstances in a game. Always it's a unique input. There's a lot of different input varieties coming at you, like where the players are precisely where you're standing. Maybe your shoes are different. Maybe the surface is a little different." The neural network integrates all these variables to produce consistent, accurate outcomes—something current AI struggles with.

**Quote:** "The story is he set up a special tracking system so he can make sure the ball was sitting in the middle of the rim, not just going through. So the level of precision, these guys hit with a neural network that's noisy is actually quite high." [[00:14:21]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=14m11s)

### Yann LeCun
**Description:** Chief AI Scientist at Meta, Turing Award winner, pioneer of convolutional neural networks.

**Why Mentioned:** Referenced for his work on energy-based models, which Naveen sees as particularly relevant to unconventional's approach because "they inherently have dynamics as part of them. They're literally written as an ordinary differential equation."

**Quote:** "I have to ask just since you mentioned energy-based models and Jan McEun has been writing quite a lot about this, do you think pursuing these sorts of paths that you're talking about gets us closer on the path to AGI, whatever AGI means?" [[00:16:56]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=16m46s)

## 5. Operating Insights

### Maximize Organizational Agency by Deliberately Getting Out of the Way

Naveen articulates a clear decision-making framework: "What decisions can I make as a leader to increase agency of the org overall? Like me making top-down decision, maybe globally better for the company in the short term. But I think wrong term, we will do better if more people have agency and can try more things out." [[00:28:43]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=28m33s) His approach is to actively find ways to remove himself from decision paths when people show passion: "Okay, you really want to do this? That makes sense. Go for it. And then you own it. You own both the good and the bad, right?" This creates both accountability and learning, as people understand "okay, I fucked up. Now this wasn't real. That's okay too."

### Separate Discovery Phase from Optimization Phase to Avoid Premature Constraints

"First few years, it really is open-ended. I don't want to close doors. Like I am really specific about this. Like I always try to bring the conversation back because those people are like, oh, that's going to be hard to manufacture. They'll say, stop. Don't think about that. Will it work? First come up with existence, proofs. Then we go back and try to engineer it and all the trade-offs they're in." [[00:27:48]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=27m38s) This represents a disciplined approach to innovation: resist the natural tendency to optimize prematurely. Engineers want to immediately consider manufacturing constraints, but that narrows the solution space before fundamental feasibility is established.

### Hard Problems Attract Smart People—Use Challenge as Recruitment Tool

"What I've done really well across the company's eyes, built has been going after hard problems, which kind of ends itself with smart people wanting to come in and try solve them. They see a challenge." [[00:28:31]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=28m21s) Rather than trying to make opportunities seem easy or risk-free, Naveen explicitly frames them as difficult challenges. This self-selects for people with the right mindset and intrinsic motivation, rather than those seeking comfort or purely financial incentives.

## 6. Overlooked Insights

### Wind Tunnels Are Still Essential Because Computational Simulation Has Fundamental Limits

In discussing analog computing, Naveen casually mentions: "There's wind tunnels, our great example of an analog computer in the sets where I have a race car, a track, or an airplane. And I want to understand how the wind moves around it... doing things with computational fluid dynamics accurately is pretty hard. So people still build wind tunnels." [[00:07:48]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=7m38s) This is a profound observation that's easy to miss: despite decades of supercomputing advances, physical analog modeling (wind tunnels) remains superior for certain applications because digital simulation of complex physical systems has irreducible precision errors. This suggests whole categories of problems where analog approaches might be fundamentally better, not just more efficient—a much stronger case for Unconventional's approach than energy efficiency alone.

### The First Prototype Will Be "Probably the Largest Analog Chip People Have Ever Built"

Almost as an aside, Naveen mentions: "When we build this chip, our first prototype, it's gonna be probably one larger, maybe the largest analog chip people have ever built, which is kind of weird." [[00:25:55]](https://www.youtube.com/watch?v=wZ4DT20OHXE&t=25m45s) This is a staggering technical challenge that wasn't emphasized but reveals the magnitude of what they're attempting. Large analog chips face massive challenges with signal degradation, thermal effects, and manufacturing variation across the die. That they're going directly to extreme scale suggests either extraordinary confidence in their architecture's robustness to these issues, or that the computational requirements simply demand this scale from the start. This detail suggests the technical risk is even higher than the already-ambitious framing implies.