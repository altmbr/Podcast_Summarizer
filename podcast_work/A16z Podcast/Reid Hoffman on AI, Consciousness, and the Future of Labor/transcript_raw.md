# Reid Hoffman on AI, Consciousness, and the Future of Labor

**Podcast:** A16z Podcast
**Date:** 2025-10-22
**Video ID:** brjL6iyoEhI
**Video URL:** https://www.youtube.com/watch?v=brjL6iyoEhI

---

[00:00:00] This is actually one of the things that I think people don't realize that Silicon Valley.
[00:00:02] SPEAKER_02: You start with what's the amazing thing that you can suddenly create?
[00:00:07] SPEAKER_02: Lots of these companies, and you go, what's your business model?
[00:00:09] SPEAKER_02: You go, I don't know.
[00:00:10] SPEAKER_02: You're like, yeah, we're going to try to work it out.
[00:00:12] SPEAKER_02: But I can create something amazing here.
[00:00:15] SPEAKER_02: And that's actually one of the fundamental,
[00:00:17] SPEAKER_02: call it the religion of Silicon Valley and the knowledge of Silicon Valley that I so much
[00:00:21] SPEAKER_02: love and admire and embody.
[00:00:26] Read Welcome Thesoncy Podcast.
[00:00:28] SPEAKER_01: It's great to be here.
[00:00:29] SPEAKER_01: So you're one of the most successful Web2 investors of that era,
[00:00:33] SPEAKER_01: you know, Facebook, LinkedIn, obviously, which you co-created.
[00:00:36] SPEAKER_01: Airbnb, many, many others, and you had several frameworks self.
[00:00:39] SPEAKER_01: You do that, one of which was the Seven Deadly Sins,
[00:00:41] SPEAKER_01: which we talk about often in love.
[00:00:42] SPEAKER_01: As you're thinking about AI investing, what's a framework or a worldview that you take to your AI investing?
[00:00:49] So obviously we're all looking through a glass darkly, looking through a fog,
[00:00:55] SPEAKER_02: with strobe lights that don't really, you know,
[00:00:57] SPEAKER_02: a hard to understand what's going on.
[00:00:59] SPEAKER_02: So we're all navigating this new universe.
[00:01:01] SPEAKER_02: So I don't know if I have as Christopher from the Seven Deadly Sins to work,
[00:01:05] SPEAKER_02: because that's a question of what is infrastructure,
[00:01:08] SPEAKER_02: psychological infrastructure across all 8 billion plus human beings.
[00:01:14] SPEAKER_02: But I'd say there's a couple things.
[00:01:16] SPEAKER_02: So first is there is going to be a set of things that are the kind of the obvious line of sight,
[00:01:25] SPEAKER_02: obvious line of sight, bunch of stellar chatbots,
[00:01:27] SPEAKER_02: bunch of subproductivity, coding assistance, you know,
[00:01:29] SPEAKER_02: da da da da da.
[00:01:31] And so, and by the way, it's still worth investing in,
[00:01:34] SPEAKER_02: but obviously obvious line of sight means it's obvious to everybody,
[00:01:38] SPEAKER_02: line of sight.
[00:01:39] SPEAKER_02: And so, so, you know, doing a differential investment is harder.
[00:01:43] SPEAKER_02: The second area is, well, what does this mean?
[00:01:47] SPEAKER_02: Because too often people say in an area of disruption that everything changes,
[00:01:51] SPEAKER_02: as opposed to significant things change.
[00:01:53] SPEAKER_02: So, like you were mentioning Web2O and LinkedIn,
[00:01:56] SPEAKER_02: and obviously, you know, part of this with a platform change, you go,
[00:02:00] SPEAKER_02: okay, well, are there now new LinkedIn's that are possible because of AI or something like that?
[00:02:04] SPEAKER_02: And of course, like, give my own heritage.
[00:02:06] SPEAKER_02: I would love LinkedIn to be that.
[00:02:08] SPEAKER_02: But, you know, it's just what I've heard.
[00:02:09] SPEAKER_02: I'm always probably innovation entrepreneurship.
[00:02:11] SPEAKER_02: The best possible thing for humanity.
[00:02:14] SPEAKER_02: But like, what are the kind of more traditional,
[00:02:16] SPEAKER_02: like the kind of things that haven't changed, network effects, you know,
[00:02:21] SPEAKER_02: enterprise integration?
[00:02:22] SPEAKER_02: You know, other kinds of things that the new platform
[00:02:27] SPEAKER_02: upsets the Apple cart, but you're still going to be putting that Apple cart kind of back together
[00:02:31] SPEAKER_02: in some way. And what is that?
[00:02:33] SPEAKER_02: And then the third, which is probably where I've been putting most of my time,
[00:02:39] SPEAKER_02: has been what I think of Silicon Valley blind spots.
[00:02:43] SPEAKER_02: Because what we tend to be like Silicon Valley is one of the most amazing places in the world.
[00:02:49] SPEAKER_02: There's a network of intense co-option, learning, you know, invention, you know, kind of building
[00:02:57] SPEAKER_02: new things, etc., which is just great. But we also have our cannons. We have our kind of blind
[00:03:03] SPEAKER_02: spots. And a classic one for us tends to be what everything should be done in CS. Everything
[00:03:08] SPEAKER_02: should be done software. Everything should be done in bits. And that's the most relevant thing.
[00:03:11] SPEAKER_02: Because by the way, it's a great area to invest. But it was like, okay, what are the areas where the
[00:03:16] SPEAKER_02: AI revolution will be magical, but won't be within the Silicon Valley blind spots. And that's
[00:03:24] SPEAKER_02: probably where I've been putting the majority of my co-founding time, invention time,
[00:03:33] SPEAKER_02: you know, kind of investment time, etc., because like, I think usually a blind spot on something
[00:03:39] SPEAKER_02: that's very, very big, right, is precisely the kinds of things that you go, okay, you have a
[00:03:45] SPEAKER_02: long runway to create something that could be like another one of the iconic companies.
[00:03:51] SPEAKER_02: Yeah, that's good deeper on that because we're also talking just before this about how people
[00:03:56] SPEAKER_01: focus so much on the productivity, that the workflow sides, but they're missing other elements.
[00:04:00] SPEAKER_01: So say more about other other things that you find more interesting there.
[00:04:04] SPEAKER_01: Well, so, so one of the things I, you know, kind of told my partners back at Greylock in 2015,
[00:04:12] SPEAKER_02: so it's like 10 years ago, was I said, look, there's going to be a bunch of different things on
[00:04:19] SPEAKER_02: productivity or our AI. I'll help, right? Like, you know, you know, you have companies you want me to
[00:04:26] SPEAKER_02: to work with that you're doing. Great. That's awesome. You know, enterprise productivity, etc.,
[00:04:30] SPEAKER_02: you know, things at Greylock tends to specialize on. When I said, actually, in fact, what I think
[00:04:37] SPEAKER_02: that's here, getting the blind spots is, is also going to be some things like, you know, what,
[00:04:45] SPEAKER_02: you know, as you guys both know, Matt SAI, which is how do we create a drug discovery factory that
[00:04:51] SPEAKER_02: works at the speed of software? Right. Now, obviously, there's regulatory, obviously, there's
[00:04:56] SPEAKER_02: biological bits, obviously, and so they won't be purely a speed of software, but how do we do this?
[00:05:03] SPEAKER_02: And they said, oh, well, what do you know about biology? The answer is zero. Well, it may be not
[00:05:08] SPEAKER_02: quite zero, you know, but on the board of BioHub for 10 years, I'm on the board of our arc, etc.,
[00:05:12] SPEAKER_02: like I've been thinking about the intersection of the worlds of atoms and the worlds of bits,
[00:05:17] SPEAKER_02: and you have biological bits, which are kind of halfway between atoms and bits in various ways.
[00:05:21] SPEAKER_02: I've been thinking about this a lot and kind of what the things are, not so much with a specific
[00:05:26] SPEAKER_02: company focus, as much as a, what are things that elevate human life, you know, kind of focus?
[00:05:33] SPEAKER_02: Part of reason why BioHub, part of reason why I arc, but then I was like, well, wait a minute,
[00:05:38] SPEAKER_02: actually now with AI, and you have the acceleration, because like, for example, actually, this
[00:05:43] SPEAKER_02: detour will be fun. So roughly also around 10 years ago, I was asked to give a talk to the Stanford
[00:05:52] SPEAKER_02: Long-Term Planning Commission, and what I told them was that they should basically divert
[00:06:02] SPEAKER_02: and put all of their energy into AI tools for every single discipline. And this is a
[00:06:07] SPEAKER_02: well before Chattche, PT, and all the rest. And the metaphor I used was a search metaphor,
[00:06:13] SPEAKER_02: because think if you had a custom search productivity tool in every single discipline, now back then,
[00:06:19] SPEAKER_02: I could imagine it, I could build one for every discipline other than theoretical math,
[00:06:23] SPEAKER_02: theoretical physics. Today, you might even be able to do theoretical math and theoretical physics.
[00:06:29] SPEAKER_02: Right, exactly. And so do that, like transform knowledge generation, knowledge communication,
[00:06:35] SPEAKER_02: knowledge analysis. Well, that kind of same thing, now thinking, well, well, the biological system
[00:06:41] SPEAKER_02: is still too complex to simulate. We've got all these amazing things with LLMs, but like the classic
[00:06:46] SPEAKER_02: Silicon Valley blindspot is, oh, we'll just put it all in simulation, and drugs will fall out.
[00:06:53] SPEAKER_02: That simulation is difficult. Now, part of the insight that you begin to see from the work
[00:06:59] SPEAKER_02: with Alpha, you know, go and Alpha zero is, because people just think, physical materials can
[00:07:05] SPEAKER_02: take quantum acuning. Now, quantum computing could do really amazing things, but actually simply doing
[00:07:11] SPEAKER_02: prediction and getting that prediction right. And by the way, it doesn't have to be right 100%
[00:07:15] SPEAKER_02: of time has to be really like 1% of the time, because you can validate the other 99% work,
[00:07:21] SPEAKER_02: work right, and then finding that one thing. And so literally, it's like, it's not a needle in a
[00:07:25] SPEAKER_02: haystack. It's like a needle in a solar system. Right. And it's like, but you can possibly do that.
[00:07:33] SPEAKER_02: And that's part of what led to like, okay, Silicon Valley will classically go, we'll put it all in
[00:07:38] SPEAKER_02: simulation, and that will solve it. Nope, that's not going to work. Or oh, no, we're going to have
[00:07:42] SPEAKER_02: a super intelligent drug researcher, and that will be two years down the thing. I actually look,
[00:07:48] SPEAKER_02: maybe someday, not soon. Right. So anyway, that was the kind of thing that was the the in other
[00:07:56] SPEAKER_02: different areas. Now, part of it's also, you know, kind of what a lot of people don't realize.
[00:08:02] SPEAKER_02: Actually, if I'm not going too long, I'll go to the other example that I gave, because
[00:08:08] SPEAKER_02: you'll love this. This will echo some of our conversations from 10, 15 years ago.
[00:08:15] SPEAKER_02: So I am prepping for a debate about on Sunday, this week, on whether or not AI's will replace
[00:08:25] SPEAKER_02: all doctors in a small number of years. Now, the pro case is very easy, which is we have massively
[00:08:32] SPEAKER_02: increasing capabilities. If you look at chat GBT today, you'd go like, for example, advice to
[00:08:39] SPEAKER_02: everyone who's listening to this, if you're not using chat GBT or equivalent as a second opinion,
[00:08:44] SPEAKER_02: you're out of your mind, you're ignorant. You get a serious result, check it as a second
[00:08:50] SPEAKER_02: opinion. And by the way, if it diverts, then go get a third. And so the diagnostic capabilities,
[00:08:56] SPEAKER_02: these are much better knowledge stores than any human being on the planet.
[00:09:01] SPEAKER_02: So you go, well, if a doctor is just a knowledge store, yeah, that's going away. However,
[00:09:09] the question is actually think things that really do mean doctor. And it's not like, oh,
[00:09:13] SPEAKER_02: someone will hold your hand and says, oh, it's okay, et cetera. You know, I actually think there
[00:09:19] SPEAKER_02: will be a position for a doctor 10 years and now 20 years from now, it won't be as the knowledge
[00:09:27] SPEAKER_02: store. It will be as a user of an as an expert user of the knowledge store. But it's not going to be,
[00:09:34] oh, because I went to med school for 10 years and I memorized things intensely. That's why I'm a
[00:09:39] SPEAKER_02: doctor. That all going away. Great. That part, but that, but there's a lot of other parts to being
[00:09:44] SPEAKER_02: a doctor. Now, so I went to chat, you be T pro, you know, using deep research, I went to
[00:09:52] SPEAKER_02: Claude, you know, four point, opus four point five deep research. I went to Gemini, ultra. I went
[00:10:00] SPEAKER_02: to co pilot deep research and I, and all of these things, I was doing everything I knew about
[00:10:05] SPEAKER_02: prompting for you to give me the best possible arguments for my position. Because I thought, well,
[00:10:10] SPEAKER_02: that's a bit debate on AI. Of course, I should be using AI debate. The answers were B minus,
[00:10:16] SPEAKER_02: or B, despite absolute topping. And I'm not like, maybe there's probably better prompters in the
[00:10:23] SPEAKER_02: world, but I've been doing this since I got access to GPD for six months before the public did.
[00:10:29] SPEAKER_02: Right. So I've got some experience in the whole prompting thing. It's not like I'm an amateur
[00:10:33] SPEAKER_02: profitor. And so I looked at this and I went, oh, this is very interesting. And I'm telling
[00:10:39] SPEAKER_02: of where current LLMs are limited in their reason capabilities. Because what it did is it
[00:10:47] SPEAKER_02: basically did, you know, 10 to 15 minutes of like 32 GPU compute clusters doing inference,
[00:10:55] SPEAKER_02: bringing all of all in amazing work relative to a work that an analyst would have produced in
[00:11:00] SPEAKER_02: three days was produced in 10 minutes. And of course, I set it up all in parallel, you know,
[00:11:05] SPEAKER_02: with different browser tabs, all all going into the different systems and then ran the comparisons
[00:11:10] SPEAKER_02: across them. But it's flaw was that it was giving me a consensus opinion about how articles in
[00:11:19] SPEAKER_02: good magazines, good things are arguing for that position today. And all of that was weak.
[00:11:25] SPEAKER_02: Because it was kind of like, oh, you need to have humans cross check the diagnosis. Right. Like
[00:11:30] SPEAKER_02: was a common theme across it. So well, by the way, very clearly, we know as technologists that human
[00:11:37] SPEAKER_02: cross checking the diagnosis, we're going to have AI's cross checking the diagnosis. We're going to
[00:11:41] SPEAKER_02: have AI's cross checking the AI's across checking the diagnosis. And sure, there'll be humans around
[00:11:45] SPEAKER_02: here somewhere. But like that's not going to be the central place to say in 20 years, doctors are
[00:11:51] SPEAKER_02: going to be cross checking the diagnosis. Because by the way, what doctors should be learning very quickly is
[00:11:55] SPEAKER_02: if you believe something different than the consensus opinion that an AI gives you,
[00:12:01] SPEAKER_02: you'd better have a very good reason and you're going to go do some investigation. Doesn't mean
[00:12:06] SPEAKER_02: the AI is always right. That's actually part of what you're like, what we're going to need in all
[00:12:10] SPEAKER_02: of our professions is is more sideways thinking, more lateral thinking. The okay, this is good consensus
[00:12:16] SPEAKER_02: opinion. Now, what if it's not consensus opinion? That's what doctors need to be doing. That's what
[00:12:22] SPEAKER_02: Lorenzo and you're doing. That's what coders will need to be doing. That's what it is.
[00:12:26] SPEAKER_02: And LLMs are still pretty structurally limited there. That's funny. My favorite saying is by Richard
[00:12:31] SPEAKER_00: Feynman's science is the belief in the ignorance of experts. Yes. And there are so many professions where
[00:12:36] SPEAKER_00: the credentialism is the expertness. It's like it's if this than that. And it's like I have MD,
[00:12:42] SPEAKER_00: therefore I know I have JD, therefore I know. And that's why coding is actually a little bit ahead of
[00:12:48] SPEAKER_00: it. It's like I don't care where you got your degree. This is kind of ahead of the rest of society.
[00:12:53] SPEAKER_00: Now, it's funny. Milton Friedman, one time he got asked because he was famous libertarian,
[00:12:59] SPEAKER_00: don't you think that brain surgeon should be credentials? And it's like, yeah, the market will
[00:13:02] SPEAKER_00: figure that out. Same is kind of crazy, right? But that's how we now do coding when you're in the
[00:13:07] SPEAKER_00: world of bits. But it feels like a lot of the reasons why you have this very not very advanced thinking
[00:13:15] SPEAKER_00: is because so much of it is built upon layers of credentialism. And that's that's a very good
[00:13:20] SPEAKER_00: heuristic historically. It has been. If you have a doctor that graduated at the top of their class
[00:13:25] SPEAKER_00: from Harvard Medical School, it's like probably a good doctor. Yes. Another way you critically wanted
[00:13:29] SPEAKER_02: that. Yes. Three years ago. Right. Right. It's like, no, no, I need someone who has the knowledge
[00:13:33] SPEAKER_02: base. You have it. Great. Right. But now we have a knowledge base. Yeah. I totally agree. That was
[00:13:38] SPEAKER_02: the reason I was saying you would love this because it echoes of our expertise. I thought you were
[00:13:41] SPEAKER_00: going to get into, you know, bits versus atoms where it's kind of interesting right now where it's
[00:13:46] SPEAKER_00: like all this high value work like Goldman Sachs, cell site analyst, that's deep research. Right?
[00:13:51] SPEAKER_00: Whereas fold by laundry, that's a hundred thousand dollars of cat X. It doesn't work as well as
[00:13:56] SPEAKER_00: somebody that you could pay $10 an hour to. Yes. And it's like the atom stuff is so hard to actually
[00:14:01] SPEAKER_00: disrupt. Yes. And we're going to get there eventually. But that's where Silicon Valley certainly
[00:14:05] SPEAKER_00: has a blind spot. But it's like a cat X versus op X or like, you know, bits versus atoms.
[00:14:09] SPEAKER_00: The atoms is another part. But that's also a reason why bio because bios are the are the are the
[00:14:14] SPEAKER_02: bitty atoms. Yes. Yes. Yes. Right. And what's the what's the best explanation for why it's so hard
[00:14:19] SPEAKER_01: to figure out folding laundry, but so easy to figure out? Well, it's actually not that hard to figure
[00:14:25] SPEAKER_00: out. Or why it's taken us much longer, much more expensive because we couldn't it would have been
[00:14:28] SPEAKER_01: hard to foresee that in advance. Well, I'm alright. I talked to Ilya about this a few years ago. And
[00:14:32] SPEAKER_00: it's like, why is it that if you read an Asimov novel where it talked about like how, you know,
[00:14:37] SPEAKER_00: people cook for you and fold your lawn like, why have none of these things happened? And it's like,
[00:14:41] SPEAKER_00: well, you just never had a brain that was smart enough. This was part of the problem is that you
[00:14:44] SPEAKER_00: could, I mean, yes, you have things like, you know, how do you actually pick up this water bottle?
[00:14:48] SPEAKER_00: And it turns out your hands are very, very well. Like, why are humans more advanced than every other
[00:14:52] SPEAKER_00: species? So there are two reasons. Number one is we have opposable thumbs. And then number two is
[00:14:57] SPEAKER_00: we've come up with the language system that we could pass down from generation to generation,
[00:15:00] SPEAKER_00: which is writing dolphins are very smart. Like there was actually a whole theory, which is it wasn't
[00:15:05] SPEAKER_00: just brain size. It was brain to body size. So humans were the highest. Nope, not true. And now that
[00:15:12] SPEAKER_00: we've actually measured every single animal, there are a lot of animals that have more brain over
[00:15:16] SPEAKER_00: body size. Like that, that that ratio is in tilt of an elephant or a dolphin or I forgot the
[00:15:22] SPEAKER_00: numbers, but they're a bunch that are actually more advanced than humans, but they don't have
[00:15:25] SPEAKER_00: opposable thumbs. And because of that, they never developed writing. So they can't actually iterate
[00:15:29] SPEAKER_00: from generation to generation. And humans did. And then of course, like the human edition was like
[00:15:34] SPEAKER_00: it was this and then the industrial revolution then went like that. And now it's continued like this.
[00:15:38] SPEAKER_00: But this is the reason why in the last four or five years, one of the things I realized is,
[00:15:42] SPEAKER_02: you know, because of the classic classification of human beings is homo sapiens. I actually think
[00:15:48] SPEAKER_02: we're a homo technique because it's that iteration through technology. Yes, yes, exactly. Whatever
[00:15:54] SPEAKER_02: version, writing, typing, you know, but it's we iterate through technology. That's the actual thing
[00:16:00] SPEAKER_02: goes to future generations, builds on science, you know, all the rest of it. And that's what I think
[00:16:04] SPEAKER_02: is really key. A couple of other explanations could be that we have more training data on white
[00:16:09] SPEAKER_01: collar work than sort of, you know, picking, picking things up. Or some people make this evolutionary
[00:16:14] SPEAKER_01: argument that we've been using our disposable thumbs for way longer than we've been, say, you know,
[00:16:18] SPEAKER_01: reading. Well, yeah, it's the lizard brain. Like most of your brain is not the neocortex. And like
[00:16:23] SPEAKER_00: that's the like drawn paint and everything else, which is actually very, very hard. You can't
[00:16:27] SPEAKER_00: find a dolphin that can draw our paint. And that's probably because they don't have opposable
[00:16:30] SPEAKER_00: thumbs. But it's also like maybe that part of the brain has it developed. But you have like
[00:16:33] SPEAKER_00: millions of years of evolution for these somewhat autonomous responses like a fighter flight that's
[00:16:39] SPEAKER_00: been around for a long, long time well before drawing and painting. But I think the main issue is
[00:16:44] SPEAKER_00: just like you have battery chemistry problems. Like I can't like it turns out like a lithium ion
[00:16:49] SPEAKER_00: battery is pretty cool. But the energy density of that is terrible relative to ATP with cells,
[00:16:54] SPEAKER_00: right? Like you have all of these reasons why robotics don't work. But first and foremost is
[00:17:00] SPEAKER_00: the brain was never very good. So you had robotics like fanook, which makes assembly line robots.
[00:17:05] SPEAKER_00: Those work really well. But it's like very deterministic or highly deterministic. But once you go
[00:17:09] SPEAKER_00: into like, you know, multiple degrees of freedom, you have to get so many things to work. And the
[00:17:13] SPEAKER_00: cat X it's like, I need a hundred thousand dollars to have a robot fault my laundry. And we have so
[00:17:18] SPEAKER_00: many extra people that will do that work. The economics never made sense. But this is why Japan
[00:17:23] SPEAKER_00: is a leader in robotics because they can't hire anybody. So therefore I might as well build the
[00:17:28] SPEAKER_00: true story. I went bowling in Japan and they had a robot to get like a vending machine robot that would
[00:17:33] SPEAKER_00: give you your bowling shoes. And then it would clean the bowling shoes. And it's like you would never
[00:17:38] SPEAKER_00: build that here. You'd hire sub guys for the local high school. Yes. And he go do that. Yeah. And
[00:17:43] SPEAKER_00: much cheaper and actually more effective. But it's this cap at like a cat X line in the office line
[00:17:47] SPEAKER_00: when they cross. Yeah. Yeah. Then it's like, oh, I should build robots. So that's the other thing
[00:17:51] SPEAKER_00: you probably need. But if the cost goes down, then of course it goes in in favor of cat X versus
[00:17:56] SPEAKER_00: op X. I think there's a couple things to go deeper on the robot side. So one is the density,
[00:18:02] SPEAKER_02: the the the bits to value. Yeah. Right. So like in language, when we encapsulated all these things
[00:18:08] SPEAKER_02: even into like romance novels, there's a high bits to value. Whereas when you're going to in the
[00:18:14] SPEAKER_02: whole world, there's a lot of like, how do you we abstract from all those bits? And how do you
[00:18:19] SPEAKER_02: abstract them? There's another part of it, which is kind of common sense awareness. Like this is
[00:18:23] SPEAKER_02: one of the things that like when I look at you know, GBD 2, 3, 4, 5, it's a progression of savants.
[00:18:31] SPEAKER_02: Right. And the savants are amazing. It doesn't mean the savants, but like when it makes mistakes,
[00:18:35] SPEAKER_02: like as a classic thing, so Microsoft has had running for years now agents talking to each other
[00:18:42] SPEAKER_02: long for like just like, let's go for a year and do and see what happens. And so often they get
[00:18:47] SPEAKER_02: in there like, Oh, thank you. No, thank you. No, thank you. One month later. Thank you. No, thank you,
[00:18:54] SPEAKER_02: which human beings like stop. Right. Like just like it's and that's like a that's a simple way of
[00:18:59] SPEAKER_02: putting the context awareness thing of saying, no, no, no, let's let's stay very context aware.
[00:19:05] SPEAKER_02: And even as magical as the progression has been, like much, much better data, much, much better
[00:19:11] SPEAKER_02: reasoning, much, much better personalization, et cetera, et cetera, context awareness only
[00:19:17] SPEAKER_02: is a proxy of that. Yeah. Yeah. I want to go deeper on your question about doctors reading.
[00:19:24] SPEAKER_01: Because Alex, we just released one of your talks around software eating labor. And I'm curious
[00:19:28] SPEAKER_01: where you and how you was sort of frameworks you have for thinking about what spaces are going to
[00:19:32] SPEAKER_01: have more of this co-pilot model or what spaces it's going to be in what sort of replacing the work entirely.
[00:19:37] I have I wish I could I'm going to use a no I'm too good. I'm going to get a B minus.
[00:19:42] SPEAKER_00: Maybe I'll answer and I get a B plus. I think a lot of it is like the natural like there's
[00:19:47] SPEAKER_00: the skew morphing version, which is okay. Well, I trust the doctor. Everybody trusts the doctor.
[00:19:51] SPEAKER_00: The heuristic is where did you go to medical school? Apparently two thirds of doctors now use
[00:19:54] SPEAKER_00: open evidence, which is like chat GPT, but it ingested the New England Journal medicine. I have
[00:20:00] SPEAKER_00: like a license to that. Yeah. Daniel Nadler. Yeah. Kenchewa. Right. So yeah. So so that seems like
[00:20:07] SPEAKER_00: there's no reason not to do that. Like my my seven deadly sins version. I'll simplify it,
[00:20:12] SPEAKER_00: which is like everybody wants to be lazier and richer. So this is a way that I can like get more
[00:20:16] SPEAKER_00: patients and do less work. Of course, people are going to use this. There's no reason not to.
[00:20:22] SPEAKER_00: But does it replace that particular thing? Actually, but most of like the the software eats labor
[00:20:26] SPEAKER_00: thing, it doesn't actually eat labor right now. The thing that's working the best is not like, hey,
[00:20:30] SPEAKER_00: I have a product where everybody's going to lose their job. Nobody's going to buy that product.
[00:20:34] SPEAKER_00: It's very, very hard to get that distributed. As opposed to I will give you this magic product
[00:20:38] SPEAKER_00: that allows you to be lazier. Obviously, it's not framed this way. Like lazy rich. It sounds kind of
[00:20:43] SPEAKER_00: not not great, but I'm going to let you work fewer hours and make more money. And that's that's a very
[00:20:48] SPEAKER_00: killer combo. And if you have a product like that, and it's delivered by somebody that already has
[00:20:53] SPEAKER_00: that heuristic of expertise, these are just going to go one after another and get adopted,
[00:20:59] SPEAKER_00: adopted, adopted, adopted. And then eventually you're going to have cases like the one that you
[00:21:02] SPEAKER_00: mentioned where if you don't use chat, GPT when you get a medical diagnosis, you're insane.
[00:21:06] SPEAKER_00: But that is not fully diffused across the population. Well, it's barely diffused. No, I know.
[00:21:12] SPEAKER_02: You were saying not fully. I mean, part of the reason that everyone start doing it. Yes.
[00:21:17] SPEAKER_02: 100%. What's the fastest growing product of all time? Yeah, it's barely. That's why I'm
[00:21:21] SPEAKER_00: convinced that AI is massively underhyped. Because in Silicon Valley, you might not make that claim.
[00:21:26] SPEAKER_00: Maybe it's overhyped. Maybe valuation. We all don't think it's overhyped. But I think once I meet
[00:21:31] SPEAKER_00: somebody in the real world and I show them this stuff, they have no idea. And part of it is like
[00:21:35] SPEAKER_00: they see the IBM Watson commercials and like, oh, that's AI. No, that's not AI. They see the fake AI.
[00:21:40] SPEAKER_00: They've seen chat GPT two years ago. It didn't solve a problem. And it's funny. I made this blog post.
[00:21:46] SPEAKER_00: Back when you were my investor at trial day, I called it never judge people on the present. And
[00:21:51] SPEAKER_00: this is a mistake. It's a category error that a lot of big company people make. But I mean,
[00:21:55] SPEAKER_00: that almost metaphorically. And the way that I wrote this blog post was I found a video of Tiger Woods.
[00:21:59] SPEAKER_00: He was two and a half years old. He hit a perfectly straight path. And he was on, you know,
[00:22:04] SPEAKER_00: not the, I think the tonight show or something. And there are two ways of watching that video. You
[00:22:09] SPEAKER_00: could say, well, I'm 44. I can hit a drive much further than that kid, which is correct. Or you could
[00:22:13] SPEAKER_00: say, wow, if that two and a half year old kid keeps that up, he could be really, really good. And most
[00:22:17] SPEAKER_00: people judge things on the present. Yes. And that's why it's underhyped. Because it's like, they tried
[00:22:22] SPEAKER_00: it at some point in time. Yes. There's a distribution of when they tried it, like probably
[00:22:26] SPEAKER_00: blistically, it's in the past. And like, oh, that didn't work for my use case. It doesn't work.
[00:22:30] SPEAKER_00: And that's, that's bad. But so I think it's going to diffuse largely around this like lazy rich
[00:22:36] SPEAKER_00: concept. And that's where a lot of these things have taken off. And I see it less at the very,
[00:22:41] SPEAKER_00: very big companies because you have a principal agent problem at the very big companies. Like, okay,
[00:22:45] SPEAKER_00: my company made money or saved money. I'm a director of XYZ. Like, all I know is that I want to leave
[00:22:51] SPEAKER_00: earlier and get promoted. And how does that actually help me? It helps the ethereal being of the
[00:22:56] SPEAKER_00: corporation. Whereas at a smaller business or a sole proprietor or an individual doctor,
[00:23:01] SPEAKER_00: where I run a dermatology clinic and somehow I can have five times as many patients or I'm a
[00:23:05] SPEAKER_00: plaintiffs attorney, I can have five times as many settlements. It's like, of course,
[00:23:09] SPEAKER_00: I'm going to use that because I get to be lazier and richer. Yeah. No, 100%. That's a great model.
[00:23:14] SPEAKER_02: But anyway, the other one you're reminding me, Ethan Mollock has a quote here that I use often
[00:23:19] SPEAKER_02: that every time. Yes. The worst AI you're going to use is the AI you're using today. Correct.
[00:23:24] SPEAKER_02: Because it's your mind you use it tomorrow. Yeah. Yeah. And a lot of the skeptics is exactly this.
[00:23:29] SPEAKER_00: It's like, well, I tried it two months ago and didn't solve this problem. Therefore, it's bad.
[00:23:32] SPEAKER_00: It's less easier judging on the present. Like, you have to extrapolate. Yes. And you don't want to get
[00:23:36] SPEAKER_00: like two extrapolatory, I'm like, you know, oh, LLMs have this. Like, you actually have, I feel like
[00:23:41] SPEAKER_00: the two types of people that are under-hyping AI are people that know nothing and people that know
[00:23:46] SPEAKER_00: everything. It's really interesting. It's like the meme where it's like the idiot meme where it's like
[00:23:51] SPEAKER_00: the people it's in the people in the this part of the distribution of correct. Normally,
[00:23:55] SPEAKER_00: the meme is the opposite. These people are smart. You know, they're dumb. These people are smart.
[00:23:59] SPEAKER_00: You know, they're smart. Everybody here like this is this part of the curve is actually correct.
[00:24:03] SPEAKER_00: Because they're the ones that are using it to get richer and be lazier.
[00:24:06] SPEAKER_00: The other thing I also tell people is if you haven't found a use of AI that helps you
[00:24:12] SPEAKER_02: on something serious today, not just ride a sonnet for your kids birthday or, you know, I've got
[00:24:17] SPEAKER_02: these ingredients in my fridge which I make do those too. But if you haven't for something like
[00:24:21] SPEAKER_02: work for like something as serious about what you're doing, you're not trying hard enough.
[00:24:26] SPEAKER_02: Yeah. Yeah. It doesn't that it does everything. Like, for example, I still think if I put in
[00:24:31] like how should read Hoffman make money investing in AI and I'll go try that again. I want
[00:24:35] SPEAKER_02: I suspect that I will still get what I think is the Bozo business professor answer versus the actual
[00:24:41] SPEAKER_02: game name of the game. But everyone should be trying. I, you know, like, for example, we put
[00:24:50] SPEAKER_02: when we get decks, we put them in and say, give me a due diligence plan, right? If not everybody
[00:24:56] SPEAKER_02: here doing that, that's a mistake. Yeah. Because you five minutes, you get one and you go, oh, no,
[00:25:01] SPEAKER_02: not two, not five. Oh, but three is good. And it would have taken me a day to getting to about three.
[00:25:06] SPEAKER_02: Yeah. Yeah. In terms of it, let's go back to extrapolation. Obviously, the last few years have had
[00:25:12] SPEAKER_01: incredible growth. You were involved, of course, with open access since the beginning. When we look
[00:25:17] SPEAKER_01: for the next few years, these brought a question as to whether scaling laws will hold, whether
[00:25:21] SPEAKER_01: sort of the limitations or how far we can get with with LLMs, do we need another breakthrough
[00:25:27] SPEAKER_01: of a different kind? What is your view on some of these questions? So one of the things we,
[00:25:34] SPEAKER_02: you know, we all swim in this universe of extrapolating the future. One of my things is great about
[00:25:38] SPEAKER_02: Silicon Valley. And so you get such things as, you know, theories of singularity, theories of
[00:25:44] SPEAKER_02: superintelligence, theory of exponential getting to superintelligence soon. And what I find is usually
[00:25:53] SPEAKER_02: the mistake in that is not the fact that extrapolating the future. That's smart. And people need to do
[00:25:57] SPEAKER_02: that and far too people people do. I think I remember liking your post and helping promote it if I
[00:26:02] SPEAKER_02: recall. But it's the notion of, well, what curve is that? Like if it's a subant curve, that's
[00:26:11] SPEAKER_02: different than, oh my gosh, it's an apotheosis. And now it's got, you know, it's like, no, no,
[00:26:19] SPEAKER_02: it'll be an even more amazing subant than we have. But by the way, if it's only a subant,
[00:26:23] SPEAKER_02: there's always room for us. There's always rooms for the generalists and the cross checker
[00:26:28] SPEAKER_02: and the context awareness and all the rest of that. Now, maybe, maybe it'll cross over a threshold
[00:26:33] SPEAKER_02: or not. Maybe it won't, you know, like I think there's a bunch of different questions. But that
[00:26:37] SPEAKER_02: extrapolation too often goes, well, it's exponential. So in two and a half years magic. And you're like,
[00:26:44] SPEAKER_02: well, look, it is magic, but it's not all magic is the is the kind of way you're doing it now.
[00:26:50] SPEAKER_02: So my own personal belief is that, so the critics of albums make a mistake in that. And you
[00:26:59] SPEAKER_02: know, we can go through all the different critics. Oh, not in all the representation. And it,
[00:27:03] SPEAKER_02: it screws up on, you know, prime numbers and, you know, blah, blah, blah, blah. We've all
[00:27:07] SPEAKER_02: in ours and stronger. Yeah, exactly. Yeah. You know, and I'll see it's broken. And you're like,
[00:27:13] SPEAKER_02: you're missing the magic, right? Like, yes, maybe there's some structural things that over time,
[00:27:19] SPEAKER_02: even in three to five years will continue to be a difficult problem for LMS. But AI is not just
[00:27:25] SPEAKER_02: the one LLM to rule them all. It's a combination of models. We already have combination of models.
[00:27:30] SPEAKER_02: We use the fusion models for various image and video tasks. Now, by the way, they wouldn't work,
[00:27:35] SPEAKER_02: but also, well, also having LLMs in order to have the ontology to say, create me an Eric
[00:27:41] SPEAKER_02: Tornberg as a Star Trek captain, you know, going out to, you know, explore the universe and meeting,
[00:27:48] SPEAKER_02: making first contact with the Vulcans and so forth, which, you know, now with our phone,
[00:27:53] SPEAKER_02: we could do that, right? And it would be there courtesy open AI and, you know, VO, because Google's
[00:27:59] SPEAKER_02: models also very good. But it needs the LLMs for that. But the thing that people on track is,
[00:28:04] SPEAKER_02: it's going to be LLMs and the fusion models. And I think other things with a fabric across them.
[00:28:11] SPEAKER_02: Now, one of the interesting questions is, is the fabric fundamental LLMs is the fabric of the
[00:28:16] SPEAKER_02: things? I think that's a TBD on this and the degree to which it gets to intelligence is an
[00:28:21] SPEAKER_02: interesting question. Now, one of the things I think is a, you know, I talk to all the critics
[00:28:27] SPEAKER_02: intensely, not because I necessarily agree with the criticism, but I'm trying to get to the,
[00:28:31] SPEAKER_02: what's the kernel of insight? And like one of the things that I loved about, you know, kind of a
[00:28:37] SPEAKER_02: set of recent conversations with Stuart Russell was saying, hey, if we could actually get the fabric
[00:28:42] SPEAKER_02: of these models to be more predictable, that would greatly, uh, uh, uh, lay the fears of what happens
[00:28:51] SPEAKER_02: if something goes amok. Okay, let's try to do that. Now, I don't think the whole verification
[00:28:56] SPEAKER_02: about puts like, like logical, like we can't even do verification of coding, right? Like,
[00:29:01] SPEAKER_02: verification of this strikes me as very heart and a brilliant man. Maybe we'll figure it out.
[00:29:06] SPEAKER_02: But the, um, but, but on the other hand, the, hey, this is a good goal. Can we make that more
[00:29:13] programmable, reliable? I think that is a good goal that people, that very smart people should be
[00:29:19] SPEAKER_02: working on. And by the way, smart eyes. Well, that's some of the math side is like, if you think
[00:29:25] SPEAKER_00: about the foundation of the world, I mean, uh, philosophy is the basis of everything. Actually,
[00:29:29] SPEAKER_00: math came from philosophy. It's called the Cartesian play in Afro-Day Cart. You know, you're a
[00:29:32] SPEAKER_00: philosophy based, right? So you have, you have, uh, philosophy, math, physics, like why did Newton
[00:29:39] SPEAKER_00: build calculus to understand the real world? So math, physics, physics gets you chemistry, chemistry
[00:29:45] SPEAKER_00: gets you biology and then biology gets you psychology. So that's kind of the stack. So if you
[00:29:50] SPEAKER_00: solve math, that's actually quite interesting because, um, there's a professor at Rutger is a
[00:29:54] SPEAKER_00: contorvitch who's written about this a lot. Um, I find this part fascinating. This is a former
[00:30:00] SPEAKER_00: mathematician because there are some very, very hard problems. Um, there, there's a rumor that the
[00:30:06] SPEAKER_00: Navier-Stokes equation is going to be solved by deep mind, which would be huge. That's one of the
[00:30:10] SPEAKER_00: claim math problems. But, you know, the Riemann hypothesis, like this is not, there's no eval.
[00:30:15] SPEAKER_00: Yes. Right? If it's like, uh, this is why if you look at the progression of AI, there is the Amy,
[00:30:20] SPEAKER_00: the American Invitational Math Examination, where you, the answers are all just like three,
[00:30:25] SPEAKER_00: it's just integers. It's like zero to nine hundred and ninety nine is the answer. And then of course,
[00:30:29] SPEAKER_00: you can keep trying different things. Then you either get the right answer. You don't, and it's very,
[00:30:32] SPEAKER_00: very easy to do that. Whereas once you get to proofs, very, very hard. Yes. Um, and if you solve that,
[00:30:38] SPEAKER_00: I mean, is that a G I know because the goalposts keep changing out a G I, but math is just so
[00:30:44] SPEAKER_00: interesting. A G I is the AI. We have an embedding. Exactly. That's exactly it's the corollary to
[00:30:48] SPEAKER_00: it's like, you know, the worst AI you're going to try is today. Well, A G I is what you're going to
[00:30:52] SPEAKER_00: have to borrow. That's the same kind of thing. But math is a very, very interesting one as well.
[00:30:57] SPEAKER_00: Because you have these things. It's not like solving a school math. Right. This is like, if you're
[00:31:01] SPEAKER_00: able to actually logically construct a proof or something and then validate it. Yeah.
[00:31:06] SPEAKER_00: There's a whole programming language called lean, which is for that. Like that, that stuff is also
[00:31:09] SPEAKER_00: fascinating. So there's so many different vectors of attack, which is the other, the other way of
[00:31:13] SPEAKER_00: thinking about it. So as you just mentioned, Alex, read your philosophy major, but you're also
[00:31:19] SPEAKER_01: very interested in deep and neuroscience. And some people say that, hey, we'll never create AI
[00:31:23] SPEAKER_01: with its own consciousness because we don't understand our own consciousness. We don't understand
[00:31:27] SPEAKER_01: our own brain works. And then the broader question is, oh, will AI have its own goals or will
[00:31:32] SPEAKER_01: have its own agency? What would it sort of view on some of these questions around in consciousness?
[00:31:37] SPEAKER_01: Wait, say, yeah. Well, consciousness is its own
[00:31:41] SPEAKER_02: tarball, which I will say a few things about. I think agency and goals is almost certain.
[00:31:51] SPEAKER_02: There is a question. I think this is one of the areas where we want to
[00:31:54] SPEAKER_02: ex have some clarity and control. That was a little bit like the kind of question. What kind of
[00:31:59] SPEAKER_02: compute fabric holds it together? Because you can't get complex problem solving without it being
[00:32:05] SPEAKER_02: able to set its own minimum sub goals and other kinds of things. And so so goal setting and
[00:32:11] SPEAKER_02: behavior and inference from it. And that's where you get the class to kind of like, well, you tell
[00:32:15] SPEAKER_02: it to maximize, you tell it to maximize paper clips and it tries to convert the entire planet
[00:32:19] SPEAKER_02: in the paper clips. And there's one thing that's definitely old-comp computer word of that, which
[00:32:25] SPEAKER_02: is no context awareness, something I even worry about modern AI systems. But on the other hand,
[00:32:29] SPEAKER_02: it's like, look, if you're actually creating intelligence, they don't go, oh, let me go flip.
[00:32:35] SPEAKER_02: Let me just go try to convert everything in a paper clip. It's like it's actually in fact not that
[00:32:40] SPEAKER_02: simple in terms of how it plays. Now consciousness is an interesting question because you got some
[00:32:45] SPEAKER_02: very smart people, Roger Penrose, who I actually interviewed way back when on Emperor's New Mind,
[00:32:51] SPEAKER_02: speaking of mathematicians. And who are like, look, actually, in fact, there's some thing about
[00:32:58] SPEAKER_02: our form of intelligence, our form of of of computational intelligence that's quantum-based
[00:33:04] SPEAKER_02: that has to do with how our physics work that has to do with things like tubularisms or
[00:33:09] SPEAKER_02: and by the way, it's not impossible. Like that's that's that's a it's a coherent theory from a very
[00:33:15] SPEAKER_02: smart mathematician like one of the world's smartest, right? Like it's kind of in the category of
[00:33:21] SPEAKER_02: there's other people as smart, but there's no one smarter, right? In the in the convector. And so
[00:33:26] SPEAKER_02: so that's possible. I don't think you need consciousness for goal setting or reasoning.
[00:33:35] SPEAKER_02: I'm not even sure you need consciousness for certain forms of self-awareness. There may be
[00:33:40] SPEAKER_02: some forms of self-awareness that consciousness is necessary for. It's a tricky thing.
[00:33:45] SPEAKER_02: Philosophers have been trying to address this not very well for as long as we've got records of
[00:33:51] SPEAKER_02: philosophy, right? And philosophers agree. I'm not. The philosophers wouldn't think I was
[00:33:55] SPEAKER_02: throwing a month of the bus with this. They're like, yeah, this is our problem because it ties
[00:33:59] SPEAKER_02: to agency and free will and a bunch of other things. And and I think that the right thing to do is
[00:34:03] SPEAKER_02: keep an open mind. Now, part of keeping an open mind, I think Mustafa Suleiman wrote a very good
[00:34:07] SPEAKER_02: piece in the last month or two on like semi-consciousness, which is we make too many mistakes all of the
[00:34:14] SPEAKER_02: touring test, the piece of brilliance, which is, well, it talks to us. So therefore, it's fully
[00:34:20] SPEAKER_02: intelligence and all the rest. And so similarly, you had that kind of, you know, kind of nutty
[00:34:25] SPEAKER_02: event from that Google engineers that I asked this earlier model. Was it conscious? And it said,
[00:34:30] SPEAKER_02: yes. So therefore it is. Yes, QE. You're just like, no, no, no, no. Like you have to be not
[00:34:36] SPEAKER_02: misled by that kind of thing. And like, for example, you know, the kind of thing that, you know,
[00:34:41] SPEAKER_02: what would I actually think most people obsess about the wrong things when it comes to AI,
[00:34:47] SPEAKER_02: obsess about the climate change stuff because actually, in fact, if you apply intelligence at the
[00:34:52] SPEAKER_02: scale and availability of electricity, you're going to help climate change. You're going to solve
[00:34:57] SPEAKER_02: grids and appliances and a bunch of other stuff. And just like, no, this will be net super positive.
[00:35:02] SPEAKER_02: And by the way, you already see elements of it. Google applied its algorithms to its own data
[00:35:06] SPEAKER_02: centers, which are some of the best tune grid systems that were all 40% energy savings. I mean,
[00:35:13] SPEAKER_02: just, you know, just that did it. And just applying it. So that's the mistake. But one of the areas,
[00:35:18] SPEAKER_02: I think, is this question around like, what is the way that we want children growing up with AI?
[00:35:24] SPEAKER_02: What is their epistemology? What is their learning curves? You know, what are the things that kind of
[00:35:30] SPEAKER_02: play to this? Because that kind of question is something that we want to be very intentional about
[00:35:35] SPEAKER_02: in terms of how we're doing it. And I think that's like, like, if you want to go ask a good question
[00:35:40] SPEAKER_02: that we should be trying to get good answers that you could do something again and contributing good
[00:35:44] SPEAKER_02: answers to, that's a good one. Yeah. Well, the most coach and argument that I've heard against
[00:35:49] SPEAKER_00: free will is just that we are biochemical machines. So if you want to test somebody's free will,
[00:35:54] SPEAKER_00: get them very hungry, very angry, like all of these things, where it's just there's a hormone. It's
[00:35:58] SPEAKER_00: like, no, app and effort. It's just like, that makes you act a particular way. It's like an override.
[00:36:03] SPEAKER_00: So you have this like free will thing, but then you just insert a certain chemical and then like,
[00:36:07] SPEAKER_00: boom, it changes. Are you saying you're not a Cartesian? You don't have a little pineal gland that
[00:36:12] SPEAKER_02: connects the two senses. I don't know. So, but it's true. It's like, like, hangar is, yeah, I'm
[00:36:18] SPEAKER_00: hangry. Like, that's a thing. Yes. And you know, what is the, like, do you actually want if you're
[00:36:23] SPEAKER_00: developing super intelligence, do you want to have this like kind of silly override? I mean, the reason
[00:36:28] SPEAKER_00: why people go to jail sometimes that are perfectly normal is they get very angry. They do
[00:36:32] SPEAKER_00: things that are kind of like out of character, but it's actually not out of character if you think
[00:36:36] SPEAKER_00: about this free will override of just like chemicals going through your bloodstream, which is kind
[00:36:39] SPEAKER_00: of crazy to think about. Look, since we're on a geeky nerdy podcast, I'm going to say two geeky
[00:36:44] SPEAKER_02: nerdy things are one, the classic one is people say, yes, we're biocomical machines, but let's not
[00:36:48] SPEAKER_02: be overly simplistic on what about chemical machine is. That's like the pen rows, quantum computing,
[00:36:53] SPEAKER_02: et cetera. And you get to this weird stuff in quantum, which is, well, it's, it's a probabilistic
[00:37:01] SPEAKER_02: dual supervisitional form until it's measured. Why is there magic and measurement? And is that
[00:37:06] SPEAKER_02: magic and measurement something that's conscious? You know, there's a bunch of stuff there. The other
[00:37:12] SPEAKER_02: thing that I think is interesting that we're seeing as a resurgence and philosophy a little bit
[00:37:17] SPEAKER_02: is idealism. Like, we would have thought as physical materialists that that we go, no, no,
[00:37:23] SPEAKER_02: idealists were disproven, they're gone, but actually at the beginning to say, no, actually,
[00:37:27] SPEAKER_02: in fact, what exists is thinking and that all of the physical things around us come from that
[00:37:34] SPEAKER_02: thinking. And obviously we see versions of this because, you know, I find myself entertained
[00:37:40] SPEAKER_02: frequently here in Silicon Valley by people saying, we're living in a simulation. I know what you
[00:37:43] SPEAKER_02: know it. And you're like, well, your simulation theory is very much like Christian intelligent design
[00:37:49] SPEAKER_02: theory. It's the, I have things that I can't explain. So therefore creator, no, therefore simulation,
[00:37:56] SPEAKER_02: no, therefore creator of simulation. You're like, no, no, no, but I, you're so clearly I'm not
[00:38:01] SPEAKER_02: an idealist, but that's why I see some resurgence of idealism happening. Why suspect? I suspect we'll
[00:38:08] SPEAKER_01: solve for AGI before we solve for the, for various definitions of AGI before we solve for the hard
[00:38:13] SPEAKER_01: problems of consciousness. Yes. I want to return to LinkedIn. How we began the conversation,
[00:38:20] SPEAKER_01: because we were lucky to, or I was lucky to work many years with you, we would get pitches every
[00:38:25] SPEAKER_01: week about a LinkedIn disruptor last 20 years, right? And so, and nothing's come even close.
[00:38:32] SPEAKER_01: And so it's fascinating. I'm curious why people sort of underrated how hard it was. And people
[00:38:38] SPEAKER_01: have this about Twitter too, or other things that kind of look simple, perhaps, but are actually
[00:38:42] SPEAKER_01: very, very difficult to unseat and have a lot of staying power. And it's interesting, you know,
[00:38:46] SPEAKER_01: open AI, they said they're coming out with a job service to quote, use AI to help find the
[00:38:51] SPEAKER_01: perfect matches between what companies need and what workers can offer. I'm curious how you
[00:38:55] SPEAKER_01: think about sort of LinkedIn's durability. So look, I obviously think LinkedIn is durable,
[00:38:59] SPEAKER_02: but first and foremost, I kind of look at this as humanity's society industry. So first and
[00:39:05] SPEAKER_02: foremost is what are the things that are good for humanity, then what's good for society,
[00:39:08] SPEAKER_02: then what's good for industry? And by the way, we do industry to be good for society humanity.
[00:39:13] SPEAKER_02: It's not, and it's not oppositional. It's just a, you know, how you're making these decisions
[00:39:17] SPEAKER_02: and what you're thinking about. So I would be delighted if there were new amazing things that helped people,
[00:39:24] SPEAKER_02: you know, kind of make productive work, fine productive work in Nekhan, do them. We're having
[00:39:30] SPEAKER_02: going to have all this job transition coming from technological disruption with AI. Like it would be
[00:39:35] SPEAKER_02: awesome. Of course, it would be extra awesome. If it was LinkedIn, bringing it, just given my own
[00:39:41] SPEAKER_02: personal craft, my hands and pride at what we built and all the rest now. The thing with LinkedIn,
[00:39:48] SPEAKER_02: and you know, Alex was with me on a lot of this journey, you know, as I sought his advice under his
[00:39:53] SPEAKER_02: things. The LinkedIn was one of those things where it's where the turtle eventually actually, in fact,
[00:40:03] SPEAKER_02: like grows into something huge, because for many, many years, the general scuttle butt in Silicon
[00:40:09] SPEAKER_02: Valley was LinkedIn was the, was the the Burt Dull boring useless thing, etc. And it was going to be
[00:40:16] SPEAKER_02: friendship. Probably most people listening to this don't know what friendship is, then my space,
[00:40:20] SPEAKER_02: maybe a few people have heard of that. Right. You know, and that of course we got, you know, Facebook
[00:40:25] SPEAKER_02: and Metta and you know, TikTok and all rest. And part of the thing for LinkedIn is it's built a
[00:40:31] SPEAKER_02: network that's hard to build. Right. Because it doesn't have the same sizzle and pizzazz that
[00:40:37] SPEAKER_02: photo sharing has. It doesn't have the same sizzle and pizzazz that, you know, you know, like one of the
[00:40:43] SPEAKER_02: things that, you know, you were referencing the Seven Deadly Sins comment. And back when I started
[00:40:49] SPEAKER_02: doing that 2002, yes, I left my walker at the door. The thing that I used to say was Twitter was
[00:40:56] SPEAKER_02: identity. I actually mistook it. It's wrath. Right. And so it doesn't have the wrath, you know,
[00:41:01] SPEAKER_02: kind of component of it. And so, and so the, you know, the thing that, and you said with like
[00:41:09] SPEAKER_02: minimal opinions, greed, great, you know, because seven Deadly Sins kind of, you know, because,
[00:41:13] SPEAKER_02: because that's, you know, a motivation that's very common across a lot of your things. Rich and lazy.
[00:41:17] SPEAKER_02: Yes, exactly. And so, or, you know, you're putting it in the punchy way, but simply being productive.
[00:41:25] SPEAKER_02: Yeah. Yeah. More value creation and accruing some of that value to yourself. And so, um,
[00:41:31] and so I think the reason why it's been difficult to create a, a disruptor to LinkedIn is it's a very
[00:41:38] SPEAKER_02: hard network to build. It's actually not easy. And, um, and by staying really true to it,
[00:41:45] SPEAKER_02: you end up getting a lot of people going, well, this is, this is where I am for that. And now I have
[00:41:48] SPEAKER_02: a network of people with us on it. And we are here together collaborating and doing stuff together.
[00:41:54] SPEAKER_02: And that's the thing that a new thing would have to be. And, you know, I, you know, I,
[00:42:02] SPEAKER_02: when I saw GVD4, um, and knew that Microsoft had access to this. I called the LinkedIn people and
[00:42:10] SPEAKER_02: said, you guys have got to get in the room to see this. Right. Because you need to start thinking
[00:42:17] SPEAKER_02: about what are the ways we help people more with that? Because you start with, this is actually one
[00:42:21] SPEAKER_02: of the things that I think people don't realize but Silicon Valley. Because, you know, the general
[00:42:24] SPEAKER_02: discussion is, oh, you're trying to make all this money through equity and all this revenue.
[00:42:28] SPEAKER_02: Of course, you know, business people are trying to do that. But they don't realize as you start with
[00:42:32] SPEAKER_02: what's the amazing thing that you can suddenly create. And part of it is like lots of these companies
[00:42:38] SPEAKER_02: like get started with and you go, what's your business money? I don't know.
[00:42:41] SPEAKER_02: Like, yeah, we're going to try to work it out. But I can create something amazing here. And that's
[00:42:47] SPEAKER_02: actually one of the fundamental like places of what the, you know, call it the religion of Silicon
[00:42:52] SPEAKER_02: Valley and the knowledge of Silicon Valley that I so much, you know, love and admire and embody.
[00:42:58] SPEAKER_02: That's actually a question that I have. So I'll say one thing. There's a huge compliment to LinkedIn.
[00:43:02] SPEAKER_00: It's anti-fragile. Yes. And that like Facebook, oh, nobody goes there anymore. It's like the Yogi
[00:43:06] SPEAKER_00: Baron. It's too crowded. Nobody goes there anymore. It's other too many parents there. And there's
[00:43:10] SPEAKER_00: always been a new one. Like, where did it snap? Like how did snap start? Like all these other networks
[00:43:14] SPEAKER_00: started. These people didn't want to hang out with their boomer parents. My kid won't let me follow
[00:43:18] SPEAKER_00: him on Instagram. Right. He does want to use Facebook. So LinkedIn has survived through all of that.
[00:43:23] SPEAKER_00: But you reference something that I think is a very interesting point, which is back in like Web 2.
[00:43:28] SPEAKER_00: It was like get lots of traffic. Yes. Get amazing retention. You know, smile curve.
[00:43:32] SPEAKER_00: Yes. And then you will figure out monetization. Yes. And like that isn't happening right now.
[00:43:37] SPEAKER_00: It's not like get lobby. Yes, it happens at GBT. It was like it's $20. Right. Like the monetization
[00:43:42] SPEAKER_00: was kind of built in very, very clear subscription versus like become giant. Yes.
[00:43:46] SPEAKER_00: Build a giant. Like do you think there will be new ones of those with a guy? Yes. And there will be
[00:43:51] SPEAKER_02: new kind of free movements. It's part of our tool chest. Now part of the reason why it's more
[00:43:55] SPEAKER_02: tricky, especially when you're doing open a eyes because the like the cogs are changed a little.
[00:44:01] SPEAKER_02: Yes. Right. Yes. No, but like, and so you just can't. This is one of reasons why at PayPal,
[00:44:06] SPEAKER_02: we had to change to like we, as you know, because you were close to us there. Like we had to change
[00:44:11] SPEAKER_02: to a paid model because we're like, Oh, look, we have expanentiating volume, which means
[00:44:15] SPEAKER_02: expanentiating cost curve, which means despite having raised hundreds of millions of dollars,
[00:44:20] SPEAKER_02: we could literally count the, we could point to the hour that we'd go out of business.
[00:44:24] SPEAKER_02: Right. Because you know, no, you can't have an expanentiating cost curve. So I think that's one
[00:44:28] SPEAKER_02: of the reasons why some of it has been different in AI because you like, you can't have an
[00:44:33] SPEAKER_02: expanentiating cost curve without at least a following revenue curve. Right. But it's it's almost
[00:44:37] SPEAKER_00: no fun. It's like Pinterest. It's like, how are they going to make money now? A big public company.
[00:44:41] SPEAKER_00: It's like, there were a lot of these during that era. And now it's like, they're burning lots of
[00:44:44] SPEAKER_00: money. They're raising lots of money. But the subscription revenue is baked in from day zero.
[00:44:48] SPEAKER_00: And that's that's the fun. But they have to because of the cost. They have to exactly. Yeah.
[00:44:52] SPEAKER_00: So I'm waiting for like one of these like, you know, net new companies that appeals to probably
[00:44:55] SPEAKER_00: one of the seven deadly sins that has to be the new counter part. Yeah. Well, I'd be happy to work
[00:45:01] SPEAKER_00: on it. Yes. Well, it is fascinating. Some people, many people have tried sort of different angles on
[00:45:06] SPEAKER_01: LinkedIn. One that I was curious about a few years ago was sort of this idea of could you get,
[00:45:10] SPEAKER_01: what's on LinkedIn is resumes, but not necessarily references. But the same way that resumes are
[00:45:14] SPEAKER_01: viral references or like anti-viral or anti-mimetic and people don't want them on the internet. If
[00:45:19] SPEAKER_01: there was this data set that people wanted on the internet, LinkedIn would have done it to some
[00:45:23] SPEAKER_01: degree. But yeah, I think most people who try these attempts don't kind of appreciate
[00:45:28] SPEAKER_01: sort of the subtleties of. And I actually, I mean, we do have the equivalent of bookpler
[00:45:33] SPEAKER_02: references on endorsements. Yes. endorsements. You don't have a negative reference. Well, but
[00:45:37] SPEAKER_02: but if I have a way, part of the reason why negative references is you have complexity and
[00:45:41] SPEAKER_02: social relationships, that's the negative virality point that you were just making. And then you
[00:45:45] SPEAKER_02: also have complexity on like, you know, kind of not just legal liability, but social relationships
[00:45:51] SPEAKER_02: and a bunch of other stuff. Now, LinkedIn is still the best way to find a negative reference. I
[00:45:55] SPEAKER_02: mean, that's actually one of the things that that I use LinkedIn to figure out who might know a person.
[00:46:01] SPEAKER_02: Yeah. And I have a standard email. You probably got a bunch of these for me where I've, where I've,
[00:46:06] SPEAKER_02: I email people saying, um, could you rate this person for me from one to 10 or or reply call me?
[00:46:15] SPEAKER_02: A negative one. Yes. And when you get a call, me, you're like, okay,
[00:46:20] SPEAKER_02: they're gonna take the call. Yeah. I understand. Right. And by the way, sometimes you go
[00:46:26] SPEAKER_02: when a person writes back to you, like, really? Like, best person you know, right? But what you're
[00:46:32] SPEAKER_02: looking for is like a set of eight nons. And if you get a set of eight nons, you might still call
[00:46:36] SPEAKER_02: and get some, get some information, but you're like, okay, I got, I got a quick or a French
[00:46:40] SPEAKER_02: information. Whereas by the way, more often than not, you know, when you're checking someone,
[00:46:44] SPEAKER_02: you really know, you get a couple call me's. Yeah. Cause, cause, and it's just that quick,
[00:46:48] SPEAKER_02: because it's email one sentence thing, get back call me. You're like, okay, I understand. Yeah.
[00:46:56] We have about 10 minutes left just to just check a couple of last things we'll get into.
[00:47:00] SPEAKER_01: Is there anything you wanted to make sure? But we can do this again. This is always fun.
[00:47:03] SPEAKER_02: Yes. That's great. The, um, I'm curious read as you've sort of continued to up level in your
[00:47:09] SPEAKER_01: career and have more opportunities and they seem to come compound, especially, you know, post-selling
[00:47:12] SPEAKER_01: LinkedIn. How have you decided where is the highest leverage use for your time? Where can you
[00:47:17] SPEAKER_01: have the biggest impact? What's your mental framework? So, I mean, one of the things that I'm
[00:47:25] SPEAKER_02: sure I speak for all three of us is an amazing time to be alive. I mean, this AI and the
[00:47:30] SPEAKER_02: transformation of what it means for evolving homo tech, and what, what is possible in life,
[00:47:37] SPEAKER_02: and in society and work and all of us, just amazing. And so I stay as, uh, involved with that as
[00:47:44] SPEAKER_02: I possibly can't like, it has to be something that's so important that I will stop doing that. Yeah.
[00:47:51] SPEAKER_02: Now, within that, you know, part of that was, you know, co-founding, I'm honest AI with
[00:47:56] SPEAKER_02: Sajar Tammuqijee, CEO, and per-author, Bimperol Malatis, inventor of some T cell therapies. So it's
[00:48:04] SPEAKER_02: all like, like, for example, getting an instruction from him on the FDA process, you know, that's the kind
[00:48:09] SPEAKER_02: of thing that makes us all run the screening for the hills, right, as an instance. And so,
[00:48:16] SPEAKER_02: uh, you know, that kind of stuff, but also, um, you know, like one of the things I think is really
[00:48:21] SPEAKER_02: important is as technology drives more and more of everything that's going on in society,
[00:48:26] SPEAKER_02: how do we make government more intelligent on technology? And so, you know, every kind of,
[00:48:31] SPEAKER_02: you know, kind of well-ordered Western democracy, I've done, been doing this for at least 20 to 25
[00:48:37] SPEAKER_02: years. If, if a minister, you know, or kind of senior person from my, from a democracy comes and
[00:48:44] SPEAKER_02: asks for advice, I give it to them. So, you know, just last week I was in France talking with
[00:48:49] SPEAKER_02: Macron because he's trying to figure out like, how do I help French industry, French society,
[00:48:54] SPEAKER_02: French people? What are the things I need to be doing? You know, if all the frontier models are
[00:48:59] SPEAKER_02: going to be built in the US and maybe China, what does that mean for how I help, you know, our
[00:49:04] SPEAKER_02: people and so forth? And, and he's doing the exact right thing, which is I understand that I have
[00:49:09] SPEAKER_02: a potential challenge. What do I do to help my people? How do I reach out? How do I talk? Sure,
[00:49:14] SPEAKER_02: they've got Miss Rale, they've got some other things, but like, how do I max only help what I'm
[00:49:19] SPEAKER_02: doing? And so, putting a bunch of time into that as well. Yeah. I remember seeing your, your,
[00:49:23] SPEAKER_01: your calendar and it was, it seemed like seven days a week, meetings absolutely stacked. And one of
[00:49:29] SPEAKER_01: the ways in which I've gone to six and a half days. Okay. So, I've had you calm down. So, one of the
[00:49:34] SPEAKER_01: ways in which you're able to do that, one, it's important problems, but two, you work on projects with
[00:49:39] SPEAKER_01: friends, sometimes over decades. And you, you, me will close here, you've thought a lot about
[00:49:43] SPEAKER_01: friendship. You've, you've written about it, you've spoken about it. I'm curious what you've found
[00:49:48] SPEAKER_01: most remarkable or most surprising about French ever where you think more people should appreciate,
[00:49:52] SPEAKER_01: especially as we enter this AI era where people, yeah, sort of our questioning, you know, the next
[00:49:57] SPEAKER_01: narration, what's there going to be relationship to? I actually'm going to write a bunch about this
[00:50:01] SPEAKER_02: specifically because AI is now bringing some very important things that people need to understand,
[00:50:06] SPEAKER_02: which is friendship is a joint relationship. It's not a, oh, you're just loyal to me or you just
[00:50:11] SPEAKER_02: do things for me. Oh, this person does things for me. Well, there's a lot of people who do things for
[00:50:16] SPEAKER_02: you. Your bus driver does things for you. You know, like, like, but that doesn't mean that you're friends.
[00:50:21] SPEAKER_02: Friends, like, for example, like a classic way of putting is, oh, I got a really bad day and I show up
[00:50:26] SPEAKER_02: with my friend Alex and I want to talk to him. And then Alex's like, oh, my god, here's my day. I'm like,
[00:50:30] SPEAKER_02: oh, your day's much worse. We're going to talk about your day versus my day. You know, that's the
[00:50:34] SPEAKER_02: kind of thing that happens because what I think fundamentally happens with friends is two people
[00:50:39] SPEAKER_02: agree to help each other become the best possible versions of themselves. And by the way, sometimes
[00:50:45] SPEAKER_02: that bleeds to friendship conversations that are tough love. They're like, yeah, you're f***ing this
[00:50:50] SPEAKER_02: up and I need to talk to you about it. Right? It's not, I tell you like, you know, the whole sick of
[00:50:55] SPEAKER_02: and see phase and AI thing. It's not that it's like the how do how do I help you? But it's part of
[00:51:02] SPEAKER_02: also the thing that I gave the commencement speech and Vanderbilt a few years back and was on
[00:51:08] SPEAKER_02: friendship. And part of it was to say, look, part of friends is not just does, does Alex help me,
[00:51:16] SPEAKER_02: but Alex allows me to help him. Right? And as part of that, that's part of how I become a deeper
[00:51:22] SPEAKER_02: friend. I'd learn things from it. It's not just helping that helping Alex. That joint relationships
[00:51:26] SPEAKER_02: really important. And you're going to see all kinds of nutty people saying, oh, I have your AI
[00:51:31] SPEAKER_02: friend right here and say, no, you don't. It's not a bidirectional relation. Maybe awesome companion.
[00:51:37] SPEAKER_02: Like just spectacular, but it's not a friend. And you need to understand like part of friend is part
[00:51:42] SPEAKER_02: of when we begin to realize that life's not just about us that we that it's a team sport. We go into
[00:51:47] SPEAKER_02: it together that sometimes friendship conversations are wonderful and difficult. And that kind of thing.
[00:51:54] SPEAKER_02: And I think that's what's really important. And now that we've got this blurriness that AI
[00:51:59] SPEAKER_02: has created, it's like, shoot, I have to go write some of this very soon. So the people understand
[00:52:04] SPEAKER_02: how to navigate it. Why they should not think about AI anytime soon as friends.
[00:52:10] SPEAKER_01: Well, it's been one thing I've always always appreciate about you as well as you're able to be
[00:52:14] SPEAKER_01: friends with people for whom you have disagreements with or people for whom you know, you are not
[00:52:19] SPEAKER_01: close to for a few years, but you reconnect and sort of that ability is about us making each other
[00:52:26] SPEAKER_02: the better versions ourselves. And sometimes that, you know, those sometimes those go through rough
[00:52:31] SPEAKER_02: patches. Yeah, I think it's a great place to close. Read the so much for coming up on my pleasure.
[00:52:34] SPEAKER_01: And I hope we do this again. Yeah, excellent.