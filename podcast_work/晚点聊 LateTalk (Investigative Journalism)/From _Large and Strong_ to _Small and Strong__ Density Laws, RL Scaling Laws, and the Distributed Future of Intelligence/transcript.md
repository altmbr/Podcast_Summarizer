# From "Large and Strong" to "Small and Strong": Density Laws, RL Scaling Laws, and the Distributed Future of Intelligence

**Podcast:** 晚点聊 LateTalk (Investigative Journalism)
**Date:** 2025-12-11
**Video ID:** 693b46ec79debc3231f1d7bf
**Video URL:** https://www.xiaoyuzhoufm.com/episode/693b46ec79debc3231f1d7bf

---

[00:00:00] 歡迎收聽完點了 我是曼奇
[00:00:07] 曼奇: 今天的嘉賓是清華大學的劉志遠和蕭朝軍
[00:00:10] 曼奇: 劉志遠是清華計算機器副教授和面臂智能的首席科學家
[00:00:14] 曼奇: 蕭朝軍現在在清華做博士後
[00:00:16] 曼奇: 也是面臂命力CPM系列的文本模型負責人
[00:00:20] 曼奇: 他們的團隊剛在11月的自然雜誌機器學習紙看上
[00:00:24] 曼奇: 發表了封面文章
[00:00:25] 曼奇: Dancing Law of LLMS大模型的密度法則
[00:00:28] 曼奇: 所謂密度就是用更少的算例和數據
[00:00:31] 曼奇: 獲得相當乃至更多的智能
[00:00:33] 曼奇: 我們討論了密度法則研究的原則
[00:00:35] 曼奇: 雖然你說高興是不言自明的
[00:00:37] 劉志遠: 但是從事實上來講
[00:00:39] 劉志遠: 2022年拆的這個體型星期以後
[00:00:41] 劉志遠: 其實在全球範圍內大的話語體系
[00:00:44] 劉志遠: 其實是給你了
[00:00:45] 劉志遠: 就是在23年初的時候
[00:00:47] 劉志遠: 曾經有過一些這個巨頭說
[00:00:50] 劉志遠: 這個世界上不需要超過幾個大夢形
[00:00:53] 劉志遠: 1943年的時候
[00:00:54] 劉志遠: IBM的董事長曾經說
[00:00:56] 劉志遠: 全球不需要超過五台主機
[00:00:57] 劉志遠: 也展開了了業界提升模型能力密度的具體做法
[00:01:01] 曼奇: 因為價格數據
[00:01:03] 蕭朝軍: 然後學習算法
[00:01:04] 蕭朝軍: 然後軟硬硬體
[00:01:06] 蕭朝軍: 就是英法四個層面
[00:01:07] 蕭朝軍: 所以我要就是四個方面我們
[00:01:09] 蕭朝軍: 應該可以看到現在市面上有很多相關的工作
[00:01:12] 蕭朝軍: 而在往後
[00:01:13] 曼奇: 更大的密度提升
[00:01:14] 曼奇: 可能需要一些全新方法
[00:01:16] 曼奇: 強化學習做到現在
[00:01:17] 蕭朝軍: 其實大家還沒有解決的一個問題
[00:01:19] 蕭朝軍: 是強化學習的Skating的問題
[00:01:21] 蕭朝軍: 它現在有兩種技術路線
[00:01:23] 蕭朝軍: 在流程源的設想中
[00:01:24] 曼奇: 未來更高密度的模型
[00:01:26] 曼奇: 會支持每個人在端策
[00:01:27] 曼奇: 擁有專屬大模型
[00:01:29] 曼奇: 智能會分布式的存在
[00:01:31] 曼奇: 比如說你的眼睛也好
[00:01:32] 劉志遠: 甚至你的耳機
[00:01:33] 劉志遠: 你的手錶
[00:01:34] 劉志遠: 各種各樣的這種智能的這種硬件
[00:01:36] 劉志遠: 它完全是可以有一個共同的一個
[00:01:39] 劉志遠: 提供智能服務的一個中端
[00:01:41] 劉志遠: 就有點像家庭裡面的NAS
[00:01:43] 劉志遠: 只是說你是一個變形的
[00:01:45] 劉志遠: 可以跟著你走的這麼一個NAS
[00:01:47] 劉志遠: 它是一個典型的技術樂觀主義者
[00:01:49] 曼奇: 關於未來你有什麼擔心的理統嗎
[00:01:51] 曼奇: 我擔心的地方是
[00:01:52] 劉志遠: 我們人類自己
[00:01:54] 劉志遠: 會舒服我們自己的潛進的步伐
[00:01:57] 劉志遠: 像國文章和討論的鏈接
[00:01:58] 曼奇: 我會貼在Shoe Loats裡
[00:02:00] 曼奇: 下面我們正式進入本期節目吧
[00:02:04] 今天很高興的邀請到了
[00:02:05] 曼奇: 清華計算機器的劉志源老師
[00:02:07] 曼奇: 和他之前的博士生
[00:02:09] 曼奇: 現在清華的博士後
[00:02:10] 曼奇: 蕭朝軍
[00:02:11] 曼奇: 朝軍今年初野做科了萬頂療
[00:02:13] 曼奇: 和汪玉老師的學生
[00:02:14] 曼奇: 復天女一起分享了
[00:02:16] 曼奇: 注意力機制
[00:02:17] 曼奇: 尤其是吸收注意力的改進
[00:02:18] 曼奇: 這個也和今天的話題有關
[00:02:20] 曼奇: 我們之後可以展開
[00:02:21] 曼奇: 兩位可以先後
[00:02:22] 曼奇: 我們聽有簡單的大招呼
[00:02:23] 曼奇: 大家好我是劉志源
[00:02:24] 蕭朝軍: 大家好我是蕭朝軍
[00:02:26] 蕭朝軍: 然後現在做文問
[00:02:27] 蕭朝軍: 加個五方面的研究
[00:02:29] 蕭朝軍: 好了密度法則之前
[00:02:30] 曼奇: 因為最近正好
[00:02:30] 曼奇: 趕上就是行業裡有很多新的模型
[00:02:33] 曼奇: 包括GBT5.1
[00:02:34] 曼奇: Grocks1
[00:02:35] 曼奇: Jemleye3
[00:02:36] 曼奇: 還有最近也剛發了Cloud Office 5
[00:02:39] 曼奇: 我們可以先從這些行業動向來了
[00:02:40] 曼奇: 就是兩位最近在這些新的模型
[00:02:43] 曼奇: 上就是你們看到的
[00:02:44] 曼奇: 比較有意思的部分
[00:02:45] 曼奇: 或是說亮點是什麼
[00:02:46] 曼奇: 我其實感覺上其實他這些東西
[00:02:49] 蕭朝軍: 我覺得只要有兩個很明顯的趨勢
[00:02:51] 蕭朝軍: 第一個趨勢
[00:02:52] 蕭朝軍: 這就還是大家今年
[00:02:53] 蕭朝軍: 就應該是所有大模型
[00:02:54] 蕭朝軍: 從業者都關注到了
[00:02:55] 蕭朝軍: 一個智能體化
[00:02:56] 蕭朝軍: 其實你看到其實所有的這些模型
[00:02:58] 蕭朝軍: 他在智能體
[00:02:59] 蕭朝軍: 任務上都有了非常出色的表現
[00:03:01] 蕭朝軍: 所以直觀體現
[00:03:02] 蕭朝軍: 就是大家這些模型
[00:03:03] 蕭朝軍: 所有發文之後
[00:03:04] 蕭朝軍: 大家都會在這個
[00:03:06] 蕭朝軍: 代碼任務上去測試
[00:03:08] 蕭朝軍: 說他能幫我解決多少八個了
[00:03:10] 蕭朝軍: 這是第一個大家很明顯的趨勢
[00:03:12] 蕭朝軍: 第二個趨勢
[00:03:13] 蕭朝軍: 我其實是從Nanu
[00:03:15] 蕭朝軍: Nanu和Nobuk
[00:03:17] 蕭朝軍: Jemleye支持了這些任務上
[00:03:20] 蕭朝軍: 然後觀察到的
[00:03:21] 蕭朝軍: 所以你可以看到他相比於
[00:03:23] 蕭朝軍: 他模型不同的一個點
[00:03:24] 蕭朝軍: 在於他現在深層突變
[00:03:27] 蕭朝軍: 深層的文字會非常的精準
[00:03:30] 蕭朝軍: 就是在之前的Defroamedo
[00:03:31] 蕭朝軍: 然後一直看到的
[00:03:33] 蕭朝軍: 然後在SO
[00:03:35] 蕭朝軍: 就是 Open i 今年發文
[00:03:37] 蕭朝軍: 能思問文一下
[00:03:38] 蕭朝軍: 看到一些暑光
[00:03:39] 蕭朝軍: 然後Nanu不難了
[00:03:40] 蕭朝軍: 他把它進一步發展
[00:03:41] 蕭朝軍: 發揚光大了
[00:03:43] 蕭朝軍: 那麼有什麼我覺得
[00:03:43] 蕭朝軍: 這件事情會很有意思的
[00:03:45] 蕭朝軍: 他會讓我們看到了一個
[00:03:47] 蕭朝軍: 新的Skilling的方向
[00:03:49] 蕭朝軍: 我們今天一直在說Skilling
[00:03:51] 蕭朝軍: 然後文本模型的數據也枯減
[00:03:54] 蕭朝軍: 然後但是一直大家都會提
[00:03:56] 蕭朝軍: 那是不是我們可以用更多模型的數據
[00:03:58] 蕭朝軍: 但實際上是你一直沒做成
[00:04:00] 蕭朝軍: 就是大家用更多模型的數據
[00:04:02] 蕭朝軍: 他沒有辦法講這個模型真的
[00:04:04] 蕭朝軍: 就是只能水平有在一步的越深
[00:04:06] 蕭朝軍: 但我現在感覺上
[00:04:08] 蕭朝軍: 假設要通過這種土向深層
[00:04:10] 蕭朝軍: 然後統一的自回歸式的方式
[00:04:12] 蕭朝軍: 是不是有可能能實現一點的事情
[00:04:15] 蕭朝軍: 但這件事情也還沒有辦法確定
[00:04:17] 蕭朝軍: 因為沒有幾乎細節
[00:04:18] 蕭朝軍: 對
[00:04:18] 蕭朝軍: 因為不知道這個Jemleye
[00:04:19] 曼奇: 三就是怎麼實現的
[00:04:20] 曼奇: 對
[00:04:21] 曼奇: 對
[00:04:22] 曼奇: 對
[00:04:22] 曼奇: 然後他是
[00:04:23] 蕭朝軍: 比如說是一種前面上的優化
[00:04:25] 蕭朝軍: 還是說他真的一體化模型上的優化
[00:04:27] 蕭朝軍: 可能這個
[00:04:28] 蕭朝軍: 等待一個兩一到兩個別我們可以再看看
[00:04:31] 蕭朝軍: 我會覺得
[00:04:32] 劉志遠: 就是最近這六年的時間
[00:04:34] 劉志遠: 其實是一個
[00:04:35] 劉志遠: 就是高速發展的一個結段
[00:04:37] 劉志遠: 就基本上我現在會看到的
[00:04:39] 劉志遠: 就是咱們幾乎每周的進展
[00:04:41] 劉志遠: 頂得上我大概我讀研究生的時候
[00:04:44] 劉志遠: 一年的進展
[00:04:45] 劉志遠: 每天都會各種吐挑
[00:04:47] 劉志遠: 那我想就是這裡面
[00:04:49] 劉志遠: 在我來看
[00:04:50] 劉志遠: 就是大致可以劃分為兩個大的方面
[00:04:53] 劉志遠: 就一個方面
[00:04:54] 劉志遠: 其實我把它總結位
[00:04:56] 劉志遠: 就是能力更強
[00:04:57] 劉志遠: 就是要這個模型
[00:04:59] 劉志遠: 它能力變得越來越強
[00:05:00] 劉志遠: 因為我們其實可以看到
[00:05:02] 劉志遠: 就是從2018年預訓練模型出現之後
[00:05:06] 劉志遠: 其實大概是美國幾年的時間
[00:05:08] 劉志遠: 就會讓這個模型的能力
[00:05:09] 劉志遠: 有一個飛躍
[00:05:11] 劉志遠: 從2022年底拆的GVT出現
[00:05:14] 劉志遠: 那它通過Instrasion to you
[00:05:16] 劉志遠: 讓這個模型能聽懂人的指令
[00:05:18] 劉志遠: 2024年底2025年出通過這種
[00:05:21] 劉志遠: 大規模強化學習
[00:05:22] 劉志遠: 能夠讓這個模型學會深度思考
[00:05:24] 劉志遠: 那麼其實我們會看到
[00:05:26] 劉志遠: 這樣一條非常清晰的出現
[00:05:28] 劉志遠: 就是這個模型在變得越來越同有
[00:05:31] 劉志遠: 越來越接近
[00:05:32] 劉志遠: 甚至是超越我們人類的這個智能的水平
[00:05:34] 劉志遠: 那同時的話
[00:05:35] 劉志遠: 就是我和朝軍
[00:05:37] 劉志遠: 我們團隊所發表的這篇論文
[00:05:40] 劉志遠: 其實想給大家呈現另外一條出現
[00:05:42] 劉志遠: 其實就是能效更高
[00:05:44] 劉志遠: 也就是說
[00:05:45] 劉志遠: 我們會看到過去六年的時間
[00:05:48] 劉志遠: 就是大家經常提大模型
[00:05:51] 劉志遠: 它的發展的邏輯是規模法則
[00:05:54] 劉志遠: 那麼其實規模法則我理解
[00:05:56] 劉志遠: 就是它背後
[00:05:57] 劉志遠: 其實是告訴我們
[00:05:59] 劉志遠: 我們能夠找到一條這種基於大數據
[00:06:02] 劉志遠: 加大算力
[00:06:03] 劉志遠: 然後加大參數
[00:06:04] 劉志遠: 這樣的一條通用的智能的解決方案
[00:06:07] 劉志遠: 那麼在這樣的解決方案下
[00:06:08] 劉志遠: 只要我有足夠多數據
[00:06:10] 劉志遠: 有足夠多算力
[00:06:11] 劉志遠: 然後我訓練足夠大的模型
[00:06:13] 劉志遠: 那我就可以讓這個模型能力
[00:06:15] 劉志遠: 可以持續地提升
[00:06:16] 劉志遠: 就這個是規模法則的這個邏輯
[00:06:19] 劉志遠: 但是我們會說
[00:06:20] 劉志遠: 規模法則同時給我們帶來的
[00:06:22] 劉志遠: 非常大的問題
[00:06:23] 劉志遠: 其實就是你模型勳得更大之後
[00:06:25] 劉志遠: 其實意味著它的訓練成本
[00:06:27] 劉志遠: 和它的使用成本
[00:06:28] 劉志遠: 都會相應得變得更高
[00:06:30] 劉志遠: 它其實基本上是一個現心
[00:06:32] 劉志遠: 是一個超現心增長的一個狀態
[00:06:35] 劉志遠: 所以我們會認為
[00:06:36] 劉志遠: 就是像歷史上的任何一個對
[00:06:38] 劉志遠: 全人類都產生深遠影響的這種科技
[00:06:41] 劉志遠: 那麼它一定是要追求一個更加高效的
[00:06:43] 劉志遠: 整個發展的這個模式
[00:06:45] 劉志遠: 也就是說一旦你找到了這條通用的
[00:06:48] 劉志遠: 這種技術方案之後
[00:06:49] 劉志遠: 你一定要讓這個技術方案的方方面面
[00:06:52] 劉志遠: 都更加的精進
[00:06:54] 劉志遠: 變成一個更加精密的一個體系
[00:06:56] 劉志遠: 來讓我們製造出來的這個產品
[00:06:59] 劉志遠: 它的成本更低
[00:07:00] 劉志遠: 它的能力更強
[00:07:01] 劉志遠: 這個其實是我們所提出來的Dancing Law
[00:07:04] 劉志遠: 其實想要告訴大家的
[00:07:06] 劉志遠: 這麼一個發展方向
[00:07:07] 劉志遠: 就我自己的一個感覺
[00:07:08] 曼奇: 就是現在這些新的模型的發布
[00:07:10] 曼奇: 我覺得就可能從外界來看
[00:07:12] 曼奇: 它更多是您剛才說的
[00:07:13] 曼奇: 這兩條主線裡面的性能的提升
[00:07:15] 曼奇: 就是那類的這個提升
[00:07:17] 曼奇: 然後在效率上
[00:07:18] 曼奇: 就您說這個第二條主線
[00:07:20] 曼奇: 你覺得最近業界的這些進展
[00:07:21] 曼奇: 它有什麼體現
[00:07:23] 劉志遠: 因為本身這個AGI的發展的這個路徑
[00:07:27] 劉志遠: 還沒有結束
[00:07:28] 劉志遠: 我們會看到像GMTi
[00:07:30] 劉志遠: 最新的這些進展
[00:07:31] 劉志遠: 就會讓大家眼前一亮
[00:07:33] 劉志遠: 所以我會看到的
[00:07:35] 劉志遠: 就是像那這個能力更強
[00:07:38] 劉志遠: 這條主線是更有顯示度的
[00:07:40] 劉志遠: 另外一條線
[00:07:41] 劉志遠: 其實我覺得在全球來看
[00:07:43] 劉志遠: 應該說也有非常多的工作
[00:07:46] 劉志遠: 因為大家其實會意識到
[00:07:48] 劉志遠: 就是我花那麼多的算力
[00:07:51] 劉志遠: 那麼多的經費
[00:07:52] 劉志遠: 然後我們來去宣誓一個模型
[00:07:54] 劉志遠: 那麼必然導致這樣的一個模型
[00:07:56] 劉志遠: 其實是沒有辦法
[00:07:57] 劉志遠: 在更多的場合能夠用起來的
[00:08:00] 劉志遠: 對吧
[00:08:00] 劉志遠: 你只能夠通過API
[00:08:01] 劉志遠: 然後來去調用
[00:08:02] 劉志遠: 那這件事情其實本身是極大的限制
[00:08:06] 劉志遠: 人工智能的真正的這個普及
[00:08:08] 劉志遠: 智能本身應該是高度分布式的
[00:08:10] 劉志遠: 它是在各個領域
[00:08:12] 劉志遠: 都會有潛在的廣泛的這麼一個應用
[00:08:15] 劉志遠: 所以從另外一個角度看
[00:08:17] 劉志遠: 就是如何讓這個模型更加高效
[00:08:20] 劉志遠: 其實是我們真正實現科技革命
[00:08:24] 劉志遠: 就是這輪人工智能的科技革命
[00:08:26] 劉志遠: 非常重要的一個底層的驅動
[00:08:28] 劉志遠: 這個問題我可以再補充一下
[00:08:30] 蕭朝軍: 所以今年一個很重要的旋律圈
[00:08:32] 蕭朝軍: 可以看到的是效率的改進
[00:08:34] 蕭朝軍: 就是我們先談開源
[00:08:36] 蕭朝軍: 就可以看到現在開源界
[00:08:38] 蕭朝軍: 然後我想今年的minimax
[00:08:40] 蕭朝軍: 然後想簽文也做簽文next
[00:08:42] 蕭朝軍: Depthic就跟不用說了
[00:08:43] 蕭朝軍: 也今年V3.2也是做了細酥的騰訊
[00:08:46] 蕭朝軍: 包括我們也在做細酥的騰訊
[00:08:48] 蕭朝軍: 所以可以看到現在在加工上
[00:08:50] 蕭朝軍: 去提升文件效率
[00:08:51] 蕭朝軍: 已經成為一個非常共識的一件事情
[00:08:54] 蕭朝軍: 就是在開源界
[00:08:55] 蕭朝軍: 那你從閉源界
[00:08:56] 蕭朝軍: 你就可以也可以看到類似的事情
[00:08:59] 蕭朝軍: 就像這個open.i它也會去搞
[00:09:01] 蕭朝軍: 各種mini系列的模型
[00:09:03] 蕭朝軍: 然後已經在Gimli3火機前
[00:09:06] 蕭朝軍: 就是讓使有一個Gimli的Diffusion
[00:09:08] 蕭朝軍: 一個文文模型
[00:09:09] 蕭朝軍: 它宣稱它的文文形容的速度
[00:09:12] 蕭朝軍: 會比其他的模型快個強多很多倍
[00:09:14] 蕭朝軍: 因為它不開源
[00:09:15] 蕭朝軍: 所以你也不知道它距離是時間什麼
[00:09:17] 蕭朝軍: 但是你可以看到
[00:09:18] 蕭朝軍: 我覺得它內部也在追尋這種效率的提升
[00:09:22] 蕭朝軍: 因為其實對它們來說
[00:09:23] 蕭朝軍: 當然固然可以看到
[00:09:25] 蕭朝軍: 它們的資源量會比我們大很多
[00:09:27] 蕭朝軍: 但實際上就是任何的
[00:09:29] 蕭朝軍: 再多的資源面向AGA的投入
[00:09:32] 蕭朝軍: 現在都還是有限的
[00:09:33] 蕭朝軍: 我們都得把這件事做到
[00:09:34] 蕭朝軍: 所以就是從公眾來看的話
[00:09:36] 劉志遠: 就是能力提升是明顯
[00:09:39] 劉志遠: 但是按線應該是能效更高
[00:09:42] 劉志遠: 因為其實你可以看到的歷史上
[00:09:44] 劉志遠: 比如說我們說過去的幾十年的新西革命
[00:09:47] 劉志遠: 它的明顯其實讓我們看到的是
[00:09:50] 劉志遠: 我們的計算設備在逐漸的小型化
[00:09:53] 劉志遠: 我們原來的大型機
[00:09:55] 劉志遠: 到了80年代是小型機PC
[00:09:57] 劉志遠: 到了2000年之後是手機
[00:09:59] 劉志遠: 到10年之後是我們的智能手機
[00:10:02] 劉志遠: 我們的智能化的設備
[00:10:04] 劉志遠: 其實它就已經是讓我們的新一化的
[00:10:07] 劉志遠: 生活工作變得非常的方便
[00:10:09] 劉志遠: 這是主線
[00:10:10] 劉志遠: 這是明線
[00:10:11] 劉志遠: 但是它的按線其實是
[00:10:13] 劉志遠: 新年行業的快速發展
[00:10:15] 劉志遠: 就是摩爾定律
[00:10:16] 劉志遠: 所以其實我們之所以提這個密度法則
[00:10:20] 劉志遠: 其實是想尋找大模型的摩爾定律
[00:10:23] 劉志遠: 我們接下來正是掌握了一下這個密度法則
[00:10:26] 曼奇: 因為我最開始看到這個研究的時候
[00:10:28] 曼奇: 我自己其實有個以後的
[00:10:29] 曼奇: 就是這個研究它揭示了
[00:10:32] 曼奇: 規律或發展的賣落
[00:10:33] 曼奇: 就是隨時間推移摩星的訓練和推理效率提高
[00:10:36] 曼奇: 就好對業界來說是不言自民的一個現象
[00:10:40] 曼奇: 當然你們這次是有一個非常定量的描述
[00:10:42] 曼奇: 就可以講上就是這麼做的背景和意義
[00:10:45] 曼奇: 以及說驅動這個研究背後的核心問題
[00:10:47] 曼奇: 意識是什麼
[00:10:49] 我覺得第一個角度
[00:10:52] 劉志遠: 就是從人工智能發展的角度來講
[00:10:55] 劉志遠: 我們會認為它一定要追求高效
[00:10:57] 劉志遠: 追求高效它肯定不能夠是一個說口號而已
[00:11:02] 劉志遠: 說我就是要高效
[00:11:03] 劉志遠: 我們其實是要有一套體系
[00:11:05] 劉志遠: 就像你剛才問的
[00:11:07] 劉志遠: 到底是哪些因素會影響這個摩星的密度呢
[00:11:10] 劉志遠: 會影響我們的這個能效呢
[00:11:12] 劉志遠: 那麼從這個角度來講
[00:11:14] 劉志遠: 我覺得我們就達成了我們提出密度法則的
[00:11:17] 劉志遠: 這麼一個目標了
[00:11:18] 劉志遠: 也就是說我們要提出一個指標來
[00:11:20] 劉志遠: 然後讓這個指標成為一個客觀的事實
[00:11:23] 劉志遠: 能夠團結更多的研究者
[00:11:25] 劉志遠: 來去更細緻的 更定量的
[00:11:28] 劉志遠: 然後來去探索
[00:11:29] 劉志遠: 更高效的這麼一個實現的這麼一個方式
[00:11:32] 劉志遠: 那麼第二個呢就是
[00:11:34] 劉志遠: 雖然你說這個高效是不言自明的
[00:11:37] 劉志遠: 但是從事實上來講
[00:11:39] 劉志遠: 2022年 拆到這個體系星期以後
[00:11:42] 劉志遠: 其實在全球範圍內
[00:11:44] 劉志遠: 大的話語體系 其實是死給你了
[00:11:46] 劉志遠: 就幾乎所有的機構都會去說
[00:11:49] 劉志遠: 我只有更多的數據 更多的算力
[00:11:53] 劉志遠: 我才能遜更大的摩星
[00:11:55] 劉志遠: 我才能追求更強的能力
[00:11:57] 劉志遠: 這是從2022年一直到2024年底
[00:12:01] 劉志遠: 然後一直以來的這麼一個大的這麼一個話語
[00:12:04] 劉志遠: 那麼甚至說有 Open i 有音韋大
[00:12:07] 劉志遠: 然後他們也都是在去說
[00:12:09] 劉志遠: 其他人就不要去遜大摩星了
[00:12:12] 劉志遠: 對吧
[00:12:12] 劉志遠: 然後只有那些有10萬張卡的這些團隊
[00:12:16] 劉志遠: 然後10萬張卡的機構
[00:12:18] 劉志遠: 才有資格去遜大摩星
[00:12:20] 劉志遠: 然後你其他人就用就可以了
[00:12:22] 劉志遠: 他嘗試著去構建一個
[00:12:24] 劉志遠: 圍算利論的這麼一個話語的這麼一個體系
[00:12:29] 劉志遠: 那在這種情況下
[00:12:31] 劉志遠: 我們會認為就是 Dancing Law 密度法則
[00:12:34] 劉志遠: 我覺得也非常地重要
[00:12:36] 劉志遠: 那麼其實我想從今年初
[00:12:39] 劉志遠: Dipsyke V3其實會告訴大家
[00:12:41] 劉志遠: 其實我不需要那麼多算力
[00:12:43] 劉志遠: 當然絕對算力仍然很大
[00:12:45] 劉志遠: 但是絕對不是像
[00:12:47] 劉志遠: 很多的這些國際的企業和機構所宣稱的
[00:12:51] 劉志遠: 在我們來看
[00:12:53] 劉志遠: 就是我們其實是要有責任
[00:12:55] 劉志遠: 然後來去在整個這個人工智能發展的
[00:12:59] 劉志遠: 這麼一個過程中
[00:13:00] 劉志遠: 我們要把這個正確的發展方向
[00:13:03] 劉志遠: 能夠告訴大家
[00:13:04] 劉志遠: 因為其實就是你作為
[00:13:07] 劉志遠: 譬如說深入了解
[00:13:08] 劉志遠: 整個人工智能這個泉帽的
[00:13:11] 劉志遠: 可能會知道高效很重要
[00:13:13] 劉志遠: 但是就是說從很多很多人來講
[00:13:16] 劉志遠: 甚至包括角色層等等
[00:13:18] 劉志遠: 那他肯定很多時候
[00:13:20] 劉志遠: 他所接受的信息
[00:13:21] 劉志遠: 仍然會覺得規模法則是第一性原理
[00:13:24] 劉志遠: 我就得有10萬張卡
[00:13:26] 劉志遠: 我才有可能做
[00:13:27] 劉志遠: 同樣人工智能
[00:13:28] 劉志遠: 我在我來看就這件事情
[00:13:30] 劉志遠: 是非常錯誤的一個信號
[00:13:33] 曼奇: 因為你剛才也提到
[00:13:34] 曼奇: 其實這個
[00:13:35] 曼奇: 它是有點像摩爾丁維的嗎
[00:13:36] 曼奇: 我之前看新片戰爭的時候
[00:13:38] 曼奇: 我覺得他裡面對摩爾丁維的描述
[00:13:39] 曼奇: 也很精平
[00:13:40] 曼奇: 就是他雖然叫定維
[00:13:41] 曼奇: 但他實際上並不是一個物理規律
[00:13:44] 曼奇: 他其實是一種敘事
[00:13:45] 曼奇: 或者說一種共識
[00:13:46] 曼奇: 他是指揮整個產業鏈
[00:13:48] 曼奇: 和這個行業裡的很多環節
[00:13:50] 曼奇: 然後大家有一個共同的目標
[00:13:53] 曼奇: 是的
[00:13:53] 曼奇: 所以摩爾丁維
[00:13:55] 劉志遠: 反正約定俗成了
[00:13:56] 劉志遠: 就叫摩爾丁維
[00:13:58] 劉志遠: 一開始團隊內部還要說叫密度定律
[00:14:01] 劉志遠: 後來我們就覺得
[00:14:02] 劉志遠: 在中文的這個華語提供內
[00:14:05] 劉志遠: 這個定律
[00:14:06] 劉志遠: 更多的跟一個物理規律可能會有關係
[00:14:08] 劉志遠: 所以我們就會覺得
[00:14:10] 劉志遠: 可能就不能夠叫定律
[00:14:12] 劉志遠: 因為其實在英文裡面
[00:14:13] 劉志遠: 這個Law
[00:14:14] 劉志遠: 它其實就是一個規律
[00:14:16] 劉志遠: 對吧
[00:14:16] 劉志遠: 它沒有說一定是物理定律的
[00:14:18] 劉志遠: 這麼一個層面
[00:14:20] 劉志遠: 但是在中文裡面
[00:14:21] 劉志遠: Law翻譯成定律的話
[00:14:22] 劉志遠: 就顯得特別高大
[00:14:24] 劉志遠: 就是它等於掌握萬事萬物的
[00:14:27] 劉志遠: 這麼一個背後的這麼一個法則
[00:14:29] 劉志遠: 或者規律
[00:14:29] 劉志遠: 所以我們就後來
[00:14:31] 劉志遠: 就是基本上會去提
[00:14:33] 劉志遠: 所謂的叫密度法則
[00:14:35] 劉志遠: 相當於說更體現
[00:14:37] 劉志遠: 這個Law本身它的這個含義
[00:14:39] 劉志遠: 所以我非常認可
[00:14:41] 劉志遠: 就是摩爾丁維和密度法則
[00:14:43] 劉志遠: 其實都是一種人工的自我實現
[00:14:46] 劉志遠: 摩爾丁維
[00:14:47] 劉志遠: 你如果去看
[00:14:48] 劉志遠: 就是摩爾在1965年
[00:14:50] 劉志遠: 它最原始的那篇文章
[00:14:52] 劉志遠: 其實它自己都沒有把自己
[00:14:54] 劉志遠: 成為是一個定律
[00:14:55] 劉志遠: 或者是一個Law
[00:14:56] 劉志遠: 其實是過了十幾年之後
[00:14:59] 劉志遠: 是由其他人說
[00:15:01] 劉志遠: 摩爾在1965年
[00:15:03] 劉志遠: 根據過去的十幾年總結的規律
[00:15:05] 劉志遠: 竟然在它之後
[00:15:07] 劉志遠: 還Work了這個十幾年
[00:15:09] 劉志遠: 所以就把它定義成了摩爾摩爾斯羅
[00:15:12] 劉志遠: 那麼甚至說在1970年之後
[00:15:15] 劉志遠: 然後又持續的這個影響整個這個行業
[00:15:19] 劉志遠: 那麼所以就是摩爾丁維
[00:15:21] 劉志遠: 它的影響力就越來越大
[00:15:23] 劉志遠: 就是我們人類社會
[00:15:25] 劉志遠: 通過工業化
[00:15:26] 劉志遠: 通過這種技術創新的
[00:15:28] 劉志遠: 這麼一種自我實現
[00:15:29] 劉志遠: 那我們對密度法則
[00:15:32] 劉志遠: 我們也認為它也是一種自我實現
[00:15:34] 劉志遠: 因為所有的追求密度更高
[00:15:37] 劉志遠: 因為你想摩爾丁維
[00:15:39] 劉志遠: 它也是要追求
[00:15:40] 劉志遠: 在這個芯片上
[00:15:42] 劉志遠: 把更多的電路
[00:15:43] 劉志遠: 給放進去
[00:15:44] 劉志遠: 密度法則本身就是你這個世界的
[00:15:47] 劉志遠: 這個上增的這麼一個趨勻而行呢
[00:15:50] 劉志遠: 它本身就是在通過紀錄創新
[00:15:52] 劉志遠: 來追求某一種內災的秩序
[00:15:54] 劉志遠: 然後把更多的內容
[00:15:56] 劉志遠: 然後給壓到一個更小的空間裡面去
[00:15:59] 劉志遠: 因為這個宇宙是大爆炸的
[00:16:00] 劉志遠: 上增的
[00:16:01] 劉志遠: 是不斷的擴散的
[00:16:03] 劉志遠: 我們讓它密度更高
[00:16:04] 劉志遠: 這件事情顯然是需要外力
[00:16:06] 劉志遠: 對於我們人類社會來講
[00:16:07] 劉志遠: 外力是什麼
[00:16:08] 劉志遠: 就是我們不斷的創新
[00:16:09] 劉志遠: 上這個知識
[00:16:11] 劉志遠: 能夠壓到一個更小的空間裡面
[00:16:13] 劉志遠: 不管是芯片也好
[00:16:14] 劉志遠: 模型也好
[00:16:15] 劉志遠: 未來一定是這個發展趨勢
[00:16:17] 劉志遠: 就剛才那個問題
[00:16:18] 蕭朝軍: 我也在補充說一下
[00:16:19] 蕭朝軍: 像人本身這個
[00:16:21] 蕭朝軍: 在自然的發展的共產黨中
[00:16:23] 蕭朝軍: 我們會講這個物競天則
[00:16:25] 蕭朝軍: 等中自然法則
[00:16:26] 蕭朝軍: 還其實並不是說
[00:16:27] 蕭朝軍: 它是一種物理規律
[00:16:28] 蕭朝軍: 但是就是說在天然的
[00:16:30] 蕭朝軍: 自然收線的場景下
[00:16:31] 蕭朝軍: 你就更好的遵循這個法則的
[00:16:34] 蕭朝軍: 那一批人
[00:16:35] 蕭朝軍: 它才能夠承諾下來
[00:16:36] 蕭朝軍: 然後繼續這個
[00:16:38] 蕭朝軍: 把你的基金衣上下去
[00:16:39] 蕭朝軍: 所以你可以看到
[00:16:40] 蕭朝軍: 人的腦子並沒有無限值的增長
[00:16:43] 蕭朝軍: 而是一直在一個有限的體積下
[00:16:45] 蕭朝軍: 就是在有限的能源消耗下
[00:16:47] 蕭朝軍: 讓我的質地不斷地提升
[00:16:48] 蕭朝軍: 所以其實它是我們的一個追求
[00:16:50] 蕭朝軍: 然後也是在有限自然下的
[00:16:52] 蕭朝軍: 一個可能你的一個必須品
[00:16:54] 蕭朝軍: 小君說的這個點特別有意思
[00:16:56] 劉志遠: 就是你要說
[00:16:57] 劉志遠: 尊循自然界
[00:16:59] 劉志遠: 就是生物群羅的這個
[00:17:01] 劉志遠: 所謂的物競天則的話
[00:17:03] 劉志遠: 說不定
[00:17:03] 劉志遠: 你可以設想就是
[00:17:05] 劉志遠: 模爾定律為什麼能夠
[00:17:06] 劉志遠: work可能也跟這個商業的
[00:17:08] 劉志遠: 互相在這種競爭的環境
[00:17:10] 劉志遠: 其實會有關係
[00:17:11] 劉志遠: 對吧
[00:17:12] 劉志遠: 你不追求用更低的成本
[00:17:14] 劉志遠: 實現更強的這個算力
[00:17:16] 劉志遠: 你的這個企業
[00:17:18] 劉志遠: 你的這個芯片就沒有競爭力
[00:17:19] 劉志遠: 對吧
[00:17:19] 劉志遠: 是我們內部的一個物競天則
[00:17:21] 劉志遠: 那你就可以設想
[00:17:22] 劉志遠: 未來的模型的發展
[00:17:24] 劉志遠: 肯定也是這個樣子
[00:17:25] 劉志遠: 如果你實現的模型的效率
[00:17:27] 劉志遠: 不如另外一家廠商
[00:17:29] 劉志遠: 那麼你就是競爭力
[00:17:30] 劉志遠: 就沒有那麼強
[00:17:32] 劉志遠: 本身密度法則和規模法則
[00:17:34] 劉志遠: 本身也不是說是對立的
[00:17:36] 劉志遠: 其實是像我像成的
[00:17:37] 劉志遠: 其實所有的一線的大模型團隊
[00:17:40] 劉志遠: 它一定是要去追求
[00:17:42] 劉志遠: 它的模型的高效的
[00:17:43] 劉志遠: 對
[00:17:44] 曼奇: 這就是你也知道
[00:17:45] 曼奇: 你不會在同一個模型裡面完成
[00:17:47] 劉志遠: 你會看到就是GPT的那些模型
[00:17:49] 劉志遠: 它的那個價格
[00:17:50] 劉志遠: API的價格
[00:17:51] 劉志遠: 其實就是在快速的下降
[00:17:53] 劉志遠: 就這個本身就是一個
[00:17:55] 劉志遠: 它內部也在不斷做事先的一個過程
[00:17:57] 劉志遠: 或者是說你換一個理解
[00:17:59] 蕭朝軍: 就是Skeleton它強調的是
[00:18:01] 蕭朝軍: 這個計算量和能力之間的
[00:18:04] 蕭朝軍: 這麼一個密度關係
[00:18:05] 蕭朝軍: 但是我們想其實強調的
[00:18:07] 蕭朝軍: 就是說這個密度件
[00:18:09] 蕭朝軍: 其實可能是那個協率
[00:18:11] 蕭朝軍: 就是說我用計算量換取的智能
[00:18:14] 蕭朝軍: 它就一個轉化率是要越來越高的
[00:18:16] 蕭朝軍: 所以可以看到
[00:18:17] 蕭朝軍: 像是說OPEN AI我們會說它智源很多
[00:18:19] 蕭朝軍: 但是它一直在講說
[00:18:20] 蕭朝軍: 我們已經不夠智源去做
[00:18:22] 蕭朝軍: 這個這個一些跟前央的事情
[00:18:25] 蕭朝軍: 包括伊莉亞
[00:18:26] 蕭朝軍: 為什麼會從OPEN AI走
[00:18:27] 蕭朝軍: 因為它覺得它拿不到
[00:18:28] 蕭朝軍: 足夠多的資源去追尋它的
[00:18:30] 蕭朝軍: 對 前央做Safe的一樣
[00:18:32] 曼奇: 對
[00:18:32] 曼奇: 所以其實核心問題
[00:18:33] 蕭朝軍: 就是大家還是智源所行
[00:18:35] 蕭朝軍: 就是其實放在OPEN AI來看
[00:18:36] 蕭朝軍: 它也因為智源所行產生了矛盾
[00:18:39] 蕭朝軍: 所以他們內部來說
[00:18:40] 蕭朝軍: 去追尋這種效率
[00:18:41] 蕭朝軍: 也是一個非常這個其智的事情
[00:18:43] 蕭朝軍: 回到你這次的這個研究
[00:18:45] 曼奇: 它的一些核心動作
[00:18:46] 曼奇: 會結論是什麼樣
[00:18:47] 曼奇: 核心動作其實我們
[00:18:49] 蕭朝軍: 雖然名字叫密度法則
[00:18:50] 蕭朝軍: 講的是這個能力密度
[00:18:52] 蕭朝軍: 不斷的隨著時間指數翻倍
[00:18:54] 蕭朝軍: 但我其實覺得
[00:18:55] 蕭朝軍: 我們更想強調的是一個觀點
[00:18:57] 蕭朝軍: 就是我們要去追尋的是
[00:18:59] 蕭朝軍: 單位參數一下
[00:19:00] 蕭朝軍: 或者是單位開銷一下
[00:19:02] 蕭朝軍: 我們能夠轉化出來的智能的能力
[00:19:06] 蕭朝軍: 這件事情是我們這個相當就是
[00:19:08] 蕭朝軍: 立了一個FLY
[00:19:09] 蕭朝軍: 和立了一個目標
[00:19:10] 蕭朝軍: 說我們大家應該追尋這件事情
[00:19:12] 蕭朝軍: 而不是一位的說
[00:19:13] 蕭朝軍: 我這個Performance有多高
[00:19:14] 蕭朝軍: 因為我們這個工作
[00:19:15] 蕭朝軍: 其實是二四年底正式完成
[00:19:17] 蕭朝軍: 然後當然經過投稿到現在來說
[00:19:19] 蕭朝軍: 經過了會一年的時間才中重稿
[00:19:22] 蕭朝軍: 那其實在那個時間點的話
[00:19:23] 蕭朝軍: 大概其實都在講
[00:19:25] 蕭朝軍: 那我這個我要怎麼把這個模型
[00:19:28] 蕭朝軍: 殘酌再放大一點
[00:19:29] 蕭朝軍: 或者就計算量再放大一點
[00:19:30] 蕭朝軍: 那我們會認為
[00:19:31] 蕭朝軍: 再這樣一個發展模式下的話
[00:19:33] 蕭朝軍: 其實是不對的
[00:19:34] 蕭朝軍: 這樣的發展模式
[00:19:35] 蕭朝軍: 只會導這個問題
[00:19:36] 蕭朝軍: 就是所有人都在瘋狂的雜錢
[00:19:38] 蕭朝軍: 但是實際上
[00:19:39] 蕭朝軍: 你沒有這個把你的這個轉化率
[00:19:42] 蕭朝軍: 就是智能的轉化率提升
[00:19:44] 蕭朝軍: 那這樣的話
[00:19:44] 蕭朝軍: 遲早會引來的就是這個
[00:19:46] 蕭朝軍: 大家的資源已經不夠了
[00:19:48] 蕭朝軍: 然後就大家就引來了一些所謂的寒冬
[00:19:50] 蕭朝軍: 就像拉馬
[00:19:51] 蕭朝軍: 他那個時候發了拉馬3405幣
[00:19:54] 蕭朝軍: 那他假設下一步發展
[00:19:55] 蕭朝軍: 然後需要發了這個1000幣
[00:19:57] 蕭朝軍: 2000幣的這個重命模型
[00:19:59] 蕭朝軍: 我覺得這樣的話
[00:20:00] 蕭朝軍: 那顯得這個這個事情
[00:20:01] 蕭朝軍: 就這個AI的發展
[00:20:03] 蕭朝軍: 就沒有辦法持續下去
[00:20:04] 蕭朝軍: 這在那個時間點
[00:20:05] 蕭朝軍: 我們想要去做出去
[00:20:06] 蕭朝軍: 會文章的持續想
[00:20:07] 蕭朝軍: 全程說這個
[00:20:09] 蕭朝軍: 對於技術的發展而言
[00:20:10] 蕭朝軍: 追求這個密度
[00:20:11] 蕭朝軍: 或者去求這個智能轉化率
[00:20:13] 蕭朝軍: 實際上是我們這個
[00:20:15] 蕭朝軍: 這個技術發展
[00:20:16] 蕭朝軍: 一個很重要的一個主線
[00:20:18] 蕭朝軍: 所以據於說它是3.5個月
[00:20:20] 蕭朝軍: 反應倍還是一年反應倍
[00:20:22] 蕭朝軍: 我覺得這個數值本身
[00:20:23] 蕭朝軍: 可能反而沒有那麼的
[00:20:24] 蕭朝軍: 對你們現在發現的是平均3.3個月
[00:20:27] 曼奇: 反應倍差不多100天
[00:20:28] 曼奇: 但後來修正了一些
[00:20:29] 劉志遠: 就是在把這個2025年的
[00:20:32] 劉志遠: 新的模型發展之後
[00:20:33] 劉志遠: 就變成3.5個月了
[00:20:35] 劉志遠: 當然就是會有一些抖動
[00:20:37] 劉志遠: 核心的觀察就是是在加速的
[00:20:40] 劉志遠: 因為就是2023年
[00:20:41] 劉志遠: 應該是2023年1月份
[00:20:43] 劉志遠: 就是拆在GP出現前和後
[00:20:46] 劉志遠: 其實它的這個費增的速度
[00:20:48] 劉志遠: 其實是在加速
[00:20:49] 劉志遠: 它增加了這幾個月
[00:20:51] 劉志遠: 然後由原來就是24年
[00:20:53] 劉志遠: 它當時算出來的是3.3
[00:20:56] 劉志遠: 然後把2025年的加進來
[00:20:58] 劉志遠: 變成3.5
[00:20:59] 劉志遠: 我們理解這個可能就是一個正常的抖動
[00:21:01] 劉志遠: 因為你們是24年底做完這個研究嗎
[00:21:03] 曼奇: 所以做開始的有這個想法
[00:21:05] 曼奇: 做這個研究時間是更早的
[00:21:06] 曼奇: 對其實我們去年年中的時候
[00:21:08] 蕭朝軍: 就已經在提
[00:21:09] 蕭朝軍: 但是我們其實叫密度
[00:21:10] 蕭朝軍: 支持密度
[00:21:11] 蕭朝軍: 支持密度24年年中
[00:21:13] 曼奇: 核心是這個故事可能就要再搞早一點
[00:21:16] 劉志遠: 這個其實會找到2023年下半年
[00:21:19] 劉志遠: 那個時候其實就是你大概
[00:21:21] 劉志遠: 咱們如果拉回到2023年
[00:21:24] 劉志遠: 2023年初的時候是拆在GP出來
[00:21:26] 劉志遠: 震驚全球
[00:21:27] 劉志遠: 就是讓所有的人都發現
[00:21:29] 劉志遠: 哇大模型這麼好用
[00:21:31] 劉志遠: 所以才新起了就是內一螺
[00:21:33] 劉志遠: 特別大的浪潮
[00:21:34] 劉志遠: 就是國內的溜小虎
[00:21:35] 劉志遠: 然後這些相關的
[00:21:37] 劉志遠: 那其實在2023年的主選率是什麼呢
[00:21:40] 劉志遠: 就是從2023年初
[00:21:42] 劉志遠: 一直到2023年底
[00:21:43] 劉志遠: 大家的主要任務是
[00:21:45] 劉志遠: 追趕拆在GP的這個腐陷
[00:21:47] 劉志遠: 就是把這個模型能力腐陷出來
[00:21:49] 劉志遠: 那麼腐陷出來呢
[00:21:51] 劉志遠: 大概國內的一線團隊
[00:21:53] 劉志遠: 就是大致是在2023年的9月份10月份
[00:21:56] 劉志遠: 那段時間能夠腐陷出來
[00:21:59] 劉志遠: 因為我們那時候的估計就是
[00:22:01] 劉志遠: 國內跟這個國際的最先進的水平
[00:22:03] 劉志遠: 差不多也就是差個一年左右的時間
[00:22:06] 劉志遠: 基本上就都能腐陷出來
[00:22:07] 劉志遠: 因為主要的點其實就是那個
[00:22:09] 劉志遠: Instagram 求你
[00:22:11] 劉志遠: 腐陷出來了
[00:22:12] 劉志遠: 腐陷出來了之後呢
[00:22:13] 劉志遠: 其實這個時候就面臨著一個決策的問題
[00:22:17] 劉志遠: 這個OpenA其實是在2023年的4月份
[00:22:20] 劉志遠: 發布了GP4
[00:22:21] 劉志遠: 所以呢就是在2023年的這個10月份左右
[00:22:25] 劉志遠: 就是一旦它把插在GP能力腐陷出來之後
[00:22:29] 劉志遠: 那很自然的一個決策就是
[00:22:31] 劉志遠: 我把當前的這個參數再誇大一些
[00:22:34] 劉志遠: 然後數據再對多一點
[00:22:37] 劉志遠: 算了一再弄得大一點
[00:22:38] 劉志遠: 然後我們就去追求GP4
[00:22:41] 劉志遠: 絕大部分團隊也都是按著這條思路去走的
[00:22:45] 劉志遠: 所以你就會看到2024年上半年
[00:22:48] 劉志遠: 其實就是幾乎所有的這些團隊都會去推
[00:22:52] 劉志遠: GP4水平的這個大模型
[00:22:54] 劉志遠: 那我們團隊呢
[00:22:55] 劉志遠: 其實是在2023年的下半年
[00:22:57] 劉志遠: 在腐陷出這個差的GP水平的模型能力之後
[00:23:01] 劉志遠: 我們的第一方案也是說
[00:23:04] 劉志遠: 我們去追求一下GP4水平的模型能力
[00:23:07] 劉志遠: 但是我們經過合算呢
[00:23:09] 劉志遠: 就會發現這樣一個模型
[00:23:12] 劉志遠: 它所需要的這個模型的參數規模
[00:23:15] 劉志遠: 大概當時應該算出來的是140B的這麼一個規模
[00:23:19] 劉志遠: 那麼對應的這個成本是多少呢
[00:23:22] 劉志遠: 就大概就是一個大幾千萬的一個水平
[00:23:24] 劉志遠: 我們就多想一下
[00:23:26] 劉志遠: 我們會說我花幾千萬的
[00:23:28] 劉志遠: 是訊這樣的一個模型
[00:23:29] 劉志遠: 大概率在2024年上半年
[00:23:32] 劉志遠: 國內一線的團隊至少有5加以上
[00:23:35] 劉志遠: 能夠實現出類似的能力
[00:23:37] 劉志遠: 那我如何能夠收回
[00:23:39] 劉志遠: 我訓練這個模型的成本呢
[00:23:41] 劉志遠: 我們會認為我們找不到這樣的一個確定的答案
[00:23:45] 劉志遠: 所以我們就會認為
[00:23:46] 劉志遠: 如果說這個配方都沒有什麼變化
[00:23:49] 劉志遠: 你只是把模型參數變大了
[00:23:51] 劉志遠: 然後把數據動多一點
[00:23:53] 劉志遠: 再訓一個GP4模型
[00:23:55] 劉志遠: 對我們來講在商業上是講不通的
[00:23:57] 劉志遠: 所以我們在那個時候呢
[00:23:58] 劉志遠: 就把主要的經歷
[00:24:01] 劉志遠: 變成了我們去進行模型風動的構建
[00:24:04] 劉志遠: 就我們要去看
[00:24:05] 劉志遠: 把各個方面更加高效地去進行這個構建
[00:24:09] 劉志遠: 它對於我們提升這個模型能力
[00:24:12] 劉志遠: 效率它有什麼影響
[00:24:14] 劉志遠: 所以呢就是這一系列的工作呢
[00:24:16] 劉志遠: 就產生的結果就是2024年
[00:24:18] 劉志遠: 1月份我們發布的
[00:24:19] 劉志遠: MiniCPM的第一個版本
[00:24:21] 劉志遠: 就是用一個24億的參數就可以實現
[00:24:24] 劉志遠: 像這個當時的喇嘛兔的13B
[00:24:27] 劉志遠: 然後像當時的Mitro7B
[00:24:30] 劉志遠: 就是這樣的一個模型的效果
[00:24:32] 劉志遠: 就會發現說
[00:24:33] 劉志遠: 你就想2.4B是什麼水平
[00:24:35] 劉志遠: 就是在手機上就可以跑了
[00:24:37] 劉志遠: 所以就是2.4B
[00:24:38] 劉志遠: MiniCPM也就是2024年的1月份
[00:24:42] 劉志遠: 讓我們意識到了
[00:24:43] 劉志遠: 其實我們可以通過技術的
[00:24:45] 劉志遠: 持續的創新
[00:24:47] 劉志遠: 能夠極大的提升這個模型的
[00:24:49] 劉志遠: 它的這個效率
[00:24:51] 劉志遠: 也就是我可以用更少的參數
[00:24:53] 劉志遠: 更少的計算量
[00:24:54] 劉志遠: 然後來去實現
[00:24:55] 劉志遠: 這個相同的甚至更高的模型能力
[00:24:58] 劉志遠: 當然一個方面我們就看到了
[00:25:00] 劉志遠: 就是在端策上去部署
[00:25:02] 劉志遠: 大模型的這個可能性
[00:25:04] 劉志遠: 所以就是在2024年
[00:25:05] 劉志遠: 我們就提端策制鬧
[00:25:07] 劉志遠: 端策大模型
[00:25:08] 劉志遠: 但是與此同時呢
[00:25:10] 劉志遠: 就是從暗線上
[00:25:12] 劉志遠: 朝軍我們其實就再去想
[00:25:15] 劉志遠: 就是它背後的規律會是什麼
[00:25:17] 劉志遠: 所以那個時候其實就是
[00:25:19] 劉志遠: 24年的整個的這個1年的時間
[00:25:22] 劉志遠: 其實我們在嘗試的
[00:25:24] 劉志遠: 就是尋找這樣的規律
[00:25:25] 劉志遠: 那當然就是受到模耳定律的這個啟示
[00:25:28] 劉志遠: 我們就想像說提出來
[00:25:30] 劉志遠: 這個支持密度的是密個概念
[00:25:32] 劉志遠: 然後在支持密度的技術上
[00:25:34] 劉志遠: 我們去探究如何去找到
[00:25:38] 劉志遠: 就是所有的這些模型
[00:25:40] 劉志遠: 通過技術上新
[00:25:41] 劉志遠: 它的這個密度的這個變化的一些規律
[00:25:44] 劉志遠: 這個其實是它的元氣
[00:25:46] 劉志遠: 所以就至少在面臂內部的話
[00:25:48] 曼奇: 這個其實成為了你們一個目標之一
[00:25:50] 曼奇: 然後你們現在把這個東西發出去
[00:25:52] 曼奇: 其實也是因為你們覺得
[00:25:53] 曼奇: 這個時間的方向是對的
[00:25:54] 曼奇: 然後你們覺得可能在業界裡
[00:25:55] 曼奇: 會有更多的共鳴
[00:25:57] 那回到你們的這個研究的一些發現
[00:25:59] 曼奇: 就是比如說我看到你們這個數據裡面
[00:26:01] 曼奇: 還說Chag.P.E.之前和之後
[00:26:03] 曼奇: 從計算到能力的這個協率
[00:26:05] 曼奇: 它是變得懂了嗎
[00:26:06] 曼奇: Chag.P.之後是變得更懂了
[00:26:07] 曼奇: 我又有一個表號起的事
[00:26:08] 曼奇: 就是為什麼在24年9月的
[00:26:10] 曼奇: O1和21之後就是強化學習
[00:26:12] 曼奇: 後訓練的這個方法被引入之後
[00:26:14] 曼奇: 它沒有類似的這種協力變化
[00:26:16] 曼奇: 還是說現在這時間太短了
[00:26:17] 曼奇: 其實更多的時刻
[00:26:19] 蕭朝軍: 可能是基礎系列上的問題
[00:26:20] 蕭朝軍: 就是我們這個難力密度
[00:26:22] 蕭朝軍: 去年年底的時候
[00:26:23] 蕭朝軍: 第一的版本是面向要激作模型的
[00:26:26] 所以說O1和21
[00:26:27] 蕭朝軍: 它更多的是後訓練技術的一個增長
[00:26:29] 蕭朝軍: 所以其實沒有直接體驗
[00:26:30] 蕭朝軍: 在這個能力密度裡
[00:26:32] 蕭朝軍: 然後其實我們現在也在做
[00:26:33] 蕭朝軍: 相應的2.0的版本
[00:26:35] 蕭朝軍: 就是希望能夠把所有的後訓練的
[00:26:38] 蕭朝軍: 這些技術的改進
[00:26:39] 蕭朝軍: 體現在這個能力密度的這個指標上
[00:26:42] 蕭朝軍: 然後我會把這個
[00:26:43] 蕭朝軍: 因為或你可以這麼理解
[00:26:44] 蕭朝軍: 就是我們現在提到這個能力密度
[00:26:46] 蕭朝軍: 對應的是Skilling了
[00:26:48] 蕭朝軍: 但是其實O1和21
[00:26:49] 蕭朝軍: 它的增長的是TESSA Time Skilling
[00:26:52] 蕭朝軍: 或叫Infinite Time Skilling了
[00:26:54] 蕭朝軍: 它其實去兌應的兩個不同的階段
[00:26:56] 蕭朝軍: 然後它同樣的
[00:26:57] 蕭朝軍: 我們的Dancing Law
[00:26:58] 蕭朝軍: 可能也要有這個面向Skilling Law的一個版本
[00:27:01] 蕭朝軍: 然後面向後面後訓練的一個版本
[00:27:04] 蕭朝軍: 不過相關的這個建模
[00:27:06] 蕭朝軍: 我們還在進展過程當中
[00:27:08] 蕭朝軍: 對 那我具體怎麼去實現這個密度的提升
[00:27:11] 曼奇: 就可以展開說一說
[00:27:12] 曼奇: 其實氛圍就是架構數據
[00:27:15] 蕭朝軍: 然後學習算法
[00:27:16] 蕭朝軍: 然後軟硬議體就是Infinite四個層面
[00:27:19] 蕭朝軍: 其實這已經是整個模型PAPE Lite
[00:27:21] 蕭朝軍: 上比較完整的四個方面
[00:27:24] 蕭朝軍: 我們現在通過做這個
[00:27:26] 劉志遠: 相當於是大模型的密度法則的
[00:27:29] 劉志遠: 相關的工作
[00:27:29] 劉志遠: 其實我們會發現
[00:27:31] 劉志遠: 不管是模型的架構
[00:27:33] 劉志遠: 包括就是技術注意力
[00:27:34] 劉志遠: 包括像Depthsick引領的
[00:27:37] 劉志遠: 就是今年的相當於系列度的
[00:27:39] 劉志遠: 這種MOE架構
[00:27:41] 劉志遠: 那麼其實都是在這個方面的
[00:27:43] 劉志遠: 這個非常典型的這個工作
[00:27:45] 劉志遠: 那麼這種在模型架構上上行
[00:27:48] 劉志遠: 那它一定是會往更高效的這個角度去做
[00:27:52] 劉志遠: 第二個方面就是數據治理
[00:27:54] 劉志遠: 也就是說我這個世界上的
[00:27:56] 劉志遠: 已有的數據
[00:27:57] 劉志遠: 和這個世界上還沒有的數據
[00:27:59] 劉志遠: 那我如何能夠為了這個模型能力的提升
[00:28:03] 劉志遠: 我能夠更好的去尋找學習的這個教材
[00:28:06] 劉志遠: 那這件事情本身現在看
[00:28:08] 劉志遠: 對於這個模型能力提升的這個效率
[00:28:11] 劉志遠: 和它的上限影響也都非常大
[00:28:14] 劉志遠: 你就可以向向一些一個小朋友
[00:28:16] 劉志遠: 它從小到大
[00:28:17] 劉志遠: 你給它的是一套非常精心挑選的這個教材
[00:28:21] 劉志遠: 精心準備的教材
[00:28:22] 劉志遠: 還是說啥也不管
[00:28:24] 劉志遠: 就是直接給它這個亂七八糟的東西讓它去讀
[00:28:27] 劉志遠: 顯然結果是不一樣的
[00:28:29] 劉志遠: 那麼第三個其實就是我們
[00:28:31] 劉志遠: 把它稱為叫做模型風動
[00:28:33] 劉志遠: 也就是我們要尋找這個模型成長的這個規律
[00:28:37] 劉志遠: 那我們能不能做到一小間大
[00:28:39] 劉志遠: 我們做大量的小的模型的實驗
[00:28:42] 劉志遠: 我們書籍的這些經驗的這種數據
[00:28:46] 劉志遠: 能夠讓我們預測出
[00:28:47] 劉志遠: 我們即將要訓練的那個大模型
[00:28:49] 劉志遠: 在什麼參數
[00:28:51] 劉志遠: 什麼數據配置下
[00:28:52] 劉志遠: 然後它能夠達到更高的效果
[00:28:55] 劉志遠: 我們能不能在這個模型還沒有訓練之前
[00:28:58] 劉志遠: 我們就有可能預測出它的這個能力上限
[00:29:02] 劉志遠: 會是多少
[00:29:03] 劉志遠: 那麼這種對智能的能力成長的這種規律的把握
[00:29:07] 劉志遠: 那麼當年在OPR是被稱為叫做Preditle skating
[00:29:11] 劉志遠: 也就是說它提出 skating load
[00:29:13] 劉志遠: 本身不只是說找到了這條Lower
[00:29:15] 劉志遠: 這條發展的這個這個這個曲線
[00:29:18] 劉志遠: 而是同時能夠找到
[00:29:21] 劉志遠: 我可以通過小模型來去預測
[00:29:24] 劉志遠: 大模型能力的這麼一個規律
[00:29:26] 劉志遠: 那麼這個規律其實對於我們去更好的去
[00:29:29] 劉志遠: 利用這個規律
[00:29:30] 劉志遠: 然後來更好的去設置
[00:29:32] 劉志遠: 那個大模型的相關的這種配置
[00:29:35] 劉志遠: 就具有舉足輕重的作用
[00:29:37] 劉志遠: 那麼第四個就是我們會看到
[00:29:40] 劉志遠: 模型它是要伴隨著硬件來這個共同成長的
[00:29:44] 劉志遠: 也就是說這個模型它是要在一個具體的硬件上
[00:29:47] 劉志遠: 然後來去發揮它的作用
[00:29:49] 劉志遠: 所以未來呢也一定是要在訓練和使用的階段
[00:29:53] 劉志遠: 都要實現一個軟硬血統的這麼一個設計和優化
[00:29:57] 劉志遠: 所以就是這這幾個要素
[00:29:59] 劉志遠: 其實都對這個模型的密度
[00:30:01] 劉志遠: 對這個模型的這個效果
[00:30:03] 劉志遠: 就是它到底是用越少的計算量
[00:30:07] 劉志遠: 越少的這個能量的消耗
[00:30:09] 劉志遠: 來去實現一個更強的能力
[00:30:11] 劉志遠: 那麼都具有非常重要的這個作用
[00:30:13] 劉志遠: 我們其實在推出這個密度法則之後
[00:30:16] 劉志遠: 其實我們現在正在推進的工作
[00:30:19] 劉志遠: 就是希望能夠定量的發現
[00:30:21] 劉志遠: 就是模型的價格數據的治理
[00:30:23] 劉志遠: 然後以及模型的風度等等
[00:30:26] 劉志遠: 然後他們對於這個模型密度
[00:30:29] 劉志遠: 對這個模型的能效的這麼一個影響的這個關係
[00:30:33] 劉志遠: 就是他們之間的這個定量關係
[00:30:35] 劉志遠: 現在來看其實還沒有找到特別的清晰
[00:30:39] 劉志遠: 但是我們有相應的這些經驗性的結果
[00:30:42] 劉志遠: 其實我要這個四個方面我們
[00:30:44] 蕭朝軍: 應該可以看到現在市面上有很多相關的工作
[00:30:47] 蕭朝軍: 第一個就是架構
[00:30:48] 蕭朝軍: 就可以看到其實傳統的就兩個部分
[00:30:51] 蕭朝軍: FFM和這個等下
[00:30:53] 蕭朝軍: 還有他們現在基本上大家已經共識
[00:30:55] 蕭朝軍: 就是要去做這個系列的Moe
[00:30:57] 蕭朝軍: 然後系列的細膚
[00:30:59] 蕭朝軍: 我們就解釋一下FF
[00:31:00] 蕭朝軍: 我覺得Feed4Won Network
[00:31:03] 蕭朝軍: 這個解釋可能就是一個
[00:31:04] 蕭朝軍: 怎麼說呢一個比較蟲密的一個取正程
[00:31:07] 蕭朝軍: 然後細膚的Moe
[00:31:09] 蕭朝軍: 其實就是說這個
[00:31:11] 蕭朝軍: 在那麼大了一個取正裡面
[00:31:13] 蕭朝軍: 我切分了好幾塊
[00:31:14] 蕭朝軍: 然後每一次計算我就選擇其中一塊
[00:31:17] 蕭朝軍: 這個其實是Depsy可接受給大家的
[00:31:19] 蕭朝軍: 為什麼Moe這個時候是Depsy可接受給大家的
[00:31:22] 曼奇: 不是上Bisit說這些不是更早做Moe嗎
[00:31:24] 曼奇: 對但是現在是Miche要做的那些Moe
[00:31:27] 蕭朝軍: 我後面的Max也更早在做Moe
[00:31:29] 曼奇: 其實Moe很早我們在做
[00:31:31] 劉志遠: 就是原來我有一個博士生
[00:31:33] 劉志遠: 其實大概張正彥
[00:31:35] 劉志遠: 大概2021年
[00:31:38] 劉志遠: 其實在早期就開始做Moe的這個架構
[00:31:42] 劉志遠: 就Mexal experts本身這個概念
[00:31:44] 劉志遠: 應該是上個世紀90年代就已經有
[00:31:47] 蕭朝軍: 然後用到了這個Transformer裡面
[00:31:50] 劉志遠: 應該其實是2020年左右
[00:31:53] 劉志遠: 當時有Google 有我們團隊
[00:31:55] 劉志遠: 其實都在做
[00:31:56] 劉志遠: 但的確科官上來講是由於Depsy可
[00:31:59] 劉志遠: 然後把它真正的做到了一個
[00:32:01] 劉志遠: 這種最大的模型上
[00:32:04] 劉志遠: 然後做Work
[00:32:05] 劉志遠: 因為其實它的規模化
[00:32:07] 劉志遠: 然後以及說它真正的能夠支持高效
[00:32:10] 劉志遠: 就真正事實上還是有很多工程上的挑戰
[00:32:13] 劉志遠: 所以它是一個就是Depsy可的解釋的意思是說
[00:32:16] 曼奇: 它從一個大家你做了一段時間的
[00:32:18] 曼奇: 這種想法和裡面
[00:32:20] 曼奇: 然後它給它在一個比較大的規模上實現了
[00:32:23] 曼奇: 然後大家覺得這個生存有效
[00:32:25] 曼奇: 對其實這個事情
[00:32:26] 蕭朝軍: 你可以在大模型的這個翻轉行業裡面
[00:32:28] 蕭朝軍: 你可以看到從祭福上來說
[00:32:30] 蕭朝軍: 其實沒有太多型的模型
[00:32:32] 蕭朝軍: 因為看到現在大家很火的
[00:32:33] 蕭朝軍: 喬娃學系也是上個世紀
[00:32:35] 蕭朝軍: 就已經非常火熱了
[00:32:37] 蕭朝軍: 然後你穿這個生存學機
[00:32:39] 蕭朝軍: 這個BP等等
[00:32:40] 蕭朝軍: 這都是上個世紀的強勢
[00:32:41] 蕭朝軍: 然後我放到現在
[00:32:43] 蕭朝軍: 其實讓核心還是因為算力的增長
[00:32:45] 蕭朝軍: 數據的增長
[00:32:45] 蕭朝軍: 使得它能夠發揮它的這個作用
[00:32:48] 蕭朝軍: 所以我們講Depsy可解釋
[00:32:49] 蕭朝軍: 其實也不是說Depsy可發明的件事情
[00:32:51] 蕭朝軍: 因為很早這個
[00:32:53] 蕭朝軍: Gb4出來的時候
[00:32:54] 蕭朝軍: 大家就在討論它就是M1架構
[00:32:56] 蕭朝軍: 但是具體說它怎麼樣
[00:32:57] 蕭朝軍: Skilling到那麼大
[00:32:58] 蕭朝軍: 那其實這個沒有形成一個共識
[00:33:01] 蕭朝軍: 然後大家也不敢一下子
[00:33:03] 蕭朝軍: 就是說我花很多的算力
[00:33:04] 蕭朝軍: 我就是選擇一條跟主流
[00:33:07] 蕭朝軍: 不一樣的道路
[00:33:08] 蕭朝軍: 所以其實Depsy可做到的
[00:33:09] 蕭朝軍: 今天事情大家就開始風冷
[00:33:11] 蕭朝軍: 而它是開源的
[00:33:12] 曼奇: 對
[00:33:13] 蕭朝軍: 其實就是接識的事情
[00:33:14] 蕭朝軍: 表示它從一個非共識到共識的
[00:33:17] 蕭朝軍: 就能一個轉接點
[00:33:18] 蕭朝軍: 你可以去說就是在FF分
[00:33:19] 曼奇: 大家會一個比較大的共識
[00:33:21] 曼奇: 是做M1的這個架構
[00:33:23] 曼奇: 然後你就可以看到
[00:33:24] 蕭朝軍: 今年一個很重要的一個轉變
[00:33:26] 蕭朝軍: 其實是就剛才提到的Egentic
[00:33:28] 蕭朝軍: 以及Senscom
[00:33:29] 蕭朝軍: 其實這兩個點其實都接受了
[00:33:31] 蕭朝軍: 這個大模型要變成長模型
[00:33:34] 蕭朝軍: 就是我要這個讓模型
[00:33:35] 蕭朝軍: 它能夠接觸到足夠多的上下輪
[00:33:37] 蕭朝軍: 就是這個
[00:33:39] 蕭朝軍: 你比如說你給的代碼的一個倉庫
[00:33:41] 蕭朝軍: 然後或者是Depsy的色系
[00:33:43] 蕭朝軍: 就是海浪的外部型系
[00:33:45] 蕭朝軍: 那再這樣一個場景下的話
[00:33:46] 蕭朝軍: 那這個它要接受很多的輸入
[00:33:49] 蕭朝軍: 然後產生很多的輸出
[00:33:50] 蕭朝軍: 那大模型是不是變成長模型
[00:33:52] 蕭朝軍: 所以很少就成為了非常重要的這個品勁
[00:33:55] 蕭朝軍: 所以現在大家都在做這件事情
[00:33:57] 蕭朝軍: 就包括現行也好
[00:33:58] 蕭朝軍: 包括西蘇也
[00:34:00] 蕭朝軍: 然後其實也有一些傳聞
[00:34:02] 蕭朝軍: 就是什麼Gemini等等
[00:34:04] 蕭朝軍: 就是都是用斯萊丁Window
[00:34:06] 蕭朝軍: 華東闖口這個模型
[00:34:07] 蕭朝軍: 叫西蘇注意力
[00:34:08] 蕭朝軍: 然後混合
[00:34:09] 蕭朝軍: 蟲密注意力來做的
[00:34:10] 蕭朝軍: 你覺得看到
[00:34:11] 蕭朝軍: 其實不管是開源幣
[00:34:12] 蕭朝軍: 大家都在幹的事情
[00:34:14] 蕭朝軍: 那其實這個事情加構體現的就是
[00:34:17] 蕭朝軍: 我們很形象的成為它是智能的容器
[00:34:21] 蕭朝軍: 就是說我們這個用
[00:34:23] 蕭朝軍: 更少的計算量達到相通的效果
[00:34:25] 蕭朝軍: 那其實是加構它帶了一個
[00:34:27] 蕭朝軍: 一個比較重要的改變
[00:34:29] 蕭朝軍: 然後另外一個其實輸入這個層面
[00:34:31] 蕭朝軍: 其實今年也會有一個比較大的一個共識
[00:34:35] 蕭朝軍: 就是因為去年也這個很多人都提到了
[00:34:38] 蕭朝軍: 就是死給靈魯撞牆了
[00:34:40] 蕭朝軍: 然後公開口禍去的數據哭解了
[00:34:43] 蕭朝軍: 然後這個那其實下一步
[00:34:45] 蕭朝軍: 很重要的一個點
[00:34:46] 蕭朝軍: 還有一個增長點是合成數據
[00:34:48] 蕭朝軍: 然後以及更高級的清洗數據
[00:34:51] 蕭朝軍: 其實這兩個增長點是
[00:34:52] 蕭朝軍: 大家現在也都在發揮
[00:34:54] 蕭朝軍: 很難做我們在做的
[00:34:55] 蕭朝軍: 這點上你們可以分享自己是怎麼做了嗎
[00:34:57] 曼奇: 對 這個的話
[00:34:58] 劉志遠: 我們其實會有一個內部的一個拍攝
[00:35:01] 劉志遠: 這個publand大概是從L0一直到L4
[00:35:05] 劉志遠: 就是大概不同的Livo
[00:35:06] 劉志遠: 你大家可以設想就是L0這個層次
[00:35:09] 劉志遠: 是相當於是數據的數據
[00:35:11] 劉志遠: 我去抓取也好 採買也好
[00:35:14] 劉志遠: 形成了最原始的數據
[00:35:16] 劉志遠: 那麼L1相當於是過濾
[00:35:18] 劉志遠: 相當於說把一些重複
[00:35:20] 劉志遠: 把一些垃圾給過濾跳
[00:35:22] 劉志遠: L2相當於是選擇
[00:35:24] 劉志遠: 就是相當於再從這些數據裡面
[00:35:26] 劉志遠: 選擇我們認為高質量的這麼一個數據
[00:35:29] 劉志遠: L3相當於是合成
[00:35:31] 劉志遠: 相當於說我們去再去做這個數據的合成的這種工作
[00:35:36] 劉志遠: 就是合成或者是改善等等的
[00:35:38] 劉志遠: 就相當於說它不再是對EO數據的
[00:35:41] 劉志遠: 簡單的這麼一個處理
[00:35:43] 劉志遠: 而是說我會去產生一些這個世界上
[00:35:45] 劉志遠: 沒有過的數據
[00:35:46] 劉志遠: L4呢 其實就是相當於是
[00:35:49] 劉志遠: 我們把它成為叫驗證數據
[00:35:50] 劉志遠: 就是相當於我們會經過一些形式化
[00:35:53] 劉志遠: 或者是人工的方式去確認
[00:35:56] 劉志遠: 這些數據是教採機的這麼一些數據
[00:35:59] 劉志遠: 比如說最近在國際上有一個非常有名的
[00:36:03] 劉志遠: 於訓練的數據 其叫翻外版
[00:36:06] 劉志遠: 我們做了點什麼事呢
[00:36:07] 劉志遠: 就是我們用我們的這套數據
[00:36:11] 劉志遠: 預處理的這個PAPLIN
[00:36:12] 劉志遠: 把這個翻外版的我們做了一個經驗
[00:36:15] 劉志遠: 就相當於我們又去精選了它的這個相應的數據
[00:36:19] 劉志遠: 得到了一個它的十分之一不到的
[00:36:21] 劉志遠: 這麼一個叫做Ouch翻外版
[00:36:23] 劉志遠: 那我們就會發現
[00:36:24] 劉志遠: 用這個Ouch翻外版訓練的這個模型能力
[00:36:27] 劉志遠: 比這個原始的這個翻外版訓練的模型能力還要再高
[00:36:32] 劉志遠: 那你就可以設想我就是用不到十分之一的數據
[00:36:35] 劉志遠: 然後我就可以學一個更好的模型
[00:36:37] 劉志遠: 那顯然我的訓練的成本
[00:36:39] 劉志遠: 就可以下降到十分之一
[00:36:41] 劉志遠: 就這個其實是我們能看到的
[00:36:43] 劉志遠: 就是你數據治理本身帶來的證明一個價值
[00:36:46] 劉志遠: 那麼其實這還只是於訓練的這個部分
[00:36:49] 劉志遠: 那還有後訓練就是包括SFT
[00:36:51] 劉志遠: 包括這個這個強化學習
[00:36:55] 劉志遠: 相應的這些數據你的這個合成的這個程度
[00:37:00] 劉志遠: 合成的這個水平
[00:37:02] 劉志遠: 其實都非常大的去影響
[00:37:04] 劉志遠: 就是你的這個模型的能力的上限和效率
[00:37:07] 劉志遠: 比如說上海交大有一位教授叫劉鵬菲
[00:37:11] 劉志遠: 這個非常年輕的一個老師
[00:37:12] 劉志遠: 他最近就是做了一系列的工作
[00:37:14] 劉志遠: 叫Less Eastmore
[00:37:15] 劉志遠: 就是說我能用更少的數據
[00:37:18] 劉志遠: 可以去得到更強的能力
[00:37:20] 劉志遠: 本身也是這個方面的一些這個非常好的這些結果
[00:37:24] 劉志遠: 所以我們其實會看到
[00:37:26] 劉志遠: 就是在數據治理這個方面
[00:37:29] 劉志遠: 有非常多的信號告訴我們
[00:37:31] 劉志遠: 其實我們需要做一個更精緻的
[00:37:34] 劉志遠: 更精烈的這麼一個數據來讓模型去學
[00:37:38] 劉志遠: 其實一個非常終極的問題就是
[00:37:40] 劉志遠: 作為人工智能發展到大模型的結段
[00:37:43] 劉志遠: 其實它非常典型的特點就是數據驅動
[00:37:46] 劉志遠: 就是你所有的智能都是來自於數據的
[00:37:48] 劉志遠: 那我們就要問一個問題
[00:37:50] 劉志遠: 假如說我是構建某一個水平的模型的能力
[00:37:56] 劉志遠: 我可以構建的最小化的那個數據機會是什麼
[00:38:01] 劉志遠: 就是這件問題這個問題其實我覺得
[00:38:03] 劉志遠: 可以持續地去探索
[00:38:06] 劉志遠: 那麼這個事情也有助於我們去
[00:38:08] 劉志遠: 想像是說去追尋就是人工智能比較終極的一些問題
[00:38:14] 劉志遠: 也就是智能到這是什麼
[00:38:15] 劉志遠: 總之的話就是我們內部有一套比較複雜的
[00:38:19] 劉志遠: 這個完備的一個分層的一個數據智能的體系
[00:38:22] 劉志遠: 當然我們應該會在最近幾個月會發布一個
[00:38:25] 劉志遠: 這個方面的一個就是我們的報告
[00:38:28] 劉志遠: 然後以及我們整個的這個數據的樣力
[00:38:32] 劉志遠: 就是做一個數據的整體的一個開源
[00:38:34] 劉志遠: 你說這個數據的開源是指開源
[00:38:36] 曼奇: 最後數據的結果還是說這個PAPA拉一本什麼會開的
[00:38:40] 曼奇: 數據的結果和我們處理這個數據的一個整體的一個架構
[00:38:45] 劉志遠: 就是這個我們會一天論文的顯示來告訴大家
[00:38:48] 劉志遠: 報告的顯示就是那個數據體系會開源
[00:38:50] 劉志遠: 可能應該開明一步子
[00:38:51] 劉志遠: 對 會告訴大家就是說你大概需要做成什麼樣子
[00:38:55] 劉志遠: 其實我們在過去幾年也在數據這個方面開放了很多
[00:38:58] 劉志遠: 這個樣力包括像R2CatchR3的Bike等等
[00:39:02] 劉志遠: 包括剛才提到的R2翻WiP
[00:39:05] 劉志遠: 其實我們想告訴大家的就是數據的質量和數量
[00:39:09] 劉志遠: 其實的確是這個模型
[00:39:10] 劉志遠: 它能夠達到一個更高水平的一個非常重要的基礎
[00:39:14] 劉志遠: 剛才說模型架構是智能的容器
[00:39:17] 劉志遠: 那麼數據的話就是模型學習的教材
[00:39:19] 劉志遠: 這個是沒有什麼好不公開的
[00:39:21] 劉志遠: 因為本身對於AI這個領域開源共享是共識
[00:39:26] 劉志遠: 但是同時的話其實我們會看到就是自從大模型出現之後
[00:39:30] 劉志遠: 其實這個構建一個大模型或者構建一個智能的系統
[00:39:34] 劉志遠: 本身會變成一個非常複雜的一個精密的體系
[00:39:38] 劉志遠: 它裡面包含的環節特別多
[00:39:40] 劉志遠: 我們只是列出來了裡面比較重要的四個方面
[00:39:44] 劉志遠: 對吧 架構 數據 學習還有這個硬件
[00:39:48] 劉志遠: 但是裡面的每一個部分又包含非常複雜的這麼一個體系
[00:39:53] 劉志遠: 其實你看像DipSync它公布的DipSync V3
[00:39:57] 劉志遠: 它的Country Beauty大概是100多人
[00:40:01] 劉志遠: 我認為本身已經算是一個非常精亮的體系了
[00:40:06] 劉志遠: 你就可以設想就是在全世界方面來看
[00:40:09] 劉志遠: 可能是數百人的一個團隊
[00:40:11] 劉志遠: 其實上千人可能都是有可能的
[00:40:13] 劉志遠: 我覺得它知識和成果產生的方式也很不同
[00:40:17] 曼奇: 因為以前的諾文可能就是說四、五個或者什麼七、八個的作者
[00:40:21] 曼奇: 現在就是報告下面都是
[00:40:23] 曼奇: 是的
[00:40:24] 劉志遠: 我們可以把這個時候玩
[00:40:25] 曼奇: 因為剛才說了是這個模型的架構還有數據
[00:40:28] 曼奇: 在往下還有算法和這個軟硬件
[00:40:30] 蕭朝軍: 對 其實算法也是一個很重要的點
[00:40:31] 蕭朝軍: 其實算法大家是比較能有感觸的
[00:40:34] 蕭朝軍: 因為去這個御徐念
[00:40:37] 蕭朝軍: 然後到今年大家都很關注的強化學習
[00:40:41] 蕭朝軍: 然後今年事情其實強化學習做到現在
[00:40:44] 蕭朝軍: 其實大家還沒有解決一個問題
[00:40:46] 蕭朝軍: 就是強化學習的Skilling的問題
[00:40:48] 蕭朝軍: 所以之前我們一直會講PrettyChannelSkilling
[00:40:51] 蕭朝軍: 那還有Skilling Law
[00:40:53] 蕭朝軍: 但是強化學習它的Skilling Law是什麼
[00:40:55] 蕭朝軍: 以及我們怎麼樣讓強化學習能夠持續地訓下去
[00:40:59] 蕭朝軍: 這件事情是大家一個臺體持續探索的事情
[00:41:03] 蕭朝軍: 這個可以講開講一下
[00:41:04] 曼奇: 就是強化學習它沒有Skilling這個是指什麼
[00:41:07] 曼奇: 我看依賴那個博會也在討論這個
[00:41:09] 曼奇: 它覺得就不應該用Skilling去描述現在強化學習的狀態
[00:41:12] 曼奇: 對 因為這個事情是大家現在也還是一個非共識的事情
[00:41:16] 蕭朝軍: 就是你可以看到OE當時去年你下面年的時候
[00:41:19] 蕭朝軍: 它就講它已經有了是大規模強化學習得到的OE糟的系統
[00:41:23] 蕭朝軍: 然後你其實可以看到最比御徐念來說
[00:41:25] 蕭朝軍: 強化學習的Skilling還是完全沒有做的
[00:41:28] 蕭朝軍: 這特別好 御徐念現在基本上有幾十T頭肯
[00:41:31] 蕭朝軍: 然後御徐念的部署可能有幾十萬
[00:41:34] 蕭朝軍: 你現在強化學習基本上大家訓個幾千步
[00:41:37] 蕭朝軍: 然後RE他公布的那個報告也就訓了不到一萬步
[00:41:40] 蕭朝軍: 這已經達到在某個任務上拿得很好的效果
[00:41:43] 蕭朝軍: 但其實他還遠沒有到我們認為
[00:41:45] 蕭朝軍: 他已經Skilling足夠好的一個程度
[00:41:48] 蕭朝軍: 所以其實大家在強調強化學習的Skilling
[00:41:51] 蕭朝軍: 誰會講的是說環境的Skilling然後數據的Skilling
[00:41:55] 蕭朝軍: 因為現在的強化學習很強的VirreFellable的Reword
[00:42:00] 蕭朝軍: 所以你可驗證的Reword
[00:42:01] 蕭朝軍: 但現在來說大家公開了以知的很好或許的這種可驗證的Reword
[00:42:06] 蕭朝軍: 誰就是數學和蛋馬
[00:42:07] 蕭朝軍: 然後但是更多的場景
[00:42:09] 蕭朝軍: 就是你雖然說蛋馬已經很強了
[00:42:11] 蕭朝軍: 但是你像現在這個他只是在競賽題
[00:42:14] 蕭朝軍: 但是比如我要去做這個真實的開發編程
[00:42:17] 蕭朝軍: 那怎麼辦
[00:42:18] 蕭朝軍: 開發編程經驗的時候你就比競賽題更佛達
[00:42:21] 蕭朝軍: 因為競賽題只涉及非常簡單的拍攝環境
[00:42:24] 蕭朝軍: 那如果要去做這種更佛達的編程開發
[00:42:27] 蕭朝軍: 就會有背後更佛達的什麼第三方庫等等
[00:42:30] 蕭朝軍: 那其實這些事情都是大家還沒做到的
[00:42:32] 蕭朝軍: 所以Ios怎麼Skilling是一個很大的問題
[00:42:37] 蕭朝軍: 然後你是可以看到就OPEN AI之前這個機前
[00:42:41] 蕭朝軍: 有那個研究委員的接通委
[00:42:43] 蕭朝軍: 他就在發推的時候這個Iodoma的Magic
[00:42:46] 蕭朝軍: 然後他的Matcher的核心在於一個Unhackbo的Invirement
[00:42:50] 蕭朝軍: 就是不可被Hack的環境
[00:42:52] 蕭朝軍: 那其實這個點其實他背後硬設的就是
[00:42:55] 蕭朝軍: 我要想辦法給強化學習大件好足夠好的
[00:42:58] 蕭朝軍: 一個Skilling的一個平台
[00:43:01] 蕭朝軍: 這樣的話這個模型他可以在上面
[00:43:03] 蕭朝軍: 真的持續的學到東西
[00:43:05] 蕭朝軍: 這個持續是大家現在在學習範圍上一直在講
[00:43:08] 蕭朝軍: 強化學習的強化學習突破
[00:43:11] 蕭朝軍: 當然這個醫療他提到的這個說
[00:43:13] 蕭朝軍: 他說強化學習不可思給令
[00:43:15] 蕭朝軍: 是他認為這種Unhackbo的Invimement的實際上是不太存在的
[00:43:20] 蕭朝軍: 或者是說他會認為人類的學習是不是這個樣子的
[00:43:23] 蕭朝軍: 因為這個事情也已簽涉到我們對ADI下一步的定位
[00:43:27] 蕭朝軍: 就是你看我們現在的強化學習
[00:43:29] 蕭朝軍: 他能做到一個點是你只要在任何一個任務上
[00:43:32] 蕭朝軍: 就要有充足的標誌輸入
[00:43:34] 蕭朝軍: 有充足的人類反饋
[00:43:36] 蕭朝軍: 他在這個任務上就能夠達到這個前面的一萬分級人類的水平
[00:43:40] 蕭朝軍: 但是現在什麼MNO金牌
[00:43:42] 蕭朝軍: I-5I金牌就已經很多大廠都已經Clam自己做到了
[00:43:46] 蕭朝軍: 但是這個這樣的話其實你可以想想想
[00:43:50] 蕭朝軍: 加上加上一家公司他做的一個就是某個具體的業務
[00:43:53] 蕭朝軍: 他可以在這個業務上接著足夠的數據通過強化學習
[00:43:56] 蕭朝軍: 使得他在這個業務上非常的驚通
[00:43:59] 蕭朝軍: 現在有趨勢吧
[00:44:00] 曼奇: 或者叫這個封口就RL2B什麼的
[00:44:03] 曼奇: 所以我去針對一個真實的商業環境去做這個
[00:44:06] 蕭朝軍: 其實但是這樣的ADI不是我們真的想要的ADI
[00:44:10] 蕭朝軍: 因為我們人類他的學習是很高效的
[00:44:13] 蕭朝軍: 我可以在一個新的任務下
[00:44:16] 蕭朝軍: 然後從我少量的反饋、少量的動作
[00:44:19] 蕭朝軍: 我就能夠把這個任務學得很好
[00:44:21] 蕭朝軍: 所以這個其實是學習範疇上
[00:44:24] 蕭朝軍: 我們可能會面臨的轉變
[00:44:26] 蕭朝軍: 就是有兩種和地園就是計學RL
[00:44:29] 蕭朝軍: 然後怎麼保持RL
[00:44:30] 蕭朝軍: 對怎麼死給你下去
[00:44:31] 蕭朝軍: 另外一種就是你可以看到
[00:44:33] 蕭朝軍: 遇上聯合RL經歷的一個區別
[00:44:35] 蕭朝軍: 或者有監督微調的RL一個區別
[00:44:37] 蕭朝軍: 就是學習效率更高
[00:44:38] 蕭朝軍: 就會我們叫三倍飛勝隧變高了
[00:44:41] 蕭朝軍: 在進一步的RL還是不夠
[00:44:43] 蕭朝軍: 那我未來怎麼讓RL他的學習的效率會更高
[00:44:47] 蕭朝軍: 這裡的學習效率就是怎麼樣跟著好的利用環境反饋
[00:44:51] 蕭朝軍: 然後跟少量的人物標準
[00:44:53] 蕭朝軍: 他能夠在一個新任務上達到一個足夠好的效果
[00:44:56] 所以現在有人會認為RMOS肛伶似直
[00:44:58] 曼奇: 比如說我給他加更多的算力
[00:45:00] 曼奇: 然後讓他睡更多的部署
[00:45:02] 曼奇: 他並不一定帶來一個更好的效果
[00:45:03] 曼奇: 他只能他在特定的任務上
[00:45:05] 蕭朝軍: 他能帶來更好的效果
[00:45:06] 蕭朝軍: 但是他沒有
[00:45:07] 曼奇: 他沒有就是說在更多的下游任務上
[00:45:10] 曼奇: 帶來更好的
[00:45:11] 曼奇: 他只能針對利用環境的任務
[00:45:13] 曼奇: 他的效果更好了
[00:45:14] 曼奇: 是的
[00:45:15] 蕭朝軍: 所以現在我們就說大模型領域
[00:45:17] 蕭朝軍: 我們期間很多時候叫Problem Engine Neural
[00:45:19] 蕭朝軍: 現在我們會求什麼Reward Engine Neural
[00:45:22] 蕭朝軍: 或者是叫這個配環境
[00:45:24] 蕭朝軍: Inverment Engine Neural
[00:45:25] 蕭朝軍: 這都是現在來說
[00:45:27] 蕭朝軍: 不同大模型時代的新的一些產物
[00:45:30] 蕭朝軍: 所以你認為他繼續去RMOS這個方式
[00:45:33] 曼奇: 可能不是我們一般閃耗中的AGA
[00:45:35] 曼奇: 但是有可能他在一些聚險場景的時候
[00:45:37] 曼奇: 很多用處的
[00:45:37] 曼奇: 其實像Coding就已經是一個這樣的例子了
[00:45:40] 曼奇: 也不完全
[00:45:41] 蕭朝軍: 就是你想這個Skating Engine事情
[00:45:43] 蕭朝軍: 他有可能帶來範化的
[00:45:45] 蕭朝軍: 就是你想跟我們之前講預訓練
[00:45:47] 蕭朝軍: 講有監督微調
[00:45:48] 蕭朝軍: 或有監督學習精神
[00:45:50] 蕭朝軍: 在這個生肚學習時代一直是在做的
[00:45:53] 蕭朝軍: 但是他真正出現
[00:45:55] 蕭朝軍: 他能出現範化就是通過預訓練
[00:45:58] 蕭朝軍: 就是Skating上去之後他就範化了
[00:46:00] 蕭朝軍: RMSkating能不能帶來這一個點
[00:46:03] 蕭朝軍: 其實是不好講的
[00:46:04] 蕭朝軍: 所以我會講他有現在有長久的技術路線
[00:46:06] 蕭朝軍: 就是一個技術Skating下去
[00:46:08] 蕭朝軍: 在足夠多的環境下Skating
[00:46:10] 蕭朝軍: 那他是不是重點在新的任務上範化
[00:46:13] 蕭朝軍: 然後或者是我們就真的需要一種新的學習方式
[00:46:15] 蕭朝軍: 我有個小白的聽證的問題
[00:46:17] 曼奇: 就是RM你不能給他夠造一個
[00:46:19] 曼奇: 就是很複雜的任務
[00:46:20] 曼奇: 很多樣的環境嗎
[00:46:22] 就很難
[00:46:23] 蕭朝軍: 就是你這個多樣的環境其實就很難
[00:46:26] 蕭朝軍: 或許因為你要想
[00:46:27] 蕭朝軍: 就是最簡單的比如說我們人
[00:46:29] 蕭朝軍: 他能夠在這個世界上會獲取很多的反饋
[00:46:32] 蕭朝軍: 比如說我跟你的聊天
[00:46:33] 蕭朝軍: 在我看到你的這個表情已經很嚴肅了
[00:46:36] 蕭朝軍: 那我會認為我可能說的話已經是不好了
[00:46:39] 蕭朝軍: 那我這個其實就是這種反饋
[00:46:41] 蕭朝軍: 但是對強化學習來說我就給告訴你
[00:46:43] 蕭朝軍: 就是一這是富裔
[00:46:44] 蕭朝軍: 那見識經其實就已經很多樣了
[00:46:47] 蕭朝軍: 你我怎麼能夠把整個世界給建模起來
[00:46:50] 蕭朝軍: 給他一個人握的
[00:46:51] 蕭朝軍: 這個其實就有點像是立亞獎的那個Value方式
[00:46:54] 蕭朝軍: 就是這種價值寒附是怎麼給到模型的
[00:46:57] 蕭朝軍: 那這個其實是一個非常非常複雜的一個問題
[00:47:00] 蕭朝軍: 對於我想想一下就是因為人所在這個真實世界的環境裡
[00:47:03] 曼奇: 它其實有人是面臨很多樣的目標的
[00:47:05] 曼奇: 但是人可能在某一個時刻
[00:47:06] 曼奇: 它的注意力是被某一些目標
[00:47:08] 曼奇: 或者說某一些這種反饋給吸引住的
[00:47:11] 曼奇: 但是換了一個場景
[00:47:12] 曼奇: 換了一個時空
[00:47:12] 曼奇: 你可能又在注意別的事情
[00:47:14] 曼奇: 所以其實這種價值寒附
[00:47:15] 蕭朝軍: 或者是這種RMOLO其實是一個很難的一個事情
[00:47:18] 蕭朝軍: 然後我們世界會下一想
[00:47:20] 蕭朝軍: 就是這種RMOLO和真的會做件事情
[00:47:23] 蕭朝軍: 就是可能它是一個同的難度的這個場景
[00:47:26] 蕭朝軍: 我就有點機身彈單身機的一個
[00:47:27] 蕭朝軍: 假設我能對這個世界建模的很好
[00:47:29] 蕭朝軍: 你所認同的事情我都能給你足夠的反饋
[00:47:32] 蕭朝軍: 那這個RMOLO或者是說這個價值寒附
[00:47:35] 蕭朝軍: 它為什麼天然不是一個AGA呢
[00:47:37] 蕭朝軍: 那所以就是說我是能夠有一個AGA
[00:47:39] 蕭朝軍: 我從頭上到處另外一個AGA人感覺
[00:47:41] 蕭朝軍: 那你們自己現在就是在這個算法的去怎麼提升的
[00:47:45] 曼奇: 能力密度上
[00:47:46] 曼奇: 比如說你們接下來可能會重點做的方式是什麼
[00:47:48] 曼奇: 對其實還是回到剛才那兩個事情
[00:47:51] 蕭朝軍: 就是我們團隊吧
[00:47:53] 蕭朝軍: 其實也有在嘗試去補出
[00:47:54] 蕭朝軍: Ios給你上的一些規律
[00:47:57] 蕭朝軍: 然後我們團隊也有些同學
[00:47:59] 蕭朝軍: 然後做了一些什麼Ios當中的商變化
[00:48:02] 蕭朝軍: 這種商其實就是可以認為大模型的這種探索能力
[00:48:06] 蕭朝軍: 你覺得這個商詞商詞商詞商詞的那個商害
[00:48:08] 曼奇: 對商詞商的那個商
[00:48:09] 蕭朝軍: 其實反應到這個模型裡面是那個Token Diversity
[00:48:12] 蕭朝軍: 混亂度的變化嗎
[00:48:13] 曼奇: 對
[00:48:13] 曼奇: 或者是說你說在當前就快了一下
[00:48:17] 蕭朝軍: 然後我這個模型它能覺得通過都是彩樣
[00:48:20] 蕭朝軍: 然後成成多少種
[00:48:22] 蕭朝軍: 非常不一樣的這種回答
[00:48:24] 蕭朝軍: 就是一個非常指望的理解
[00:48:26] 蕭朝軍: 然後會發現就是Ios的過程
[00:48:28] 蕭朝軍: 其實這種商然後變化成這種Ecclacy的一個過程
[00:48:32] 蕭朝軍: 所以我們也在嘗試去構建這種機制上的理解
[00:48:36] 蕭朝軍: 然後還有一些就是這個Miance scaling
[00:48:39] 蕭朝軍: 我們也會去做自國高校的Skilling的框架
[00:48:42] 蕭朝軍: 就是我們希望說能夠使得這個模型
[00:48:46] 蕭朝軍: 在Ios的過程裡面能夠把這個算力用的足夠滿
[00:48:48] 蕭朝軍: 然後這樣的話我就能夠讓它尋得足夠快
[00:48:51] 蕭朝軍: 那未來就有可能是我從這些算力比如說
[00:48:54] 蕭朝軍: 選擇幾萬步這些Ios
[00:48:56] 蕭朝軍: 然後還有一些更多探索的這個環節
[00:48:58] 蕭朝軍: 比如說像這種立向開放魚的強調學習
[00:49:01] 蕭朝軍: 就除了這個數學代碼以外
[00:49:03] 蕭朝軍: 那像這種比如說寫篇論文
[00:49:06] 蕭朝軍: 那這個什麼樣的Reword是好等等
[00:49:08] 蕭朝軍: 那其實我們在企業在做一些這種嘗試
[00:49:11] 蕭朝軍: 不過目前來說就是後面開放魚的長時間
[00:49:13] 蕭朝軍: 其實還沒有包括業界
[00:49:16] 蕭朝軍: 也沒有一個特別懲罰的一個共識
[00:49:18] 蕭朝軍: 其實我們現在在做企業的事情
[00:49:20] 蕭朝軍: 那最後可以講最後一種就是軟硬的深度優化
[00:49:23] 曼奇: 然後一些調一體的深度優化
[00:49:25] 曼奇: 對 軟硬一些的深度優化
[00:49:26] 蕭朝軍: 其實這件事情應該說是更是計算機發展到現在的一個核心的主線
[00:49:31] 蕭朝軍: 你可以看到就是模特定率本身
[00:49:34] 蕭朝軍: 大家會說已經終止了
[00:49:36] 蕭朝軍: 那是因為大家會認為CPU已經不在這種通用的計算設備
[00:49:40] 蕭朝軍: 已經不在像這個模特定率預測了一樣
[00:49:43] 蕭朝軍: 就一直能夠指數的翻倍
[00:49:44] 蕭朝軍: 但是像GPU它會說因為大家還在支持這個模特定的發展
[00:49:49] 蕭朝軍: 現在會叫黃式定率
[00:49:51] 蕭朝軍: 其實都是在做的事情
[00:49:52] 蕭朝軍: 就是說我們怎麼能夠把這個算力用的足夠滿
[00:49:56] 蕭朝軍: 那其實是一個很重要的事情
[00:49:58] 蕭朝軍: 加上我GPU算力
[00:50:01] 蕭朝軍: 然後我只用滿了一半
[00:50:03] 蕭朝軍: 要生下一半的計算資源都消耗掉了
[00:50:05] 蕭朝軍: 那肯定是一個非常不經濟的做法
[00:50:08] 蕭朝軍: 所以軟硬一些東西就是在想是說
[00:50:10] 蕭朝軍: 我要怎麼去把這個算力用的足夠滿
[00:50:13] 蕭朝軍: 你可以看到
[00:50:14] 蕭朝軍: 其實可以看到就是大魔形
[00:50:16] 蕭朝軍: 或神經網路翻籃到現在核心勝利的那些架構
[00:50:19] 蕭朝軍: 勝利的那些專家
[00:50:20] 蕭朝軍: 那些方法都是軟硬一些方法
[00:50:23] 蕭朝軍: 像Transformer這早的提出
[00:50:25] 蕭朝軍: 其實核心的問題就是
[00:50:27] 蕭朝軍: 因為它認為 吞噬這種架構是能夠把GPU利用率達滿的
[00:50:31] 蕭朝軍: 所以它提出來這個點
[00:50:32] 蕭朝軍: 然後Transformer出來了
[00:50:33] 蕭朝軍: 然後Transformer
[00:50:35] 蕭朝軍: 能夠把GPU達滿之後代表
[00:50:37] 蕭朝軍: 那我就可以死給令了
[00:50:38] 蕭朝軍: 就把一個魔形做得很大
[00:50:39] 蕭朝軍: 之前的也什麼
[00:50:40] 蕭朝軍: CMCM等等都沒有辦法做到這件事情
[00:50:43] 蕭朝軍: 那Transformer就勝利了
[00:50:45] 蕭朝軍: 然後從這個出現了GPU11
[00:50:48] 蕭朝軍: 但是GPU11到GPU3的話
[00:50:49] 蕭朝軍: 那中間又會有很多這種工程心理的問題
[00:50:52] 蕭朝軍: 然後大家就需要這個怎麼辦
[00:50:54] 蕭朝軍: 這個大的基群
[00:50:56] 蕭朝軍: 然後這種通性給用滿時的
[00:50:57] 蕭朝軍: 這種大的基群能發給它足夠多的作用
[00:51:00] 蕭朝軍: 然後在網紅上說
[00:51:01] 蕭朝軍: 吞噬其實現在大家用的什麼Flash的吞噬等等
[00:51:04] 蕭朝軍: 它也是面向硬件去優化了這個
[00:51:07] 蕭朝軍: 它現在這個自身
[00:51:08] 蕭朝軍: 我們上期節目有聊過這個
[00:51:10] 曼奇: 所以其實軟硬斜同件是一直是
[00:51:12] 蕭朝軍: AI或者是乃至集團機行業發展一個主權率
[00:51:16] 蕭朝軍: 差需要兩個方面
[00:51:17] 蕭朝軍: 一個就是面向現代的硬件設備
[00:51:19] 蕭朝軍: 我們要去設計更好的算法
[00:51:21] 蕭朝軍: 那其實也是我們像是聊那個吸入注意力
[00:51:23] 蕭朝軍: 一個很重要的點
[00:51:25] 蕭朝軍: 如果我們為什麼會去設計
[00:51:26] 蕭朝軍: 這種分塊的注意力
[00:51:27] 蕭朝軍: 然後那其實天然就是因為GPU的這個集團特性倒置了
[00:51:31] 蕭朝軍: 然後它一個很重要的方向
[00:51:33] 蕭朝軍: 就是面向現在的這個AI設備
[00:51:35] 蕭朝軍: 我們怎麼去設計這個硬件
[00:51:37] 蕭朝軍: 你可以看到現在這種所謂的手機關的NPU
[00:51:40] 蕭朝軍: 然後以及這些這個InfoR層面的一些努力
[00:51:44] 蕭朝軍: 然後其實都是在做件事情
[00:51:45] 蕭朝軍: 包括Dubes這個V3
[00:51:47] 蕭朝軍: 它的那個基礎報告裡面
[00:51:48] 蕭朝軍: 也甚至就是在提到
[00:51:49] 蕭朝軍: 它甚至深落底層去改那些匯編語
[00:51:53] 蕭朝軍: 然後是的
[00:51:54] 蕭朝軍: 說它真的能夠把這個模型支持了足夠好
[00:51:57] 蕭朝軍: 那其實都是這種
[00:51:58] 蕭朝軍: 就是兩邊在現通的例子
[00:52:00] 蕭朝軍: 它還成了這麼一個進展
[00:52:02] 蕭朝軍: 因為你們自己其實今年10月
[00:52:04] 曼奇: 也更新的你們在這個應該算是
[00:52:06] 曼奇: 模型加果上的一個別變化
[00:52:08] 曼奇: 就是這個吸入注意力
[00:52:10] 曼奇: Info LLM的第二代
[00:52:12] 曼奇: 你把今天是個開源的
[00:52:13] 曼奇: 它這個相比你們之前
[00:52:15] 曼奇: 那第一代的更新和提升是什麼
[00:52:17] 曼奇: 其實我們開源是在6月份了
[00:52:18] 蕭朝軍: 就是我們6月份第一版
[00:52:21] 蕭朝軍: 那個Mini3分4的時候就已經用上了
[00:52:23] 蕭朝軍: 然後是在10月份把這個基礎報告和
[00:52:26] 蕭朝軍: 一些就用述量數據
[00:52:27] 蕭朝軍: 被這個集中模型等等傳統開放出去
[00:52:30] 蕭朝軍: 大家就使用
[00:52:31] 蕭朝軍: 它像比於第一版的一個核心的一個改變
[00:52:33] 蕭朝軍: 其實也是受到了DUC的一個啟發
[00:52:36] 蕭朝軍: 就是要做到這個語言深細數
[00:52:39] 蕭朝軍: 就是我在訓練階段它就是做過細數的
[00:52:41] 蕭朝軍: 這樣的話就是像比於這個
[00:52:44] 蕭朝軍: 我們第一版其實是在推理階段去做細數
[00:52:46] 蕭朝軍: 還有一個有點
[00:52:47] 蕭朝軍: 第一個是我訓練的水源能加速了
[00:52:49] 蕭朝軍: 第二個就有點就是如果我細數度
[00:52:51] 蕭朝軍: 能夠降低它什麼
[00:52:52] 蕭朝軍: 就是之前這種推理階段的細數
[00:52:55] 蕭朝軍: 可能50%左右的這個細數程度
[00:52:57] 蕭朝軍: 然後同時當時的推理階段的細數值
[00:53:00] 蕭朝軍: 那是我們叫Prefitting
[00:53:01] 蕭朝軍: 就是長文本輸入能夠加速
[00:53:03] 蕭朝軍: 但對於長文輸輸取做得不太好
[00:53:06] 蕭朝軍: 那在這樣一個
[00:53:07] 蕭朝軍: 第二代的版本裡面
[00:53:08] 蕭朝軍: 我們就把這個
[00:53:10] 蕭朝軍: 做到訓練裡細數度能夠降低
[00:53:12] 蕭朝軍: 然後現在基本上128K
[00:53:13] 蕭朝軍: 就需要104到6K的頭肯
[00:53:15] 蕭朝軍: 也就剝到5%的一個細數度
[00:53:18] 蕭朝軍: 然後這個同時我們這個Sand
[00:53:20] 蕭朝軍: 也能支持是一個長輸出
[00:53:22] 蕭朝軍: 這樣的話我們去做這種深度思考
[00:53:25] 蕭朝軍: 然後包括做A-JAMT這些領域的話
[00:53:27] 蕭朝軍: 這個方法都是比較利用的
[00:53:29] 蕭朝軍: 你剛才也講到就是這個DBC
[00:53:30] 曼奇: 可能出的那個NSA
[00:53:32] 曼奇: 它是一個快狀的
[00:53:33] 曼奇: 就是去選這個頭肯的這麼一個方式
[00:53:36] 曼奇: 然後我看他們9月更新的DBC V3.2
[00:53:39] 蕭朝軍: 它的裡面的注意力又改了
[00:53:41] 曼奇: 它們又新去了一個名字
[00:53:42] 曼奇: 叫DBC Sparce Tension DSA
[00:53:44] 曼奇: 然後相比NSA的話
[00:53:46] 曼奇: 我理解它的那個快狀選取的方式
[00:53:48] 曼奇: 好像就變得更細了
[00:53:49] 曼奇: 它就怎麼去解決計算上的問題了
[00:53:52] 蕭朝軍: 但這個其實這個計算上的問題
[00:53:55] 蕭朝軍: 這個就可能太幾乎細講了
[00:53:56] 蕭朝軍: 可行可以解釋的一個點就是
[00:53:58] 蕭朝軍: DBC的那個架構
[00:54:00] 蕭朝軍: 它已經不需要解決這個訪存問題了
[00:54:03] 蕭朝軍: 它不需要解決這種存儲帶寬向的問題了
[00:54:06] 蕭朝軍: 然後它已經能夠做到
[00:54:07] 蕭朝軍: 就是準備計算給棒的做
[00:54:09] 蕭朝軍: 所以說它能夠去忽略這個問題
[00:54:11] 蕭朝軍: 但是對大部分的這個其他的模型而言
[00:54:14] 蕭朝軍: 或大部分的其他部署長進來的問題
[00:54:17] 蕭朝軍: 就存在了加上用它那個DSA的架構
[00:54:19] 蕭朝軍: 可以強調一個點
[00:54:20] 蕭朝軍: 其實它那個DSA相比於NSA
[00:54:22] 蕭朝軍: 也是我們在NSA V2裡面
[00:54:25] 蕭朝軍: 就批判了一個點
[00:54:26] 蕭朝軍: NSA暫時它做出來那個架構
[00:54:28] 蕭朝軍: 它有兩個缺點
[00:54:30] 蕭朝軍: 第一個缺點就是它對短文本身上不友好
[00:54:32] 蕭朝軍: 因為它拆成了三個注意力組件
[00:54:35] 蕭朝軍: 這樣的話對於短文本而言
[00:54:37] 蕭朝軍: 它三個注意力組件都得算
[00:54:39] 蕭朝軍: 就計算成本節成三了
[00:54:41] 蕭朝軍: 然後第二個是說它對於後尋臉不友好
[00:54:45] 蕭朝軍: 就是說對大部分的模型而言
[00:54:46] 蕭朝軍: 我們現在的尋臉犯是就是長文本的預尋臉
[00:54:50] 蕭朝軍: 然後長文本就是做到後尋臉的階段來做
[00:54:52] 蕭朝軍: 不會從頭開始做長文本的預尋臉
[00:54:55] 蕭朝軍: NSA就沒辦法做短文本的預尋臉
[00:54:58] 蕭朝軍: 長文本後尋臉這個犯事
[00:55:00] 蕭朝軍: 所以它DSA就把這兩個點給拋棄了
[00:55:03] 蕭朝軍: 然後設計了一套
[00:55:04] 蕭朝軍: 這個單身體架構起來是非常類似的
[00:55:07] 蕭朝軍: 直播時面向這種後尋臉的犯事
[00:55:09] 蕭朝軍: 然後去優化了一些設計
[00:55:11] 蕭朝軍: 我們印象明明要當時也是覺得
[00:55:13] 蕭朝軍: 它這兩個點不好
[00:55:14] 蕭朝軍: 然後我們就提出了一個新的方案
[00:55:17] 蕭朝軍: 然後因為剛才講的是吸塑注意力的改進
[00:55:19] 曼奇: 前段時間我和高材代特的核心
[00:55:22] 曼奇: 做了洋松林了過現行主義的一些改進
[00:55:25] 曼奇: 因為像那個Kun San Laks
[00:55:26] 曼奇: 還有Kimeyu一個KDA
[00:55:28] 曼奇: 因為這和DSA很像
[00:55:30] 曼奇: 然後他們都是做一些現行主義的改進
[00:55:33] 曼奇: 然後混了這個Delta and Latin和FuO transition
[00:55:35] 曼奇: 然後當時洋松林也提到
[00:55:36] 曼奇: 他覺得下一個方向
[00:55:37] 曼奇: 就大家在探索的是說
[00:55:39] 曼奇: 我直接用現行主義力去混吸塑注意力
[00:55:42] 曼奇: 你們是不是也有這方面的一些探索
[00:55:45] 曼奇: 對
[00:55:46] 蕭朝軍: 因為這件事情大家都能關注到我
[00:55:48] 蕭朝軍: 然後其實你可以這麼未必
[00:55:51] 蕭朝軍: 現行主義力它對應的方式不是全注意力
[00:55:54] 蕭朝軍: 而是Sleiding Window
[00:55:55] 蕭朝軍: 就是因為Sleiding Window天然的長速向的存幅
[00:55:59] 蕭朝軍: 然後能夠去做這種長文本的處理
[00:56:02] 蕭朝軍: 它的對應像是這個
[00:56:03] 蕭朝軍: 然後Spares的對應的向才是FuO transition
[00:56:06] 蕭朝軍: 就是全的寵秘的注意力
[00:56:08] 蕭朝軍: 所以其實這兩個方向
[00:56:10] 蕭朝軍: 在未來應該是要合避在一起的
[00:56:11] 蕭朝軍: 因為現在你其實可以看到有很多的工作像
[00:56:14] 蕭朝軍: KDA也好
[00:56:15] 蕭朝軍: Mini Max也好
[00:56:16] 蕭朝軍: 他們其實都在嘗試的做
[00:56:18] 蕭朝軍: 就是現行主義力混合寵秘主義力
[00:56:21] 蕭朝軍: 然後其實可以看到
[00:56:22] 蕭朝軍: 它能夠在很多場景下
[00:56:24] 蕭朝軍: 能夠保持對長文本的高效處理和效果
[00:56:26] 蕭朝軍: 然後那未來的話
[00:56:27] 蕭朝軍: 其實這個現行的似乎應該是一個自然的想法
[00:56:32] 蕭朝軍: 但其實我覺得現在一個
[00:56:33] 蕭朝軍: 繼續解決一個問題
[00:56:34] 蕭朝軍: 現在大家關注所謂的長文本
[00:56:36] 蕭朝軍: 其實還是關注在之前那種長輸物的問題上
[00:56:40] 蕭朝軍: 然後其實我覺得現在的長文本
[00:56:42] 蕭朝軍: 就像我剛才提到的
[00:56:43] 蕭朝軍: 它的發展的路徑一定是面向未來的生思考
[00:56:46] 蕭朝軍: 和這種A-jump的場景
[00:56:48] 蕭朝軍: 那面向生思考和A-jump的場景
[00:56:50] 蕭朝軍: 它更應該處理的內容
[00:56:52] 蕭朝軍: 就是它會有很長的這種輸出
[00:56:55] 蕭朝軍: 那所以面向這種長輸出來說
[00:56:57] 蕭朝軍: 其實現在很多這種現行主義力
[00:56:59] 蕭朝軍: 它的現實力遠不夠的
[00:57:01] 蕭朝軍: 所以你其實可以發現
[00:57:02] 蕭朝軍: Mini Max為什麼從這一次
[00:57:04] 蕭朝軍: 這一次M1做這個
[00:57:07] 蕭朝軍: Lighten的展示
[00:57:08] 曼奇: 對 現行主義力混合全選
[00:57:10] 曼奇: 對 然後到現在用全副的展示
[00:57:12] 蕭朝軍: 我覺得一個很重要的原因
[00:57:13] 蕭朝軍: 就是M2它去想主大A-jump
[00:57:16] 蕭朝軍: 但是可能即使前一過了
[00:57:17] 蕭朝軍: 會發現今天事情三部號的還是有點性能上
[00:57:20] 蕭朝軍: 的損失所以它最終選擇了
[00:57:22] 蕭朝軍: Fort 成績上這少路徑
[00:57:23] 蕭朝軍: 所以我認為其實這也是
[00:57:25] 蕭朝軍: 大家現在關注長文本
[00:57:26] 蕭朝軍: 有點偏頗導致的一個很重要的原因
[00:57:30] 蕭朝軍: 你年初的時候不就是說
[00:57:31] 曼奇: 應該更多關注長輸出嗎
[00:57:32] 曼奇: 這事兒到現在也不是共識什麼
[00:57:34] 曼奇: 我覺得是
[00:57:36] 蕭朝軍: 因為就是今天事情大家可能
[00:57:38] 蕭朝軍: 還是會認為長輸入很重要
[00:57:41] 蕭朝軍: 就是給你位一本輸入
[00:57:42] 蕭朝軍: 然後問你的QA很重要
[00:57:43] 蕭朝軍: 長輸出其實因為COT
[00:57:45] 曼奇: 所以變得更重要了
[00:57:46] 曼奇: 對 以及包括Planning
[00:57:47] 蕭朝軍: 就是我得比如說像A-jump
[00:57:49] 蕭朝軍: 我得去規劃好
[00:57:50] 蕭朝軍: 我1235個步驟
[00:57:52] 蕭朝軍: 然後你最不能做到一件事情
[00:57:54] 蕭朝軍: 我做完第二個步驟
[00:57:55] 蕭朝軍: 我把第一個步驟做什麼事情給忘
[00:57:57] 蕭朝軍: 然後反過頭再再做一遍
[00:57:58] 蕭朝軍: 所以其實就是這種
[00:57:59] 蕭朝軍: 深思考
[00:58:00] 蕭朝軍: 然後多部的Planning就有難力
[00:58:03] 蕭朝軍: 其實是對於現在的長文本
[00:58:04] 蕭朝軍: 加多一個新的挑戰
[00:58:06] 曼奇: 然後你說現行主義
[00:58:06] 曼奇: 你在這件事情經常表現的不是特別好
[00:58:08] 曼奇: 還是跟現行主義本身
[00:58:10] 曼奇: 還要更容易一忘
[00:58:11] 曼奇: 之前的一些內容有關
[00:58:12] 曼奇: 對
[00:58:13] 蕭朝軍: 在長輸出上
[00:58:14] 曼奇: 它即使混了
[00:58:14] 曼奇: Fort 成績也不能決這個問題嗎
[00:58:16] 曼奇: 就得看你很多好了
[00:58:18] 蕭朝軍: 那之前大家都很激進的
[00:58:19] 蕭朝軍: 像Mini Mad
[00:58:20] 蕭朝軍: 它是一比七的混
[00:58:21] 蕭朝軍: 只有一層複合
[00:58:22] 蕭朝軍: 氣存的現行
[00:58:23] 蕭朝軍: 但是我估計到後面
[00:58:25] 蕭朝軍: 他們有點沒公主
[00:58:26] 蕭朝軍: 然後導致他們自封選擇的複合團選
[00:58:29] 蕭朝軍: 我理解這個方向是有未來的
[00:58:31] 蕭朝軍: 就是它應該是能夠解決掉的
[00:58:33] 蕭朝軍: 但是可能還需要一點時間
[00:58:35] 蕭朝軍: 那你們在自己的下一代的
[00:58:37] 曼奇: 比如說期間模型上
[00:58:38] 曼奇: 你們會採取什麼樣的假構
[00:58:40] 曼奇: 你可以預告一下
[00:58:41] 蕭朝軍: 還是要深刻的
[00:58:43] 對 可能有一個肯定的點
[00:58:44] 蕭朝軍: 是一個全吸塑假構
[00:58:46] 蕭朝軍: 就是阿富汗分野吸塑
[00:58:47] 蕭朝軍: 騰涉野吸塑
[00:58:48] 蕭朝軍: 但距離而騰涉
[00:58:49] 蕭朝軍: 會不會再進一步的
[00:58:50] 蕭朝軍: 那可能還得我們再
[00:58:51] 蕭朝軍: 更多的戲演上去嘗試
[00:58:52] 蕭朝軍: 對
[00:58:53] 曼奇: 接下來就是想研擇
[00:58:54] 曼奇: 討論一下這個密度定律
[00:58:56] 曼奇: 就有了這個定量的描述之後
[00:58:57] 曼奇: 對研究界還有業界
[00:58:59] 曼奇: 有些什麼影響
[00:59:00] 曼奇: 就比如說有觀察到
[00:59:01] 曼奇: 比如說你們自己的行為
[00:59:02] 曼奇: 包括你
[00:59:03] 曼奇: 就其他這個行為裡的
[00:59:04] 曼奇: 其他角色的行為有什麼影響嗎
[00:59:06] 曼奇: 因為其實你們發出來
[00:59:07] 曼奇: 應該有段時間了
[00:59:08] 曼奇: 對吧
[00:59:08] 曼奇: 就是在這個雜誌看盪之前
[00:59:10] 曼奇: 其實那個首先
[00:59:11] 劉志遠: 我覺得對我們自己內部
[00:59:13] 劉志遠: 就是我覺得是一個
[00:59:14] 劉志遠: 統一思想的
[00:59:15] 劉志遠: 一個非常重要的一個點
[00:59:17] 劉志遠: 所以現在大家就是
[00:59:18] 劉志遠: 不管是做模型加構的創新
[00:59:20] 劉志遠: 數據治理的創新
[00:59:22] 劉志遠: 還是說這個奉動的建設
[00:59:24] 劉志遠: 然後以及軟硬一體 血統
[00:59:26] 劉志遠: 其實現在有了一個統一的
[00:59:28] 劉志遠: 一個評價的一個標準
[00:59:30] 劉志遠: 或者是一個指標
[00:59:31] 劉志遠: 然後這樣的話
[00:59:32] 劉志遠: 其實大家就可以更容易的
[00:59:34] 劉志遠: 去行程公司
[00:59:35] 劉志遠: 然後來共同推進
[00:59:36] 劉志遠: 因為其實這個
[00:59:38] 劉志遠: 我覺得可能它跟說
[00:59:39] 劉志遠: 我把模型簡單的去的越大
[00:59:42] 劉志遠: 然後讓它去能力更強
[00:59:44] 劉志遠: 這個我覺得還是
[00:59:45] 劉志遠: 一個不太一樣的一個取向
[00:59:47] 劉志遠: 就這個我覺得是
[00:59:48] 劉志遠: 我們內部的一個非常重要的一個體系
[00:59:50] 劉志遠: 也就是說我們自己內部
[00:59:52] 劉志遠: 其實會經常喜歡類比芯片
[00:59:55] 劉志遠: 比如說現在去構造一個大模型
[00:59:58] 劉志遠: 我們所形成的這個包含
[01:00:00] 劉志遠: 加工設計數據治理 學習方法
[01:00:04] 劉志遠: 然後以及軟硬一體的血統
[01:00:06] 劉志遠: 整個這個體系
[01:00:07] 劉志遠: 我們喜歡把它稱為叫大模型的光刻機
[01:00:11] 劉志遠: 它本身其實是一個
[01:00:12] 劉志遠: 很複雜的一個體系
[01:00:13] 劉志遠: 那其實是要有一個共同的一個目標
[01:00:16] 劉志遠: 然後才能夠把
[01:00:17] 劉志遠: 比如說我們多達上百人的一個團隊
[01:00:20] 劉志遠: 然後能夠形成一個非常好的一個血統
[01:00:23] 劉志遠: 這個其實我覺得
[01:00:24] 劉志遠: 對內部的一個價值
[01:00:25] 劉志遠: 對外部的話
[01:00:26] 劉志遠: 我們所能夠看到的
[01:00:28] 劉志遠: 就是當然其實今年初開始
[01:00:31] 劉志遠: 就是也是UEDC V3的這個影響
[01:00:34] 劉志遠: 其實你會發現像
[01:00:35] 劉志遠: SIME Ultimate
[01:00:36] 劉志遠: 然後包括Andsopic
[01:00:38] 劉志遠: 包括後來就是Mario Maker
[01:00:40] 劉志遠: 它的那個互聯網的報告
[01:00:42] 劉志遠: 其實都特別的強調
[01:00:43] 劉志遠: 就是AI的成本問題
[01:00:45] 劉志遠: 所以其實我會覺得
[01:00:47] 劉志遠: 就是這個本身是說明實事的
[01:00:49] 劉志遠: 也就是說從今年開始
[01:00:50] 劉志遠: 大家開始發現
[01:00:52] 劉志遠: 就是這個效率或者是這個
[01:00:54] 劉志遠: 能效變得非常的關鍵
[01:00:56] 劉志遠: 原因是在於隨著
[01:00:59] 劉志遠: 我們要把AI真正的廣泛
[01:01:01] 劉志遠: 的應用的各行各業
[01:01:02] 劉志遠: 那麼這個事情會變得非常關鍵
[01:01:04] 劉志遠: 同時的話我們會看到
[01:01:06] 劉志遠: 就是經常原因
[01:01:08] 劉志遠: 我們密度法則的這個報告
[01:01:10] 劉志遠: 然後相關的工作
[01:01:11] 劉志遠: 基本上是發生在這個
[01:01:13] 劉志遠: 居身智能等等的
[01:01:15] 劉志遠: 這些專家的這個報告裡面
[01:01:18] 劉志遠: 就基本上你會可以看到
[01:01:20] 劉志遠: 就是在這個居身
[01:01:22] 劉志遠: 因為本身它需要構建
[01:01:24] 劉志遠: 就是這個相當於是這個
[01:01:26] 劉志遠: 機器人的大腦小腦等等的
[01:01:28] 劉志遠: 其實他們會對
[01:01:29] 劉志遠: 就是這種端竊的這種智能的
[01:01:32] 劉志遠: 這個構建的規律
[01:01:33] 劉志遠: 可能會有更加敏感的需求
[01:01:35] 劉志遠: 就是因為它是對延遲
[01:01:36] 曼奇: 這些要求表格
[01:01:37] 曼奇: 對的
[01:01:38] 曼奇: 不可能就是
[01:01:40] 曼奇: 全部依賴雲端的這種模式
[01:01:41] 曼奇: 對的
[01:01:42] 曼奇: 因為當時其實就是
[01:01:43] 劉志遠: 這篇是發的那一只
[01:01:45] 劉志遠: 馬上引擎人士上
[01:01:46] 劉志遠: 在前面其實有一篇是
[01:01:48] 劉志遠: 發的那一只
[01:01:48] 劉志遠: Cominiqueism上
[01:01:49] 劉志遠: 就是MinicPM-Way
[01:01:51] 劉志遠: 那篇文章
[01:01:52] 劉志遠: 那篇文章其實裡面有一個
[01:01:54] 劉志遠: 非常經典的圖
[01:01:55] 劉志遠: 就是說密度法則和模二定律
[01:01:57] 劉志遠: 其實是皆是了
[01:01:58] 劉志遠: 就是我們在中端上
[01:02:00] 劉志遠: 其實可以裝得下一個
[01:02:02] 劉志遠: 在歷史上只能夠在服務器上
[01:02:04] 劉志遠: 在雲上才能裝得下跑得動的一個模型
[01:02:07] 劉志遠: 就那個圖就經常被很多
[01:02:09] 劉志遠: 這個做巨神智能的專家和團隊
[01:02:12] 劉志遠: 去使用
[01:02:13] 劉志遠: 就大家會認為
[01:02:14] 劉志遠: 這個其實是讓他們去做巨神大腦
[01:02:17] 劉志遠: 有了這個非常好的
[01:02:18] 劉志遠: 這麼一個法律的依據
[01:02:20] 劉志遠: 這個其實在幫他們預測
[01:02:21] 曼奇: 就是說
[01:02:21] 曼奇: 至少就是在這個
[01:02:23] 曼奇: 比如說算力
[01:02:23] 曼奇: 或者是這個能力密度上
[01:02:24] 曼奇: 它能跑在這個巨神機器人端策
[01:02:27] 曼奇: 的這種模型什麼時候到來
[01:02:29] 曼奇: 對
[01:02:29] 曼奇: 所以我們知道的
[01:02:30] 劉志遠: 就是別人轉給我的
[01:02:32] 劉志遠: 就是可能很多高機器人的專家
[01:02:34] 劉志遠: 可能經常特別喜歡去
[01:02:36] 劉志遠: 用我們的關於Dancing Law的
[01:02:39] 劉志遠: 相關的一些成功
[01:02:40] 劉志遠: 按照現在的就是你們
[01:02:42] 曼奇: 針對之前這兩年多
[01:02:44] 曼奇: 這些開源模型做的這個預測
[01:02:46] 曼奇: 就是巨神上要能用上
[01:02:47] 曼奇: 端策的模型
[01:02:50] 曼奇: 現在也很強的
[01:02:50] 曼奇: 能支持巨神大腦的
[01:02:52] 曼奇: 會在什麼時候出現
[01:02:53] 曼奇: 我們其實就是接下來這幾年
[01:02:56] 劉志遠: 就是從今年開始
[01:02:57] 劉志遠: 一直到2030年
[01:02:59] 劉志遠: 其實全球的依陷的新片團隊
[01:03:03] 劉志遠: 包括因為大家包括華為
[01:03:04] 劉志遠: 其實他們都會有自己的
[01:03:06] 劉志遠: 基於就是自己的製程
[01:03:08] 劉志遠: 一個路線圖
[01:03:09] 劉志遠: 也就是說它每年發布的
[01:03:11] 劉志遠: 那個新片大概是什麼樣的算力
[01:03:13] 劉志遠: 放存等等的這種規格
[01:03:15] 劉志遠: 其實都是有的
[01:03:16] 劉志遠: 我們其實根據那些
[01:03:17] 劉志遠: 相應發布的這個規格
[01:03:19] 劉志遠: 我們其實可以算出來
[01:03:21] 劉志遠: 就是在這些主流新片上
[01:03:23] 劉志遠: 就是端策的新片上
[01:03:25] 劉志遠: 然後可以去加載的這個模型的尺寸
[01:03:28] 劉志遠: 然後以及它的機會殘殊的這個規模
[01:03:31] 劉志遠: 大概的這個程度是多少
[01:03:33] 劉志遠: 所以我們大概應該是有一個孤算
[01:03:35] 劉志遠: 就是到2030年
[01:03:37] 劉志遠: 實際上是可以在端策上
[01:03:39] 劉志遠: 然後能夠部署一個超過60筆
[01:03:41] 劉志遠: 也就是600一殘殊的
[01:03:43] 劉志遠: 整個一個大的模型
[01:03:44] 劉志遠: 然後它的機會殘殊
[01:03:46] 劉志遠: 大概可以達到8筆以上
[01:03:48] 劉志遠: 大概是這麼一個水平
[01:03:50] 劉志遠: 當然就是說這個本身是現性的預測
[01:03:53] 劉志遠: 就是你也不妨礙說
[01:03:54] 劉志遠: 接下來這幾年的時間
[01:03:55] 劉志遠: 可能會出現一些飛線性的
[01:03:57] 劉志遠: 這麼一些突破
[01:03:58] 劉志遠: 就是這個單純從現性上來講
[01:04:00] 劉志遠: 大概達到這個水平
[01:04:01] 劉志遠: 所以我們同時如果跌加這個密度法則
[01:04:05] 劉志遠: 我們會認為就是到這個
[01:04:07] 劉志遠: 接下來的這五年的時間
[01:04:09] 劉志遠: 我們一定是可以把一個GT4
[01:04:11] 劉志遠: 到GT5水平模型的能力
[01:04:14] 劉志遠: 然後可以放在專程上
[01:04:16] 劉志遠: 然後就能夠形成
[01:04:18] 劉志遠: 就是每個用戶自己專屬的
[01:04:20] 劉志遠: 這麼一個能力
[01:04:21] 劉志遠: 大概是這麼一個大概孤算
[01:04:23] 劉志遠: 我覺得這個對手機
[01:04:24] 曼奇: 這類相對程序的移動設備
[01:04:26] 曼奇: 應該還是挺有用的
[01:04:27] 曼奇: 對 是
[01:04:28] 劉志遠: 所以就是手機機器人車
[01:04:31] 劉志遠: 這種機器和包裝PC
[01:04:34] 劉志遠: 就是這幾大中端
[01:04:36] 劉志遠: 現在來看它的順序
[01:04:37] 劉志遠: 應該是車現在是今年
[01:04:40] 劉志遠: 就是這個鋪開的非常快
[01:04:42] 劉志遠: 因為車它對功耗
[01:04:43] 曼奇: 這些要求更低一點
[01:04:44] 曼奇: 更低而且它空間更大
[01:04:46] 劉志遠: 那麼再次一級就是PC
[01:04:48] 劉志遠: 那麼再次一級就是手機
[01:04:50] 劉志遠: 那麼本身機器人又令到別論
[01:04:53] 劉志遠: 因為本身它現在也還處在一個快速發展
[01:04:57] 劉志遠: 還沒有固定下來的這麼一個狀態
[01:04:59] 劉志遠: 但是相對來講
[01:05:00] 劉志遠: 我們會覺得機器人的這個空間
[01:05:02] 劉志遠: 應該也還可以
[01:05:03] 劉志遠: 只是說它自己本身已經有非常大的功耗的問題
[01:05:07] 劉志遠: 所以我們會認為就是在內頂上
[01:05:09] 劉志遠: 去加點一個特別高算力的大模型
[01:05:13] 劉志遠: 可能也還是需要一個
[01:05:15] 劉志遠: 再進一步的去它所的這麼一個方向
[01:05:17] 劉志遠: 而且我覺得對手機電腦還有車來說
[01:05:20] 曼奇: 就是你在上面放一個GBT-C
[01:05:21] 曼奇: GBT-5水平的模型
[01:05:23] 曼奇: 我覺得是比較容易想像
[01:05:24] 曼奇: 它能做什麼 帶來什麼價值的
[01:05:26] 曼奇: 而去升智能
[01:05:27] 曼奇: 可能現在它有一個別的難點
[01:05:28] 曼奇: 就是說你一個什麼樣水平的模型
[01:05:31] 曼奇: 在那個上面可以 work
[01:05:32] 曼奇: 因為其實大家沒有完全研究出來說
[01:05:34] 曼奇: 這個巨生模型到底是怎麼做嗎
[01:05:36] 曼奇: 其實車你可以看成是一個
[01:05:39] 劉志遠: 各樣成熟的機器人
[01:05:40] 劉志遠: 它的相對的action比較簡單
[01:05:42] 劉志遠: 但是就是說即使在車上
[01:05:44] 劉志遠: 你比如說自動駕駛這個小腦的模型
[01:05:47] 劉志遠: 和智能做倉這個大腦的模型
[01:05:49] 劉志遠: 現在來講其實還是相對獨立的去發展的
[01:05:52] 劉志遠: 其實未來機器人
[01:05:54] 劉志遠: 它一旦它的那個小腦的那個部分
[01:05:56] 劉志遠: 相對穩定之後 然後再有大腦
[01:05:58] 劉志遠: 那其實接下來還有一個命題
[01:06:00] 劉志遠: 就是大小腦如何系統
[01:06:02] 劉志遠: 其實就是在這些方面
[01:06:04] 劉志遠: 我理解就是這個機器人這個方向
[01:06:06] 劉志遠: 它連小腦可能都還沒有穩定下來
[01:06:09] 劉志遠: 所以就還不用還談不上
[01:06:12] 劉志遠: 就是大小腦如何系統了
[01:06:14] 曼奇: 不我覺得VIA算是這種嘗試的
[01:06:16] 曼奇: 這大家設想的一個系統下的方式
[01:06:18] 曼奇: 對 所以我覺得現在還處在戰國時代
[01:06:21] 劉志遠: 就是向大家擺放期望
[01:06:23] 劉志遠: 那對於我們來說
[01:06:24] 曼奇: 其實你們很多應用也是在這些端策上 是嗎
[01:06:27] 曼奇: 我們所以就是按剛才我們所說的順序
[01:06:30] 劉志遠: 就基本上現在今年來看
[01:06:31] 劉志遠: 就是在車上的進展特別大
[01:06:33] 劉志遠: 在手機上因為它相對算力還比較數線
[01:06:37] 劉志遠: 所以你看就是我們其實也試過
[01:06:39] 劉志遠: 就是Apple Intelligence
[01:06:41] 劉志遠: 它的整體效果其實還比較數線一些
[01:06:44] 劉志遠: 但是我們根據我們剛才的估算
[01:06:46] 劉志遠: 應該會在未來的兩到三年
[01:06:48] 劉志遠: 就會產生一個非常大的一個月前
[01:06:50] 劉志遠: 可以講講你們和車起的一些合作嗎
[01:06:52] 曼奇: 就或者說在氣氏這個場景上
[01:06:54] 曼奇: 有一些落地有
[01:06:55] 曼奇: 就有落地的話是這樣子
[01:06:56] 曼奇: 就是我們今年應該最早的落地的是
[01:06:59] 劉志遠: 查馬子達的一款車型
[01:07:01] 劉志遠: 然後後面就是吉利
[01:07:03] 劉志遠: 這個有一款車型
[01:07:05] 劉志遠: 然後基本上在今年就已經量產
[01:07:08] 劉志遠: 它的我覺得就算速度還比較快
[01:07:11] 劉志遠: 因為我們是在今年上半年就入場
[01:07:14] 劉志遠: 然後開始這個相關的合作
[01:07:16] 劉志遠: 但是在今年下半年
[01:07:17] 劉志遠: 就能完成量產的這麼一個動作
[01:07:20] 劉志遠: 基本上這個車這個行業
[01:07:22] 劉志遠: 本身還是非常罕見的
[01:07:24] 劉志遠: 我們前面其實有過一個推送
[01:07:27] 劉志遠: 其實是介紹這個方面的最新的進展
[01:07:30] 劉志遠: 那麼明年的話
[01:07:31] 劉志遠: 我們陸續應該會有超過6款以上的
[01:07:34] 劉志遠: 不同的車起的車型
[01:07:37] 劉志遠: 然後會加在我們的這個模型
[01:07:39] 劉志遠: 基本上我們在一線的感受
[01:07:42] 劉志遠: 就是由於我們特別強調密度法則
[01:07:45] 劉志遠: 那你就可以想像這個在車這個領域
[01:07:48] 劉志遠: 只能做倉
[01:07:49] 劉志遠: 它本身每一款車型
[01:07:51] 劉志遠: 它都是會約定好
[01:07:52] 劉志遠: 它用哪一款芯片
[01:07:54] 劉志遠: 那麼這款芯片
[01:07:56] 劉志遠: 同時在約定好它的響應時間
[01:07:59] 劉志遠: 它的功耗之後
[01:08:00] 劉志遠: 其實你在這個芯片上
[01:08:02] 劉志遠: 能夠加在的模型的那個彩鼠規模
[01:08:05] 劉志遠: 基本上是固定下來
[01:08:06] 劉志遠: 那麼密度法則讓我們團隊
[01:08:09] 劉志遠: 能夠固定一個密度更高的模型
[01:08:11] 劉志遠: 基本上就是在這些車這個方面的
[01:08:14] 劉志遠: 相關的這個競爭
[01:08:16] 劉志遠: 就基本上是處在一個非常領先的水平
[01:08:18] 劉志遠: 就這個是我們今年就是在車這個方向
[01:08:22] 劉志遠: 尤其是只能做倉上
[01:08:23] 劉志遠: 然後取得非常快的這個推進的
[01:08:25] 劉志遠: 一個比較底層的一個落
[01:08:27] 劉志遠: 像面臂的模型在車上找出做什麼
[01:08:29] 曼奇: 因為比如說大圓模型
[01:08:30] 曼奇: 我覺得可能標好理解
[01:08:31] 曼奇: 有些運動交互省的
[01:08:32] 曼奇: 然後你們多摩太
[01:08:33] 曼奇: 其實你們多摩太的那個開源的那個
[01:08:35] 曼奇: 版本在給它不上的形式
[01:08:36] 曼奇: 它下載都挺多的
[01:08:38] 多摩太用在車上可以做什麼
[01:08:39] 曼奇: 主要是一些這個
[01:08:41] 劉志遠: 現在還處在一個智能做倉的一個
[01:08:44] 劉志遠: 初期的階段
[01:08:44] 劉志遠: 主要是用這個多摩太的模型
[01:08:46] 劉志遠: 去做一些車外和車內的
[01:08:48] 劉志遠: 這個相關的一些環境的感知
[01:08:51] 劉志遠: 然後同時通過自然圓的方式
[01:08:53] 劉志遠: 跟這個車內的這些不同的位置的人
[01:08:56] 劉志遠: 然後來進行這麼一個交互
[01:08:58] 劉志遠: 來滿足他們的這個相關的一些需求
[01:09:01] 劉志遠: 並且做相應的這個提醒
[01:09:03] 劉志遠: 這個其實現在不同的車廠
[01:09:05] 劉志遠: 它對於這個智能做倉的這個功能的定義
[01:09:08] 劉志遠: 其實有非常大的這個不同
[01:09:10] 劉志遠: 不同層次的這個車
[01:09:13] 劉志遠: 它其實也會有相應的不同的這個設計
[01:09:16] 劉志遠: 比如說一些相對比較複雜的智能做倉的需求
[01:09:19] 劉志遠: 可能它的這個功能點
[01:09:20] 劉志遠: 得能超過100多個
[01:09:22] 劉志遠: 就是相當於說你可以在各個方面
[01:09:24] 劉志遠: 然後可以跟這個智能做倉的這個應用
[01:09:27] 劉志遠: 進行一個非常好的這麼一個交互
[01:09:29] 劉志遠: 總體的目標是要讓這個做倉
[01:09:32] 劉志遠: 就是你做到這個車裡面
[01:09:34] 劉志遠: 你就能夠體驗到一個全智能化的這麼一個環境
[01:09:38] 劉志遠: 這個其實是它的這個目標
[01:09:40] 劉志遠: 但是就是目前來看智能做倉
[01:09:42] 劉志遠: 還沒有達成一個標準化的所有的車
[01:09:45] 劉志遠: 然後都有一個共識的這麼一個發展
[01:09:49] 劉志遠: 還處在一個初步的這麼一個階段
[01:09:51] 劉志遠: 就是我想知道比說像現在做大模型研發的公司
[01:09:55] 曼奇: 如果去把這個模型放到車上
[01:09:57] 曼奇: 就像你剛才說它可能以作倉
[01:09:59] 曼奇: 以後100多個功能點
[01:10:00] 曼奇: 這個模型和功能點之間
[01:10:02] 曼奇: 就是你要去試配去結合
[01:10:04] 曼奇: 主要是一個微調的過程
[01:10:06] 劉志遠: 是你們來做嗎還是這個去
[01:10:08] 曼奇: 現在來講是我們來做
[01:10:10] 劉志遠: 這個微調大概要比說要多少人調多久
[01:10:13] 曼奇: 就是因為我們本身會特別地強調這個算的標準化
[01:10:17] 劉志遠: 就譬如我們的模型
[01:10:18] 劉志遠: 然後我們其實會內部會有一個非常
[01:10:21] 劉志遠: 這個標準化的這麼一個SFT的這麼一個工具鏈
[01:10:26] 劉志遠: 然後以及就是如何去做
[01:10:27] 劉志遠: 相應的這個數據合成的這麼一個規範
[01:10:30] 劉志遠: 所以就差不多
[01:10:31] 劉志遠: 反正是相對是比較高效的這麼一個模式
[01:10:34] 劉志遠: 就是回到你們23年年中的時候
[01:10:36] 曼奇: 您當時也說就是其實面臨一個角色
[01:10:38] 曼奇: 就是在付現了CHAGVT的能力之後
[01:10:41] 曼奇: 我是繼續花大幾千萬人民幣
[01:10:43] 曼奇: 我需順一個主流140幣的模型去JGVT4
[01:10:47] 曼奇: 還是說我去做這個跟高效判斷
[01:10:49] 曼奇: 其實當時這個判斷背後是有一個商業邏輯在的
[01:10:51] 曼奇: 像這個是您會參與決策嗎還是
[01:10:54] 曼奇: 是的
[01:10:54] 曼奇: 因為我們那時候作為創業公司
[01:10:56] 劉志遠: 在2023年的下半年的時候
[01:10:58] 劉志遠: 其實也在尋找我們的商業模式
[01:11:01] 劉志遠: 所以在那個時候可能的一個選擇
[01:11:03] 劉志遠: 就是像很多的這個
[01:11:06] 劉志遠: 創業公司就是大模型的公司一樣
[01:11:09] 劉志遠: 來提供APR的服務
[01:11:10] 劉志遠: 這個其實在我們的選項之內的
[01:11:12] 劉志遠: 但是我們的研判就是
[01:11:14] 劉志遠: 會認為這種APR的服務
[01:11:16] 劉志遠: 缺少差一化
[01:11:17] 劉志遠: 比如說都是個家
[01:11:19] 劉志遠: 然後來去提供APR
[01:11:20] 劉志遠: 你的競爭力體驗的什麼地方呢
[01:11:22] 劉志遠: 當然我們說從密度法則來講
[01:11:24] 劉志遠: 我可以讓我的模型
[01:11:26] 劉志遠: 然後它的成本更低
[01:11:27] 劉志遠: 我可以讓APR的加個更低
[01:11:29] 劉志遠: 但是我們會認為在雲上這個模式
[01:11:33] 劉志遠: 不具備商業化的這個壁類
[01:11:36] 劉志遠: 原因是什麼呢
[01:11:37] 劉志遠: 原因是會有一些大廠
[01:11:39] 劉志遠: 它會來發揮它的超能力
[01:11:41] 劉志遠: 它在其他的方面
[01:11:43] 劉志遠: 它盈利非常的多
[01:11:44] 劉志遠: 對它可以燒起來
[01:11:44] 劉志遠: 它加點很厚
[01:11:45] 劉志遠: 它可以燒錢
[01:11:47] 劉志遠: 它可以選擇在這個方向上
[01:11:48] 劉志遠: 我不計成本的去進行推廣
[01:11:50] 劉志遠: 所以我們會認為在雲上
[01:11:52] 劉志遠: 來提供APR對於一個創業公司
[01:11:55] 劉志遠: 尤其是加點很不太厚的創業公司
[01:11:57] 劉志遠: 我們認為是一個風險極大的事情
[01:11:59] 劉志遠: 這個基本上也預判了
[01:12:00] 曼奇: 後面的發展
[01:12:00] 曼奇: 因為24年5月就開始加個戰
[01:12:02] 曼奇: 就開始APR的加個戰
[01:12:03] 曼奇: 對 就是那個加個戰
[01:12:04] 劉志遠: 我覺得其實是一個
[01:12:06] 劉志遠: 非常重要的一個警示
[01:12:08] 劉志遠: 因為其實那個加個戰
[01:12:09] 劉志遠: 本身就是有DPC
[01:12:10] 劉志遠: 對 所以也發的
[01:12:12] 曼奇: 然後後面大家都跟進了什麼火山引擎
[01:12:14] 曼奇: 什麼阿里這些用
[01:12:15] 曼奇: 所以DPC其實本身
[01:12:17] 劉志遠: 它也在像我們團隊一樣
[01:12:19] 劉志遠: 都在追求高效
[01:12:20] 劉志遠: 就是下來用更高的技術
[01:12:22] 劉志遠: 然後來追求用更少的參數
[01:12:24] 劉志遠: 更少的集團量
[01:12:25] 劉志遠: 來完成更高的這個模型能力
[01:12:27] 劉志遠: 那麼它在雲上
[01:12:28] 劉志遠: 的確是能夠呈現一個
[01:12:30] 劉志遠: 非常高的這麼一個競爭力
[01:12:32] 劉志遠: 對吧
[01:12:32] 劉志遠: 它可以用更低的加個
[01:12:33] 劉志遠: 然後來去進行這個服務的提供
[01:12:36] 劉志遠: 但是我覺得
[01:12:37] 劉志遠: 我們跟DPC
[01:12:37] 劉志遠: 個不一樣的地方是
[01:12:39] 劉志遠: DPC
[01:12:39] 劉志遠: 它也有非常厚的加點
[01:12:40] 劉志遠: 它完全可以不考慮盈利問題
[01:12:42] 劉志遠: 但是對於絕大部分的創業公司
[01:12:44] 劉志遠: 來講其實它還是需要考慮
[01:12:46] 劉志遠: 自己的商業的這麼一個選項
[01:12:49] 劉志遠: 你們有就是副判過
[01:12:51] 曼奇: 因為其實你們一直在追求
[01:12:52] 曼奇: 這個高效的方式
[01:12:54] 曼奇: 就它並沒有在續視和輿論上
[01:12:56] 曼奇: 形成像25年1月分
[01:12:58] 曼奇: DPC的那種生事是為什麼呢
[01:13:00] 曼奇: 我覺得還是剛才說的
[01:13:01] 劉志遠: 就是從絕大部分的公眾來看
[01:13:04] 劉志遠: 明顯還是說誰家的能力
[01:13:07] 劉志遠: 能達到一個更高的水平
[01:13:09] 劉志遠: 那這件事情
[01:13:10] 劉志遠: 我覺得顯然是你要去
[01:13:12] 劉志遠: 去遜一個更大的模型
[01:13:13] 劉志遠: 才有可能達成
[01:13:14] 劉志遠: 譬如說幾千億的
[01:13:16] 劉志遠: 甚至上萬億的這麼一個參數
[01:13:18] 劉志遠: 來達成這件事情
[01:13:19] 劉志遠: 所以就是對我們來講
[01:13:20] 劉志遠: 我覺得我們現在所走的一條路線
[01:13:24] 劉志遠: 可能是更適合我們自己的判斷
[01:13:27] 劉志遠: 然後以及我們現在的餅負的一個路線
[01:13:30] 劉志遠: 因為對我來講
[01:13:31] 劉志遠: 我不追求一泡二紅
[01:13:33] 劉志遠: 我追求的是
[01:13:34] 劉志遠: 我能夠堅定的
[01:13:36] 劉志遠: 百日百日
[01:13:37] 劉志遠: 可以達成目標的這麼一條路徑
[01:13:39] 劉志遠: 譬如說
[01:13:40] 劉志遠: 我當然可以選擇
[01:13:41] 劉志遠: 孤注一致
[01:13:42] 劉志遠: 把我們融到的所有的錢
[01:13:44] 劉志遠: 都拿來去訓一個超級大的模型
[01:13:46] 劉志遠: 但是這件事情
[01:13:47] 劉志遠: 一旦它不能夠形成商業必換
[01:13:50] 劉志遠: 那它其實就可能會帶來一個不可接受的
[01:13:53] 劉志遠: 這麼一個後果
[01:13:54] 劉志遠: 這個對我來講
[01:13:55] 劉志遠: 不是我能接受的
[01:13:57] 劉志遠: 因為AGR一定會再未來5到10年到達
[01:13:59] 劉志遠: 那AGR的這個時代的
[01:14:02] 劉志遠: 可以做的事情非常多
[01:14:03] 劉志遠: 那我為什麼要去到一個英雞的
[01:14:06] 劉志遠: 可能我們並不佔優勢的
[01:14:07] 劉志遠: 一個塞道裡面去跟別人去卷
[01:14:09] 劉志遠: 我覺得這不是一個名字的選擇
[01:14:11] 劉志遠: 對我來講
[01:14:12] 劉志遠: 所以我覺得就是
[01:14:13] 劉志遠: 我在哪兒聽到一句話
[01:14:14] 劉志遠: 我覺得特別對
[01:14:15] 劉志遠: 就別人得到的並不見得
[01:14:16] 劉志遠: 是你失去的
[01:14:17] 劉志遠: 就是Divsake火
[01:14:19] 劉志遠: 並不代表人說
[01:14:20] 劉志遠: 我也追求高校
[01:14:22] 劉志遠: 我會覺得說
[01:14:23] 劉志遠: 是不是我本來應該是
[01:14:24] 劉志遠: 想他那樣去做
[01:14:26] 劉志遠: 我覺得不見得
[01:14:27] 劉志遠: 因為其實面向未來
[01:14:28] 劉志遠: AGR真的是一個非常廣闊的天地
[01:14:31] 劉志遠: 你當然是可以追求
[01:14:33] 劉志遠: 像互聯網時代的Google
[01:14:35] 劉志遠: 這樣去提供一個公開的
[01:14:37] 劉志遠: 這種雲的這種服務
[01:14:39] 劉志遠: 但是你可以說想
[01:14:40] 劉志遠: 就AGR實在的這個智能
[01:14:42] 劉志遠: 它既可以發生在雲端
[01:14:43] 劉志遠: 也會發生在端策
[01:14:45] 劉志遠: 那麼既然端策
[01:14:46] 劉志遠: 我們看到了它的無限的這個前進
[01:14:49] 劉志遠: 既然現在其實沒有什麼人
[01:14:51] 劉志遠: 然後還注意到這個方向
[01:14:52] 劉志遠: 那我們提前去做布局
[01:14:54] 劉志遠: 我覺得這個可能是更符合
[01:14:56] 劉志遠: 一個出售公司
[01:14:57] 劉志遠: 通過這個前瞻的布局
[01:14:59] 劉志遠: 然後來去進行探索的
[01:15:01] 劉志遠: 這麼一個可能
[01:15:02] 劉志遠: 更好的這麼一個方式
[01:15:04] 劉志遠: 我追求的是
[01:15:05] 劉志遠: 充滿不確定性的吃定性
[01:15:07] 劉志遠: 剛才也提到就是
[01:15:08] 曼奇: 你們現在是Apple Intelligence
[01:15:10] 曼奇: 其實它的效果還沒有那麼好
[01:15:11] 曼奇: 就在手機上
[01:15:12] 曼奇: 因為目前手機上的算力
[01:15:15] 曼奇: 要來能支撐了這個模型的性能
[01:15:17] 曼奇: 可能還是相對受限的
[01:15:18] 曼奇: 就是如果根據你們
[01:15:20] 去觀測到的這個密度法則
[01:15:21] 曼奇: 在什麼時間上手機
[01:15:23] 曼奇: 因為這可能是大家最關注的一個硬件
[01:15:26] 曼奇: 然後用的人也是最多的
[01:15:27] 曼奇: 這個上面的模型能力
[01:15:28] 曼奇: 也會變得就是
[01:15:30] 曼奇: 能支持我們乾些
[01:15:32] 曼奇: 非常有意思的事
[01:15:33] 曼奇: 然後非常提升效率的個性
[01:15:34] 曼奇: 那我們會覺得大概到二零二七年
[01:15:37] 劉志遠: 可能會是一個比較重要的一個節點
[01:15:39] 劉志遠: 這個節點意味著什麼呢
[01:15:41] 劉志遠: 到二零二七年預計
[01:15:43] 劉志遠: 我們可以把大規模強化學習
[01:15:45] 劉志遠: 能夠實現在
[01:15:46] 劉志遠: 端策的模型上
[01:15:47] 劉志遠: 就可以進行一個非常好的
[01:15:49] 劉志遠: 一個學習的能力
[01:15:51] 劉志遠: 那就意味著什麼呢
[01:15:52] 劉志遠: 意味著說
[01:15:53] 劉志遠: 我們每個人都可以利用
[01:15:55] 劉志遠: 我們自己的數據
[01:15:56] 劉志遠: 來提供給這個模型
[01:15:59] 劉志遠: 作為它的這個學習環境
[01:16:01] 劉志遠: 來讓它持續的去進行學習
[01:16:03] 劉志遠: 也就是說在二零二七年
[01:16:04] 劉志遠: 我們可以利用一個
[01:16:05] 劉志遠: 端策的相對不錯的算力
[01:16:07] 劉志遠: 支持一個具備自我學習能力的模型
[01:16:10] 劉志遠: 能夠逐漸地成長為我們每個人
[01:16:13] 劉志遠: 專屬的大模型
[01:16:14] 劉志遠: 助手
[01:16:15] 劉志遠: 那我想它就可以成為我們所有的中端上的
[01:16:19] 劉志遠: 智能的這麼一個掌握者
[01:16:20] 劉志遠: 如果它要在眼鏡這麼少
[01:16:22] 曼奇: 要提及上也實現類似的能力
[01:16:23] 曼奇: 是不是要更遠
[01:16:25] 曼奇: 那我感覺一些眼鏡可以作為
[01:16:27] 劉志遠: 你說說我記得配件
[01:16:28] 曼奇: 對
[01:16:29] 劉志遠: 我不太認為就是一定要在眼鏡上
[01:16:32] 劉志遠: 本身裝上這個模型
[01:16:34] 劉志遠: 因為其實我最近在碰桃
[01:16:36] 劉志遠: 因為我要解肥
[01:16:38] 劉志遠: 所以我在裝一個
[01:16:39] 劉志遠: 就是監測的領舍
[01:16:41] 劉志遠: 監測血糖的這麼一個硬件
[01:16:43] 劉志遠: 是扎在那個格波上的
[01:16:45] 曼奇: 對
[01:16:45] 曼奇: 我們為什麼一定要在這頂上
[01:16:47] 劉志遠: 搞一個新片或是模型的
[01:16:49] 劉志遠: 當然也有
[01:16:50] 劉志遠: 但是它肯定不用做智能化的東西
[01:16:52] 劉志遠: 它只要把採儀的數據
[01:16:54] 劉志遠: 傳到我們的譬如說用戶
[01:16:57] 劉志遠: 它有一個自己的一個中端的盒子
[01:16:59] 劉志遠: 只要它能夠持續地去
[01:17:01] 劉志遠: 收集這個用戶的相應的數據
[01:17:03] 劉志遠: 來提供相應的智能化的服務
[01:17:05] 劉志遠: 其實又可以了
[01:17:06] 劉志遠: 我覺得不太需要說
[01:17:08] 劉志遠: 我們要讓智能真正的
[01:17:10] 劉志遠: 到所有的中端上去
[01:17:12] 劉志遠: 我覺得整個人類社會的中端
[01:17:15] 劉志遠: 應該就是每個人
[01:17:16] 劉志遠: 這個人的各種各樣的
[01:17:18] 劉志遠: 更加分佈的一些
[01:17:21] 劉志遠: 譬如說你的眼鏡也好
[01:17:22] 劉志遠: 甚至你的耳機
[01:17:23] 劉志遠: 你的手錶
[01:17:24] 劉志遠: 然後你的各種各樣的智能的硬件
[01:17:28] 劉志遠: 我想就是它完全是可以有一個
[01:17:30] 劉志遠: 共同的一個提供智能服務的中端
[01:17:34] 劉志遠: 就有點像家庭裡面的NAS
[01:17:36] 劉志遠: 只是說你是一個變形的
[01:17:38] 劉志遠: 可以跟著你走的這麼一個NAS
[01:17:40] 劉志遠: 最後想討論一下
[01:17:41] 曼奇: 你們關心的更廣泛的一些行業趨勢
[01:17:43] 曼奇: 其實前面一消息
[01:17:44] 曼奇: 也有一些聊到的
[01:17:45] 曼奇: 就是我覺得現在大家
[01:17:46] 曼奇: 也會在討論說這個大園模型
[01:17:48] 曼奇: 在預訓練加強化學習
[01:17:50] 曼奇: 後訓練這個方法
[01:17:52] 曼奇: 之後會有什麼新的方法
[01:17:54] 曼奇: 就下一個不同的方法
[01:17:55] 曼奇: 可能是什麼
[01:17:56] 曼奇: 對 因為我覺得
[01:17:57] 蕭朝軍: 這一個最重要的點
[01:17:58] 蕭朝軍: 就是我們還掐了自主學習
[01:18:00] 蕭朝軍: 或者要自我學習 自我精華
[01:18:01] 蕭朝軍: 或者還是什麼持續學習
[01:18:03] 蕭朝軍: 反正現在名字很多
[01:18:04] 蕭朝軍: 在性學習是這個意思嗎
[01:18:05] 曼奇: 在性學習
[01:18:06] 蕭朝軍: 管結屬於其中的一小部分
[01:18:08] 蕭朝軍: 就是名字其實很多
[01:18:10] 蕭朝軍: 那其實還是那個判斷
[01:18:11] 蕭朝軍: 就是說現在的強化學習
[01:18:14] 蕭朝軍: 已經能夠支持著說
[01:18:15] 蕭朝軍: 我們在任何一個任務上
[01:18:16] 蕭朝軍: 已經能夠去了非常優異的效果
[01:18:18] 蕭朝軍: 但這實際上還不是我們原來所擁有的智能
[01:18:22] 蕭朝軍: 這樣的一個呈現
[01:18:23] 蕭朝軍: 所以其實剛才我們市場的這樣一個場景
[01:18:25] 蕭朝軍: 未來應該是說這個模型
[01:18:28] 蕭朝軍: 它是一個很好的學習者
[01:18:29] 蕭朝軍: 它放在你的中端上
[01:18:31] 蕭朝軍: 或者是放在任何的設備上
[01:18:33] 蕭朝軍: 它可以主電的學習
[01:18:34] 蕭朝軍: 你所需要它學習的那些任務
[01:18:37] 蕭朝軍: 比如說幫你這個譬如說簡波客
[01:18:39] 蕭朝軍: 或者是幫你去寫代碼等等
[01:18:41] 蕭朝軍: 這些都是你的一些專業的技能
[01:18:43] 蕭朝軍: 你可以像帶一個實際生意
[01:18:46] 蕭朝軍: 把他給帶出來
[01:18:46] 蕭朝軍: 然後慢慢的這個幫助你取代你
[01:18:49] 蕭朝軍: 這我覺得是...
[01:18:50] 蕭朝軍: 幫助我取代
[01:18:51] 曼奇: 對 其實這個應該是一個
[01:18:53] 蕭朝軍: AGI 下一步非常重要的一個點
[01:18:56] 蕭朝軍: 我覺得其實現在這個
[01:18:58] 蕭朝軍: 不過強化學習也好
[01:18:59] 蕭朝軍: 包括御徐內
[01:19:00] 蕭朝軍: 其實沒有辦法做到的一個事情
[01:19:02] 蕭朝軍: 這個就是說為什麼現在會有這個事件模型
[01:19:04] 蕭朝軍: 所以它就是想說幫助模型
[01:19:07] 蕭朝軍: 去獲取一個足夠好
[01:19:08] 蕭朝軍: 或去反饋的一個平台
[01:19:10] 蕭朝軍: 還有一些其他的一些
[01:19:12] 蕭朝軍: 各種學習這樣的犯事
[01:19:14] 蕭朝軍: 包括現在那個強化學習之父
[01:19:15] 蕭朝軍: 他也去提到說大模型為什麼
[01:19:17] 蕭朝軍: 他說大模型不是同樣AGI的這種目的
[01:19:19] 蕭朝軍: 核心還是這種Data German的這種學習方式
[01:19:22] 蕭朝軍: 可能還是會阻礙模型
[01:19:24] 蕭朝軍: 就是更高效的學習一些新的技能
[01:19:27] 蕭朝軍: 然後再往後其實還會有
[01:19:28] 蕭朝軍: 比如說這個模型有了自我學習
[01:19:31] 蕭朝軍: 自我精華的能力之後
[01:19:32] 蕭朝軍: 它在你的手上會成為一個
[01:19:34] 蕭朝軍: 很專業的類似的這種播報新聞的
[01:19:37] 蕭朝軍: 然後播報科技前演的這麼一個模型
[01:19:39] 蕭朝軍: 它可能在我的手上
[01:19:40] 蕭朝軍: 它就是一個研究
[01:19:42] 蕭朝軍: 也公開的本身的一個Research
[01:19:43] 蕭朝軍: 那在這樣一個場景下
[01:19:45] 蕭朝軍: 不同的模型它可能再往下一步走
[01:19:47] 蕭朝軍: 其實是相互的和協助
[01:19:50] 蕭朝軍: 對吧 那比如我這個模型是一個AGI的Serture
[01:19:53] 蕭朝軍: 然後會有其他的模型
[01:19:54] 蕭朝軍: 是類似於就是Infart的Research
[01:19:56] 蕭朝軍: 它們怎麼樣能夠讓這個AGI模型跑得更快
[01:19:59] 蕭朝軍: 然後再往後可能是最高階的
[01:20:01] 蕭朝軍: 就是這種創造的能力
[01:20:02] 蕭朝軍: 比如說我們會去講說
[01:20:04] 蕭朝軍: 我們人類會不但的產生足夠落的AINSTOT
[01:20:08] 蕭朝軍: 它能夠根據已有的一些符號體系
[01:20:10] 蕭朝軍: 就是比如說基於牛燉定律這些符號
[01:20:13] 蕭朝軍: 體系推導出相對論
[01:20:14] 蕭朝軍: 但是對於現在的AGI模型來說
[01:20:16] 蕭朝軍: 顯然做不到一點
[01:20:17] 蕭朝軍: 它永遠只能在人類已經定義好的符號
[01:20:20] 蕭朝軍: 接於演符號上做學習
[01:20:22] 蕭朝軍: 它沒有學辦法搞新
[01:20:23] 蕭朝軍: 創造出新的這種符號級的關係
[01:20:25] 蕭朝軍: 甚至創造出一個新的符號
[01:20:27] 蕭朝軍: 然後這個可能假如達到創造能力之後
[01:20:30] 蕭朝軍: 那其實我們會認為
[01:20:32] 蕭朝軍: 現在人類社會上所有需要智能的這些
[01:20:34] 蕭朝軍: 工作可能都會被AGI取來的
[01:20:37] 蕭朝軍: 這是我們大概列的AGI的幾步走的計劃
[01:20:40] 蕭朝軍: 所以總結一下就是自主學習
[01:20:42] 曼奇: 然後是已經完成自主學習
[01:20:44] 曼奇: 這些AI之間的寫作
[01:20:45] 曼奇: 最後就是真正的創新
[01:20:47] 曼奇: 對 我也叫AISertice
[01:20:49] 蕭朝軍: 大家現在創體的概念
[01:20:51] 蕭朝軍: 不過我們提到創新可能的更多會是
[01:20:54] 蕭朝軍: 這種創新的META的能力
[01:20:56] 蕭朝軍: 而不是在某一個
[01:20:58] 蕭朝軍: 比如說具體的螺旋醫
[01:21:00] 蕭朝軍: 像無醫藥等等的接領域
[01:21:02] 蕭朝軍: 它卻做一些智能做這個理論的創新
[01:21:04] 蕭朝軍: 這個也是一個明線
[01:21:05] 劉志遠: 它的案線其實就是
[01:21:07] 劉志遠: 相當於是在底層的實現的邏輯上
[01:21:10] 劉志遠: 我們其實特別喜歡
[01:21:11] 劉志遠: 跟信息時代的新片發展其實做類比
[01:21:16] 劉志遠: 你會看到就是整個信息革命
[01:21:19] 劉志遠: 我們全球是一個信息化的社會
[01:21:21] 劉志遠: 但這個信息化其實它的底層的支撐
[01:21:25] 劉志遠: 其實就是信片 就是算力
[01:21:27] 劉志遠: 但是你就可以看一下
[01:21:29] 劉志遠: 這個算力的分布大概是什麼情況
[01:21:31] 劉志遠: 就是我們看到一個統計
[01:21:33] 劉志遠: 就是二二四年初
[01:21:34] 劉志遠: 中國電信研究院的統計
[01:21:36] 劉志遠: 它就是說二三年的時候
[01:21:37] 劉志遠: 全國的端策的
[01:21:39] 劉志遠: 就是手機上的算力加在一起的總量
[01:21:42] 劉志遠: 是全國的
[01:21:44] 劉志遠: 是數據中心的算力的12倍
[01:21:46] 劉志遠: 你大家可以說想就是說
[01:21:48] 劉志遠: 雖然雲上的算力看上去很大
[01:21:51] 劉志遠: 然後一個算力中心它的算力也很大
[01:21:54] 劉志遠: 但是你架不住全國十幾日人
[01:21:57] 劉志遠: 可能有幾十一部手機
[01:21:58] 劉志遠: 然後它手機上的算力總量
[01:22:01] 劉志遠: 其實是非常大的一個規模
[01:22:02] 劉志遠: 所以過去的80年
[01:22:04] 劉志遠: 相信在全球實現了
[01:22:06] 劉志遠: 通過算力支撐的這麼一個信息化的社會
[01:22:09] 劉志遠: 那麼這個算力的分布是什麼樣子呢
[01:22:11] 劉志遠: 也就是對應的信息的分布是什麼樣子呢
[01:22:14] 劉志遠: 其實是分布式的
[01:22:15] 劉志遠: 是分布在這些中端上的
[01:22:17] 劉志遠: 我們每個人都擁有自己的一個
[01:22:20] 劉志遠: 以個人為中心的這麼一個信息
[01:22:23] 劉志遠: 這個中端
[01:22:24] 劉志遠: 我們手機我們的PC我們的Pi的等等
[01:22:27] 劉志遠: 那麼你就可以設想未來的智能社會
[01:22:31] 劉志遠: 智能的分布會是什麼樣子呢
[01:22:33] 劉志遠: 就是在二三年初的時候
[01:22:34] 劉志遠: 曾經全球有過一些巨頭說
[01:22:38] 劉志遠: 這個世界上不需要超過幾個大夢想
[01:22:41] 劉志遠: 就可以滿足全球的智能化的需求
[01:22:44] 劉志遠: 這個跟信息化剛剛開始
[01:22:47] 劉志遠: 1943年的時候
[01:22:48] 劉志遠: 就是IBM的董事長
[01:22:49] 劉志遠: 曾經說全球不需要超過舞台主機
[01:22:51] 劉志遠: 就可以滿足全球計算的需求一樣
[01:22:55] 劉志遠: 所以就是我們可以設想未來幾年之後
[01:22:58] 劉志遠: 智能化它也一定是分布式的
[01:23:00] 劉志遠: 原因就是這個世界上
[01:23:02] 劉志遠: 本身智能也都是分布在我們每個人的頭腦裡面
[01:23:06] 劉志遠: 我們每個人的這個中端上
[01:23:08] 劉志遠: 所以就是我們認為未來智能化的社會
[01:23:12] 劉志遠: 一定是要在我們的中端的社會上
[01:23:15] 劉志遠: 要為每個人每個智能的中端提供
[01:23:19] 劉志遠: 這種智能化的證明一個服務
[01:23:21] 劉志遠: 這個其實我覺得是未來的一個基本形態
[01:23:24] 劉志遠: 那麼在這樣的一個形態的支撐下
[01:23:26] 劉志遠: 你就可以想它具備了自主學習能力
[01:23:29] 劉志遠: 那就意味著說在中端上
[01:23:31] 劉志遠: 我們就可以根據這個用戶個人的數據
[01:23:34] 劉志遠: 去形成這個人的專屬的大模型
[01:23:37] 劉志遠: 這個大模型就是你的個人的終身的助手
[01:23:41] 它最懂你
[01:23:41] 它是這個世界上最懂你的那個智能大模型
[01:23:44] 劉志遠: 那麼在雲上會有各種各樣的這種專家大模型
[01:23:47] 劉志遠: 有美團大模型專門懂這個外賣
[01:23:50] 劉志遠: 這個有滴滴大模型專門懂
[01:23:53] 劉志遠: 如何去規劃這個形成
[01:23:55] 劉志遠: 然後有這個抖音大模型懂
[01:23:57] 劉志遠: 你的各種各樣的這種娛樂的這種喜好
[01:24:00] 劉志遠: 但是就是它一定是一個端的你的個性化大模型
[01:24:04] 劉志遠: 和這些雲的專家大模型互相之間的一個系統
[01:24:08] 劉志遠: 然後來完成相應的這麼一個工作
[01:24:10] 劉志遠: 那麼在這種情況下
[01:24:11] 劉志遠: 會形成所謂的智能體的互聯網
[01:24:14] 劉志遠: 這個其實我們會對應
[01:24:16] 劉志遠: 就是它說的這種寫作的寫作的這個模式
[01:24:18] 劉志遠: 那麼這個其實它就是在底層的一個案線
[01:24:22] 劉志遠: 也就是說不為公眾所廣為認知的
[01:24:25] 劉志遠: 它其實是發生在這些計算設備
[01:24:28] 劉志遠: 發生在我們的底層的這麼一個相應的
[01:24:32] 劉志遠: 這麼一個邏輯
[01:24:33] 劉志遠: 這個邏輯其實我們會覺得
[01:24:35] 劉志遠: Dancing Law會發揮一個非常重要的作用
[01:24:37] 劉志遠: 因為它就可以幾乎以非常非常低
[01:24:40] 劉志遠: 這個用戶所沒有感知的這麼一個成分
[01:24:43] 劉志遠: 來開始廣為的服務我們每個用戶
[01:24:46] 劉志遠: 在這個未來的設想裡面
[01:24:48] 曼奇: 它的這個經濟模式是怎麼運轉的樣子
[01:24:50] 劉志遠: 當然我這是我自己的觀點
[01:24:52] 劉志遠: 就是不見得對
[01:24:55] 劉志遠: 在我來看就是歷史上工業革命
[01:24:58] 劉志遠: 一輪一輪的
[01:24:59] 劉志遠: 比如說蒸氣革命 電氣革命
[01:25:01] 劉志遠: 其實它大致是屬於工業革命
[01:25:03] 劉志遠: 它替代的是人的體力勞動
[01:25:06] 劉志遠: 相當於是利用機器
[01:25:07] 劉志遠: 然後來去達到替代人的機械化的體力勞動
[01:25:12] 劉志遠: 甚至說比人的體力勞動
[01:25:14] 劉志遠: 能力還要再強
[01:25:15] 劉志遠: 那我們這些輪的智能革命是在幹什麼呢
[01:25:18] 劉志遠: 我是覺得它其實是跟上一輪的信息革命
[01:25:22] 劉志遠: 再加上這一輪的智能革命
[01:25:24] 劉志遠: 其實是在替代我們人的腦力勞動
[01:25:26] 劉志遠: 我們所有的知識工作者
[01:25:28] 劉志遠: 我們所有的需要這個人類
[01:25:31] 劉志遠: 關於這個世界的知識
[01:25:32] 劉志遠: 才能完成的那些工作
[01:25:34] 劉志遠: 那麼未來只要它是機械化的 重顧化
[01:25:37] 劉志遠: 那其實就應該由這個人工智能來去完成
[01:25:40] 劉志遠: 那我怎麼賺錢呢
[01:25:41] 曼奇: 我如果是一個個人的話
[01:25:42] 曼奇: 首先我覺得第一步
[01:25:44] 劉志遠: 你的個人的智能中端
[01:25:46] 劉志遠: 其實是能夠讓你的工作效率變得更高
[01:25:49] 劉志遠: 變得更有市場競爭力
[01:25:50] 劉志遠: 就像單純說你在你的個人住手的知識下
[01:25:53] 劉志遠: 你完成的這個工作
[01:25:55] 劉志遠: 水平比別人更高
[01:25:56] 劉志遠: 然後完成的這個工作量比別人更多
[01:25:58] 劉志遠: 那你就可以能夠得到更多的
[01:26:00] 劉志遠: 這麼一個勞動的生活
[01:26:03] 劉志遠: 對不對
[01:26:03] 劉志遠: 因為本身你是通過知識
[01:26:05] 劉志遠: 然後來完成相應的工作
[01:26:07] 劉志遠: 然後來去獲得相應的輸入的
[01:26:09] 劉志遠: 通過學這個體能想像的
[01:26:10] 曼奇: 我覺得在那個描述的環境
[01:26:12] 曼奇: 你又感覺好像也不超貨幣了
[01:26:14] 也不超過工作了
[01:26:15] 曼奇: 那我就聚一個今年的例子
[01:26:17] 劉志遠: 比如說今年的例子就是
[01:26:18] 劉志遠: 代碼大模型非常地厲害
[01:26:20] 劉志遠: 那不就是相當於是所有的這些程序員
[01:26:24] 劉志遠: 其實都要加載上相應的AI
[01:26:26] 劉志遠: 然後來輔助著它工作
[01:26:28] 劉志遠: 來提高它的工作效率
[01:26:29] 劉志遠: 那麼對應的結果就是
[01:26:31] 劉志遠: 你有了AI的輔助你的生產效率就變得極高
[01:26:36] 劉志遠: 那麼你其實就可以獲得
[01:26:37] 劉志遠: 更好的這麼一個輸入
[01:26:39] 劉志遠: 假如說你未來說
[01:26:41] 劉志遠: 好
[01:26:41] 劉志遠: 那大家用一個通用的
[01:26:43] 劉志遠: 這個AI的代碼大模型都不足夠了
[01:26:46] 劉志遠: 你需要自己去積累出
[01:26:48] 劉志遠: 你自己的一個知識庫
[01:26:49] 劉志遠: 你自己的這麼一些
[01:26:51] 劉志遠: 已經編寫過代碼的這麼一些經驗
[01:26:53] 劉志遠: 那麼你通過這些經驗
[01:26:55] 劉志遠: 所構造的這個你個人的這個助手
[01:26:57] 劉志遠: 能夠更好的幫助你去寫
[01:27:00] 劉志遠: 別人寫不了的代碼
[01:27:01] 劉志遠: 那你就在這個市場上
[01:27:03] 劉志遠: 就會更有競爭力
[01:27:04] 劉志遠: 對吧
[01:27:04] 劉志遠: 那他變相呢
[01:27:05] 劉志遠: 其實是相當於提高了整個
[01:27:07] 劉志遠: 這個社會的生產力
[01:27:08] 劉志遠: 生產效率
[01:27:09] 劉志遠: 那今年還有的發生的時候
[01:27:11] 曼奇: 就是剛剛亞馬遜採了一點四萬人
[01:27:13] 曼奇: 或者其實應該是說
[01:27:14] 蕭朝軍: 可能是一種模式是說未來
[01:27:16] 蕭朝軍: 其實現在來說
[01:27:17] 蕭朝軍: 你你賺錢的方式
[01:27:18] 蕭朝軍: 通過你的知識和技能賺錢
[01:27:20] 蕭朝軍: 但是在未來AI能自舞學習之後
[01:27:22] 蕭朝軍: 你可以設想這個AI
[01:27:24] 蕭朝軍: 本身是為你的知識
[01:27:25] 蕭朝軍: 就是你的知識和技能外化的一個提升
[01:27:28] 蕭朝軍: 所以其實未來可能還是那麼容易
[01:27:30] 蕭朝軍: 就是人機寫成了模式
[01:27:31] 蕭朝軍: 你的AI
[01:27:32] 蕭朝軍: 它的智能水平有多高
[01:27:33] 蕭朝軍: 可能還是取決於你
[01:27:34] 蕭朝軍: 怎麼跟它進行寫統
[01:27:36] 蕭朝軍: 然後你怎麼給它足夠多的經驗
[01:27:38] 蕭朝軍: 然後去交它去帶它
[01:27:40] 蕭朝軍: 我覺得可能下一步
[01:27:41] 蕭朝軍: 第一個改變可能會是這樣
[01:27:43] 蕭朝軍: 所以未來大家給你付費
[01:27:44] 蕭朝軍: 是因為你加你的AI很強
[01:27:47] 劉志遠: 所以就是說你設想
[01:27:48] 劉志遠: 就是在AI時代還能夠去工作的人
[01:27:51] 劉志遠: 那麼它一定是一個高水平的知識工作者
[01:27:54] 劉志遠: 因為低水平的知識工作
[01:27:55] 劉志遠: 已經被AI所期待了
[01:27:57] 劉志遠: 那麼在這種基礎上的話
[01:27:58] 劉志遠: 那就是說所有的這種高水平的知識工作者
[01:28:01] 劉志遠: 那它一定是由AI來付出的
[01:28:03] 劉志遠: 那在這個AI時代
[01:28:04] 曼奇: 一個低水平的工作
[01:28:05] 曼奇: 就如何變成成長為一個高水平的知識工作者
[01:28:08] 曼奇: 你的專業領域之所以還需要你
[01:28:11] 劉志遠: 是因為AI肯定做不了
[01:28:13] 劉志遠: 肯定有AI做不了的事
[01:28:14] 劉志遠: 因為我們是一個人類社會
[01:28:16] 劉志遠: 譬如說醫學
[01:28:18] 劉志遠: 譬如說心理諮詢
[01:28:19] 劉志遠: 譬如說金融
[01:28:20] 劉志遠: 譬如說教育
[01:28:21] 劉志遠: 所有的這些知識工作
[01:28:23] 劉志遠: 特別重要的方向包括法律
[01:28:25] 劉志遠: 那不可能離開人
[01:28:27] 劉志遠: 所以它一定第一階段
[01:28:28] 劉志遠: 一定是AI是付出
[01:28:30] 劉志遠: 但是AI在付出的過程中
[01:28:32] 劉志遠: 它可以通過你的行為
[01:28:33] 劉志遠: 可以通過你的反饋
[01:28:35] 劉志遠: 它來不斷地成長
[01:28:37] 劉志遠: 它成長到一定的水平
[01:28:39] 劉志遠: 那它是不是就可以在某些場景
[01:28:41] 劉志遠: 可以發揮一些作用
[01:28:42] 劉志遠: 或者是說可能是你一個人開了一家公司
[01:28:45] 蕭朝軍: 然後你的員工是你的AI
[01:28:48] 蕭朝軍: 然後你的成長過程
[01:28:50] 蕭朝軍: 其實就是你這家公司創業發展的過去
[01:28:53] 蕭朝軍: 這種生產力無限大的未來還挺難想像的
[01:28:57] 曼奇: 這你會覺得這個生產就非常過勝了
[01:28:59] 曼奇: 我真的真正充滿了期待
[01:29:01] 劉志遠: 原因是什麼呢
[01:29:02] 劉志遠: 我會覺得過去的這幾十年
[01:29:04] 劉志遠: 我覺得人類的發展其實是被棒的住的
[01:29:08] 為什麼棒的住
[01:29:09] 劉志遠: 被我們人類自己的知識給棒的住了
[01:29:11] 劉志遠: 你想我們過去的這幾十年
[01:29:13] 劉志遠: 這個新西大爆炸
[01:29:14] 劉志遠: 知識大爆炸
[01:29:16] 我覺得帶來的結果
[01:29:17] 劉志遠: 就是每個人
[01:29:18] 劉志遠: 你都只能成為某個特別小的方向上的專家
[01:29:21] 劉志遠: 對
[01:29:22] 劉志遠: 所以我覺得就是這個社會
[01:29:24] 劉志遠: 我們人類歷史上
[01:29:25] 劉志遠: 積累的知識總量已經大到了
[01:29:28] 劉志遠: 我們任何一個人
[01:29:30] 劉志遠: 只能在裡面是一個很小很小的一個拼圖
[01:29:32] 劉志遠: 我會覺得如果仍然只是依賴我們人
[01:29:36] 劉志遠: 來去完成相應的
[01:29:38] 劉志遠: 這麼一個前言探索和創新
[01:29:40] 劉志遠: 來更好的繼續認識這個世界
[01:29:42] 劉志遠: 改造這個世界
[01:29:43] 劉志遠: 我會覺得變得越來越難
[01:29:45] 劉志遠: 因為你這一輩子
[01:29:46] 劉志遠: 你光學心知識就已經學破了
[01:29:48] 劉志遠: 對吧
[01:29:49] 劉志遠: 那不是他有一個例子
[01:29:50] 劉志遠: 說舉了一個例子
[01:29:52] 劉志遠: 說人類知識是一個大的一個球
[01:29:55] 劉志遠: 對吧
[01:29:55] 劉志遠: 然後任何一個人的學習
[01:29:57] 劉志遠: 到了博士階段
[01:29:58] 劉志遠: 可能就是在球的邊緣
[01:30:00] 劉志遠: 可能是在做那麼一個小的突起
[01:30:03] 劉志遠: 那你在往後
[01:30:04] 劉志遠: 你可能你窮念一生
[01:30:06] 劉志遠: 可能都到達不了那個邊緣了
[01:30:07] 劉志遠: 那怎麼辦
[01:30:08] 劉志遠: 我是覺得就得有AI的幫助
[01:30:10] 劉志遠: 所以在我來看
[01:30:12] 劉志遠: 我反而是覺得AI的出現
[01:30:14] 劉志遠: 局部上某些方面會帶來我們的
[01:30:17] 劉志遠: 這個就是生產歷過生看上去
[01:30:19] 劉志遠: 比如說代碼程序員
[01:30:21] 劉志遠: 很像大比例的參員或者怎麼樣
[01:30:24] 劉志遠: 但是同時我們其實看到
[01:30:25] 劉志遠: 人類作為整體
[01:30:27] 劉志遠: 其實對我們人類已經積累的知識的掌握
[01:30:30] 劉志遠: 其實是完全失控了的
[01:30:32] 劉志遠: 這個學術到你說第三階段
[01:30:33] 曼奇: 對吧
[01:30:34] 曼奇: 就是AI
[01:30:34] 曼奇: 他能幫助一些發現
[01:30:36] 曼奇: 我覺得
[01:30:37] 劉志遠: 現在其實已經譬如AIFileScience
[01:30:39] 劉志遠: 現在國內
[01:30:40] 劉志遠: 很多的
[01:30:41] 劉志遠: 包括國家機構等等的
[01:30:43] 劉志遠: 都在非常權力的布局
[01:30:45] 劉志遠: 你看美國剛剛發的那個創始機計畫
[01:30:48] 劉志遠: 也是在做這件事情
[01:30:49] 劉志遠: 原因就是在於大家都看到了
[01:30:51] 劉志遠: 就是AI本身對我們人類
[01:30:54] 劉志遠: 這個發現和改造這個世界
[01:30:56] 劉志遠: 其實具有非常重要的這個作用
[01:30:58] 劉志遠: 你原來可能需要花十幾年
[01:31:00] 劉志遠: 幾十年才能完成的工作
[01:31:02] 劉志遠: 未來有了AI的幫助
[01:31:03] 劉志遠: 你可能用幾個小時就可以去完成
[01:31:06] 劉志遠: 這是完全有可能的
[01:31:07] 劉志遠: 所以在我來看
[01:31:08] 劉志遠: 我會覺得我們在很多很多方向
[01:31:11] 劉志遠: 其實對這個世界還完全缺少認知
[01:31:13] 劉志遠: 材料能源
[01:31:15] 劉志遠: 整個這個宇宙
[01:31:16] 劉志遠: 等等各個方面
[01:31:17] 劉志遠: 包括AI本身
[01:31:19] 劉志遠: 智能本質是什麼
[01:31:20] 劉志遠: 不知道
[01:31:21] 劉志遠: 包括我們的腦
[01:31:22] 劉志遠: 到底是什麼樣的一個機制
[01:31:24] 劉志遠: 它的機率是什麼
[01:31:25] 劉志遠: 這個時間有太多的位置了
[01:31:27] 劉志遠: 所以我是覺得
[01:31:28] 劉志遠: 這個智能時代最讓人找米的地方
[01:31:31] 劉志遠: 不只是說我們實現了AI
[01:31:33] 劉志遠: 而是AI
[01:31:34] 劉志遠: 能夠讓我們更快的
[01:31:36] 劉志遠: 更好的
[01:31:37] 劉志遠: 能夠認識
[01:31:38] 劉志遠: 一個更大的世界
[01:31:39] 劉志遠: 所以我覺得完全不用擔心生產離過生
[01:31:42] 劉志遠: 因為就像劉慈興說的相存教師
[01:31:44] 劉志遠: 對吧
[01:31:45] 劉志遠: 這麼低頻的這種貸款
[01:31:47] 劉志遠: 對吧
[01:31:47] 劉志遠: 口口相傳所積累的
[01:31:49] 劉志遠: 這麼一個人類的知識
[01:31:50] 劉志遠: 到了我們現代
[01:31:51] 劉志遠: 我覺得已經難以危機了
[01:31:53] 劉志遠: 所以我是覺得現在
[01:31:54] 劉志遠: 咱們探討什麼生產離過生
[01:31:56] 劉志遠: 什麼程序員都失業
[01:31:58] 劉志遠: 我覺得這都是倉海一素
[01:32:00] 劉志遠: 你再過十年二十年
[01:32:02] 劉志遠: 當我們有了新的材料
[01:32:04] 劉志遠: 有了新的能源
[01:32:06] 劉志遠: 我們的生命科學
[01:32:07] 劉志遠: 我們受命
[01:32:08] 劉志遠: 各個方面都有一個非常大的突破
[01:32:11] 劉志遠: 當我們的足跡
[01:32:12] 劉志遠: 能夠超越地球
[01:32:14] 劉志遠: 對吧
[01:32:15] 劉志遠: 我們能夠辯部全宇宙
[01:32:16] 劉志遠: 那我們還要擔心
[01:32:18] 劉志遠: 程序員實業的問題嗎
[01:32:19] 劉志遠: 對
[01:32:20] 曼奇: 但我這個擔心不是從紅官層面的
[01:32:22] 曼奇: 它是從每個人的角度的
[01:32:24] 曼奇: 因為你實業對一個人
[01:32:25] 曼奇: 或者說對一個家庭來說
[01:32:26] 曼奇: 肯定是個衝擊嗎
[01:32:27] 曼奇: 但是對我們全人類來講
[01:32:29] 劉志遠: 我覺得是一次大的解放
[01:32:30] 劉志遠: 關於未來你有什麼擔心的地方嗎
[01:32:32] 曼奇: 你剛說都是很樂觀的部分
[01:32:34] 曼奇: 擔心的地方
[01:32:34] 曼奇: 擔心的地方
[01:32:35] 劉志遠: 我擔心的地方是
[01:32:37] 劉志遠: 我們人類自己
[01:32:38] 劉志遠: 會舒服我們自己的潛進的步伐
[01:32:42] 劉志遠: 你這什麼
[01:32:43] 曼奇: 你是質譬如說監管
[01:32:44] 曼奇: 或者說
[01:32:45] 對
[01:32:45] 你看現在已經有很多人
[01:32:47] 劉志遠: 會說
[01:32:48] 劉志遠: 不要去研究Superintile的真實
[01:32:50] 劉志遠: 不要去幹這個
[01:32:51] 劉志遠: 不要去幹那個
[01:32:52] 劉志遠: 我覺得這個是完全不合理的
[01:32:54] 劉志遠: 你就跟說
[01:32:55] 劉志遠: 在1940年
[01:32:56] 劉志遠: 因為說我們有computer這個職業
[01:32:59] 劉志遠: 我們完全可以用算盤來去完成一些計算
[01:33:01] 劉志遠: 所以為什麼要研究超級計算機呢
[01:33:04] 劉志遠: 為什麼要研究大型計算機呢
[01:33:06] 劉志遠: 說這些大型計算機都是用來做軍事的
[01:33:09] 劉志遠: 用來去做這個毀滅人類的
[01:33:11] 劉志遠: 我們為什麼要去研究它
[01:33:12] 劉志遠: 所以你的擔心是其他人對AI未來退估擔心
[01:33:15] 曼奇: 你不覺得嗎
[01:33:16] 劉志遠: 如果說回到1940年
[01:33:19] 劉志遠: 因為一些人說computer這個東西
[01:33:23] 劉志遠: 電子計算機這個東西是用來做軍事的
[01:33:25] 劉志遠: 所以我們不應該研究它
[01:33:27] 劉志遠: 你覺得是合理的嗎
[01:33:28] 劉志遠: 我覺得可以審慎
[01:33:29] 劉志遠: 但我是覺得
[01:33:30] 劉志遠: 反而是能夠提高我們探索這個世界能力的事情
[01:33:33] 劉志遠: 我們一定要支持
[01:33:35] 劉志遠: 不然的話
[01:33:35] 劉志遠: 我們就永遠的只能夠停留在一個
[01:33:37] 劉志遠: 這個盲目的位置之餘
[01:33:39] 劉志遠: 不過剩階段
[01:33:40] 曼奇: 我覺得就是擔憂未來的這種擔憂
[01:33:43] 曼奇: 然後迎來更多的監管
[01:33:45] 曼奇: 包括這個什麼約束目前還不是這個業界的主要
[01:33:48] 曼奇: 是的
[01:33:49] 劉志遠: 這其實比較少數派的事情
[01:33:51] 曼奇: 對
[01:33:51] 曼奇: 所以我也沒什麼好擔憂的
[01:33:54] 劉志遠: 我們就努力往前走就好了
[01:33:56] 曼奇: 最後想問的是
[01:33:57] 曼奇: 未來一年內
[01:33:57] 曼奇: 你們有什麼想驗證的問題嗎
[01:33:59] 曼奇: 可能祭福上的話
[01:34:00] 蕭朝軍: 還是就是
[01:34:01] 蕭朝軍: I-O到底能走多遠
[01:34:03] 蕭朝軍: 然後能走多遠
[01:34:04] 曼奇: 對
[01:34:04] 曼奇: 然後自入學習到底應該以那樣的
[01:34:07] 蕭朝軍: 前往的方式存在
[01:34:08] 蕭朝軍: 我覺得這是一個問題
[01:34:09] 蕭朝軍: 然後可能未來一年
[01:34:11] 蕭朝軍: 我們還得再思考了一個新的問題
[01:34:12] 蕭朝軍: 就是最早到來的那個AGA
[01:34:14] 蕭朝軍: 它的形態會是什麼樣
[01:34:16] 蕭朝軍: 我覺得應該會要在未來一到兩年
[01:34:17] 蕭朝軍: 這內逐漸的呈現出來
[01:34:19] 蕭朝軍: 最早到來的AGA的形態
[01:34:20] 曼奇: 或者是什麼樣子是什麼
[01:34:21] 曼奇: 比如說哪一天
[01:34:22] 蕭朝軍: OPENY 宣稱自己
[01:34:24] 蕭朝軍: 模型進到了AGA這個水平
[01:34:26] 蕭朝軍: 你覺得它會是什麼樣的一個模型
[01:34:27] 蕭朝軍: 或者覺得它是會怎樣的一個系統
[01:34:29] 蕭朝軍: 我覺得這是一個很值得思考的問題
[01:34:31] 蕭朝軍: 因為大家現在覺得這種對話
[01:34:33] 蕭朝軍: 已經很通用了
[01:34:34] 蕭朝軍: 但我覺得還遠不到
[01:34:35] 蕭朝軍: 其實我現在感覺有在思考這些問題
[01:34:37] 蕭朝軍: 比如說大家現在會說
[01:34:39] 蕭朝軍: AGA未來就要做的那時候
[01:34:41] 蕭朝軍: 能做的事情
[01:34:42] 蕭朝軍: 但是其實這件事情我覺得是不對的
[01:34:44] 蕭朝軍: 那其實很簡單一個問題
[01:34:45] 蕭朝軍: 就是自動駕駛
[01:34:46] 蕭朝軍: 一件事情未來可能有大模型來做
[01:34:48] 蕭朝軍: 我覺得顯然不可能
[01:34:50] 它一定會是一個非常專用的想模型
[01:34:52] 蕭朝軍: 做的會比大模型要更好
[01:34:54] 蕭朝軍: 我們會說
[01:34:55] 蕭朝軍: 因為這個大模型做不了
[01:34:56] 蕭朝軍: 比如說人類會做駕駛的事情
[01:34:58] 蕭朝軍: 說它不是AGA
[01:34:59] 蕭朝軍: 我覺得其實肯定也不能這麼定論
[01:35:01] 蕭朝軍: 所以說那AGA到底是什麼
[01:35:03] 蕭朝軍: 所以說剛才那個定義
[01:35:04] 蕭朝軍: 就是人類能做的事情
[01:35:05] 蕭朝軍: 它都能做是錯的
[01:35:07] 蕭朝軍: 其實你就得思考它的形態到底會是什麼樣
[01:35:09] 蕭朝軍: 然後我現在一個感覺就是
[01:35:12] 蕭朝軍: 未來的一個可能AGA的形態
[01:35:14] 蕭朝軍: 是一個生產AI的AI
[01:35:17] 蕭朝軍: 比如說它是一個
[01:35:19] 蕭朝軍: AGA可能就是說
[01:35:20] 蕭朝軍: 之前那個定義可能是
[01:35:21] 蕭朝軍: 它是一個TUC的一個定義
[01:35:23] 蕭朝軍: 但是我覺得AGA應該是一個Tubi的一個定義
[01:35:26] 蕭朝軍: 就是說未來我們現在
[01:35:27] 蕭朝軍: 你可以看到我們會有很多的公司
[01:35:29] 蕭朝軍: 很多的這種需要僱傭人
[01:35:31] 蕭朝軍: 去做很多的生產勞動
[01:35:33] 蕭朝軍: 但是我覺得未來的可能就是
[01:35:35] 蕭朝軍: 會有AGA來幫人家的事情
[01:35:37] 蕭朝軍: 就是我跟他說
[01:35:38] 蕭朝軍: 現在我想要有一個自動駕駛的模型
[01:35:41] 蕭朝軍: 然後跟AGA說
[01:35:42] 蕭朝軍: AGA開始就是去構建這個模型
[01:35:45] 蕭朝軍: 比如說生產數據 收集數據
[01:35:47] 蕭朝軍: 然後去不斷的優化這個模型
[01:35:49] 蕭朝軍: 讓它不斷能夠在那個特定的算量上跑起來
[01:35:52] 蕭朝軍: 這樣的話我們不需要AGA
[01:35:53] 蕭朝軍: 啥都會
[01:35:54] 蕭朝軍: 但它需要會的一個最合理的事情就是生產而已
[01:35:57] 蕭朝軍: 對 但是這個也不是一個這種答案
[01:35:59] 蕭朝軍: 但是我覺得這個問題
[01:36:01] 蕭朝軍: 一定得再不會來一到兩年來去思考
[01:36:03] 蕭朝軍: 這樣才能夠分好
[01:36:04] 蕭朝軍: 就說我們說的什麼自主學習
[01:36:06] 蕭朝軍: 這些都是能力
[01:36:07] 蕭朝軍: 能力它距離應該在寫了一個產品
[01:36:09] 蕭朝軍: 或者什麼樣一個模型上呈現
[01:36:11] 蕭朝軍: 或者是一個什麼用途上去做的
[01:36:13] 曼奇: 對
[01:36:14] 蕭朝軍: 我覺得這是一個很重要的問題
[01:36:16] 劉志遠: 因為朝廷說的那個
[01:36:17] 劉志遠: 就是你從歷史上來看
[01:36:19] 劉志遠: 就是工業革命的一個
[01:36:20] 劉志遠: 這個特徵就是用機器制造機器
[01:36:23] 劉志遠: 就是你不是用手工制造機器
[01:36:25] 劉志遠: 用機器制造機器
[01:36:26] 劉志遠: 是機器大生產的一個標誌
[01:36:29] 那其實未來的AI大生產
[01:36:31] 劉志遠: 就是你到了智能時代
[01:36:32] 劉志遠: 真正的各行各業都能夠把這個AI
[01:36:35] 劉志遠: 能夠用起來
[01:36:36] 劉志遠: 那其實是用AI制造AI是它的本質
[01:36:38] 劉志遠: 那麼在這個裡面其實很重要的一個點
[01:36:40] 劉志遠: 其實就是咱們說的這個自主學習
[01:36:43] 劉志遠: 因為自主學習就是意味著說
[01:36:44] 劉志遠: 這個模型它可以自己去實現一個
[01:36:48] 劉志遠: 在某種環境裡面的這種成長
[01:36:50] 劉志遠: 其實我覺得是一種
[01:36:51] 劉志遠: 用AI制造AI的一個出型了
[01:36:54] 劉志遠: 就像這個AI本身它會自我的
[01:36:56] 劉志遠: 其聲它自己的這個能力
[01:36:58] 劉志遠: 所以其實這件事
[01:36:59] 劉志遠: 我覺得還是特別期待這個最早明年
[01:37:02] 劉志遠: 最晚後年
[01:37:03] 劉志遠: 然後能夠有一個這種學習能力的
[01:37:05] 劉志遠: 這麼一個提升
[01:37:06] 劉志遠: 那主老師您自己想認真的一個問題是什麼
[01:37:08] 曼奇: 就是我剛才說的這個
[01:37:10] 劉志遠: 就是自主學習
[01:37:11] 劉志遠: 自主學習
[01:37:13] 劉志遠: 明年就會出現自主學習
[01:37:14] 曼奇: 我這麼感覺業界現在對怎麼做這個事
[01:37:17] 並沒有很多信號
[01:37:18] 曼奇: 這才是很有意思的點
[01:37:20] 劉志遠: 就是剛才其實朝鮮已經提到了
[01:37:22] 劉志遠: 就是Reward的和Nierin
[01:37:23] 劉志遠: 就是其實現在已經是在相關的
[01:37:26] 劉志遠: 一些特定的領域
[01:37:28] 劉志遠: 其實是可以看到
[01:37:29] 劉志遠: 就是如何去通過設計
[01:37:32] 劉志遠: 特殊的這種Reward的這種機制
[01:37:35] 劉志遠: 然後來實現一種持續的這種學習
[01:37:38] 劉志遠: 就不只是停留在數學
[01:37:40] 劉志遠: 待馬這兩個領域
[01:37:41] 劉志遠: 會開始扩展到其他領域
[01:37:43] 劉志遠: 最新扩展到了什麼領域
[01:37:44] 曼奇: 從屬於和待馬
[01:37:45] 曼奇: 就是可以通過形式化
[01:37:47] 劉志遠: 或者是通過相關的這種方式
[01:37:49] 劉志遠: 來提供Reward的這些領域
[01:37:51] 劉志遠: 比如最近像物理等等的這些領域
[01:37:53] 劉志遠: 其實有非常大的進展
[01:37:56] 劉志遠: 何先生還是那幾個學科吧
[01:37:58] 蕭朝軍: 學科
[01:37:58] 蕭朝軍: 對的離課那些進展
[01:38:00] 蕭朝軍: 而且我其實覺得就是
[01:38:01] 蕭朝軍: 現在我們看不到任何苗頭
[01:38:02] 蕭朝軍: 但是你還想記住發展
[01:38:04] 蕭朝軍: 它是一個有個死給領的過程
[01:38:06] 蕭朝軍: 因為你看到
[01:38:07] 蕭朝軍: 這個預生的模型出現
[01:38:08] 蕭朝軍: 到真的廣播一人
[01:38:10] 蕭朝軍: 直到Gb3到CityPT
[01:38:11] 蕭朝軍: 其實有很長的一段死給領的過程
[01:38:13] 蕭朝軍: 當然最近因為大家觀眾的多
[01:38:15] 蕭朝軍: 這個死給領的過程迅速的在縮短
[01:38:17] 蕭朝軍: 但是我覺得明年自我學習
[01:38:19] 蕭朝軍: 已經是已經會成為大家能看到的一個出現
[01:38:21] 蕭朝軍: 比如說它可能沒有那麼飯化
[01:38:23] 蕭朝軍: 但是如果在某個任務上
[01:38:25] 蕭朝軍: 它能夠出現變得更好
[01:38:26] 蕭朝軍: 所以這個明年其實說的是
[01:38:27] 曼奇: 它可能是這個信號出現苗頭出現
[01:38:29] 曼奇: 到不是說被我們被大眾接受的
[01:38:31] 曼奇: 不止啊
[01:38:32] 劉志遠: 你想之所以能夠成為Reward的
[01:38:34] 劉志遠: Nierry也就是說
[01:38:36] 劉志遠: ARO這件事情開始在各行各業
[01:38:38] 劉志遠: 大家通過人力
[01:38:40] 劉志遠: 然後來幫著去構建這個Reward的
[01:38:42] 劉志遠: 道理從這來是比較好的
[01:38:44] 劉志遠: 那麼再進一步
[01:38:45] 劉志遠: 這個人力越來越少
[01:38:47] 劉志遠: 那麼其實就可以形成一個真正的
[01:38:49] 劉志遠: 這種各樣自主的這麼一個模式
[01:38:51] 劉志遠: 這個董事本來也是一個
[01:38:52] 劉志遠: 亮變產生之變的過程
[01:38:54] 劉志遠: 你就當然可以想像一下
[01:38:55] 劉志遠: 現在的ARO已經開始在非常廣泛的
[01:38:58] 劉志遠: 在各個領域來嘗試它的這麼一個應用了
[01:39:01] 劉志遠: 然後明年可能會看到數學代碼
[01:39:03] 曼奇: 之外的更多的這個場景的成果
[01:39:05] 曼奇: 對 然後慢慢的再能夠回去到一起
[01:39:08] 劉志遠: 它現在先飯花的是環境
[01:39:10] 劉志遠: 然後如果我們把這些所有的環境
[01:39:12] 劉志遠: 都讓一個模型
[01:39:13] 劉志遠: 然後去進行這個學習
[01:39:15] 劉志遠: 那是不是它能夠掌握更高層的
[01:39:18] 劉志遠: 這種自主判斷這個Reward的能力
[01:39:20] 劉志遠: 其實回到我們中間討論過的
[01:39:21] 曼奇: 很多問題
[01:39:22] 曼奇: 對 所以我覺得這個
[01:39:23] 劉志遠: 是一個很自然的一個過程
[01:39:24] 劉志遠: 好的好的 那今天非常感謝
[01:39:26] 曼奇: 劉柱元老師還有小朋友如我是
[01:39:29] 曼奇: 做客晚點聊
[01:39:30] 曼奇: 然後分享了就是
[01:39:31] 曼奇: 清華還有面臂合作去研究了這個
[01:39:35] 曼奇: 密度法則
[01:39:35] 曼奇: 它皆是一個什麼樣的
[01:39:37] 曼奇: 業界發展的規律
[01:39:38] 曼奇: 以及這樣一個去世可能會幫助
[01:39:40] 曼奇: 大家怎麼去統一接下來的目標
[01:39:42] 曼奇: 在一個就是更高效的方式
[01:39:45] 曼奇: 去獲得更多的智能
[01:39:46] 曼奇: 然後我們也延長了到了就是
[01:39:48] 曼奇: 面臂自己在這個密度法則的
[01:39:50] 曼奇: 支援下的一些模型研發
[01:39:52] 曼奇: 還有業界落地的探索
[01:39:54] 曼奇: 以及就是說這個密度法則
[01:39:56] 曼奇: 對更廣泛的行業有什麼影響
[01:39:58] 曼奇: 那謝謝兩位參加我們的節目
[01:40:00] 曼奇: 各位拜拜
[01:40:01] 蕭朝軍: 謝謝
[01:41:02] 曼奇: 長輸出也變得很重要
[01:41:04] 曼奇: 十個十個月
[01:41:05] 曼奇: 朝軍這一次提到
[01:41:06] 曼奇: 他認為長輸出的重要性
[01:41:08] 曼奇: 還是沒有成為一個行業共識
[01:41:10] 曼奇: 是一個急續優化的方向
[01:41:12] 曼奇: 1037即使對注意力
[01:41:13] 曼奇: 機制發展的一個很好的輸理和科普
[01:41:16] 曼奇: 當時的一些思考也必不過時
[01:41:18] 曼奇: 比如內氣最後
[01:41:19] 曼奇: 我們也討論了Fossign
[01:41:21] 曼奇: 這在今年也成為了一個
[01:41:22] 曼奇: 更明確的發展方向
[01:41:26] 本期節目就到這裡
[01:41:27] 曼奇: 歡迎收聽
[01:41:28] 曼奇: 如果你對今天聊的話
[01:41:29] 曼奇: 有關查好奇會疑問
[01:41:30] 曼奇: 歡迎在評論區分享想法
[01:41:32] 曼奇: 這也會成為我們節目的一部分
[01:41:34] 曼奇: 讓整個討論更完整
[01:41:35] 曼奇: 也可以把我們的節目
[01:41:36] 曼奇: 分享給對這個話題感興趣的朋友
[01:41:38] 曼奇: 歡迎推薦更多你想聽的主題和嘉賓
[01:41:41] 曼奇: 你可以從小宇宙
[01:41:42] 曼奇: 蘋果泡的Costre等曲到
[01:41:43] 曼奇: 關注晚點聊Late Talk
[01:41:45] 曼奇: 也歡迎關注我們的公眾號
[01:41:46] 曼奇: 晚點Late Post
[01:41:47] 曼奇: 下期再見