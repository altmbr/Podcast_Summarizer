# From "Large and Strong" to "Small and Strong": Density Laws, RL Scaling Laws, and the Distributed Future of Intelligence

**Podcast:** 晚点聊 LateTalk (Investigative Journalism)
**Date:** 2025-12-11
**Video ID:** 693b46ec79debc3231f1d7bf
**Video URL:** https://www.xiaoyuzhoufm.com/episode/693b46ec79debc3231f1d7bf

---

[00:00:00] 歡迎收聽完點了 我是曼奇
[00:00:07] SPEAKER_00: 今天的嘉賓是清華大學的劉志遠和蕭朝軍
[00:00:10] SPEAKER_00: 劉志遠是清華計算機器副教授和面臂智能的首席科學家
[00:00:14] SPEAKER_00: 蕭朝軍現在在清華做博士後
[00:00:16] SPEAKER_00: 也是面臂命力CPM系列的文本模型負責人
[00:00:20] SPEAKER_00: 他們的團隊剛在11月的自然雜誌機器學習紙看上
[00:00:24] SPEAKER_00: 發表了封面文章
[00:00:25] SPEAKER_00: Dancing Law of LLMS大模型的密度法則
[00:00:28] SPEAKER_00: 所謂密度就是用更少的算例和數據
[00:00:31] SPEAKER_00: 獲得相當乃至更多的智能
[00:00:33] SPEAKER_00: 我們討論了密度法則研究的原則
[00:00:35] SPEAKER_00: 雖然你說高興是不言自明的
[00:00:37] SPEAKER_01: 但是從事實上來講
[00:00:39] SPEAKER_01: 2022年拆的這個體型星期以後
[00:00:41] SPEAKER_01: 其實在全球範圍內大的話語體系
[00:00:44] SPEAKER_01: 其實是給你了
[00:00:45] SPEAKER_01: 就是在23年初的時候
[00:00:47] SPEAKER_01: 曾經有過一些這個巨頭說
[00:00:50] SPEAKER_01: 這個世界上不需要超過幾個大夢形
[00:00:53] SPEAKER_01: 1943年的時候
[00:00:54] SPEAKER_01: IBM的董事長曾經說
[00:00:56] SPEAKER_01: 全球不需要超過五台主機
[00:00:57] SPEAKER_01: 也展開了了業界提升模型能力密度的具體做法
[00:01:01] SPEAKER_00: 因為價格數據
[00:01:03] SPEAKER_02: 然後學習算法
[00:01:04] SPEAKER_02: 然後軟硬硬體
[00:01:06] SPEAKER_02: 就是英法四個層面
[00:01:07] SPEAKER_02: 所以我要就是四個方面我們
[00:01:09] SPEAKER_02: 應該可以看到現在市面上有很多相關的工作
[00:01:12] SPEAKER_02: 而在往後
[00:01:13] SPEAKER_00: 更大的密度提升
[00:01:14] SPEAKER_00: 可能需要一些全新方法
[00:01:16] SPEAKER_00: 強化學習做到現在
[00:01:17] SPEAKER_02: 其實大家還沒有解決的一個問題
[00:01:19] SPEAKER_02: 是強化學習的Skating的問題
[00:01:21] SPEAKER_02: 它現在有兩種技術路線
[00:01:23] SPEAKER_02: 在流程源的設想中
[00:01:24] SPEAKER_00: 未來更高密度的模型
[00:01:26] SPEAKER_00: 會支持每個人在端策
[00:01:27] SPEAKER_00: 擁有專屬大模型
[00:01:29] SPEAKER_00: 智能會分布式的存在
[00:01:31] SPEAKER_00: 比如說你的眼睛也好
[00:01:32] SPEAKER_01: 甚至你的耳機
[00:01:33] SPEAKER_01: 你的手錶
[00:01:34] SPEAKER_01: 各種各樣的這種智能的這種硬件
[00:01:36] SPEAKER_01: 它完全是可以有一個共同的一個
[00:01:39] SPEAKER_01: 提供智能服務的一個中端
[00:01:41] SPEAKER_01: 就有點像家庭裡面的NAS
[00:01:43] SPEAKER_01: 只是說你是一個變形的
[00:01:45] SPEAKER_01: 可以跟著你走的這麼一個NAS
[00:01:47] SPEAKER_01: 它是一個典型的技術樂觀主義者
[00:01:49] SPEAKER_00: 關於未來你有什麼擔心的理統嗎
[00:01:51] SPEAKER_00: 我擔心的地方是
[00:01:52] SPEAKER_01: 我們人類自己
[00:01:54] SPEAKER_01: 會舒服我們自己的潛進的步伐
[00:01:57] SPEAKER_01: 像國文章和討論的鏈接
[00:01:58] SPEAKER_00: 我會貼在Shoe Loats裡
[00:02:00] SPEAKER_00: 下面我們正式進入本期節目吧
[00:02:04] 今天很高興的邀請到了
[00:02:05] SPEAKER_00: 清華計算機器的劉志源老師
[00:02:07] SPEAKER_00: 和他之前的博士生
[00:02:09] SPEAKER_00: 現在清華的博士後
[00:02:10] SPEAKER_00: 蕭朝軍
[00:02:11] SPEAKER_00: 朝軍今年初野做科了萬頂療
[00:02:13] SPEAKER_00: 和汪玉老師的學生
[00:02:14] SPEAKER_00: 復天女一起分享了
[00:02:16] SPEAKER_00: 注意力機制
[00:02:17] SPEAKER_00: 尤其是吸收注意力的改進
[00:02:18] SPEAKER_00: 這個也和今天的話題有關
[00:02:20] SPEAKER_00: 我們之後可以展開
[00:02:21] SPEAKER_00: 兩位可以先後
[00:02:22] SPEAKER_00: 我們聽有簡單的大招呼
[00:02:23] SPEAKER_00: 大家好我是劉志源
[00:02:24] SPEAKER_02: 大家好我是蕭朝軍
[00:02:26] SPEAKER_02: 然後現在做文問
[00:02:27] SPEAKER_02: 加個五方面的研究
[00:02:29] SPEAKER_02: 好了密度法則之前
[00:02:30] SPEAKER_00: 因為最近正好
[00:02:30] SPEAKER_00: 趕上就是行業裡有很多新的模型
[00:02:33] SPEAKER_00: 包括GBT5.1
[00:02:34] SPEAKER_00: Grocks1
[00:02:35] SPEAKER_00: Jemleye3
[00:02:36] SPEAKER_00: 還有最近也剛發了Cloud Office 5
[00:02:39] SPEAKER_00: 我們可以先從這些行業動向來了
[00:02:40] SPEAKER_00: 就是兩位最近在這些新的模型
[00:02:43] SPEAKER_00: 上就是你們看到的
[00:02:44] SPEAKER_00: 比較有意思的部分
[00:02:45] SPEAKER_00: 或是說亮點是什麼
[00:02:46] SPEAKER_00: 我其實感覺上其實他這些東西
[00:02:49] SPEAKER_02: 我覺得只要有兩個很明顯的趨勢
[00:02:51] SPEAKER_02: 第一個趨勢
[00:02:52] SPEAKER_02: 這就還是大家今年
[00:02:53] SPEAKER_02: 就應該是所有大模型
[00:02:54] SPEAKER_02: 從業者都關注到了
[00:02:55] SPEAKER_02: 一個智能體化
[00:02:56] SPEAKER_02: 其實你看到其實所有的這些模型
[00:02:58] SPEAKER_02: 他在智能體
[00:02:59] SPEAKER_02: 任務上都有了非常出色的表現
[00:03:01] SPEAKER_02: 所以直觀體現
[00:03:02] SPEAKER_02: 就是大家這些模型
[00:03:03] SPEAKER_02: 所有發文之後
[00:03:04] SPEAKER_02: 大家都會在這個
[00:03:06] SPEAKER_02: 代碼任務上去測試
[00:03:08] SPEAKER_02: 說他能幫我解決多少八個了
[00:03:10] SPEAKER_02: 這是第一個大家很明顯的趨勢
[00:03:12] SPEAKER_02: 第二個趨勢
[00:03:13] SPEAKER_02: 我其實是從Nanu
[00:03:15] SPEAKER_02: Nanu和Nobuk
[00:03:17] SPEAKER_02: Jemleye支持了這些任務上
[00:03:20] SPEAKER_02: 然後觀察到的
[00:03:21] SPEAKER_02: 所以你可以看到他相比於
[00:03:23] SPEAKER_02: 他模型不同的一個點
[00:03:24] SPEAKER_02: 在於他現在深層突變
[00:03:27] SPEAKER_02: 深層的文字會非常的精準
[00:03:30] SPEAKER_02: 就是在之前的Defroamedo
[00:03:31] SPEAKER_02: 然後一直看到的
[00:03:33] SPEAKER_02: 然後在SO
[00:03:35] SPEAKER_02: 就是 Open i 今年發文
[00:03:37] SPEAKER_02: 能思問文一下
[00:03:38] SPEAKER_02: 看到一些暑光
[00:03:39] SPEAKER_02: 然後Nanu不難了
[00:03:40] SPEAKER_02: 他把它進一步發展
[00:03:41] SPEAKER_02: 發揚光大了
[00:03:43] SPEAKER_02: 那麼有什麼我覺得
[00:03:43] SPEAKER_02: 這件事情會很有意思的
[00:03:45] SPEAKER_02: 他會讓我們看到了一個
[00:03:47] SPEAKER_02: 新的Skilling的方向
[00:03:49] SPEAKER_02: 我們今天一直在說Skilling
[00:03:51] SPEAKER_02: 然後文本模型的數據也枯減
[00:03:54] SPEAKER_02: 然後但是一直大家都會提
[00:03:56] SPEAKER_02: 那是不是我們可以用更多模型的數據
[00:03:58] SPEAKER_02: 但實際上是你一直沒做成
[00:04:00] SPEAKER_02: 就是大家用更多模型的數據
[00:04:02] SPEAKER_02: 他沒有辦法講這個模型真的
[00:04:04] SPEAKER_02: 就是只能水平有在一步的越深
[00:04:06] SPEAKER_02: 但我現在感覺上
[00:04:08] SPEAKER_02: 假設要通過這種土向深層
[00:04:10] SPEAKER_02: 然後統一的自回歸式的方式
[00:04:12] SPEAKER_02: 是不是有可能能實現一點的事情
[00:04:15] SPEAKER_02: 但這件事情也還沒有辦法確定
[00:04:17] SPEAKER_02: 因為沒有幾乎細節
[00:04:18] SPEAKER_02: 對
[00:04:18] SPEAKER_02: 因為不知道這個Jemleye
[00:04:19] SPEAKER_00: 三就是怎麼實現的
[00:04:20] SPEAKER_00: 對
[00:04:21] SPEAKER_00: 對
[00:04:22] SPEAKER_00: 對
[00:04:22] SPEAKER_00: 然後他是
[00:04:23] SPEAKER_02: 比如說是一種前面上的優化
[00:04:25] SPEAKER_02: 還是說他真的一體化模型上的優化
[00:04:27] SPEAKER_02: 可能這個
[00:04:28] SPEAKER_02: 等待一個兩一到兩個別我們可以再看看
[00:04:31] SPEAKER_02: 我會覺得
[00:04:32] SPEAKER_01: 就是最近這六年的時間
[00:04:34] SPEAKER_01: 其實是一個
[00:04:35] SPEAKER_01: 就是高速發展的一個結段
[00:04:37] SPEAKER_01: 就基本上我現在會看到的
[00:04:39] SPEAKER_01: 就是咱們幾乎每周的進展
[00:04:41] SPEAKER_01: 頂得上我大概我讀研究生的時候
[00:04:44] SPEAKER_01: 一年的進展
[00:04:45] SPEAKER_01: 每天都會各種吐挑
[00:04:47] SPEAKER_01: 那我想就是這裡面
[00:04:49] SPEAKER_01: 在我來看
[00:04:50] SPEAKER_01: 就是大致可以劃分為兩個大的方面
[00:04:53] SPEAKER_01: 就一個方面
[00:04:54] SPEAKER_01: 其實我把它總結位
[00:04:56] SPEAKER_01: 就是能力更強
[00:04:57] SPEAKER_01: 就是要這個模型
[00:04:59] SPEAKER_01: 它能力變得越來越強
[00:05:00] SPEAKER_01: 因為我們其實可以看到
[00:05:02] SPEAKER_01: 就是從2018年預訓練模型出現之後
[00:05:06] SPEAKER_01: 其實大概是美國幾年的時間
[00:05:08] SPEAKER_01: 就會讓這個模型的能力
[00:05:09] SPEAKER_01: 有一個飛躍
[00:05:11] SPEAKER_01: 從2022年底拆的GVT出現
[00:05:14] SPEAKER_01: 那它通過Instrasion to you
[00:05:16] SPEAKER_01: 讓這個模型能聽懂人的指令
[00:05:18] SPEAKER_01: 2024年底2025年出通過這種
[00:05:21] SPEAKER_01: 大規模強化學習
[00:05:22] SPEAKER_01: 能夠讓這個模型學會深度思考
[00:05:24] SPEAKER_01: 那麼其實我們會看到
[00:05:26] SPEAKER_01: 這樣一條非常清晰的出現
[00:05:28] SPEAKER_01: 就是這個模型在變得越來越同有
[00:05:31] SPEAKER_01: 越來越接近
[00:05:32] SPEAKER_01: 甚至是超越我們人類的這個智能的水平
[00:05:34] SPEAKER_01: 那同時的話
[00:05:35] SPEAKER_01: 就是我和朝軍
[00:05:37] SPEAKER_01: 我們團隊所發表的這篇論文
[00:05:40] SPEAKER_01: 其實想給大家呈現另外一條出現
[00:05:42] SPEAKER_01: 其實就是能效更高
[00:05:44] SPEAKER_01: 也就是說
[00:05:45] SPEAKER_01: 我們會看到過去六年的時間
[00:05:48] SPEAKER_01: 就是大家經常提大模型
[00:05:51] SPEAKER_01: 它的發展的邏輯是規模法則
[00:05:54] SPEAKER_01: 那麼其實規模法則我理解
[00:05:56] SPEAKER_01: 就是它背後
[00:05:57] SPEAKER_01: 其實是告訴我們
[00:05:59] SPEAKER_01: 我們能夠找到一條這種基於大數據
[00:06:02] SPEAKER_01: 加大算力
[00:06:03] SPEAKER_01: 然後加大參數
[00:06:04] SPEAKER_01: 這樣的一條通用的智能的解決方案
[00:06:07] SPEAKER_01: 那麼在這樣的解決方案下
[00:06:08] SPEAKER_01: 只要我有足夠多數據
[00:06:10] SPEAKER_01: 有足夠多算力
[00:06:11] SPEAKER_01: 然後我訓練足夠大的模型
[00:06:13] SPEAKER_01: 那我就可以讓這個模型能力
[00:06:15] SPEAKER_01: 可以持續地提升
[00:06:16] SPEAKER_01: 就這個是規模法則的這個邏輯
[00:06:19] SPEAKER_01: 但是我們會說
[00:06:20] SPEAKER_01: 規模法則同時給我們帶來的
[00:06:22] SPEAKER_01: 非常大的問題
[00:06:23] SPEAKER_01: 其實就是你模型勳得更大之後
[00:06:25] SPEAKER_01: 其實意味著它的訓練成本
[00:06:27] SPEAKER_01: 和它的使用成本
[00:06:28] SPEAKER_01: 都會相應得變得更高
[00:06:30] SPEAKER_01: 它其實基本上是一個現心
[00:06:32] SPEAKER_01: 是一個超現心增長的一個狀態
[00:06:35] SPEAKER_01: 所以我們會認為
[00:06:36] SPEAKER_01: 就是像歷史上的任何一個對
[00:06:38] SPEAKER_01: 全人類都產生深遠影響的這種科技
[00:06:41] SPEAKER_01: 那麼它一定是要追求一個更加高效的
[00:06:43] SPEAKER_01: 整個發展的這個模式
[00:06:45] SPEAKER_01: 也就是說一旦你找到了這條通用的
[00:06:48] SPEAKER_01: 這種技術方案之後
[00:06:49] SPEAKER_01: 你一定要讓這個技術方案的方方面面
[00:06:52] SPEAKER_01: 都更加的精進
[00:06:54] SPEAKER_01: 變成一個更加精密的一個體系
[00:06:56] SPEAKER_01: 來讓我們製造出來的這個產品
[00:06:59] SPEAKER_01: 它的成本更低
[00:07:00] SPEAKER_01: 它的能力更強
[00:07:01] SPEAKER_01: 這個其實是我們所提出來的Dancing Law
[00:07:04] SPEAKER_01: 其實想要告訴大家的
[00:07:06] SPEAKER_01: 這麼一個發展方向
[00:07:07] SPEAKER_01: 就我自己的一個感覺
[00:07:08] SPEAKER_00: 就是現在這些新的模型的發布
[00:07:10] SPEAKER_00: 我覺得就可能從外界來看
[00:07:12] SPEAKER_00: 它更多是您剛才說的
[00:07:13] SPEAKER_00: 這兩條主線裡面的性能的提升
[00:07:15] SPEAKER_00: 就是那類的這個提升
[00:07:17] SPEAKER_00: 然後在效率上
[00:07:18] SPEAKER_00: 就您說這個第二條主線
[00:07:20] SPEAKER_00: 你覺得最近業界的這些進展
[00:07:21] SPEAKER_00: 它有什麼體現
[00:07:23] SPEAKER_01: 因為本身這個AGI的發展的這個路徑
[00:07:27] SPEAKER_01: 還沒有結束
[00:07:28] SPEAKER_01: 我們會看到像GMTi
[00:07:30] SPEAKER_01: 最新的這些進展
[00:07:31] SPEAKER_01: 就會讓大家眼前一亮
[00:07:33] SPEAKER_01: 所以我會看到的
[00:07:35] SPEAKER_01: 就是像那這個能力更強
[00:07:38] SPEAKER_01: 這條主線是更有顯示度的
[00:07:40] SPEAKER_01: 另外一條線
[00:07:41] SPEAKER_01: 其實我覺得在全球來看
[00:07:43] SPEAKER_01: 應該說也有非常多的工作
[00:07:46] SPEAKER_01: 因為大家其實會意識到
[00:07:48] SPEAKER_01: 就是我花那麼多的算力
[00:07:51] SPEAKER_01: 那麼多的經費
[00:07:52] SPEAKER_01: 然後我們來去宣誓一個模型
[00:07:54] SPEAKER_01: 那麼必然導致這樣的一個模型
[00:07:56] SPEAKER_01: 其實是沒有辦法
[00:07:57] SPEAKER_01: 在更多的場合能夠用起來的
[00:08:00] SPEAKER_01: 對吧
[00:08:00] SPEAKER_01: 你只能夠通過API
[00:08:01] SPEAKER_01: 然後來去調用
[00:08:02] SPEAKER_01: 那這件事情其實本身是極大的限制
[00:08:06] SPEAKER_01: 人工智能的真正的這個普及
[00:08:08] SPEAKER_01: 智能本身應該是高度分布式的
[00:08:10] SPEAKER_01: 它是在各個領域
[00:08:12] SPEAKER_01: 都會有潛在的廣泛的這麼一個應用
[00:08:15] SPEAKER_01: 所以從另外一個角度看
[00:08:17] SPEAKER_01: 就是如何讓這個模型更加高效
[00:08:20] SPEAKER_01: 其實是我們真正實現科技革命
[00:08:24] SPEAKER_01: 就是這輪人工智能的科技革命
[00:08:26] SPEAKER_01: 非常重要的一個底層的驅動
[00:08:28] SPEAKER_01: 這個問題我可以再補充一下
[00:08:30] SPEAKER_02: 所以今年一個很重要的旋律圈
[00:08:32] SPEAKER_02: 可以看到的是效率的改進
[00:08:34] SPEAKER_02: 就是我們先談開源
[00:08:36] SPEAKER_02: 就可以看到現在開源界
[00:08:38] SPEAKER_02: 然後我想今年的minimax
[00:08:40] SPEAKER_02: 然後想簽文也做簽文next
[00:08:42] SPEAKER_02: Depthic就跟不用說了
[00:08:43] SPEAKER_02: 也今年V3.2也是做了細酥的騰訊
[00:08:46] SPEAKER_02: 包括我們也在做細酥的騰訊
[00:08:48] SPEAKER_02: 所以可以看到現在在加工上
[00:08:50] SPEAKER_02: 去提升文件效率
[00:08:51] SPEAKER_02: 已經成為一個非常共識的一件事情
[00:08:54] SPEAKER_02: 就是在開源界
[00:08:55] SPEAKER_02: 那你從閉源界
[00:08:56] SPEAKER_02: 你就可以也可以看到類似的事情
[00:08:59] SPEAKER_02: 就像這個open.i它也會去搞
[00:09:01] SPEAKER_02: 各種mini系列的模型
[00:09:03] SPEAKER_02: 然後已經在Gimli3火機前
[00:09:06] SPEAKER_02: 就是讓使有一個Gimli的Diffusion
[00:09:08] SPEAKER_02: 一個文文模型
[00:09:09] SPEAKER_02: 它宣稱它的文文形容的速度
[00:09:12] SPEAKER_02: 會比其他的模型快個強多很多倍
[00:09:14] SPEAKER_02: 因為它不開源
[00:09:15] SPEAKER_02: 所以你也不知道它距離是時間什麼
[00:09:17] SPEAKER_02: 但是你可以看到
[00:09:18] SPEAKER_02: 我覺得它內部也在追尋這種效率的提升
[00:09:22] SPEAKER_02: 因為其實對它們來說
[00:09:23] SPEAKER_02: 當然固然可以看到
[00:09:25] SPEAKER_02: 它們的資源量會比我們大很多
[00:09:27] SPEAKER_02: 但實際上就是任何的
[00:09:29] SPEAKER_02: 再多的資源面向AGA的投入
[00:09:32] SPEAKER_02: 現在都還是有限的
[00:09:33] SPEAKER_02: 我們都得把這件事做到
[00:09:34] SPEAKER_02: 所以就是從公眾來看的話
[00:09:36] SPEAKER_01: 就是能力提升是明顯
[00:09:39] SPEAKER_01: 但是按線應該是能效更高
[00:09:42] SPEAKER_01: 因為其實你可以看到的歷史上
[00:09:44] SPEAKER_01: 比如說我們說過去的幾十年的新西革命
[00:09:47] SPEAKER_01: 它的明顯其實讓我們看到的是
[00:09:50] SPEAKER_01: 我們的計算設備在逐漸的小型化
[00:09:53] SPEAKER_01: 我們原來的大型機
[00:09:55] SPEAKER_01: 到了80年代是小型機PC
[00:09:57] SPEAKER_01: 到了2000年之後是手機
[00:09:59] SPEAKER_01: 到10年之後是我們的智能手機
[00:10:02] SPEAKER_01: 我們的智能化的設備
[00:10:04] SPEAKER_01: 其實它就已經是讓我們的新一化的
[00:10:07] SPEAKER_01: 生活工作變得非常的方便
[00:10:09] SPEAKER_01: 這是主線
[00:10:10] SPEAKER_01: 這是明線
[00:10:11] SPEAKER_01: 但是它的按線其實是
[00:10:13] SPEAKER_01: 新年行業的快速發展
[00:10:15] SPEAKER_01: 就是摩爾定律
[00:10:16] SPEAKER_01: 所以其實我們之所以提這個密度法則
[00:10:20] SPEAKER_01: 其實是想尋找大模型的摩爾定律
[00:10:23] SPEAKER_01: 我們接下來正是掌握了一下這個密度法則
[00:10:26] SPEAKER_00: 因為我最開始看到這個研究的時候
[00:10:28] SPEAKER_00: 我自己其實有個以後的
[00:10:29] SPEAKER_00: 就是這個研究它揭示了
[00:10:32] SPEAKER_00: 規律或發展的賣落
[00:10:33] SPEAKER_00: 就是隨時間推移摩星的訓練和推理效率提高
[00:10:36] SPEAKER_00: 就好對業界來說是不言自民的一個現象
[00:10:40] SPEAKER_00: 當然你們這次是有一個非常定量的描述
[00:10:42] SPEAKER_00: 就可以講上就是這麼做的背景和意義
[00:10:45] SPEAKER_00: 以及說驅動這個研究背後的核心問題
[00:10:47] SPEAKER_00: 意識是什麼
[00:10:49] 我覺得第一個角度
[00:10:52] SPEAKER_01: 就是從人工智能發展的角度來講
[00:10:55] SPEAKER_01: 我們會認為它一定要追求高效
[00:10:57] SPEAKER_01: 追求高效它肯定不能夠是一個說口號而已
[00:11:02] SPEAKER_01: 說我就是要高效
[00:11:03] SPEAKER_01: 我們其實是要有一套體系
[00:11:05] SPEAKER_01: 就像你剛才問的
[00:11:07] SPEAKER_01: 到底是哪些因素會影響這個摩星的密度呢
[00:11:10] SPEAKER_01: 會影響我們的這個能效呢
[00:11:12] SPEAKER_01: 那麼從這個角度來講
[00:11:14] SPEAKER_01: 我覺得我們就達成了我們提出密度法則的
[00:11:17] SPEAKER_01: 這麼一個目標了
[00:11:18] SPEAKER_01: 也就是說我們要提出一個指標來
[00:11:20] SPEAKER_01: 然後讓這個指標成為一個客觀的事實
[00:11:23] SPEAKER_01: 能夠團結更多的研究者
[00:11:25] SPEAKER_01: 來去更細緻的 更定量的
[00:11:28] SPEAKER_01: 然後來去探索
[00:11:29] SPEAKER_01: 更高效的這麼一個實現的這麼一個方式
[00:11:32] SPEAKER_01: 那麼第二個呢就是
[00:11:34] SPEAKER_01: 雖然你說這個高效是不言自明的
[00:11:37] SPEAKER_01: 但是從事實上來講
[00:11:39] SPEAKER_01: 2022年 拆到這個體系星期以後
[00:11:42] SPEAKER_01: 其實在全球範圍內
[00:11:44] SPEAKER_01: 大的話語體系 其實是死給你了
[00:11:46] SPEAKER_01: 就幾乎所有的機構都會去說
[00:11:49] SPEAKER_01: 我只有更多的數據 更多的算力
[00:11:53] SPEAKER_01: 我才能遜更大的摩星
[00:11:55] SPEAKER_01: 我才能追求更強的能力
[00:11:57] SPEAKER_01: 這是從2022年一直到2024年底
[00:12:01] SPEAKER_01: 然後一直以來的這麼一個大的這麼一個話語
[00:12:04] SPEAKER_01: 那麼甚至說有 Open i 有音韋大
[00:12:07] SPEAKER_01: 然後他們也都是在去說
[00:12:09] SPEAKER_01: 其他人就不要去遜大摩星了
[00:12:12] SPEAKER_01: 對吧
[00:12:12] SPEAKER_01: 然後只有那些有10萬張卡的這些團隊
[00:12:16] SPEAKER_01: 然後10萬張卡的機構
[00:12:18] SPEAKER_01: 才有資格去遜大摩星
[00:12:20] SPEAKER_01: 然後你其他人就用就可以了
[00:12:22] SPEAKER_01: 他嘗試著去構建一個
[00:12:24] SPEAKER_01: 圍算利論的這麼一個話語的這麼一個體系
[00:12:29] SPEAKER_01: 那在這種情況下
[00:12:31] SPEAKER_01: 我們會認為就是 Dancing Law 密度法則
[00:12:34] SPEAKER_01: 我覺得也非常地重要
[00:12:36] SPEAKER_01: 那麼其實我想從今年初
[00:12:39] SPEAKER_01: Dipsyke V3其實會告訴大家
[00:12:41] SPEAKER_01: 其實我不需要那麼多算力
[00:12:43] SPEAKER_01: 當然絕對算力仍然很大
[00:12:45] SPEAKER_01: 但是絕對不是像
[00:12:47] SPEAKER_01: 很多的這些國際的企業和機構所宣稱的
[00:12:51] SPEAKER_01: 在我們來看
[00:12:53] SPEAKER_01: 就是我們其實是要有責任
[00:12:55] SPEAKER_01: 然後來去在整個這個人工智能發展的
[00:12:59] SPEAKER_01: 這麼一個過程中
[00:13:00] SPEAKER_01: 我們要把這個正確的發展方向
[00:13:03] SPEAKER_01: 能夠告訴大家
[00:13:04] SPEAKER_01: 因為其實就是你作為
[00:13:07] SPEAKER_01: 譬如說深入了解
[00:13:08] SPEAKER_01: 整個人工智能這個泉帽的
[00:13:11] SPEAKER_01: 可能會知道高效很重要
[00:13:13] SPEAKER_01: 但是就是說從很多很多人來講
[00:13:16] SPEAKER_01: 甚至包括角色層等等
[00:13:18] SPEAKER_01: 那他肯定很多時候
[00:13:20] SPEAKER_01: 他所接受的信息
[00:13:21] SPEAKER_01: 仍然會覺得規模法則是第一性原理
[00:13:24] SPEAKER_01: 我就得有10萬張卡
[00:13:26] SPEAKER_01: 我才有可能做
[00:13:27] SPEAKER_01: 同樣人工智能
[00:13:28] SPEAKER_01: 我在我來看就這件事情
[00:13:30] SPEAKER_01: 是非常錯誤的一個信號
[00:13:33] SPEAKER_00: 因為你剛才也提到
[00:13:34] SPEAKER_00: 其實這個
[00:13:35] SPEAKER_00: 它是有點像摩爾丁維的嗎
[00:13:36] SPEAKER_00: 我之前看新片戰爭的時候
[00:13:38] SPEAKER_00: 我覺得他裡面對摩爾丁維的描述
[00:13:39] SPEAKER_00: 也很精平
[00:13:40] SPEAKER_00: 就是他雖然叫定維
[00:13:41] SPEAKER_00: 但他實際上並不是一個物理規律
[00:13:44] SPEAKER_00: 他其實是一種敘事
[00:13:45] SPEAKER_00: 或者說一種共識
[00:13:46] SPEAKER_00: 他是指揮整個產業鏈
[00:13:48] SPEAKER_00: 和這個行業裡的很多環節
[00:13:50] SPEAKER_00: 然後大家有一個共同的目標
[00:13:53] SPEAKER_00: 是的
[00:13:53] SPEAKER_00: 所以摩爾丁維
[00:13:55] SPEAKER_01: 反正約定俗成了
[00:13:56] SPEAKER_01: 就叫摩爾丁維
[00:13:58] SPEAKER_01: 一開始團隊內部還要說叫密度定律
[00:14:01] SPEAKER_01: 後來我們就覺得
[00:14:02] SPEAKER_01: 在中文的這個華語提供內
[00:14:05] SPEAKER_01: 這個定律
[00:14:06] SPEAKER_01: 更多的跟一個物理規律可能會有關係
[00:14:08] SPEAKER_01: 所以我們就會覺得
[00:14:10] SPEAKER_01: 可能就不能夠叫定律
[00:14:12] SPEAKER_01: 因為其實在英文裡面
[00:14:13] SPEAKER_01: 這個Law
[00:14:14] SPEAKER_01: 它其實就是一個規律
[00:14:16] SPEAKER_01: 對吧
[00:14:16] SPEAKER_01: 它沒有說一定是物理定律的
[00:14:18] SPEAKER_01: 這麼一個層面
[00:14:20] SPEAKER_01: 但是在中文裡面
[00:14:21] SPEAKER_01: Law翻譯成定律的話
[00:14:22] SPEAKER_01: 就顯得特別高大
[00:14:24] SPEAKER_01: 就是它等於掌握萬事萬物的
[00:14:27] SPEAKER_01: 這麼一個背後的這麼一個法則
[00:14:29] SPEAKER_01: 或者規律
[00:14:29] SPEAKER_01: 所以我們就後來
[00:14:31] SPEAKER_01: 就是基本上會去提
[00:14:33] SPEAKER_01: 所謂的叫密度法則
[00:14:35] SPEAKER_01: 相當於說更體現
[00:14:37] SPEAKER_01: 這個Law本身它的這個含義
[00:14:39] SPEAKER_01: 所以我非常認可
[00:14:41] SPEAKER_01: 就是摩爾丁維和密度法則
[00:14:43] SPEAKER_01: 其實都是一種人工的自我實現
[00:14:46] SPEAKER_01: 摩爾丁維
[00:14:47] SPEAKER_01: 你如果去看
[00:14:48] SPEAKER_01: 就是摩爾在1965年
[00:14:50] SPEAKER_01: 它最原始的那篇文章
[00:14:52] SPEAKER_01: 其實它自己都沒有把自己
[00:14:54] SPEAKER_01: 成為是一個定律
[00:14:55] SPEAKER_01: 或者是一個Law
[00:14:56] SPEAKER_01: 其實是過了十幾年之後
[00:14:59] SPEAKER_01: 是由其他人說
[00:15:01] SPEAKER_01: 摩爾在1965年
[00:15:03] SPEAKER_01: 根據過去的十幾年總結的規律
[00:15:05] SPEAKER_01: 竟然在它之後
[00:15:07] SPEAKER_01: 還Work了這個十幾年
[00:15:09] SPEAKER_01: 所以就把它定義成了摩爾摩爾斯羅
[00:15:12] SPEAKER_01: 那麼甚至說在1970年之後
[00:15:15] SPEAKER_01: 然後又持續的這個影響整個這個行業
[00:15:19] SPEAKER_01: 那麼所以就是摩爾丁維
[00:15:21] SPEAKER_01: 它的影響力就越來越大
[00:15:23] SPEAKER_01: 就是我們人類社會
[00:15:25] SPEAKER_01: 通過工業化
[00:15:26] SPEAKER_01: 通過這種技術創新的
[00:15:28] SPEAKER_01: 這麼一種自我實現
[00:15:29] SPEAKER_01: 那我們對密度法則
[00:15:32] SPEAKER_01: 我們也認為它也是一種自我實現
[00:15:34] SPEAKER_01: 因為所有的追求密度更高
[00:15:37] SPEAKER_01: 因為你想摩爾丁維
[00:15:39] SPEAKER_01: 它也是要追求
[00:15:40] SPEAKER_01: 在這個芯片上
[00:15:42] SPEAKER_01: 把更多的電路
[00:15:43] SPEAKER_01: 給放進去
[00:15:44] SPEAKER_01: 密度法則本身就是你這個世界的
[00:15:47] SPEAKER_01: 這個上增的這麼一個趨勻而行呢
[00:15:50] SPEAKER_01: 它本身就是在通過紀錄創新
[00:15:52] SPEAKER_01: 來追求某一種內災的秩序
[00:15:54] SPEAKER_01: 然後把更多的內容
[00:15:56] SPEAKER_01: 然後給壓到一個更小的空間裡面去
[00:15:59] SPEAKER_01: 因為這個宇宙是大爆炸的
[00:16:00] SPEAKER_01: 上增的
[00:16:01] SPEAKER_01: 是不斷的擴散的
[00:16:03] SPEAKER_01: 我們讓它密度更高
[00:16:04] SPEAKER_01: 這件事情顯然是需要外力
[00:16:06] SPEAKER_01: 對於我們人類社會來講
[00:16:07] SPEAKER_01: 外力是什麼
[00:16:08] SPEAKER_01: 就是我們不斷的創新
[00:16:09] SPEAKER_01: 上這個知識
[00:16:11] SPEAKER_01: 能夠壓到一個更小的空間裡面
[00:16:13] SPEAKER_01: 不管是芯片也好
[00:16:14] SPEAKER_01: 模型也好
[00:16:15] SPEAKER_01: 未來一定是這個發展趨勢
[00:16:17] SPEAKER_01: 就剛才那個問題
[00:16:18] SPEAKER_02: 我也在補充說一下
[00:16:19] SPEAKER_02: 像人本身這個
[00:16:21] SPEAKER_02: 在自然的發展的共產黨中
[00:16:23] SPEAKER_02: 我們會講這個物競天則
[00:16:25] SPEAKER_02: 等中自然法則
[00:16:26] SPEAKER_02: 還其實並不是說
[00:16:27] SPEAKER_02: 它是一種物理規律
[00:16:28] SPEAKER_02: 但是就是說在天然的
[00:16:30] SPEAKER_02: 自然收線的場景下
[00:16:31] SPEAKER_02: 你就更好的遵循這個法則的
[00:16:34] SPEAKER_02: 那一批人
[00:16:35] SPEAKER_02: 它才能夠承諾下來
[00:16:36] SPEAKER_02: 然後繼續這個
[00:16:38] SPEAKER_02: 把你的基金衣上下去
[00:16:39] SPEAKER_02: 所以你可以看到
[00:16:40] SPEAKER_02: 人的腦子並沒有無限值的增長
[00:16:43] SPEAKER_02: 而是一直在一個有限的體積下
[00:16:45] SPEAKER_02: 就是在有限的能源消耗下
[00:16:47] SPEAKER_02: 讓我的質地不斷地提升
[00:16:48] SPEAKER_02: 所以其實它是我們的一個追求
[00:16:50] SPEAKER_02: 然後也是在有限自然下的
[00:16:52] SPEAKER_02: 一個可能你的一個必須品
[00:16:54] SPEAKER_02: 小君說的這個點特別有意思
[00:16:56] SPEAKER_01: 就是你要說
[00:16:57] SPEAKER_01: 尊循自然界
[00:16:59] SPEAKER_01: 就是生物群羅的這個
[00:17:01] SPEAKER_01: 所謂的物競天則的話
[00:17:03] SPEAKER_01: 說不定
[00:17:03] SPEAKER_01: 你可以設想就是
[00:17:05] SPEAKER_01: 模爾定律為什麼能夠
[00:17:06] SPEAKER_01: work可能也跟這個商業的
[00:17:08] SPEAKER_01: 互相在這種競爭的環境
[00:17:10] SPEAKER_01: 其實會有關係
[00:17:11] SPEAKER_01: 對吧
[00:17:12] SPEAKER_01: 你不追求用更低的成本
[00:17:14] SPEAKER_01: 實現更強的這個算力
[00:17:16] SPEAKER_01: 你的這個企業
[00:17:18] SPEAKER_01: 你的這個芯片就沒有競爭力
[00:17:19] SPEAKER_01: 對吧
[00:17:19] SPEAKER_01: 是我們內部的一個物競天則
[00:17:21] SPEAKER_01: 那你就可以設想
[00:17:22] SPEAKER_01: 未來的模型的發展
[00:17:24] SPEAKER_01: 肯定也是這個樣子
[00:17:25] SPEAKER_01: 如果你實現的模型的效率
[00:17:27] SPEAKER_01: 不如另外一家廠商
[00:17:29] SPEAKER_01: 那麼你就是競爭力
[00:17:30] SPEAKER_01: 就沒有那麼強
[00:17:32] SPEAKER_01: 本身密度法則和規模法則
[00:17:34] SPEAKER_01: 本身也不是說是對立的
[00:17:36] SPEAKER_01: 其實是像我像成的
[00:17:37] SPEAKER_01: 其實所有的一線的大模型團隊
[00:17:40] SPEAKER_01: 它一定是要去追求
[00:17:42] SPEAKER_01: 它的模型的高效的
[00:17:43] SPEAKER_01: 對
[00:17:44] SPEAKER_00: 這就是你也知道
[00:17:45] SPEAKER_00: 你不會在同一個模型裡面完成
[00:17:47] SPEAKER_01: 你會看到就是GPT的那些模型
[00:17:49] SPEAKER_01: 它的那個價格
[00:17:50] SPEAKER_01: API的價格
[00:17:51] SPEAKER_01: 其實就是在快速的下降
[00:17:53] SPEAKER_01: 就這個本身就是一個
[00:17:55] SPEAKER_01: 它內部也在不斷做事先的一個過程
[00:17:57] SPEAKER_01: 或者是說你換一個理解
[00:17:59] SPEAKER_02: 就是Skeleton它強調的是
[00:18:01] SPEAKER_02: 這個計算量和能力之間的
[00:18:04] SPEAKER_02: 這麼一個密度關係
[00:18:05] SPEAKER_02: 但是我們想其實強調的
[00:18:07] SPEAKER_02: 就是說這個密度件
[00:18:09] SPEAKER_02: 其實可能是那個協率
[00:18:11] SPEAKER_02: 就是說我用計算量換取的智能
[00:18:14] SPEAKER_02: 它就一個轉化率是要越來越高的
[00:18:16] SPEAKER_02: 所以可以看到
[00:18:17] SPEAKER_02: 像是說OPEN AI我們會說它智源很多
[00:18:19] SPEAKER_02: 但是它一直在講說
[00:18:20] SPEAKER_02: 我們已經不夠智源去做
[00:18:22] SPEAKER_02: 這個這個一些跟前央的事情
[00:18:25] SPEAKER_02: 包括伊莉亞
[00:18:26] SPEAKER_02: 為什麼會從OPEN AI走
[00:18:27] SPEAKER_02: 因為它覺得它拿不到
[00:18:28] SPEAKER_02: 足夠多的資源去追尋它的
[00:18:30] SPEAKER_02: 對 前央做Safe的一樣
[00:18:32] SPEAKER_00: 對
[00:18:32] SPEAKER_00: 所以其實核心問題
[00:18:33] SPEAKER_02: 就是大家還是智源所行
[00:18:35] SPEAKER_02: 就是其實放在OPEN AI來看
[00:18:36] SPEAKER_02: 它也因為智源所行產生了矛盾
[00:18:39] SPEAKER_02: 所以他們內部來說
[00:18:40] SPEAKER_02: 去追尋這種效率
[00:18:41] SPEAKER_02: 也是一個非常這個其智的事情
[00:18:43] SPEAKER_02: 回到你這次的這個研究
[00:18:45] SPEAKER_00: 它的一些核心動作
[00:18:46] SPEAKER_00: 會結論是什麼樣
[00:18:47] SPEAKER_00: 核心動作其實我們
[00:18:49] SPEAKER_02: 雖然名字叫密度法則
[00:18:50] SPEAKER_02: 講的是這個能力密度
[00:18:52] SPEAKER_02: 不斷的隨著時間指數翻倍
[00:18:54] SPEAKER_02: 但我其實覺得
[00:18:55] SPEAKER_02: 我們更想強調的是一個觀點
[00:18:57] SPEAKER_02: 就是我們要去追尋的是
[00:18:59] SPEAKER_02: 單位參數一下
[00:19:00] SPEAKER_02: 或者是單位開銷一下
[00:19:02] SPEAKER_02: 我們能夠轉化出來的智能的能力
[00:19:06] SPEAKER_02: 這件事情是我們這個相當就是
[00:19:08] SPEAKER_02: 立了一個FLY
[00:19:09] SPEAKER_02: 和立了一個目標
[00:19:10] SPEAKER_02: 說我們大家應該追尋這件事情
[00:19:12] SPEAKER_02: 而不是一位的說
[00:19:13] SPEAKER_02: 我這個Performance有多高
[00:19:14] SPEAKER_02: 因為我們這個工作
[00:19:15] SPEAKER_02: 其實是二四年底正式完成
[00:19:17] SPEAKER_02: 然後當然經過投稿到現在來說
[00:19:19] SPEAKER_02: 經過了會一年的時間才中重稿
[00:19:22] SPEAKER_02: 那其實在那個時間點的話
[00:19:23] SPEAKER_02: 大概其實都在講
[00:19:25] SPEAKER_02: 那我這個我要怎麼把這個模型
[00:19:28] SPEAKER_02: 殘酌再放大一點
[00:19:29] SPEAKER_02: 或者就計算量再放大一點
[00:19:30] SPEAKER_02: 那我們會認為
[00:19:31] SPEAKER_02: 再這樣一個發展模式下的話
[00:19:33] SPEAKER_02: 其實是不對的
[00:19:34] SPEAKER_02: 這樣的發展模式
[00:19:35] SPEAKER_02: 只會導這個問題
[00:19:36] SPEAKER_02: 就是所有人都在瘋狂的雜錢
[00:19:38] SPEAKER_02: 但是實際上
[00:19:39] SPEAKER_02: 你沒有這個把你的這個轉化率
[00:19:42] SPEAKER_02: 就是智能的轉化率提升
[00:19:44] SPEAKER_02: 那這樣的話
[00:19:44] SPEAKER_02: 遲早會引來的就是這個
[00:19:46] SPEAKER_02: 大家的資源已經不夠了
[00:19:48] SPEAKER_02: 然後就大家就引來了一些所謂的寒冬
[00:19:50] SPEAKER_02: 就像拉馬
[00:19:51] SPEAKER_02: 他那個時候發了拉馬3405幣
[00:19:54] SPEAKER_02: 那他假設下一步發展
[00:19:55] SPEAKER_02: 然後需要發了這個1000幣
[00:19:57] SPEAKER_02: 2000幣的這個重命模型
[00:19:59] SPEAKER_02: 我覺得這樣的話
[00:20:00] SPEAKER_02: 那顯得這個這個事情
[00:20:01] SPEAKER_02: 就這個AI的發展
[00:20:03] SPEAKER_02: 就沒有辦法持續下去
[00:20:04] SPEAKER_02: 這在那個時間點
[00:20:05] SPEAKER_02: 我們想要去做出去
[00:20:06] SPEAKER_02: 會文章的持續想
[00:20:07] SPEAKER_02: 全程說這個
[00:20:09] SPEAKER_02: 對於技術的發展而言
[00:20:10] SPEAKER_02: 追求這個密度
[00:20:11] SPEAKER_02: 或者去求這個智能轉化率
[00:20:13] SPEAKER_02: 實際上是我們這個
[00:20:15] SPEAKER_02: 這個技術發展
[00:20:16] SPEAKER_02: 一個很重要的一個主線
[00:20:18] SPEAKER_02: 所以據於說它是3.5個月
[00:20:20] SPEAKER_02: 反應倍還是一年反應倍
[00:20:22] SPEAKER_02: 我覺得這個數值本身
[00:20:23] SPEAKER_02: 可能反而沒有那麼的
[00:20:24] SPEAKER_02: 對你們現在發現的是平均3.3個月
[00:20:27] SPEAKER_00: 反應倍差不多100天
[00:20:28] SPEAKER_00: 但後來修正了一些
[00:20:29] SPEAKER_01: 就是在把這個2025年的
[00:20:32] SPEAKER_01: 新的模型發展之後
[00:20:33] SPEAKER_01: 就變成3.5個月了
[00:20:35] SPEAKER_01: 當然就是會有一些抖動
[00:20:37] SPEAKER_01: 核心的觀察就是是在加速的
[00:20:40] SPEAKER_01: 因為就是2023年
[00:20:41] SPEAKER_01: 應該是2023年1月份
[00:20:43] SPEAKER_01: 就是拆在GP出現前和後
[00:20:46] SPEAKER_01: 其實它的這個費增的速度
[00:20:48] SPEAKER_01: 其實是在加速
[00:20:49] SPEAKER_01: 它增加了這幾個月
[00:20:51] SPEAKER_01: 然後由原來就是24年
[00:20:53] SPEAKER_01: 它當時算出來的是3.3
[00:20:56] SPEAKER_01: 然後把2025年的加進來
[00:20:58] SPEAKER_01: 變成3.5
[00:20:59] SPEAKER_01: 我們理解這個可能就是一個正常的抖動
[00:21:01] SPEAKER_01: 因為你們是24年底做完這個研究嗎
[00:21:03] SPEAKER_00: 所以做開始的有這個想法
[00:21:05] SPEAKER_00: 做這個研究時間是更早的
[00:21:06] SPEAKER_00: 對其實我們去年年中的時候
[00:21:08] SPEAKER_02: 就已經在提
[00:21:09] SPEAKER_02: 但是我們其實叫密度
[00:21:10] SPEAKER_02: 支持密度
[00:21:11] SPEAKER_02: 支持密度24年年中
[00:21:13] SPEAKER_00: 核心是這個故事可能就要再搞早一點
[00:21:16] SPEAKER_01: 這個其實會找到2023年下半年
[00:21:19] SPEAKER_01: 那個時候其實就是你大概
[00:21:21] SPEAKER_01: 咱們如果拉回到2023年
[00:21:24] SPEAKER_01: 2023年初的時候是拆在GP出來
[00:21:26] SPEAKER_01: 震驚全球
[00:21:27] SPEAKER_01: 就是讓所有的人都發現
[00:21:29] SPEAKER_01: 哇大模型這麼好用
[00:21:31] SPEAKER_01: 所以才新起了就是內一螺
[00:21:33] SPEAKER_01: 特別大的浪潮
[00:21:34] SPEAKER_01: 就是國內的溜小虎
[00:21:35] SPEAKER_01: 然後這些相關的
[00:21:37] SPEAKER_01: 那其實在2023年的主選率是什麼呢
[00:21:40] SPEAKER_01: 就是從2023年初
[00:21:42] SPEAKER_01: 一直到2023年底
[00:21:43] SPEAKER_01: 大家的主要任務是
[00:21:45] SPEAKER_01: 追趕拆在GP的這個腐陷
[00:21:47] SPEAKER_01: 就是把這個模型能力腐陷出來
[00:21:49] SPEAKER_01: 那麼腐陷出來呢
[00:21:51] SPEAKER_01: 大概國內的一線團隊
[00:21:53] SPEAKER_01: 就是大致是在2023年的9月份10月份
[00:21:56] SPEAKER_01: 那段時間能夠腐陷出來
[00:21:59] SPEAKER_01: 因為我們那時候的估計就是
[00:22:01] SPEAKER_01: 國內跟這個國際的最先進的水平
[00:22:03] SPEAKER_01: 差不多也就是差個一年左右的時間
[00:22:06] SPEAKER_01: 基本上就都能腐陷出來
[00:22:07] SPEAKER_01: 因為主要的點其實就是那個
[00:22:09] SPEAKER_01: Instagram 求你
[00:22:11] SPEAKER_01: 腐陷出來了
[00:22:12] SPEAKER_01: 腐陷出來了之後呢
[00:22:13] SPEAKER_01: 其實這個時候就面臨著一個決策的問題
[00:22:17] SPEAKER_01: 這個OpenA其實是在2023年的4月份
[00:22:20] SPEAKER_01: 發布了GP4
[00:22:21] SPEAKER_01: 所以呢就是在2023年的這個10月份左右
[00:22:25] SPEAKER_01: 就是一旦它把插在GP能力腐陷出來之後
[00:22:29] SPEAKER_01: 那很自然的一個決策就是
[00:22:31] SPEAKER_01: 我把當前的這個參數再誇大一些
[00:22:34] SPEAKER_01: 然後數據再對多一點
[00:22:37] SPEAKER_01: 算了一再弄得大一點
[00:22:38] SPEAKER_01: 然後我們就去追求GP4
[00:22:41] SPEAKER_01: 絕大部分團隊也都是按著這條思路去走的
[00:22:45] SPEAKER_01: 所以你就會看到2024年上半年
[00:22:48] SPEAKER_01: 其實就是幾乎所有的這些團隊都會去推
[00:22:52] SPEAKER_01: GP4水平的這個大模型
[00:22:54] SPEAKER_01: 那我們團隊呢
[00:22:55] SPEAKER_01: 其實是在2023年的下半年
[00:22:57] SPEAKER_01: 在腐陷出這個差的GP水平的模型能力之後
[00:23:01] SPEAKER_01: 我們的第一方案也是說
[00:23:04] SPEAKER_01: 我們去追求一下GP4水平的模型能力
[00:23:07] SPEAKER_01: 但是我們經過合算呢
[00:23:09] SPEAKER_01: 就會發現這樣一個模型
[00:23:12] SPEAKER_01: 它所需要的這個模型的參數規模
[00:23:15] SPEAKER_01: 大概當時應該算出來的是140B的這麼一個規模
[00:23:19] SPEAKER_01: 那麼對應的這個成本是多少呢
[00:23:22] SPEAKER_01: 就大概就是一個大幾千萬的一個水平
[00:23:24] SPEAKER_01: 我們就多想一下
[00:23:26] SPEAKER_01: 我們會說我花幾千萬的
[00:23:28] SPEAKER_01: 是訊這樣的一個模型
[00:23:29] SPEAKER_01: 大概率在2024年上半年
[00:23:32] SPEAKER_01: 國內一線的團隊至少有5加以上
[00:23:35] SPEAKER_01: 能夠實現出類似的能力
[00:23:37] SPEAKER_01: 那我如何能夠收回
[00:23:39] SPEAKER_01: 我訓練這個模型的成本呢
[00:23:41] SPEAKER_01: 我們會認為我們找不到這樣的一個確定的答案
[00:23:45] SPEAKER_01: 所以我們就會認為
[00:23:46] SPEAKER_01: 如果說這個配方都沒有什麼變化
[00:23:49] SPEAKER_01: 你只是把模型參數變大了
[00:23:51] SPEAKER_01: 然後把數據動多一點
[00:23:53] SPEAKER_01: 再訓一個GP4模型
[00:23:55] SPEAKER_01: 對我們來講在商業上是講不通的
[00:23:57] SPEAKER_01: 所以我們在那個時候呢
[00:23:58] SPEAKER_01: 就把主要的經歷
[00:24:01] SPEAKER_01: 變成了我們去進行模型風動的構建
[00:24:04] SPEAKER_01: 就我們要去看
[00:24:05] SPEAKER_01: 把各個方面更加高效地去進行這個構建
[00:24:09] SPEAKER_01: 它對於我們提升這個模型能力
[00:24:12] SPEAKER_01: 效率它有什麼影響
[00:24:14] SPEAKER_01: 所以呢就是這一系列的工作呢
[00:24:16] SPEAKER_01: 就產生的結果就是2024年
[00:24:18] SPEAKER_01: 1月份我們發布的
[00:24:19] SPEAKER_01: MiniCPM的第一個版本
[00:24:21] SPEAKER_01: 就是用一個24億的參數就可以實現
[00:24:24] SPEAKER_01: 像這個當時的喇嘛兔的13B
[00:24:27] SPEAKER_01: 然後像當時的Mitro7B
[00:24:30] SPEAKER_01: 就是這樣的一個模型的效果
[00:24:32] SPEAKER_01: 就會發現說
[00:24:33] SPEAKER_01: 你就想2.4B是什麼水平
[00:24:35] SPEAKER_01: 就是在手機上就可以跑了
[00:24:37] SPEAKER_01: 所以就是2.4B
[00:24:38] SPEAKER_01: MiniCPM也就是2024年的1月份
[00:24:42] SPEAKER_01: 讓我們意識到了
[00:24:43] SPEAKER_01: 其實我們可以通過技術的
[00:24:45] SPEAKER_01: 持續的創新
[00:24:47] SPEAKER_01: 能夠極大的提升這個模型的
[00:24:49] SPEAKER_01: 它的這個效率
[00:24:51] SPEAKER_01: 也就是我可以用更少的參數
[00:24:53] SPEAKER_01: 更少的計算量
[00:24:54] SPEAKER_01: 然後來去實現
[00:24:55] SPEAKER_01: 這個相同的甚至更高的模型能力
[00:24:58] SPEAKER_01: 當然一個方面我們就看到了
[00:25:00] SPEAKER_01: 就是在端策上去部署
[00:25:02] SPEAKER_01: 大模型的這個可能性
[00:25:04] SPEAKER_01: 所以就是在2024年
[00:25:05] SPEAKER_01: 我們就提端策制鬧
[00:25:07] SPEAKER_01: 端策大模型
[00:25:08] SPEAKER_01: 但是與此同時呢
[00:25:10] SPEAKER_01: 就是從暗線上
[00:25:12] SPEAKER_01: 朝軍我們其實就再去想
[00:25:15] SPEAKER_01: 就是它背後的規律會是什麼
[00:25:17] SPEAKER_01: 所以那個時候其實就是
[00:25:19] SPEAKER_01: 24年的整個的這個1年的時間
[00:25:22] SPEAKER_01: 其實我們在嘗試的
[00:25:24] SPEAKER_01: 就是尋找這樣的規律
[00:25:25] SPEAKER_01: 那當然就是受到模耳定律的這個啟示
[00:25:28] SPEAKER_01: 我們就想像說提出來
[00:25:30] SPEAKER_01: 這個支持密度的是密個概念
[00:25:32] SPEAKER_01: 然後在支持密度的技術上
[00:25:34] SPEAKER_01: 我們去探究如何去找到
[00:25:38] SPEAKER_01: 就是所有的這些模型
[00:25:40] SPEAKER_01: 通過技術上新
[00:25:41] SPEAKER_01: 它的這個密度的這個變化的一些規律
[00:25:44] SPEAKER_01: 這個其實是它的元氣
[00:25:46] SPEAKER_01: 所以就至少在面臂內部的話
[00:25:48] SPEAKER_00: 這個其實成為了你們一個目標之一
[00:25:50] SPEAKER_00: 然後你們現在把這個東西發出去
[00:25:52] SPEAKER_00: 其實也是因為你們覺得
[00:25:53] SPEAKER_00: 這個時間的方向是對的
[00:25:54] SPEAKER_00: 然後你們覺得可能在業界裡
[00:25:55] SPEAKER_00: 會有更多的共鳴
[00:25:57] 那回到你們的這個研究的一些發現
[00:25:59] SPEAKER_00: 就是比如說我看到你們這個數據裡面
[00:26:01] SPEAKER_00: 還說Chag.P.E.之前和之後
[00:26:03] SPEAKER_00: 從計算到能力的這個協率
[00:26:05] SPEAKER_00: 它是變得懂了嗎
[00:26:06] SPEAKER_00: Chag.P.之後是變得更懂了
[00:26:07] SPEAKER_00: 我又有一個表號起的事
[00:26:08] SPEAKER_00: 就是為什麼在24年9月的
[00:26:10] SPEAKER_00: O1和21之後就是強化學習
[00:26:12] SPEAKER_00: 後訓練的這個方法被引入之後
[00:26:14] SPEAKER_00: 它沒有類似的這種協力變化
[00:26:16] SPEAKER_00: 還是說現在這時間太短了
[00:26:17] SPEAKER_00: 其實更多的時刻
[00:26:19] SPEAKER_02: 可能是基礎系列上的問題
[00:26:20] SPEAKER_02: 就是我們這個難力密度
[00:26:22] SPEAKER_02: 去年年底的時候
[00:26:23] SPEAKER_02: 第一的版本是面向要激作模型的
[00:26:26] 所以說O1和21
[00:26:27] SPEAKER_02: 它更多的是後訓練技術的一個增長
[00:26:29] SPEAKER_02: 所以其實沒有直接體驗
[00:26:30] SPEAKER_02: 在這個能力密度裡
[00:26:32] SPEAKER_02: 然後其實我們現在也在做
[00:26:33] SPEAKER_02: 相應的2.0的版本
[00:26:35] SPEAKER_02: 就是希望能夠把所有的後訓練的
[00:26:38] SPEAKER_02: 這些技術的改進
[00:26:39] SPEAKER_02: 體現在這個能力密度的這個指標上
[00:26:42] SPEAKER_02: 然後我會把這個
[00:26:43] SPEAKER_02: 因為或你可以這麼理解
[00:26:44] SPEAKER_02: 就是我們現在提到這個能力密度
[00:26:46] SPEAKER_02: 對應的是Skilling了
[00:26:48] SPEAKER_02: 但是其實O1和21
[00:26:49] SPEAKER_02: 它的增長的是TESSA Time Skilling
[00:26:52] SPEAKER_02: 或叫Infinite Time Skilling了
[00:26:54] SPEAKER_02: 它其實去兌應的兩個不同的階段
[00:26:56] SPEAKER_02: 然後它同樣的
[00:26:57] SPEAKER_02: 我們的Dancing Law
[00:26:58] SPEAKER_02: 可能也要有這個面向Skilling Law的一個版本
[00:27:01] SPEAKER_02: 然後面向後面後訓練的一個版本
[00:27:04] SPEAKER_02: 不過相關的這個建模
[00:27:06] SPEAKER_02: 我們還在進展過程當中
[00:27:08] SPEAKER_02: 對 那我具體怎麼去實現這個密度的提升
[00:27:11] SPEAKER_00: 就可以展開說一說
[00:27:12] SPEAKER_00: 其實氛圍就是架構數據
[00:27:15] SPEAKER_02: 然後學習算法
[00:27:16] SPEAKER_02: 然後軟硬議體就是Infinite四個層面
[00:27:19] SPEAKER_02: 其實這已經是整個模型PAPE Lite
[00:27:21] SPEAKER_02: 上比較完整的四個方面
[00:27:24] SPEAKER_02: 我們現在通過做這個
[00:27:26] SPEAKER_01: 相當於是大模型的密度法則的
[00:27:29] SPEAKER_01: 相關的工作
[00:27:29] SPEAKER_01: 其實我們會發現
[00:27:31] SPEAKER_01: 不管是模型的架構
[00:27:33] SPEAKER_01: 包括就是技術注意力
[00:27:34] SPEAKER_01: 包括像Depthsick引領的
[00:27:37] SPEAKER_01: 就是今年的相當於系列度的
[00:27:39] SPEAKER_01: 這種MOE架構
[00:27:41] SPEAKER_01: 那麼其實都是在這個方面的
[00:27:43] SPEAKER_01: 這個非常典型的這個工作
[00:27:45] SPEAKER_01: 那麼這種在模型架構上上行
[00:27:48] SPEAKER_01: 那它一定是會往更高效的這個角度去做
[00:27:52] SPEAKER_01: 第二個方面就是數據治理
[00:27:54] SPEAKER_01: 也就是說我這個世界上的
[00:27:56] SPEAKER_01: 已有的數據
[00:27:57] SPEAKER_01: 和這個世界上還沒有的數據
[00:27:59] SPEAKER_01: 那我如何能夠為了這個模型能力的提升
[00:28:03] SPEAKER_01: 我能夠更好的去尋找學習的這個教材
[00:28:06] SPEAKER_01: 那這件事情本身現在看
[00:28:08] SPEAKER_01: 對於這個模型能力提升的這個效率
[00:28:11] SPEAKER_01: 和它的上限影響也都非常大
[00:28:14] SPEAKER_01: 你就可以向向一些一個小朋友
[00:28:16] SPEAKER_01: 它從小到大
[00:28:17] SPEAKER_01: 你給它的是一套非常精心挑選的這個教材
[00:28:21] SPEAKER_01: 精心準備的教材
[00:28:22] SPEAKER_01: 還是說啥也不管
[00:28:24] SPEAKER_01: 就是直接給它這個亂七八糟的東西讓它去讀
[00:28:27] SPEAKER_01: 顯然結果是不一樣的
[00:28:29] SPEAKER_01: 那麼第三個其實就是我們
[00:28:31] SPEAKER_01: 把它稱為叫做模型風動
[00:28:33] SPEAKER_01: 也就是我們要尋找這個模型成長的這個規律
[00:28:37] SPEAKER_01: 那我們能不能做到一小間大
[00:28:39] SPEAKER_01: 我們做大量的小的模型的實驗
[00:28:42] SPEAKER_01: 我們書籍的這些經驗的這種數據
[00:28:46] SPEAKER_01: 能夠讓我們預測出
[00:28:47] SPEAKER_01: 我們即將要訓練的那個大模型
[00:28:49] SPEAKER_01: 在什麼參數
[00:28:51] SPEAKER_01: 什麼數據配置下
[00:28:52] SPEAKER_01: 然後它能夠達到更高的效果
[00:28:55] SPEAKER_01: 我們能不能在這個模型還沒有訓練之前
[00:28:58] SPEAKER_01: 我們就有可能預測出它的這個能力上限
[00:29:02] SPEAKER_01: 會是多少
[00:29:03] SPEAKER_01: 那麼這種對智能的能力成長的這種規律的把握
[00:29:07] SPEAKER_01: 那麼當年在OPR是被稱為叫做Preditle skating
[00:29:11] SPEAKER_01: 也就是說它提出 skating load
[00:29:13] SPEAKER_01: 本身不只是說找到了這條Lower
[00:29:15] SPEAKER_01: 這條發展的這個這個這個曲線
[00:29:18] SPEAKER_01: 而是同時能夠找到
[00:29:21] SPEAKER_01: 我可以通過小模型來去預測
[00:29:24] SPEAKER_01: 大模型能力的這麼一個規律
[00:29:26] SPEAKER_01: 那麼這個規律其實對於我們去更好的去
[00:29:29] SPEAKER_01: 利用這個規律
[00:29:30] SPEAKER_01: 然後來更好的去設置
[00:29:32] SPEAKER_01: 那個大模型的相關的這種配置
[00:29:35] SPEAKER_01: 就具有舉足輕重的作用
[00:29:37] SPEAKER_01: 那麼第四個就是我們會看到
[00:29:40] SPEAKER_01: 模型它是要伴隨著硬件來這個共同成長的
[00:29:44] SPEAKER_01: 也就是說這個模型它是要在一個具體的硬件上
[00:29:47] SPEAKER_01: 然後來去發揮它的作用
[00:29:49] SPEAKER_01: 所以未來呢也一定是要在訓練和使用的階段
[00:29:53] SPEAKER_01: 都要實現一個軟硬血統的這麼一個設計和優化
[00:29:57] SPEAKER_01: 所以就是這這幾個要素
[00:29:59] SPEAKER_01: 其實都對這個模型的密度
[00:30:01] SPEAKER_01: 對這個模型的這個效果
[00:30:03] SPEAKER_01: 就是它到底是用越少的計算量
[00:30:07] SPEAKER_01: 越少的這個能量的消耗
[00:30:09] SPEAKER_01: 來去實現一個更強的能力
[00:30:11] SPEAKER_01: 那麼都具有非常重要的這個作用
[00:30:13] SPEAKER_01: 我們其實在推出這個密度法則之後
[00:30:16] SPEAKER_01: 其實我們現在正在推進的工作
[00:30:19] SPEAKER_01: 就是希望能夠定量的發現
[00:30:21] SPEAKER_01: 就是模型的價格數據的治理
[00:30:23] SPEAKER_01: 然後以及模型的風度等等
[00:30:26] SPEAKER_01: 然後他們對於這個模型密度
[00:30:29] SPEAKER_01: 對這個模型的能效的這麼一個影響的這個關係
[00:30:33] SPEAKER_01: 就是他們之間的這個定量關係
[00:30:35] SPEAKER_01: 現在來看其實還沒有找到特別的清晰
[00:30:39] SPEAKER_01: 但是我們有相應的這些經驗性的結果
[00:30:42] SPEAKER_01: 其實我要這個四個方面我們
[00:30:44] SPEAKER_02: 應該可以看到現在市面上有很多相關的工作
[00:30:47] SPEAKER_02: 第一個就是架構
[00:30:48] SPEAKER_02: 就可以看到其實傳統的就兩個部分
[00:30:51] SPEAKER_02: FFM和這個等下
[00:30:53] SPEAKER_02: 還有他們現在基本上大家已經共識
[00:30:55] SPEAKER_02: 就是要去做這個系列的Moe
[00:30:57] SPEAKER_02: 然後系列的細膚
[00:30:59] SPEAKER_02: 我們就解釋一下FF
[00:31:00] SPEAKER_02: 我覺得Feed4Won Network
[00:31:03] SPEAKER_02: 這個解釋可能就是一個
[00:31:04] SPEAKER_02: 怎麼說呢一個比較蟲密的一個取正程
[00:31:07] SPEAKER_02: 然後細膚的Moe
[00:31:09] SPEAKER_02: 其實就是說這個
[00:31:11] SPEAKER_02: 在那麼大了一個取正裡面
[00:31:13] SPEAKER_02: 我切分了好幾塊
[00:31:14] SPEAKER_02: 然後每一次計算我就選擇其中一塊
[00:31:17] SPEAKER_02: 這個其實是Depsy可接受給大家的
[00:31:19] SPEAKER_02: 為什麼Moe這個時候是Depsy可接受給大家的
[00:31:22] SPEAKER_00: 不是上Bisit說這些不是更早做Moe嗎
[00:31:24] SPEAKER_00: 對但是現在是Miche要做的那些Moe
[00:31:27] SPEAKER_02: 我後面的Max也更早在做Moe
[00:31:29] SPEAKER_00: 其實Moe很早我們在做
[00:31:31] SPEAKER_01: 就是原來我有一個博士生
[00:31:33] SPEAKER_01: 其實大概張正彥
[00:31:35] SPEAKER_01: 大概2021年
[00:31:38] SPEAKER_01: 其實在早期就開始做Moe的這個架構
[00:31:42] SPEAKER_01: 就Mexal experts本身這個概念
[00:31:44] SPEAKER_01: 應該是上個世紀90年代就已經有
[00:31:47] SPEAKER_02: 然後用到了這個Transformer裡面
[00:31:50] SPEAKER_01: 應該其實是2020年左右
[00:31:53] SPEAKER_01: 當時有Google 有我們團隊
[00:31:55] SPEAKER_01: 其實都在做
[00:31:56] SPEAKER_01: 但的確科官上來講是由於Depsy可
[00:31:59] SPEAKER_01: 然後把它真正的做到了一個
[00:32:01] SPEAKER_01: 這種最大的模型上
[00:32:04] SPEAKER_01: 然後做Work
[00:32:05] SPEAKER_01: 因為其實它的規模化
[00:32:07] SPEAKER_01: 然後以及說它真正的能夠支持高效
[00:32:10] SPEAKER_01: 就真正事實上還是有很多工程上的挑戰
[00:32:13] SPEAKER_01: 所以它是一個就是Depsy可的解釋的意思是說
[00:32:16] SPEAKER_00: 它從一個大家你做了一段時間的
[00:32:18] SPEAKER_00: 這種想法和裡面
[00:32:20] SPEAKER_00: 然後它給它在一個比較大的規模上實現了
[00:32:23] SPEAKER_00: 然後大家覺得這個生存有效
[00:32:25] SPEAKER_00: 對其實這個事情
[00:32:26] SPEAKER_02: 你可以在大模型的這個翻轉行業裡面
[00:32:28] SPEAKER_02: 你可以看到從祭福上來說
[00:32:30] SPEAKER_02: 其實沒有太多型的模型
[00:32:32] SPEAKER_02: 因為看到現在大家很火的
[00:32:33] SPEAKER_02: 喬娃學系也是上個世紀
[00:32:35] SPEAKER_02: 就已經非常火熱了
[00:32:37] SPEAKER_02: 然後你穿這個生存學機
[00:32:39] SPEAKER_02: 這個BP等等
[00:32:40] SPEAKER_02: 這都是上個世紀的強勢
[00:32:41] SPEAKER_02: 然後我放到現在
[00:32:43] SPEAKER_02: 其實讓核心還是因為算力的增長
[00:32:45] SPEAKER_02: 數據的增長
[00:32:45] SPEAKER_02: 使得它能夠發揮它的這個作用
[00:32:48] SPEAKER_02: 所以我們講Depsy可解釋
[00:32:49] SPEAKER_02: 其實也不是說Depsy可發明的件事情
[00:32:51] SPEAKER_02: 因為很早這個
[00:32:53] SPEAKER_02: Gb4出來的時候
[00:32:54] SPEAKER_02: 大家就在討論它就是M1架構
[00:32:56] SPEAKER_02: 但是具體說它怎麼樣
[00:32:57] SPEAKER_02: Skilling到那麼大
[00:32:58] SPEAKER_02: 那其實這個沒有形成一個共識
[00:33:01] SPEAKER_02: 然後大家也不敢一下子
[00:33:03] SPEAKER_02: 就是說我花很多的算力
[00:33:04] SPEAKER_02: 我就是選擇一條跟主流
[00:33:07] SPEAKER_02: 不一樣的道路
[00:33:08] SPEAKER_02: 所以其實Depsy可做到的
[00:33:09] SPEAKER_02: 今天事情大家就開始風冷
[00:33:11] SPEAKER_02: 而它是開源的
[00:33:12] SPEAKER_00: 對
[00:33:13] SPEAKER_02: 其實就是接識的事情
[00:33:14] SPEAKER_02: 表示它從一個非共識到共識的
[00:33:17] SPEAKER_02: 就能一個轉接點
[00:33:18] SPEAKER_02: 你可以去說就是在FF分
[00:33:19] SPEAKER_00: 大家會一個比較大的共識
[00:33:21] SPEAKER_00: 是做M1的這個架構
[00:33:23] SPEAKER_00: 然後你就可以看到
[00:33:24] SPEAKER_02: 今年一個很重要的一個轉變
[00:33:26] SPEAKER_02: 其實是就剛才提到的Egentic
[00:33:28] SPEAKER_02: 以及Senscom
[00:33:29] SPEAKER_02: 其實這兩個點其實都接受了
[00:33:31] SPEAKER_02: 這個大模型要變成長模型
[00:33:34] SPEAKER_02: 就是我要這個讓模型
[00:33:35] SPEAKER_02: 它能夠接觸到足夠多的上下輪
[00:33:37] SPEAKER_02: 就是這個
[00:33:39] SPEAKER_02: 你比如說你給的代碼的一個倉庫
[00:33:41] SPEAKER_02: 然後或者是Depsy的色系
[00:33:43] SPEAKER_02: 就是海浪的外部型系
[00:33:45] SPEAKER_02: 那再這樣一個場景下的話
[00:33:46] SPEAKER_02: 那這個它要接受很多的輸入
[00:33:49] SPEAKER_02: 然後產生很多的輸出
[00:33:50] SPEAKER_02: 那大模型是不是變成長模型
[00:33:52] SPEAKER_02: 所以很少就成為了非常重要的這個品勁
[00:33:55] SPEAKER_02: 所以現在大家都在做這件事情
[00:33:57] SPEAKER_02: 就包括現行也好
[00:33:58] SPEAKER_02: 包括西蘇也
[00:34:00] SPEAKER_02: 然後其實也有一些傳聞
[00:34:02] SPEAKER_02: 就是什麼Gemini等等
[00:34:04] SPEAKER_02: 就是都是用斯萊丁Window
[00:34:06] SPEAKER_02: 華東闖口這個模型
[00:34:07] SPEAKER_02: 叫西蘇注意力
[00:34:08] SPEAKER_02: 然後混合
[00:34:09] SPEAKER_02: 蟲密注意力來做的
[00:34:10] SPEAKER_02: 你覺得看到
[00:34:11] SPEAKER_02: 其實不管是開源幣
[00:34:12] SPEAKER_02: 大家都在幹的事情
[00:34:14] SPEAKER_02: 那其實這個事情加構體現的就是
[00:34:17] SPEAKER_02: 我們很形象的成為它是智能的容器
[00:34:21] SPEAKER_02: 就是說我們這個用
[00:34:23] SPEAKER_02: 更少的計算量達到相通的效果
[00:34:25] SPEAKER_02: 那其實是加構它帶了一個
[00:34:27] SPEAKER_02: 一個比較重要的改變
[00:34:29] SPEAKER_02: 然後另外一個其實輸入這個層面
[00:34:31] SPEAKER_02: 其實今年也會有一個比較大的一個共識
[00:34:35] SPEAKER_02: 就是因為去年也這個很多人都提到了
[00:34:38] SPEAKER_02: 就是死給靈魯撞牆了
[00:34:40] SPEAKER_02: 然後公開口禍去的數據哭解了
[00:34:43] SPEAKER_02: 然後這個那其實下一步
[00:34:45] SPEAKER_02: 很重要的一個點
[00:34:46] SPEAKER_02: 還有一個增長點是合成數據
[00:34:48] SPEAKER_02: 然後以及更高級的清洗數據
[00:34:51] SPEAKER_02: 其實這兩個增長點是
[00:34:52] SPEAKER_02: 大家現在也都在發揮
[00:34:54] SPEAKER_02: 很難做我們在做的
[00:34:55] SPEAKER_02: 這點上你們可以分享自己是怎麼做了嗎
[00:34:57] SPEAKER_00: 對 這個的話
[00:34:58] SPEAKER_01: 我們其實會有一個內部的一個拍攝
[00:35:01] SPEAKER_01: 這個publand大概是從L0一直到L4
[00:35:05] SPEAKER_01: 就是大概不同的Livo
[00:35:06] SPEAKER_01: 你大家可以設想就是L0這個層次
[00:35:09] SPEAKER_01: 是相當於是數據的數據
[00:35:11] SPEAKER_01: 我去抓取也好 採買也好
[00:35:14] SPEAKER_01: 形成了最原始的數據
[00:35:16] SPEAKER_01: 那麼L1相當於是過濾
[00:35:18] SPEAKER_01: 相當於說把一些重複
[00:35:20] SPEAKER_01: 把一些垃圾給過濾跳
[00:35:22] SPEAKER_01: L2相當於是選擇
[00:35:24] SPEAKER_01: 就是相當於再從這些數據裡面
[00:35:26] SPEAKER_01: 選擇我們認為高質量的這麼一個數據
[00:35:29] SPEAKER_01: L3相當於是合成
[00:35:31] SPEAKER_01: 相當於說我們去再去做這個數據的合成的這種工作
[00:35:36] SPEAKER_01: 就是合成或者是改善等等的
[00:35:38] SPEAKER_01: 就相當於說它不再是對EO數據的
[00:35:41] SPEAKER_01: 簡單的這麼一個處理
[00:35:43] SPEAKER_01: 而是說我會去產生一些這個世界上
[00:35:45] SPEAKER_01: 沒有過的數據
[00:35:46] SPEAKER_01: L4呢 其實就是相當於是
[00:35:49] SPEAKER_01: 我們把它成為叫驗證數據
[00:35:50] SPEAKER_01: 就是相當於我們會經過一些形式化
[00:35:53] SPEAKER_01: 或者是人工的方式去確認
[00:35:56] SPEAKER_01: 這些數據是教採機的這麼一些數據
[00:35:59] SPEAKER_01: 比如說最近在國際上有一個非常有名的
[00:36:03] SPEAKER_01: 於訓練的數據 其叫翻外版
[00:36:06] SPEAKER_01: 我們做了點什麼事呢
[00:36:07] SPEAKER_01: 就是我們用我們的這套數據
[00:36:11] SPEAKER_01: 預處理的這個PAPLIN
[00:36:12] SPEAKER_01: 把這個翻外版的我們做了一個經驗
[00:36:15] SPEAKER_01: 就相當於我們又去精選了它的這個相應的數據
[00:36:19] SPEAKER_01: 得到了一個它的十分之一不到的
[00:36:21] SPEAKER_01: 這麼一個叫做Ouch翻外版
[00:36:23] SPEAKER_01: 那我們就會發現
[00:36:24] SPEAKER_01: 用這個Ouch翻外版訓練的這個模型能力
[00:36:27] SPEAKER_01: 比這個原始的這個翻外版訓練的模型能力還要再高
[00:36:32] SPEAKER_01: 那你就可以設想我就是用不到十分之一的數據
[00:36:35] SPEAKER_01: 然後我就可以學一個更好的模型
[00:36:37] SPEAKER_01: 那顯然我的訓練的成本
[00:36:39] SPEAKER_01: 就可以下降到十分之一
[00:36:41] SPEAKER_01: 就這個其實是我們能看到的
[00:36:43] SPEAKER_01: 就是你數據治理本身帶來的證明一個價值
[00:36:46] SPEAKER_01: 那麼其實這還只是於訓練的這個部分
[00:36:49] SPEAKER_01: 那還有後訓練就是包括SFT
[00:36:51] SPEAKER_01: 包括這個這個強化學習
[00:36:55] SPEAKER_01: 相應的這些數據你的這個合成的這個程度
[00:37:00] SPEAKER_01: 合成的這個水平
[00:37:02] SPEAKER_01: 其實都非常大的去影響
[00:37:04] SPEAKER_01: 就是你的這個模型的能力的上限和效率
[00:37:07] SPEAKER_01: 比如說上海交大有一位教授叫劉鵬菲
[00:37:11] SPEAKER_01: 這個非常年輕的一個老師
[00:37:12] SPEAKER_01: 他最近就是做了一系列的工作
[00:37:14] SPEAKER_01: 叫Less Eastmore
[00:37:15] SPEAKER_01: 就是說我能用更少的數據
[00:37:18] SPEAKER_01: 可以去得到更強的能力
[00:37:20] SPEAKER_01: 本身也是這個方面的一些這個非常好的這些結果
[00:37:24] SPEAKER_01: 所以我們其實會看到
[00:37:26] SPEAKER_01: 就是在數據治理這個方面
[00:37:29] SPEAKER_01: 有非常多的信號告訴我們
[00:37:31] SPEAKER_01: 其實我們需要做一個更精緻的
[00:37:34] SPEAKER_01: 更精烈的這麼一個數據來讓模型去學
[00:37:38] SPEAKER_01: 其實一個非常終極的問題就是
[00:37:40] SPEAKER_01: 作為人工智能發展到大模型的結段
[00:37:43] SPEAKER_01: 其實它非常典型的特點就是數據驅動
[00:37:46] SPEAKER_01: 就是你所有的智能都是來自於數據的
[00:37:48] SPEAKER_01: 那我們就要問一個問題
[00:37:50] SPEAKER_01: 假如說我是構建某一個水平的模型的能力
[00:37:56] SPEAKER_01: 我可以構建的最小化的那個數據機會是什麼
[00:38:01] SPEAKER_01: 就是這件問題這個問題其實我覺得
[00:38:03] SPEAKER_01: 可以持續地去探索
[00:38:06] SPEAKER_01: 那麼這個事情也有助於我們去
[00:38:08] SPEAKER_01: 想像是說去追尋就是人工智能比較終極的一些問題
[00:38:14] SPEAKER_01: 也就是智能到這是什麼
[00:38:15] SPEAKER_01: 總之的話就是我們內部有一套比較複雜的
[00:38:19] SPEAKER_01: 這個完備的一個分層的一個數據智能的體系
[00:38:22] SPEAKER_01: 當然我們應該會在最近幾個月會發布一個
[00:38:25] SPEAKER_01: 這個方面的一個就是我們的報告
[00:38:28] SPEAKER_01: 然後以及我們整個的這個數據的樣力
[00:38:32] SPEAKER_01: 就是做一個數據的整體的一個開源
[00:38:34] SPEAKER_01: 你說這個數據的開源是指開源
[00:38:36] SPEAKER_00: 最後數據的結果還是說這個PAPA拉一本什麼會開的
[00:38:40] SPEAKER_00: 數據的結果和我們處理這個數據的一個整體的一個架構
[00:38:45] SPEAKER_01: 就是這個我們會一天論文的顯示來告訴大家
[00:38:48] SPEAKER_01: 報告的顯示就是那個數據體系會開源
[00:38:50] SPEAKER_01: 可能應該開明一步子
[00:38:51] SPEAKER_01: 對 會告訴大家就是說你大概需要做成什麼樣子
[00:38:55] SPEAKER_01: 其實我們在過去幾年也在數據這個方面開放了很多
[00:38:58] SPEAKER_01: 這個樣力包括像R2CatchR3的Bike等等
[00:39:02] SPEAKER_01: 包括剛才提到的R2翻WiP
[00:39:05] SPEAKER_01: 其實我們想告訴大家的就是數據的質量和數量
[00:39:09] SPEAKER_01: 其實的確是這個模型
[00:39:10] SPEAKER_01: 它能夠達到一個更高水平的一個非常重要的基礎
[00:39:14] SPEAKER_01: 剛才說模型架構是智能的容器
[00:39:17] SPEAKER_01: 那麼數據的話就是模型學習的教材
[00:39:19] SPEAKER_01: 這個是沒有什麼好不公開的
[00:39:21] SPEAKER_01: 因為本身對於AI這個領域開源共享是共識
[00:39:26] SPEAKER_01: 但是同時的話其實我們會看到就是自從大模型出現之後
[00:39:30] SPEAKER_01: 其實這個構建一個大模型或者構建一個智能的系統
[00:39:34] SPEAKER_01: 本身會變成一個非常複雜的一個精密的體系
[00:39:38] SPEAKER_01: 它裡面包含的環節特別多
[00:39:40] SPEAKER_01: 我們只是列出來了裡面比較重要的四個方面
[00:39:44] SPEAKER_01: 對吧 架構 數據 學習還有這個硬件
[00:39:48] SPEAKER_01: 但是裡面的每一個部分又包含非常複雜的這麼一個體系
[00:39:53] SPEAKER_01: 其實你看像DipSync它公布的DipSync V3
[00:39:57] SPEAKER_01: 它的Country Beauty大概是100多人
[00:40:01] SPEAKER_01: 我認為本身已經算是一個非常精亮的體系了
[00:40:06] SPEAKER_01: 你就可以設想就是在全世界方面來看
[00:40:09] SPEAKER_01: 可能是數百人的一個團隊
[00:40:11] SPEAKER_01: 其實上千人可能都是有可能的
[00:40:13] SPEAKER_01: 我覺得它知識和成果產生的方式也很不同
[00:40:17] SPEAKER_00: 因為以前的諾文可能就是說四、五個或者什麼七、八個的作者
[00:40:21] SPEAKER_00: 現在就是報告下面都是
[00:40:23] SPEAKER_00: 是的
[00:40:24] SPEAKER_01: 我們可以把這個時候玩
[00:40:25] SPEAKER_00: 因為剛才說了是這個模型的架構還有數據
[00:40:28] SPEAKER_00: 在往下還有算法和這個軟硬件
[00:40:30] SPEAKER_02: 對 其實算法也是一個很重要的點
[00:40:31] SPEAKER_02: 其實算法大家是比較能有感觸的
[00:40:34] SPEAKER_02: 因為去這個御徐念
[00:40:37] SPEAKER_02: 然後到今年大家都很關注的強化學習
[00:40:41] SPEAKER_02: 然後今年事情其實強化學習做到現在
[00:40:44] SPEAKER_02: 其實大家還沒有解決一個問題
[00:40:46] SPEAKER_02: 就是強化學習的Skilling的問題
[00:40:48] SPEAKER_02: 所以之前我們一直會講PrettyChannelSkilling
[00:40:51] SPEAKER_02: 那還有Skilling Law
[00:40:53] SPEAKER_02: 但是強化學習它的Skilling Law是什麼
[00:40:55] SPEAKER_02: 以及我們怎麼樣讓強化學習能夠持續地訓下去
[00:40:59] SPEAKER_02: 這件事情是大家一個臺體持續探索的事情
[00:41:03] SPEAKER_02: 這個可以講開講一下
[00:41:04] SPEAKER_00: 就是強化學習它沒有Skilling這個是指什麼
[00:41:07] SPEAKER_00: 我看依賴那個博會也在討論這個
[00:41:09] SPEAKER_00: 它覺得就不應該用Skilling去描述現在強化學習的狀態
[00:41:12] SPEAKER_00: 對 因為這個事情是大家現在也還是一個非共識的事情
[00:41:16] SPEAKER_02: 就是你可以看到OE當時去年你下面年的時候
[00:41:19] SPEAKER_02: 它就講它已經有了是大規模強化學習得到的OE糟的系統
[00:41:23] SPEAKER_02: 然後你其實可以看到最比御徐念來說
[00:41:25] SPEAKER_02: 強化學習的Skilling還是完全沒有做的
[00:41:28] SPEAKER_02: 這特別好 御徐念現在基本上有幾十T頭肯
[00:41:31] SPEAKER_02: 然後御徐念的部署可能有幾十萬
[00:41:34] SPEAKER_02: 你現在強化學習基本上大家訓個幾千步
[00:41:37] SPEAKER_02: 然後RE他公布的那個報告也就訓了不到一萬步
[00:41:40] SPEAKER_02: 這已經達到在某個任務上拿得很好的效果
[00:41:43] SPEAKER_02: 但其實他還遠沒有到我們認為
[00:41:45] SPEAKER_02: 他已經Skilling足夠好的一個程度
[00:41:48] SPEAKER_02: 所以其實大家在強調強化學習的Skilling
[00:41:51] SPEAKER_02: 誰會講的是說環境的Skilling然後數據的Skilling
[00:41:55] SPEAKER_02: 因為現在的強化學習很強的VirreFellable的Reword
[00:42:00] SPEAKER_02: 所以你可驗證的Reword
[00:42:01] SPEAKER_02: 但現在來說大家公開了以知的很好或許的這種可驗證的Reword
[00:42:06] SPEAKER_02: 誰就是數學和蛋馬
[00:42:07] SPEAKER_02: 然後但是更多的場景
[00:42:09] SPEAKER_02: 就是你雖然說蛋馬已經很強了
[00:42:11] SPEAKER_02: 但是你像現在這個他只是在競賽題
[00:42:14] SPEAKER_02: 但是比如我要去做這個真實的開發編程
[00:42:17] SPEAKER_02: 那怎麼辦
[00:42:18] SPEAKER_02: 開發編程經驗的時候你就比競賽題更佛達
[00:42:21] SPEAKER_02: 因為競賽題只涉及非常簡單的拍攝環境
[00:42:24] SPEAKER_02: 那如果要去做這種更佛達的編程開發
[00:42:27] SPEAKER_02: 就會有背後更佛達的什麼第三方庫等等
[00:42:30] SPEAKER_02: 那其實這些事情都是大家還沒做到的
[00:42:32] SPEAKER_02: 所以Ios怎麼Skilling是一個很大的問題
[00:42:37] SPEAKER_02: 然後你是可以看到就OPEN AI之前這個機前
[00:42:41] SPEAKER_02: 有那個研究委員的接通委
[00:42:43] SPEAKER_02: 他就在發推的時候這個Iodoma的Magic
[00:42:46] SPEAKER_02: 然後他的Matcher的核心在於一個Unhackbo的Invirement
[00:42:50] SPEAKER_02: 就是不可被Hack的環境
[00:42:52] SPEAKER_02: 那其實這個點其實他背後硬設的就是
[00:42:55] SPEAKER_02: 我要想辦法給強化學習大件好足夠好的
[00:42:58] SPEAKER_02: 一個Skilling的一個平台
[00:43:01] SPEAKER_02: 這樣的話這個模型他可以在上面
[00:43:03] SPEAKER_02: 真的持續的學到東西
[00:43:05] SPEAKER_02: 這個持續是大家現在在學習範圍上一直在講
[00:43:08] SPEAKER_02: 強化學習的強化學習突破
[00:43:11] SPEAKER_02: 當然這個醫療他提到的這個說
[00:43:13] SPEAKER_02: 他說強化學習不可思給令
[00:43:15] SPEAKER_02: 是他認為這種Unhackbo的Invimement的實際上是不太存在的
[00:43:20] SPEAKER_02: 或者是說他會認為人類的學習是不是這個樣子的
[00:43:23] SPEAKER_02: 因為這個事情也已簽涉到我們對ADI下一步的定位
[00:43:27] SPEAKER_02: 就是你看我們現在的強化學習
[00:43:29] SPEAKER_02: 他能做到一個點是你只要在任何一個任務上
[00:43:32] SPEAKER_02: 就要有充足的標誌輸入
[00:43:34] SPEAKER_02: 有充足的人類反饋
[00:43:36] SPEAKER_02: 他在這個任務上就能夠達到這個前面的一萬分級人類的水平
[00:43:40] SPEAKER_02: 但是現在什麼MNO金牌
[00:43:42] SPEAKER_02: I-5I金牌就已經很多大廠都已經Clam自己做到了
[00:43:46] SPEAKER_02: 但是這個這樣的話其實你可以想想想
[00:43:50] SPEAKER_02: 加上加上一家公司他做的一個就是某個具體的業務
[00:43:53] SPEAKER_02: 他可以在這個業務上接著足夠的數據通過強化學習
[00:43:56] SPEAKER_02: 使得他在這個業務上非常的驚通
[00:43:59] SPEAKER_02: 現在有趨勢吧
[00:44:00] SPEAKER_00: 或者叫這個封口就RL2B什麼的
[00:44:03] SPEAKER_00: 所以我去針對一個真實的商業環境去做這個
[00:44:06] SPEAKER_02: 其實但是這樣的ADI不是我們真的想要的ADI
[00:44:10] SPEAKER_02: 因為我們人類他的學習是很高效的
[00:44:13] SPEAKER_02: 我可以在一個新的任務下
[00:44:16] SPEAKER_02: 然後從我少量的反饋、少量的動作
[00:44:19] SPEAKER_02: 我就能夠把這個任務學得很好
[00:44:21] SPEAKER_02: 所以這個其實是學習範疇上
[00:44:24] SPEAKER_02: 我們可能會面臨的轉變
[00:44:26] SPEAKER_02: 就是有兩種和地園就是計學RL
[00:44:29] SPEAKER_02: 然後怎麼保持RL
[00:44:30] SPEAKER_02: 對怎麼死給你下去
[00:44:31] SPEAKER_02: 另外一種就是你可以看到
[00:44:33] SPEAKER_02: 遇上聯合RL經歷的一個區別
[00:44:35] SPEAKER_02: 或者有監督微調的RL一個區別
[00:44:37] SPEAKER_02: 就是學習效率更高
[00:44:38] SPEAKER_02: 就會我們叫三倍飛勝隧變高了
[00:44:41] SPEAKER_02: 在進一步的RL還是不夠
[00:44:43] SPEAKER_02: 那我未來怎麼讓RL他的學習的效率會更高
[00:44:47] SPEAKER_02: 這裡的學習效率就是怎麼樣跟著好的利用環境反饋
[00:44:51] SPEAKER_02: 然後跟少量的人物標準
[00:44:53] SPEAKER_02: 他能夠在一個新任務上達到一個足夠好的效果
[00:44:56] 所以現在有人會認為RMOS肛伶似直
[00:44:58] SPEAKER_00: 比如說我給他加更多的算力
[00:45:00] SPEAKER_00: 然後讓他睡更多的部署
[00:45:02] SPEAKER_00: 他並不一定帶來一個更好的效果
[00:45:03] SPEAKER_00: 他只能他在特定的任務上
[00:45:05] SPEAKER_02: 他能帶來更好的效果
[00:45:06] SPEAKER_02: 但是他沒有
[00:45:07] SPEAKER_00: 他沒有就是說在更多的下游任務上
[00:45:10] SPEAKER_00: 帶來更好的
[00:45:11] SPEAKER_00: 他只能針對利用環境的任務
[00:45:13] SPEAKER_00: 他的效果更好了
[00:45:14] SPEAKER_00: 是的
[00:45:15] SPEAKER_02: 所以現在我們就說大模型領域
[00:45:17] SPEAKER_02: 我們期間很多時候叫Problem Engine Neural
[00:45:19] SPEAKER_02: 現在我們會求什麼Reward Engine Neural
[00:45:22] SPEAKER_02: 或者是叫這個配環境
[00:45:24] SPEAKER_02: Inverment Engine Neural
[00:45:25] SPEAKER_02: 這都是現在來說
[00:45:27] SPEAKER_02: 不同大模型時代的新的一些產物
[00:45:30] SPEAKER_02: 所以你認為他繼續去RMOS這個方式
[00:45:33] SPEAKER_00: 可能不是我們一般閃耗中的AGA
[00:45:35] SPEAKER_00: 但是有可能他在一些聚險場景的時候
[00:45:37] SPEAKER_00: 很多用處的
[00:45:37] SPEAKER_00: 其實像Coding就已經是一個這樣的例子了
[00:45:40] SPEAKER_00: 也不完全
[00:45:41] SPEAKER_02: 就是你想這個Skating Engine事情
[00:45:43] SPEAKER_02: 他有可能帶來範化的
[00:45:45] SPEAKER_02: 就是你想跟我們之前講預訓練
[00:45:47] SPEAKER_02: 講有監督微調
[00:45:48] SPEAKER_02: 或有監督學習精神
[00:45:50] SPEAKER_02: 在這個生肚學習時代一直是在做的
[00:45:53] SPEAKER_02: 但是他真正出現
[00:45:55] SPEAKER_02: 他能出現範化就是通過預訓練
[00:45:58] SPEAKER_02: 就是Skating上去之後他就範化了
[00:46:00] SPEAKER_02: RMSkating能不能帶來這一個點
[00:46:03] SPEAKER_02: 其實是不好講的
[00:46:04] SPEAKER_02: 所以我會講他有現在有長久的技術路線
[00:46:06] SPEAKER_02: 就是一個技術Skating下去
[00:46:08] SPEAKER_02: 在足夠多的環境下Skating
[00:46:10] SPEAKER_02: 那他是不是重點在新的任務上範化
[00:46:13] SPEAKER_02: 然後或者是我們就真的需要一種新的學習方式
[00:46:15] SPEAKER_02: 我有個小白的聽證的問題
[00:46:17] SPEAKER_00: 就是RM你不能給他夠造一個
[00:46:19] SPEAKER_00: 就是很複雜的任務
[00:46:20] SPEAKER_00: 很多樣的環境嗎
[00:46:22] 就很難
[00:46:23] SPEAKER_02: 就是你這個多樣的環境其實就很難
[00:46:26] SPEAKER_02: 或許因為你要想
[00:46:27] SPEAKER_02: 就是最簡單的比如說我們人
[00:46:29] SPEAKER_02: 他能夠在這個世界上會獲取很多的反饋
[00:46:32] SPEAKER_02: 比如說我跟你的聊天
[00:46:33] SPEAKER_02: 在我看到你的這個表情已經很嚴肅了
[00:46:36] SPEAKER_02: 那我會認為我可能說的話已經是不好了
[00:46:39] SPEAKER_02: 那我這個其實就是這種反饋
[00:46:41] SPEAKER_02: 但是對強化學習來說我就給告訴你
[00:46:43] SPEAKER_02: 就是一這是富裔
[00:46:44] SPEAKER_02: 那見識經其實就已經很多樣了
[00:46:47] SPEAKER_02: 你我怎麼能夠把整個世界給建模起來
[00:46:50] SPEAKER_02: 給他一個人握的
[00:46:51] SPEAKER_02: 這個其實就有點像是立亞獎的那個Value方式
[00:46:54] SPEAKER_02: 就是這種價值寒附是怎麼給到模型的
[00:46:57] SPEAKER_02: 那這個其實是一個非常非常複雜的一個問題
[00:47:00] SPEAKER_02: 對於我想想一下就是因為人所在這個真實世界的環境裡
[00:47:03] SPEAKER_00: 它其實有人是面臨很多樣的目標的
[00:47:05] SPEAKER_00: 但是人可能在某一個時刻
[00:47:06] SPEAKER_00: 它的注意力是被某一些目標
[00:47:08] SPEAKER_00: 或者說某一些這種反饋給吸引住的
[00:47:11] SPEAKER_00: 但是換了一個場景
[00:47:12] SPEAKER_00: 換了一個時空
[00:47:12] SPEAKER_00: 你可能又在注意別的事情
[00:47:14] SPEAKER_00: 所以其實這種價值寒附
[00:47:15] SPEAKER_02: 或者是這種RMOLO其實是一個很難的一個事情
[00:47:18] SPEAKER_02: 然後我們世界會下一想
[00:47:20] SPEAKER_02: 就是這種RMOLO和真的會做件事情
[00:47:23] SPEAKER_02: 就是可能它是一個同的難度的這個場景
[00:47:26] SPEAKER_02: 我就有點機身彈單身機的一個
[00:47:27] SPEAKER_02: 假設我能對這個世界建模的很好
[00:47:29] SPEAKER_02: 你所認同的事情我都能給你足夠的反饋
[00:47:32] SPEAKER_02: 那這個RMOLO或者是說這個價值寒附
[00:47:35] SPEAKER_02: 它為什麼天然不是一個AGA呢
[00:47:37] SPEAKER_02: 那所以就是說我是能夠有一個AGA
[00:47:39] SPEAKER_02: 我從頭上到處另外一個AGA人感覺
[00:47:41] SPEAKER_02: 那你們自己現在就是在這個算法的去怎麼提升的
[00:47:45] SPEAKER_00: 能力密度上
[00:47:46] SPEAKER_00: 比如說你們接下來可能會重點做的方式是什麼
[00:47:48] SPEAKER_00: 對其實還是回到剛才那兩個事情
[00:47:51] SPEAKER_02: 就是我們團隊吧
[00:47:53] SPEAKER_02: 其實也有在嘗試去補出
[00:47:54] SPEAKER_02: Ios給你上的一些規律
[00:47:57] SPEAKER_02: 然後我們團隊也有些同學
[00:47:59] SPEAKER_02: 然後做了一些什麼Ios當中的商變化
[00:48:02] SPEAKER_02: 這種商其實就是可以認為大模型的這種探索能力
[00:48:06] SPEAKER_02: 你覺得這個商詞商詞商詞商詞的那個商害
[00:48:08] SPEAKER_00: 對商詞商的那個商
[00:48:09] SPEAKER_02: 其實反應到這個模型裡面是那個Token Diversity
[00:48:12] SPEAKER_02: 混亂度的變化嗎
[00:48:13] SPEAKER_00: 對
[00:48:13] SPEAKER_00: 或者是說你說在當前就快了一下
[00:48:17] SPEAKER_02: 然後我這個模型它能覺得通過都是彩樣
[00:48:20] SPEAKER_02: 然後成成多少種
[00:48:22] SPEAKER_02: 非常不一樣的這種回答
[00:48:24] SPEAKER_02: 就是一個非常指望的理解
[00:48:26] SPEAKER_02: 然後會發現就是Ios的過程
[00:48:28] SPEAKER_02: 其實這種商然後變化成這種Ecclacy的一個過程
[00:48:32] SPEAKER_02: 所以我們也在嘗試去構建這種機制上的理解
[00:48:36] SPEAKER_02: 然後還有一些就是這個Miance scaling
[00:48:39] SPEAKER_02: 我們也會去做自國高校的Skilling的框架
[00:48:42] SPEAKER_02: 就是我們希望說能夠使得這個模型
[00:48:46] SPEAKER_02: 在Ios的過程裡面能夠把這個算力用的足夠滿
[00:48:48] SPEAKER_02: 然後這樣的話我就能夠讓它尋得足夠快
[00:48:51] SPEAKER_02: 那未來就有可能是我從這些算力比如說
[00:48:54] SPEAKER_02: 選擇幾萬步這些Ios
[00:48:56] SPEAKER_02: 然後還有一些更多探索的這個環節
[00:48:58] SPEAKER_02: 比如說像這種立向開放魚的強調學習
[00:49:01] SPEAKER_02: 就除了這個數學代碼以外
[00:49:03] SPEAKER_02: 那像這種比如說寫篇論文
[00:49:06] SPEAKER_02: 那這個什麼樣的Reword是好等等
[00:49:08] SPEAKER_02: 那其實我們在企業在做一些這種嘗試
[00:49:11] SPEAKER_02: 不過目前來說就是後面開放魚的長時間
[00:49:13] SPEAKER_02: 其實還沒有包括業界
[00:49:16] SPEAKER_02: 也沒有一個特別懲罰的一個共識
[00:49:18] SPEAKER_02: 其實我們現在在做企業的事情
[00:49:20] SPEAKER_02: 那最後可以講最後一種就是軟硬的深度優化
[00:49:23] SPEAKER_00: 然後一些調一體的深度優化
[00:49:25] SPEAKER_00: 對 軟硬一些的深度優化
[00:49:26] SPEAKER_02: 其實這件事情應該說是更是計算機發展到現在的一個核心的主線
[00:49:31] SPEAKER_02: 你可以看到就是模特定率本身
[00:49:34] SPEAKER_02: 大家會說已經終止了
[00:49:36] SPEAKER_02: 那是因為大家會認為CPU已經不在這種通用的計算設備
[00:49:40] SPEAKER_02: 已經不在像這個模特定率預測了一樣
[00:49:43] SPEAKER_02: 就一直能夠指數的翻倍
[00:49:44] SPEAKER_02: 但是像GPU它會說因為大家還在支持這個模特定的發展
[00:49:49] SPEAKER_02: 現在會叫黃式定率
[00:49:51] SPEAKER_02: 其實都是在做的事情
[00:49:52] SPEAKER_02: 就是說我們怎麼能夠把這個算力用的足夠滿
[00:49:56] SPEAKER_02: 那其實是一個很重要的事情
[00:49:58] SPEAKER_02: 加上我GPU算力
[00:50:01] SPEAKER_02: 然後我只用滿了一半
[00:50:03] SPEAKER_02: 要生下一半的計算資源都消耗掉了
[00:50:05] SPEAKER_02: 那肯定是一個非常不經濟的做法
[00:50:08] SPEAKER_02: 所以軟硬一些東西就是在想是說
[00:50:10] SPEAKER_02: 我要怎麼去把這個算力用的足夠滿
[00:50:13] SPEAKER_02: 你可以看到
[00:50:14] SPEAKER_02: 其實可以看到就是大魔形
[00:50:16] SPEAKER_02: 或神經網路翻籃到現在核心勝利的那些架構
[00:50:19] SPEAKER_02: 勝利的那些專家
[00:50:20] SPEAKER_02: 那些方法都是軟硬一些方法
[00:50:23] SPEAKER_02: 像Transformer這早的提出
[00:50:25] SPEAKER_02: 其實核心的問題就是
[00:50:27] SPEAKER_02: 因為它認為 吞噬這種架構是能夠把GPU利用率達滿的
[00:50:31] SPEAKER_02: 所以它提出來這個點
[00:50:32] SPEAKER_02: 然後Transformer出來了
[00:50:33] SPEAKER_02: 然後Transformer
[00:50:35] SPEAKER_02: 能夠把GPU達滿之後代表
[00:50:37] SPEAKER_02: 那我就可以死給令了
[00:50:38] SPEAKER_02: 就把一個魔形做得很大
[00:50:39] SPEAKER_02: 之前的也什麼
[00:50:40] SPEAKER_02: CMCM等等都沒有辦法做到這件事情
[00:50:43] SPEAKER_02: 那Transformer就勝利了
[00:50:45] SPEAKER_02: 然後從這個出現了GPU11
[00:50:48] SPEAKER_02: 但是GPU11到GPU3的話
[00:50:49] SPEAKER_02: 那中間又會有很多這種工程心理的問題
[00:50:52] SPEAKER_02: 然後大家就需要這個怎麼辦
[00:50:54] SPEAKER_02: 這個大的基群
[00:50:56] SPEAKER_02: 然後這種通性給用滿時的
[00:50:57] SPEAKER_02: 這種大的基群能發給它足夠多的作用
[00:51:00] SPEAKER_02: 然後在網紅上說
[00:51:01] SPEAKER_02: 吞噬其實現在大家用的什麼Flash的吞噬等等
[00:51:04] SPEAKER_02: 它也是面向硬件去優化了這個
[00:51:07] SPEAKER_02: 它現在這個自身
[00:51:08] SPEAKER_02: 我們上期節目有聊過這個
[00:51:10] SPEAKER_00: 所以其實軟硬斜同件是一直是
[00:51:12] SPEAKER_02: AI或者是乃至集團機行業發展一個主權率
[00:51:16] SPEAKER_02: 差需要兩個方面
[00:51:17] SPEAKER_02: 一個就是面向現代的硬件設備
[00:51:19] SPEAKER_02: 我們要去設計更好的算法
[00:51:21] SPEAKER_02: 那其實也是我們像是聊那個吸入注意力
[00:51:23] SPEAKER_02: 一個很重要的點
[00:51:25] SPEAKER_02: 如果我們為什麼會去設計
[00:51:26] SPEAKER_02: 這種分塊的注意力
[00:51:27] SPEAKER_02: 然後那其實天然就是因為GPU的這個集團特性倒置了
[00:51:31] SPEAKER_02: 然後它一個很重要的方向
[00:51:33] SPEAKER_02: 就是面向現在的這個AI設備
[00:51:35] SPEAKER_02: 我們怎麼去設計這個硬件
[00:51:37] SPEAKER_02: 你可以看到現在這種所謂的手機關的NPU
[00:51:40] SPEAKER_02: 然後以及這些這個InfoR層面的一些努力
[00:51:44] SPEAKER_02: 然後其實都是在做件事情
[00:51:45] SPEAKER_02: 包括Dubes這個V3
[00:51:47] SPEAKER_02: 它的那個基礎報告裡面
[00:51:48] SPEAKER_02: 也甚至就是在提到
[00:51:49] SPEAKER_02: 它甚至深落底層去改那些匯編語
[00:51:53] SPEAKER_02: 然後是的
[00:51:54] SPEAKER_02: 說它真的能夠把這個模型支持了足夠好
[00:51:57] SPEAKER_02: 那其實都是這種
[00:51:58] SPEAKER_02: 就是兩邊在現通的例子
[00:52:00] SPEAKER_02: 它還成了這麼一個進展
[00:52:02] SPEAKER_02: 因為你們自己其實今年10月
[00:52:04] SPEAKER_00: 也更新的你們在這個應該算是
[00:52:06] SPEAKER_00: 模型加果上的一個別變化
[00:52:08] SPEAKER_00: 就是這個吸入注意力
[00:52:10] SPEAKER_00: Info LLM的第二代
[00:52:12] SPEAKER_00: 你把今天是個開源的
[00:52:13] SPEAKER_00: 它這個相比你們之前
[00:52:15] SPEAKER_00: 那第一代的更新和提升是什麼
[00:52:17] SPEAKER_00: 其實我們開源是在6月份了
[00:52:18] SPEAKER_02: 就是我們6月份第一版
[00:52:21] SPEAKER_02: 那個Mini3分4的時候就已經用上了
[00:52:23] SPEAKER_02: 然後是在10月份把這個基礎報告和
[00:52:26] SPEAKER_02: 一些就用述量數據
[00:52:27] SPEAKER_02: 被這個集中模型等等傳統開放出去
[00:52:30] SPEAKER_02: 大家就使用
[00:52:31] SPEAKER_02: 它像比於第一版的一個核心的一個改變
[00:52:33] SPEAKER_02: 其實也是受到了DUC的一個啟發
[00:52:36] SPEAKER_02: 就是要做到這個語言深細數
[00:52:39] SPEAKER_02: 就是我在訓練階段它就是做過細數的
[00:52:41] SPEAKER_02: 這樣的話就是像比於這個
[00:52:44] SPEAKER_02: 我們第一版其實是在推理階段去做細數
[00:52:46] SPEAKER_02: 還有一個有點
[00:52:47] SPEAKER_02: 第一個是我訓練的水源能加速了
[00:52:49] SPEAKER_02: 第二個就有點就是如果我細數度
[00:52:51] SPEAKER_02: 能夠降低它什麼
[00:52:52] SPEAKER_02: 就是之前這種推理階段的細數
[00:52:55] SPEAKER_02: 可能50%左右的這個細數程度
[00:52:57] SPEAKER_02: 然後同時當時的推理階段的細數值
[00:53:00] SPEAKER_02: 那是我們叫Prefitting
[00:53:01] SPEAKER_02: 就是長文本輸入能夠加速
[00:53:03] SPEAKER_02: 但對於長文輸輸取做得不太好
[00:53:06] SPEAKER_02: 那在這樣一個
[00:53:07] SPEAKER_02: 第二代的版本裡面
[00:53:08] SPEAKER_02: 我們就把這個
[00:53:10] SPEAKER_02: 做到訓練裡細數度能夠降低
[00:53:12] SPEAKER_02: 然後現在基本上128K
[00:53:13] SPEAKER_02: 就需要104到6K的頭肯
[00:53:15] SPEAKER_02: 也就剝到5%的一個細數度
[00:53:18] SPEAKER_02: 然後這個同時我們這個Sand
[00:53:20] SPEAKER_02: 也能支持是一個長輸出
[00:53:22] SPEAKER_02: 這樣的話我們去做這種深度思考
[00:53:25] SPEAKER_02: 然後包括做A-JAMT這些領域的話
[00:53:27] SPEAKER_02: 這個方法都是比較利用的
[00:53:29] SPEAKER_02: 你剛才也講到就是這個DBC
[00:53:30] SPEAKER_00: 可能出的那個NSA
[00:53:32] SPEAKER_00: 它是一個快狀的
[00:53:33] SPEAKER_00: 就是去選這個頭肯的這麼一個方式
[00:53:36] SPEAKER_00: 然後我看他們9月更新的DBC V3.2
[00:53:39] SPEAKER_02: 它的裡面的注意力又改了
[00:53:41] SPEAKER_00: 它們又新去了一個名字
[00:53:42] SPEAKER_00: 叫DBC Sparce Tension DSA
[00:53:44] SPEAKER_00: 然後相比NSA的話
[00:53:46] SPEAKER_00: 我理解它的那個快狀選取的方式
[00:53:48] SPEAKER_00: 好像就變得更細了
[00:53:49] SPEAKER_00: 它就怎麼去解決計算上的問題了
[00:53:52] SPEAKER_02: 但這個其實這個計算上的問題
[00:53:55] SPEAKER_02: 這個就可能太幾乎細講了
[00:53:56] SPEAKER_02: 可行可以解釋的一個點就是
[00:53:58] SPEAKER_02: DBC的那個架構
[00:54:00] SPEAKER_02: 它已經不需要解決這個訪存問題了
[00:54:03] SPEAKER_02: 它不需要解決這種存儲帶寬向的問題了
[00:54:06] SPEAKER_02: 然後它已經能夠做到
[00:54:07] SPEAKER_02: 就是準備計算給棒的做
[00:54:09] SPEAKER_02: 所以說它能夠去忽略這個問題
[00:54:11] SPEAKER_02: 但是對大部分的這個其他的模型而言
[00:54:14] SPEAKER_02: 或大部分的其他部署長進來的問題
[00:54:17] SPEAKER_02: 就存在了加上用它那個DSA的架構
[00:54:19] SPEAKER_02: 可以強調一個點
[00:54:20] SPEAKER_02: 其實它那個DSA相比於NSA
[00:54:22] SPEAKER_02: 也是我們在NSA V2裡面
[00:54:25] SPEAKER_02: 就批判了一個點
[00:54:26] SPEAKER_02: NSA暫時它做出來那個架構
[00:54:28] SPEAKER_02: 它有兩個缺點
[00:54:30] SPEAKER_02: 第一個缺點就是它對短文本身上不友好
[00:54:32] SPEAKER_02: 因為它拆成了三個注意力組件
[00:54:35] SPEAKER_02: 這樣的話對於短文本而言
[00:54:37] SPEAKER_02: 它三個注意力組件都得算
[00:54:39] SPEAKER_02: 就計算成本節成三了
[00:54:41] SPEAKER_02: 然後第二個是說它對於後尋臉不友好
[00:54:45] SPEAKER_02: 就是說對大部分的模型而言
[00:54:46] SPEAKER_02: 我們現在的尋臉犯是就是長文本的預尋臉
[00:54:50] SPEAKER_02: 然後長文本就是做到後尋臉的階段來做
[00:54:52] SPEAKER_02: 不會從頭開始做長文本的預尋臉
[00:54:55] SPEAKER_02: NSA就沒辦法做短文本的預尋臉
[00:54:58] SPEAKER_02: 長文本後尋臉這個犯事
[00:55:00] SPEAKER_02: 所以它DSA就把這兩個點給拋棄了
[00:55:03] SPEAKER_02: 然後設計了一套
[00:55:04] SPEAKER_02: 這個單身體架構起來是非常類似的
[00:55:07] SPEAKER_02: 直播時面向這種後尋臉的犯事
[00:55:09] SPEAKER_02: 然後去優化了一些設計
[00:55:11] SPEAKER_02: 我們印象明明要當時也是覺得
[00:55:13] SPEAKER_02: 它這兩個點不好
[00:55:14] SPEAKER_02: 然後我們就提出了一個新的方案
[00:55:17] SPEAKER_02: 然後因為剛才講的是吸塑注意力的改進
[00:55:19] SPEAKER_00: 前段時間我和高材代特的核心
[00:55:22] SPEAKER_00: 做了洋松林了過現行主義的一些改進
[00:55:25] SPEAKER_00: 因為像那個Kun San Laks
[00:55:26] SPEAKER_00: 還有Kimeyu一個KDA
[00:55:28] SPEAKER_00: 因為這和DSA很像
[00:55:30] SPEAKER_00: 然後他們都是做一些現行主義的改進
[00:55:33] SPEAKER_00: 然後混了這個Delta and Latin和FuO transition
[00:55:35] SPEAKER_00: 然後當時洋松林也提到
[00:55:36] SPEAKER_00: 他覺得下一個方向
[00:55:37] SPEAKER_00: 就大家在探索的是說
[00:55:39] SPEAKER_00: 我直接用現行主義力去混吸塑注意力
[00:55:42] SPEAKER_00: 你們是不是也有這方面的一些探索
[00:55:45] SPEAKER_00: 對
[00:55:46] SPEAKER_02: 因為這件事情大家都能關注到我
[00:55:48] SPEAKER_02: 然後其實你可以這麼未必
[00:55:51] SPEAKER_02: 現行主義力它對應的方式不是全注意力
[00:55:54] SPEAKER_02: 而是Sleiding Window
[00:55:55] SPEAKER_02: 就是因為Sleiding Window天然的長速向的存幅
[00:55:59] SPEAKER_02: 然後能夠去做這種長文本的處理
[00:56:02] SPEAKER_02: 它的對應像是這個
[00:56:03] SPEAKER_02: 然後Spares的對應的向才是FuO transition
[00:56:06] SPEAKER_02: 就是全的寵秘的注意力
[00:56:08] SPEAKER_02: 所以其實這兩個方向
[00:56:10] SPEAKER_02: 在未來應該是要合避在一起的
[00:56:11] SPEAKER_02: 因為現在你其實可以看到有很多的工作像
[00:56:14] SPEAKER_02: KDA也好
[00:56:15] SPEAKER_02: Mini Max也好
[00:56:16] SPEAKER_02: 他們其實都在嘗試的做
[00:56:18] SPEAKER_02: 就是現行主義力混合寵秘主義力
[00:56:21] SPEAKER_02: 然後其實可以看到
[00:56:22] SPEAKER_02: 它能夠在很多場景下
[00:56:24] SPEAKER_02: 能夠保持對長文本的高效處理和效果
[00:56:26] SPEAKER_02: 然後那未來的話
[00:56:27] SPEAKER_02: 其實這個現行的似乎應該是一個自然的想法
[00:56:32] SPEAKER_02: 但其實我覺得現在一個
[00:56:33] SPEAKER_02: 繼續解決一個問題
[00:56:34] SPEAKER_02: 現在大家關注所謂的長文本
[00:56:36] SPEAKER_02: 其實還是關注在之前那種長輸物的問題上
[00:56:40] SPEAKER_02: 然後其實我覺得現在的長文本
[00:56:42] SPEAKER_02: 就像我剛才提到的
[00:56:43] SPEAKER_02: 它的發展的路徑一定是面向未來的生思考
[00:56:46] SPEAKER_02: 和這種A-jump的場景
[00:56:48] SPEAKER_02: 那面向生思考和A-jump的場景
[00:56:50] SPEAKER_02: 它更應該處理的內容
[00:56:52] SPEAKER_02: 就是它會有很長的這種輸出
[00:56:55] SPEAKER_02: 那所以面向這種長輸出來說
[00:56:57] SPEAKER_02: 其實現在很多這種現行主義力
[00:56:59] SPEAKER_02: 它的現實力遠不夠的
[00:57:01] SPEAKER_02: 所以你其實可以發現
[00:57:02] SPEAKER_02: Mini Max為什麼從這一次
[00:57:04] SPEAKER_02: 這一次M1做這個
[00:57:07] SPEAKER_02: Lighten的展示
[00:57:08] SPEAKER_00: 對 現行主義力混合全選
[00:57:10] SPEAKER_00: 對 然後到現在用全副的展示
[00:57:12] SPEAKER_02: 我覺得一個很重要的原因
[00:57:13] SPEAKER_02: 就是M2它去想主大A-jump
[00:57:16] SPEAKER_02: 但是可能即使前一過了
[00:57:17] SPEAKER_02: 會發現今天事情三部號的還是有點性能上
[00:57:20] SPEAKER_02: 的損失所以它最終選擇了
[00:57:22] SPEAKER_02: Fort 成績上這少路徑
[00:57:23] SPEAKER_02: 所以我認為其實這也是
[00:57:25] SPEAKER_02: 大家現在關注長文本
[00:57:26] SPEAKER_02: 有點偏頗導致的一個很重要的原因
[00:57:30] SPEAKER_02: 你年初的時候不就是說
[00:57:31] SPEAKER_00: 應該更多關注長輸出嗎
[00:57:32] SPEAKER_00: 這事兒到現在也不是共識什麼
[00:57:34] SPEAKER_00: 我覺得是
[00:57:36] SPEAKER_02: 因為就是今天事情大家可能
[00:57:38] SPEAKER_02: 還是會認為長輸入很重要
[00:57:41] SPEAKER_02: 就是給你位一本輸入
[00:57:42] SPEAKER_02: 然後問你的QA很重要
[00:57:43] SPEAKER_02: 長輸出其實因為COT
[00:57:45] SPEAKER_00: 所以變得更重要了
[00:57:46] SPEAKER_00: 對 以及包括Planning
[00:57:47] SPEAKER_02: 就是我得比如說像A-jump
[00:57:49] SPEAKER_02: 我得去規劃好
[00:57:50] SPEAKER_02: 我1235個步驟
[00:57:52] SPEAKER_02: 然後你最不能做到一件事情
[00:57:54] SPEAKER_02: 我做完第二個步驟
[00:57:55] SPEAKER_02: 我把第一個步驟做什麼事情給忘
[00:57:57] SPEAKER_02: 然後反過頭再再做一遍
[00:57:58] SPEAKER_02: 所以其實就是這種
[00:57:59] SPEAKER_02: 深思考
[00:58:00] SPEAKER_02: 然後多部的Planning就有難力
[00:58:03] SPEAKER_02: 其實是對於現在的長文本
[00:58:04] SPEAKER_02: 加多一個新的挑戰
[00:58:06] SPEAKER_00: 然後你說現行主義
[00:58:06] SPEAKER_00: 你在這件事情經常表現的不是特別好
[00:58:08] SPEAKER_00: 還是跟現行主義本身
[00:58:10] SPEAKER_00: 還要更容易一忘
[00:58:11] SPEAKER_00: 之前的一些內容有關
[00:58:12] SPEAKER_00: 對
[00:58:13] SPEAKER_02: 在長輸出上
[00:58:14] SPEAKER_00: 它即使混了
[00:58:14] SPEAKER_00: Fort 成績也不能決這個問題嗎
[00:58:16] SPEAKER_00: 就得看你很多好了
[00:58:18] SPEAKER_02: 那之前大家都很激進的
[00:58:19] SPEAKER_02: 像Mini Mad
[00:58:20] SPEAKER_02: 它是一比七的混
[00:58:21] SPEAKER_02: 只有一層複合
[00:58:22] SPEAKER_02: 氣存的現行
[00:58:23] SPEAKER_02: 但是我估計到後面
[00:58:25] SPEAKER_02: 他們有點沒公主
[00:58:26] SPEAKER_02: 然後導致他們自封選擇的複合團選
[00:58:29] SPEAKER_02: 我理解這個方向是有未來的
[00:58:31] SPEAKER_02: 就是它應該是能夠解決掉的
[00:58:33] SPEAKER_02: 但是可能還需要一點時間
[00:58:35] SPEAKER_02: 那你們在自己的下一代的
[00:58:37] SPEAKER_00: 比如說期間模型上
[00:58:38] SPEAKER_00: 你們會採取什麼樣的假構
[00:58:40] SPEAKER_00: 你可以預告一下
[00:58:41] SPEAKER_02: 還是要深刻的
[00:58:43] 對 可能有一個肯定的點
[00:58:44] SPEAKER_02: 是一個全吸塑假構
[00:58:46] SPEAKER_02: 就是阿富汗分野吸塑
[00:58:47] SPEAKER_02: 騰涉野吸塑
[00:58:48] SPEAKER_02: 但距離而騰涉
[00:58:49] SPEAKER_02: 會不會再進一步的
[00:58:50] SPEAKER_02: 那可能還得我們再
[00:58:51] SPEAKER_02: 更多的戲演上去嘗試
[00:58:52] SPEAKER_02: 對
[00:58:53] SPEAKER_00: 接下來就是想研擇
[00:58:54] SPEAKER_00: 討論一下這個密度定律
[00:58:56] SPEAKER_00: 就有了這個定量的描述之後
[00:58:57] SPEAKER_00: 對研究界還有業界
[00:58:59] SPEAKER_00: 有些什麼影響
[00:59:00] SPEAKER_00: 就比如說有觀察到
[00:59:01] SPEAKER_00: 比如說你們自己的行為
[00:59:02] SPEAKER_00: 包括你
[00:59:03] SPEAKER_00: 就其他這個行為裡的
[00:59:04] SPEAKER_00: 其他角色的行為有什麼影響嗎
[00:59:06] SPEAKER_00: 因為其實你們發出來
[00:59:07] SPEAKER_00: 應該有段時間了
[00:59:08] SPEAKER_00: 對吧
[00:59:08] SPEAKER_00: 就是在這個雜誌看盪之前
[00:59:10] SPEAKER_00: 其實那個首先
[00:59:11] SPEAKER_01: 我覺得對我們自己內部
[00:59:13] SPEAKER_01: 就是我覺得是一個
[00:59:14] SPEAKER_01: 統一思想的
[00:59:15] SPEAKER_01: 一個非常重要的一個點
[00:59:17] SPEAKER_01: 所以現在大家就是
[00:59:18] SPEAKER_01: 不管是做模型加構的創新
[00:59:20] SPEAKER_01: 數據治理的創新
[00:59:22] SPEAKER_01: 還是說這個奉動的建設
[00:59:24] SPEAKER_01: 然後以及軟硬一體 血統
[00:59:26] SPEAKER_01: 其實現在有了一個統一的
[00:59:28] SPEAKER_01: 一個評價的一個標準
[00:59:30] SPEAKER_01: 或者是一個指標
[00:59:31] SPEAKER_01: 然後這樣的話
[00:59:32] SPEAKER_01: 其實大家就可以更容易的
[00:59:34] SPEAKER_01: 去行程公司
[00:59:35] SPEAKER_01: 然後來共同推進
[00:59:36] SPEAKER_01: 因為其實這個
[00:59:38] SPEAKER_01: 我覺得可能它跟說
[00:59:39] SPEAKER_01: 我把模型簡單的去的越大
[00:59:42] SPEAKER_01: 然後讓它去能力更強
[00:59:44] SPEAKER_01: 這個我覺得還是
[00:59:45] SPEAKER_01: 一個不太一樣的一個取向
[00:59:47] SPEAKER_01: 就這個我覺得是
[00:59:48] SPEAKER_01: 我們內部的一個非常重要的一個體系
[00:59:50] SPEAKER_01: 也就是說我們自己內部
[00:59:52] SPEAKER_01: 其實會經常喜歡類比芯片
[00:59:55] SPEAKER_01: 比如說現在去構造一個大模型
[00:59:58] SPEAKER_01: 我們所形成的這個包含
[01:00:00] SPEAKER_01: 加工設計數據治理 學習方法
[01:00:04] SPEAKER_01: 然後以及軟硬一體的血統
[01:00:06] SPEAKER_01: 整個這個體系
[01:00:07] SPEAKER_01: 我們喜歡把它稱為叫大模型的光刻機
[01:00:11] SPEAKER_01: 它本身其實是一個
[01:00:12] SPEAKER_01: 很複雜的一個體系
[01:00:13] SPEAKER_01: 那其實是要有一個共同的一個目標
[01:00:16] SPEAKER_01: 然後才能夠把
[01:00:17] SPEAKER_01: 比如說我們多達上百人的一個團隊
[01:00:20] SPEAKER_01: 然後能夠形成一個非常好的一個血統
[01:00:23] SPEAKER_01: 這個其實我覺得
[01:00:24] SPEAKER_01: 對內部的一個價值
[01:00:25] SPEAKER_01: 對外部的話
[01:00:26] SPEAKER_01: 我們所能夠看到的
[01:00:28] SPEAKER_01: 就是當然其實今年初開始
[01:00:31] SPEAKER_01: 就是也是UEDC V3的這個影響
[01:00:34] SPEAKER_01: 其實你會發現像
[01:00:35] SPEAKER_01: SIME Ultimate
[01:00:36] SPEAKER_01: 然後包括Andsopic
[01:00:38] SPEAKER_01: 包括後來就是Mario Maker
[01:00:40] SPEAKER_01: 它的那個互聯網的報告
[01:00:42] SPEAKER_01: 其實都特別的強調
[01:00:43] SPEAKER_01: 就是AI的成本問題
[01:00:45] SPEAKER_01: 所以其實我會覺得
[01:00:47] SPEAKER_01: 就是這個本身是說明實事的
[01:00:49] SPEAKER_01: 也就是說從今年開始
[01:00:50] SPEAKER_01: 大家開始發現
[01:00:52] SPEAKER_01: 就是這個效率或者是這個
[01:00:54] SPEAKER_01: 能效變得非常的關鍵
[01:00:56] SPEAKER_01: 原因是在於隨著
[01:00:59] SPEAKER_01: 我們要把AI真正的廣泛
[01:01:01] SPEAKER_01: 的應用的各行各業
[01:01:02] SPEAKER_01: 那麼這個事情會變得非常關鍵
[01:01:04] SPEAKER_01: 同時的話我們會看到
[01:01:06] SPEAKER_01: 就是經常原因
[01:01:08] SPEAKER_01: 我們密度法則的這個報告
[01:01:10] SPEAKER_01: 然後相關的工作
[01:01:11] SPEAKER_01: 基本上是發生在這個
[01:01:13] SPEAKER_01: 居身智能等等的
[01:01:15] SPEAKER_01: 這些專家的這個報告裡面
[01:01:18] SPEAKER_01: 就基本上你會可以看到
[01:01:20] SPEAKER_01: 就是在這個居身
[01:01:22] SPEAKER_01: 因為本身它需要構建
[01:01:24] SPEAKER_01: 就是這個相當於是這個
[01:01:26] SPEAKER_01: 機器人的大腦小腦等等的
[01:01:28] SPEAKER_01: 其實他們會對
[01:01:29] SPEAKER_01: 就是這種端竊的這種智能的
[01:01:32] SPEAKER_01: 這個構建的規律
[01:01:33] SPEAKER_01: 可能會有更加敏感的需求
[01:01:35] SPEAKER_01: 就是因為它是對延遲
[01:01:36] SPEAKER_00: 這些要求表格
[01:01:37] SPEAKER_00: 對的
[01:01:38] SPEAKER_00: 不可能就是
[01:01:40] SPEAKER_00: 全部依賴雲端的這種模式
[01:01:41] SPEAKER_00: 對的
[01:01:42] SPEAKER_00: 因為當時其實就是
[01:01:43] SPEAKER_01: 這篇是發的那一只
[01:01:45] SPEAKER_01: 馬上引擎人士上
[01:01:46] SPEAKER_01: 在前面其實有一篇是
[01:01:48] SPEAKER_01: 發的那一只
[01:01:48] SPEAKER_01: Cominiqueism上
[01:01:49] SPEAKER_01: 就是MinicPM-Way
[01:01:51] SPEAKER_01: 那篇文章
[01:01:52] SPEAKER_01: 那篇文章其實裡面有一個
[01:01:54] SPEAKER_01: 非常經典的圖
[01:01:55] SPEAKER_01: 就是說密度法則和模二定律
[01:01:57] SPEAKER_01: 其實是皆是了
[01:01:58] SPEAKER_01: 就是我們在中端上
[01:02:00] SPEAKER_01: 其實可以裝得下一個
[01:02:02] SPEAKER_01: 在歷史上只能夠在服務器上
[01:02:04] SPEAKER_01: 在雲上才能裝得下跑得動的一個模型
[01:02:07] SPEAKER_01: 就那個圖就經常被很多
[01:02:09] SPEAKER_01: 這個做巨神智能的專家和團隊
[01:02:12] SPEAKER_01: 去使用
[01:02:13] SPEAKER_01: 就大家會認為
[01:02:14] SPEAKER_01: 這個其實是讓他們去做巨神大腦
[01:02:17] SPEAKER_01: 有了這個非常好的
[01:02:18] SPEAKER_01: 這麼一個法律的依據
[01:02:20] SPEAKER_01: 這個其實在幫他們預測
[01:02:21] SPEAKER_00: 就是說
[01:02:21] SPEAKER_00: 至少就是在這個
[01:02:23] SPEAKER_00: 比如說算力
[01:02:23] SPEAKER_00: 或者是這個能力密度上
[01:02:24] SPEAKER_00: 它能跑在這個巨神機器人端策
[01:02:27] SPEAKER_00: 的這種模型什麼時候到來
[01:02:29] SPEAKER_00: 對
[01:02:29] SPEAKER_00: 所以我們知道的
[01:02:30] SPEAKER_01: 就是別人轉給我的
[01:02:32] SPEAKER_01: 就是可能很多高機器人的專家
[01:02:34] SPEAKER_01: 可能經常特別喜歡去
[01:02:36] SPEAKER_01: 用我們的關於Dancing Law的
[01:02:39] SPEAKER_01: 相關的一些成功
[01:02:40] SPEAKER_01: 按照現在的就是你們
[01:02:42] SPEAKER_00: 針對之前這兩年多
[01:02:44] SPEAKER_00: 這些開源模型做的這個預測
[01:02:46] SPEAKER_00: 就是巨神上要能用上
[01:02:47] SPEAKER_00: 端策的模型
[01:02:50] SPEAKER_00: 現在也很強的
[01:02:50] SPEAKER_00: 能支持巨神大腦的
[01:02:52] SPEAKER_00: 會在什麼時候出現
[01:02:53] SPEAKER_00: 我們其實就是接下來這幾年
[01:02:56] SPEAKER_01: 就是從今年開始
[01:02:57] SPEAKER_01: 一直到2030年
[01:02:59] SPEAKER_01: 其實全球的依陷的新片團隊
[01:03:03] SPEAKER_01: 包括因為大家包括華為
[01:03:04] SPEAKER_01: 其實他們都會有自己的
[01:03:06] SPEAKER_01: 基於就是自己的製程
[01:03:08] SPEAKER_01: 一個路線圖
[01:03:09] SPEAKER_01: 也就是說它每年發布的
[01:03:11] SPEAKER_01: 那個新片大概是什麼樣的算力
[01:03:13] SPEAKER_01: 放存等等的這種規格
[01:03:15] SPEAKER_01: 其實都是有的
[01:03:16] SPEAKER_01: 我們其實根據那些
[01:03:17] SPEAKER_01: 相應發布的這個規格
[01:03:19] SPEAKER_01: 我們其實可以算出來
[01:03:21] SPEAKER_01: 就是在這些主流新片上
[01:03:23] SPEAKER_01: 就是端策的新片上
[01:03:25] SPEAKER_01: 然後可以去加載的這個模型的尺寸
[01:03:28] SPEAKER_01: 然後以及它的機會殘殊的這個規模
[01:03:31] SPEAKER_01: 大概的這個程度是多少
[01:03:33] SPEAKER_01: 所以我們大概應該是有一個孤算
[01:03:35] SPEAKER_01: 就是到2030年
[01:03:37] SPEAKER_01: 實際上是可以在端策上
[01:03:39] SPEAKER_01: 然後能夠部署一個超過60筆
[01:03:41] SPEAKER_01: 也就是600一殘殊的
[01:03:43] SPEAKER_01: 整個一個大的模型
[01:03:44] SPEAKER_01: 然後它的機會殘殊
[01:03:46] SPEAKER_01: 大概可以達到8筆以上
[01:03:48] SPEAKER_01: 大概是這麼一個水平
[01:03:50] SPEAKER_01: 當然就是說這個本身是現性的預測
[01:03:53] SPEAKER_01: 就是你也不妨礙說
[01:03:54] SPEAKER_01: 接下來這幾年的時間
[01:03:55] SPEAKER_01: 可能會出現一些飛線性的
[01:03:57] SPEAKER_01: 這麼一些突破
[01:03:58] SPEAKER_01: 就是這個單純從現性上來講
[01:04:00] SPEAKER_01: 大概達到這個水平
[01:04:01] SPEAKER_01: 所以我們同時如果跌加這個密度法則
[01:04:05] SPEAKER_01: 我們會認為就是到這個
[01:04:07] SPEAKER_01: 接下來的這五年的時間
[01:04:09] SPEAKER_01: 我們一定是可以把一個GT4
[01:04:11] SPEAKER_01: 到GT5水平模型的能力
[01:04:14] SPEAKER_01: 然後可以放在專程上
[01:04:16] SPEAKER_01: 然後就能夠形成
[01:04:18] SPEAKER_01: 就是每個用戶自己專屬的
[01:04:20] SPEAKER_01: 這麼一個能力
[01:04:21] SPEAKER_01: 大概是這麼一個大概孤算
[01:04:23] SPEAKER_01: 我覺得這個對手機
[01:04:24] SPEAKER_00: 這類相對程序的移動設備
[01:04:26] SPEAKER_00: 應該還是挺有用的
[01:04:27] SPEAKER_00: 對 是
[01:04:28] SPEAKER_01: 所以就是手機機器人車
[01:04:31] SPEAKER_01: 這種機器和包裝PC
[01:04:34] SPEAKER_01: 就是這幾大中端
[01:04:36] SPEAKER_01: 現在來看它的順序
[01:04:37] SPEAKER_01: 應該是車現在是今年
[01:04:40] SPEAKER_01: 就是這個鋪開的非常快
[01:04:42] SPEAKER_01: 因為車它對功耗
[01:04:43] SPEAKER_00: 這些要求更低一點
[01:04:44] SPEAKER_00: 更低而且它空間更大
[01:04:46] SPEAKER_01: 那麼再次一級就是PC
[01:04:48] SPEAKER_01: 那麼再次一級就是手機
[01:04:50] SPEAKER_01: 那麼本身機器人又令到別論
[01:04:53] SPEAKER_01: 因為本身它現在也還處在一個快速發展
[01:04:57] SPEAKER_01: 還沒有固定下來的這麼一個狀態
[01:04:59] SPEAKER_01: 但是相對來講
[01:05:00] SPEAKER_01: 我們會覺得機器人的這個空間
[01:05:02] SPEAKER_01: 應該也還可以
[01:05:03] SPEAKER_01: 只是說它自己本身已經有非常大的功耗的問題
[01:05:07] SPEAKER_01: 所以我們會認為就是在內頂上
[01:05:09] SPEAKER_01: 去加點一個特別高算力的大模型
[01:05:13] SPEAKER_01: 可能也還是需要一個
[01:05:15] SPEAKER_01: 再進一步的去它所的這麼一個方向
[01:05:17] SPEAKER_01: 而且我覺得對手機電腦還有車來說
[01:05:20] SPEAKER_00: 就是你在上面放一個GBT-C
[01:05:21] SPEAKER_00: GBT-5水平的模型
[01:05:23] SPEAKER_00: 我覺得是比較容易想像
[01:05:24] SPEAKER_00: 它能做什麼 帶來什麼價值的
[01:05:26] SPEAKER_00: 而去升智能
[01:05:27] SPEAKER_00: 可能現在它有一個別的難點
[01:05:28] SPEAKER_00: 就是說你一個什麼樣水平的模型
[01:05:31] SPEAKER_00: 在那個上面可以 work
[01:05:32] SPEAKER_00: 因為其實大家沒有完全研究出來說
[01:05:34] SPEAKER_00: 這個巨生模型到底是怎麼做嗎
[01:05:36] SPEAKER_00: 其實車你可以看成是一個
[01:05:39] SPEAKER_01: 各樣成熟的機器人
[01:05:40] SPEAKER_01: 它的相對的action比較簡單
[01:05:42] SPEAKER_01: 但是就是說即使在車上
[01:05:44] SPEAKER_01: 你比如說自動駕駛這個小腦的模型
[01:05:47] SPEAKER_01: 和智能做倉這個大腦的模型
[01:05:49] SPEAKER_01: 現在來講其實還是相對獨立的去發展的
[01:05:52] SPEAKER_01: 其實未來機器人
[01:05:54] SPEAKER_01: 它一旦它的那個小腦的那個部分
[01:05:56] SPEAKER_01: 相對穩定之後 然後再有大腦
[01:05:58] SPEAKER_01: 那其實接下來還有一個命題
[01:06:00] SPEAKER_01: 就是大小腦如何系統
[01:06:02] SPEAKER_01: 其實就是在這些方面
[01:06:04] SPEAKER_01: 我理解就是這個機器人這個方向
[01:06:06] SPEAKER_01: 它連小腦可能都還沒有穩定下來
[01:06:09] SPEAKER_01: 所以就還不用還談不上
[01:06:12] SPEAKER_01: 就是大小腦如何系統了
[01:06:14] SPEAKER_00: 不我覺得VIA算是這種嘗試的
[01:06:16] SPEAKER_00: 這大家設想的一個系統下的方式
[01:06:18] SPEAKER_00: 對 所以我覺得現在還處在戰國時代
[01:06:21] SPEAKER_01: 就是向大家擺放期望
[01:06:23] SPEAKER_01: 那對於我們來說
[01:06:24] SPEAKER_00: 其實你們很多應用也是在這些端策上 是嗎
[01:06:27] SPEAKER_00: 我們所以就是按剛才我們所說的順序
[01:06:30] SPEAKER_01: 就基本上現在今年來看
[01:06:31] SPEAKER_01: 就是在車上的進展特別大
[01:06:33] SPEAKER_01: 在手機上因為它相對算力還比較數線
[01:06:37] SPEAKER_01: 所以你看就是我們其實也試過
[01:06:39] SPEAKER_01: 就是Apple Intelligence
[01:06:41] SPEAKER_01: 它的整體效果其實還比較數線一些
[01:06:44] SPEAKER_01: 但是我們根據我們剛才的估算
[01:06:46] SPEAKER_01: 應該會在未來的兩到三年
[01:06:48] SPEAKER_01: 就會產生一個非常大的一個月前
[01:06:50] SPEAKER_01: 可以講講你們和車起的一些合作嗎
[01:06:52] SPEAKER_00: 就或者說在氣氏這個場景上
[01:06:54] SPEAKER_00: 有一些落地有
[01:06:55] SPEAKER_00: 就有落地的話是這樣子
[01:06:56] SPEAKER_00: 就是我們今年應該最早的落地的是
[01:06:59] SPEAKER_01: 查馬子達的一款車型
[01:07:01] SPEAKER_01: 然後後面就是吉利
[01:07:03] SPEAKER_01: 這個有一款車型
[01:07:05] SPEAKER_01: 然後基本上在今年就已經量產
[01:07:08] SPEAKER_01: 它的我覺得就算速度還比較快
[01:07:11] SPEAKER_01: 因為我們是在今年上半年就入場
[01:07:14] SPEAKER_01: 然後開始這個相關的合作
[01:07:16] SPEAKER_01: 但是在今年下半年
[01:07:17] SPEAKER_01: 就能完成量產的這麼一個動作
[01:07:20] SPEAKER_01: 基本上這個車這個行業
[01:07:22] SPEAKER_01: 本身還是非常罕見的
[01:07:24] SPEAKER_01: 我們前面其實有過一個推送
[01:07:27] SPEAKER_01: 其實是介紹這個方面的最新的進展
[01:07:30] SPEAKER_01: 那麼明年的話
[01:07:31] SPEAKER_01: 我們陸續應該會有超過6款以上的
[01:07:34] SPEAKER_01: 不同的車起的車型
[01:07:37] SPEAKER_01: 然後會加在我們的這個模型
[01:07:39] SPEAKER_01: 基本上我們在一線的感受
[01:07:42] SPEAKER_01: 就是由於我們特別強調密度法則
[01:07:45] SPEAKER_01: 那你就可以想像這個在車這個領域
[01:07:48] SPEAKER_01: 只能做倉
[01:07:49] SPEAKER_01: 它本身每一款車型
[01:07:51] SPEAKER_01: 它都是會約定好
[01:07:52] SPEAKER_01: 它用哪一款芯片
[01:07:54] SPEAKER_01: 那麼這款芯片
[01:07:56] SPEAKER_01: 同時在約定好它的響應時間
[01:07:59] SPEAKER_01: 它的功耗之後
[01:08:00] SPEAKER_01: 其實你在這個芯片上
[01:08:02] SPEAKER_01: 能夠加在的模型的那個彩鼠規模
[01:08:05] SPEAKER_01: 基本上是固定下來
[01:08:06] SPEAKER_01: 那麼密度法則讓我們團隊
[01:08:09] SPEAKER_01: 能夠固定一個密度更高的模型
[01:08:11] SPEAKER_01: 基本上就是在這些車這個方面的
[01:08:14] SPEAKER_01: 相關的這個競爭
[01:08:16] SPEAKER_01: 就基本上是處在一個非常領先的水平
[01:08:18] SPEAKER_01: 就這個是我們今年就是在車這個方向
[01:08:22] SPEAKER_01: 尤其是只能做倉上
[01:08:23] SPEAKER_01: 然後取得非常快的這個推進的
[01:08:25] SPEAKER_01: 一個比較底層的一個落
[01:08:27] SPEAKER_01: 像面臂的模型在車上找出做什麼
[01:08:29] SPEAKER_00: 因為比如說大圓模型
[01:08:30] SPEAKER_00: 我覺得可能標好理解
[01:08:31] SPEAKER_00: 有些運動交互省的
[01:08:32] SPEAKER_00: 然後你們多摩太
[01:08:33] SPEAKER_00: 其實你們多摩太的那個開源的那個
[01:08:35] SPEAKER_00: 版本在給它不上的形式
[01:08:36] SPEAKER_00: 它下載都挺多的
[01:08:38] 多摩太用在車上可以做什麼
[01:08:39] SPEAKER_00: 主要是一些這個
[01:08:41] SPEAKER_01: 現在還處在一個智能做倉的一個
[01:08:44] SPEAKER_01: 初期的階段
[01:08:44] SPEAKER_01: 主要是用這個多摩太的模型
[01:08:46] SPEAKER_01: 去做一些車外和車內的
[01:08:48] SPEAKER_01: 這個相關的一些環境的感知
[01:08:51] SPEAKER_01: 然後同時通過自然圓的方式
[01:08:53] SPEAKER_01: 跟這個車內的這些不同的位置的人
[01:08:56] SPEAKER_01: 然後來進行這麼一個交互
[01:08:58] SPEAKER_01: 來滿足他們的這個相關的一些需求
[01:09:01] SPEAKER_01: 並且做相應的這個提醒
[01:09:03] SPEAKER_01: 這個其實現在不同的車廠
[01:09:05] SPEAKER_01: 它對於這個智能做倉的這個功能的定義
[01:09:08] SPEAKER_01: 其實有非常大的這個不同
[01:09:10] SPEAKER_01: 不同層次的這個車
[01:09:13] SPEAKER_01: 它其實也會有相應的不同的這個設計
[01:09:16] SPEAKER_01: 比如說一些相對比較複雜的智能做倉的需求
[01:09:19] SPEAKER_01: 可能它的這個功能點
[01:09:20] SPEAKER_01: 得能超過100多個
[01:09:22] SPEAKER_01: 就是相當於說你可以在各個方面
[01:09:24] SPEAKER_01: 然後可以跟這個智能做倉的這個應用
[01:09:27] SPEAKER_01: 進行一個非常好的這麼一個交互
[01:09:29] SPEAKER_01: 總體的目標是要讓這個做倉
[01:09:32] SPEAKER_01: 就是你做到這個車裡面
[01:09:34] SPEAKER_01: 你就能夠體驗到一個全智能化的這麼一個環境
[01:09:38] SPEAKER_01: 這個其實是它的這個目標
[01:09:40] SPEAKER_01: 但是就是目前來看智能做倉
[01:09:42] SPEAKER_01: 還沒有達成一個標準化的所有的車
[01:09:45] SPEAKER_01: 然後都有一個共識的這麼一個發展
[01:09:49] SPEAKER_01: 還處在一個初步的這麼一個階段
[01:09:51] SPEAKER_01: 就是我想知道比說像現在做大模型研發的公司
[01:09:55] SPEAKER_00: 如果去把這個模型放到車上
[01:09:57] SPEAKER_00: 就像你剛才說它可能以作倉
[01:09:59] SPEAKER_00: 以後100多個功能點
[01:10:00] SPEAKER_00: 這個模型和功能點之間
[01:10:02] SPEAKER_00: 就是你要去試配去結合
[01:10:04] SPEAKER_00: 主要是一個微調的過程
[01:10:06] SPEAKER_01: 是你們來做嗎還是這個去
[01:10:08] SPEAKER_00: 現在來講是我們來做
[01:10:10] SPEAKER_01: 這個微調大概要比說要多少人調多久
[01:10:13] SPEAKER_00: 就是因為我們本身會特別地強調這個算的標準化
[01:10:17] SPEAKER_01: 就譬如我們的模型
[01:10:18] SPEAKER_01: 然後我們其實會內部會有一個非常
[01:10:21] SPEAKER_01: 這個標準化的這麼一個SFT的這麼一個工具鏈
[01:10:26] SPEAKER_01: 然後以及就是如何去做
[01:10:27] SPEAKER_01: 相應的這個數據合成的這麼一個規範
[01:10:30] SPEAKER_01: 所以就差不多
[01:10:31] SPEAKER_01: 反正是相對是比較高效的這麼一個模式
[01:10:34] SPEAKER_01: 就是回到你們23年年中的時候
[01:10:36] SPEAKER_00: 您當時也說就是其實面臨一個角色
[01:10:38] SPEAKER_00: 就是在付現了CHAGVT的能力之後
[01:10:41] SPEAKER_00: 我是繼續花大幾千萬人民幣
[01:10:43] SPEAKER_00: 我需順一個主流140幣的模型去JGVT4
[01:10:47] SPEAKER_00: 還是說我去做這個跟高效判斷
[01:10:49] SPEAKER_00: 其實當時這個判斷背後是有一個商業邏輯在的
[01:10:51] SPEAKER_00: 像這個是您會參與決策嗎還是
[01:10:54] SPEAKER_00: 是的
[01:10:54] SPEAKER_00: 因為我們那時候作為創業公司
[01:10:56] SPEAKER_01: 在2023年的下半年的時候
[01:10:58] SPEAKER_01: 其實也在尋找我們的商業模式
[01:11:01] SPEAKER_01: 所以在那個時候可能的一個選擇
[01:11:03] SPEAKER_01: 就是像很多的這個
[01:11:06] SPEAKER_01: 創業公司就是大模型的公司一樣
[01:11:09] SPEAKER_01: 來提供APR的服務
[01:11:10] SPEAKER_01: 這個其實在我們的選項之內的
[01:11:12] SPEAKER_01: 但是我們的研判就是
[01:11:14] SPEAKER_01: 會認為這種APR的服務
[01:11:16] SPEAKER_01: 缺少差一化
[01:11:17] SPEAKER_01: 比如說都是個家
[01:11:19] SPEAKER_01: 然後來去提供APR
[01:11:20] SPEAKER_01: 你的競爭力體驗的什麼地方呢
[01:11:22] SPEAKER_01: 當然我們說從密度法則來講
[01:11:24] SPEAKER_01: 我可以讓我的模型
[01:11:26] SPEAKER_01: 然後它的成本更低
[01:11:27] SPEAKER_01: 我可以讓APR的加個更低
[01:11:29] SPEAKER_01: 但是我們會認為在雲上這個模式
[01:11:33] SPEAKER_01: 不具備商業化的這個壁類
[01:11:36] SPEAKER_01: 原因是什麼呢
[01:11:37] SPEAKER_01: 原因是會有一些大廠
[01:11:39] SPEAKER_01: 它會來發揮它的超能力
[01:11:41] SPEAKER_01: 它在其他的方面
[01:11:43] SPEAKER_01: 它盈利非常的多
[01:11:44] SPEAKER_01: 對它可以燒起來
[01:11:44] SPEAKER_01: 它加點很厚
[01:11:45] SPEAKER_01: 它可以燒錢
[01:11:47] SPEAKER_01: 它可以選擇在這個方向上
[01:11:48] SPEAKER_01: 我不計成本的去進行推廣
[01:11:50] SPEAKER_01: 所以我們會認為在雲上
[01:11:52] SPEAKER_01: 來提供APR對於一個創業公司
[01:11:55] SPEAKER_01: 尤其是加點很不太厚的創業公司
[01:11:57] SPEAKER_01: 我們認為是一個風險極大的事情
[01:11:59] SPEAKER_01: 這個基本上也預判了
[01:12:00] SPEAKER_00: 後面的發展
[01:12:00] SPEAKER_00: 因為24年5月就開始加個戰
[01:12:02] SPEAKER_00: 就開始APR的加個戰
[01:12:03] SPEAKER_00: 對 就是那個加個戰
[01:12:04] SPEAKER_01: 我覺得其實是一個
[01:12:06] SPEAKER_01: 非常重要的一個警示
[01:12:08] SPEAKER_01: 因為其實那個加個戰
[01:12:09] SPEAKER_01: 本身就是有DPC
[01:12:10] SPEAKER_01: 對 所以也發的
[01:12:12] SPEAKER_00: 然後後面大家都跟進了什麼火山引擎
[01:12:14] SPEAKER_00: 什麼阿里這些用
[01:12:15] SPEAKER_00: 所以DPC其實本身
[01:12:17] SPEAKER_01: 它也在像我們團隊一樣
[01:12:19] SPEAKER_01: 都在追求高效
[01:12:20] SPEAKER_01: 就是下來用更高的技術
[01:12:22] SPEAKER_01: 然後來追求用更少的參數
[01:12:24] SPEAKER_01: 更少的集團量
[01:12:25] SPEAKER_01: 來完成更高的這個模型能力
[01:12:27] SPEAKER_01: 那麼它在雲上
[01:12:28] SPEAKER_01: 的確是能夠呈現一個
[01:12:30] SPEAKER_01: 非常高的這麼一個競爭力
[01:12:32] SPEAKER_01: 對吧
[01:12:32] SPEAKER_01: 它可以用更低的加個
[01:12:33] SPEAKER_01: 然後來去進行這個服務的提供
[01:12:36] SPEAKER_01: 但是我覺得
[01:12:37] SPEAKER_01: 我們跟DPC
[01:12:37] SPEAKER_01: 個不一樣的地方是
[01:12:39] SPEAKER_01: DPC
[01:12:39] SPEAKER_01: 它也有非常厚的加點
[01:12:40] SPEAKER_01: 它完全可以不考慮盈利問題
[01:12:42] SPEAKER_01: 但是對於絕大部分的創業公司
[01:12:44] SPEAKER_01: 來講其實它還是需要考慮
[01:12:46] SPEAKER_01: 自己的商業的這麼一個選項
[01:12:49] SPEAKER_01: 你們有就是副判過
[01:12:51] SPEAKER_00: 因為其實你們一直在追求
[01:12:52] SPEAKER_00: 這個高效的方式
[01:12:54] SPEAKER_00: 就它並沒有在續視和輿論上
[01:12:56] SPEAKER_00: 形成像25年1月分
[01:12:58] SPEAKER_00: DPC的那種生事是為什麼呢
[01:13:00] SPEAKER_00: 我覺得還是剛才說的
[01:13:01] SPEAKER_01: 就是從絕大部分的公眾來看
[01:13:04] SPEAKER_01: 明顯還是說誰家的能力
[01:13:07] SPEAKER_01: 能達到一個更高的水平
[01:13:09] SPEAKER_01: 那這件事情
[01:13:10] SPEAKER_01: 我覺得顯然是你要去
[01:13:12] SPEAKER_01: 去遜一個更大的模型
[01:13:13] SPEAKER_01: 才有可能達成
[01:13:14] SPEAKER_01: 譬如說幾千億的
[01:13:16] SPEAKER_01: 甚至上萬億的這麼一個參數
[01:13:18] SPEAKER_01: 來達成這件事情
[01:13:19] SPEAKER_01: 所以就是對我們來講
[01:13:20] SPEAKER_01: 我覺得我們現在所走的一條路線
[01:13:24] SPEAKER_01: 可能是更適合我們自己的判斷
[01:13:27] SPEAKER_01: 然後以及我們現在的餅負的一個路線
[01:13:30] SPEAKER_01: 因為對我來講
[01:13:31] SPEAKER_01: 我不追求一泡二紅
[01:13:33] SPEAKER_01: 我追求的是
[01:13:34] SPEAKER_01: 我能夠堅定的
[01:13:36] SPEAKER_01: 百日百日
[01:13:37] SPEAKER_01: 可以達成目標的這麼一條路徑
[01:13:39] SPEAKER_01: 譬如說
[01:13:40] SPEAKER_01: 我當然可以選擇
[01:13:41] SPEAKER_01: 孤注一致
[01:13:42] SPEAKER_01: 把我們融到的所有的錢
[01:13:44] SPEAKER_01: 都拿來去訓一個超級大的模型
[01:13:46] SPEAKER_01: 但是這件事情
[01:13:47] SPEAKER_01: 一旦它不能夠形成商業必換
[01:13:50] SPEAKER_01: 那它其實就可能會帶來一個不可接受的
[01:13:53] SPEAKER_01: 這麼一個後果
[01:13:54] SPEAKER_01: 這個對我來講
[01:13:55] SPEAKER_01: 不是我能接受的
[01:13:57] SPEAKER_01: 因為AGR一定會再未來5到10年到達
[01:13:59] SPEAKER_01: 那AGR的這個時代的
[01:14:02] SPEAKER_01: 可以做的事情非常多
[01:14:03] SPEAKER_01: 那我為什麼要去到一個英雞的
[01:14:06] SPEAKER_01: 可能我們並不佔優勢的
[01:14:07] SPEAKER_01: 一個塞道裡面去跟別人去卷
[01:14:09] SPEAKER_01: 我覺得這不是一個名字的選擇
[01:14:11] SPEAKER_01: 對我來講
[01:14:12] SPEAKER_01: 所以我覺得就是
[01:14:13] SPEAKER_01: 我在哪兒聽到一句話
[01:14:14] SPEAKER_01: 我覺得特別對
[01:14:15] SPEAKER_01: 就別人得到的並不見得
[01:14:16] SPEAKER_01: 是你失去的
[01:14:17] SPEAKER_01: 就是Divsake火
[01:14:19] SPEAKER_01: 並不代表人說
[01:14:20] SPEAKER_01: 我也追求高校
[01:14:22] SPEAKER_01: 我會覺得說
[01:14:23] SPEAKER_01: 是不是我本來應該是
[01:14:24] SPEAKER_01: 想他那樣去做
[01:14:26] SPEAKER_01: 我覺得不見得
[01:14:27] SPEAKER_01: 因為其實面向未來
[01:14:28] SPEAKER_01: AGR真的是一個非常廣闊的天地
[01:14:31] SPEAKER_01: 你當然是可以追求
[01:14:33] SPEAKER_01: 像互聯網時代的Google
[01:14:35] SPEAKER_01: 這樣去提供一個公開的
[01:14:37] SPEAKER_01: 這種雲的這種服務
[01:14:39] SPEAKER_01: 但是你可以說想
[01:14:40] SPEAKER_01: 就AGR實在的這個智能
[01:14:42] SPEAKER_01: 它既可以發生在雲端
[01:14:43] SPEAKER_01: 也會發生在端策
[01:14:45] SPEAKER_01: 那麼既然端策
[01:14:46] SPEAKER_01: 我們看到了它的無限的這個前進
[01:14:49] SPEAKER_01: 既然現在其實沒有什麼人
[01:14:51] SPEAKER_01: 然後還注意到這個方向
[01:14:52] SPEAKER_01: 那我們提前去做布局
[01:14:54] SPEAKER_01: 我覺得這個可能是更符合
[01:14:56] SPEAKER_01: 一個出售公司
[01:14:57] SPEAKER_01: 通過這個前瞻的布局
[01:14:59] SPEAKER_01: 然後來去進行探索的
[01:15:01] SPEAKER_01: 這麼一個可能
[01:15:02] SPEAKER_01: 更好的這麼一個方式
[01:15:04] SPEAKER_01: 我追求的是
[01:15:05] SPEAKER_01: 充滿不確定性的吃定性
[01:15:07] SPEAKER_01: 剛才也提到就是
[01:15:08] SPEAKER_00: 你們現在是Apple Intelligence
[01:15:10] SPEAKER_00: 其實它的效果還沒有那麼好
[01:15:11] SPEAKER_00: 就在手機上
[01:15:12] SPEAKER_00: 因為目前手機上的算力
[01:15:15] SPEAKER_00: 要來能支撐了這個模型的性能
[01:15:17] SPEAKER_00: 可能還是相對受限的
[01:15:18] SPEAKER_00: 就是如果根據你們
[01:15:20] 去觀測到的這個密度法則
[01:15:21] SPEAKER_00: 在什麼時間上手機
[01:15:23] SPEAKER_00: 因為這可能是大家最關注的一個硬件
[01:15:26] SPEAKER_00: 然後用的人也是最多的
[01:15:27] SPEAKER_00: 這個上面的模型能力
[01:15:28] SPEAKER_00: 也會變得就是
[01:15:30] SPEAKER_00: 能支持我們乾些
[01:15:32] SPEAKER_00: 非常有意思的事
[01:15:33] SPEAKER_00: 然後非常提升效率的個性
[01:15:34] SPEAKER_00: 那我們會覺得大概到二零二七年
[01:15:37] SPEAKER_01: 可能會是一個比較重要的一個節點
[01:15:39] SPEAKER_01: 這個節點意味著什麼呢
[01:15:41] SPEAKER_01: 到二零二七年預計
[01:15:43] SPEAKER_01: 我們可以把大規模強化學習
[01:15:45] SPEAKER_01: 能夠實現在
[01:15:46] SPEAKER_01: 端策的模型上
[01:15:47] SPEAKER_01: 就可以進行一個非常好的
[01:15:49] SPEAKER_01: 一個學習的能力
[01:15:51] SPEAKER_01: 那就意味著什麼呢
[01:15:52] SPEAKER_01: 意味著說
[01:15:53] SPEAKER_01: 我們每個人都可以利用
[01:15:55] SPEAKER_01: 我們自己的數據
[01:15:56] SPEAKER_01: 來提供給這個模型
[01:15:59] SPEAKER_01: 作為它的這個學習環境
[01:16:01] SPEAKER_01: 來讓它持續的去進行學習
[01:16:03] SPEAKER_01: 也就是說在二零二七年
[01:16:04] SPEAKER_01: 我們可以利用一個
[01:16:05] SPEAKER_01: 端策的相對不錯的算力
[01:16:07] SPEAKER_01: 支持一個具備自我學習能力的模型
[01:16:10] SPEAKER_01: 能夠逐漸地成長為我們每個人
[01:16:13] SPEAKER_01: 專屬的大模型
[01:16:14] SPEAKER_01: 助手
[01:16:15] SPEAKER_01: 那我想它就可以成為我們所有的中端上的
[01:16:19] SPEAKER_01: 智能的這麼一個掌握者
[01:16:20] SPEAKER_01: 如果它要在眼鏡這麼少
[01:16:22] SPEAKER_00: 要提及上也實現類似的能力
[01:16:23] SPEAKER_00: 是不是要更遠
[01:16:25] SPEAKER_00: 那我感覺一些眼鏡可以作為
[01:16:27] SPEAKER_01: 你說說我記得配件
[01:16:28] SPEAKER_00: 對
[01:16:29] SPEAKER_01: 我不太認為就是一定要在眼鏡上
[01:16:32] SPEAKER_01: 本身裝上這個模型
[01:16:34] SPEAKER_01: 因為其實我最近在碰桃
[01:16:36] SPEAKER_01: 因為我要解肥
[01:16:38] SPEAKER_01: 所以我在裝一個
[01:16:39] SPEAKER_01: 就是監測的領舍
[01:16:41] SPEAKER_01: 監測血糖的這麼一個硬件
[01:16:43] SPEAKER_01: 是扎在那個格波上的
[01:16:45] SPEAKER_00: 對
[01:16:45] SPEAKER_00: 我們為什麼一定要在這頂上
[01:16:47] SPEAKER_01: 搞一個新片或是模型的
[01:16:49] SPEAKER_01: 當然也有
[01:16:50] SPEAKER_01: 但是它肯定不用做智能化的東西
[01:16:52] SPEAKER_01: 它只要把採儀的數據
[01:16:54] SPEAKER_01: 傳到我們的譬如說用戶
[01:16:57] SPEAKER_01: 它有一個自己的一個中端的盒子
[01:16:59] SPEAKER_01: 只要它能夠持續地去
[01:17:01] SPEAKER_01: 收集這個用戶的相應的數據
[01:17:03] SPEAKER_01: 來提供相應的智能化的服務
[01:17:05] SPEAKER_01: 其實又可以了
[01:17:06] SPEAKER_01: 我覺得不太需要說
[01:17:08] SPEAKER_01: 我們要讓智能真正的
[01:17:10] SPEAKER_01: 到所有的中端上去
[01:17:12] SPEAKER_01: 我覺得整個人類社會的中端
[01:17:15] SPEAKER_01: 應該就是每個人
[01:17:16] SPEAKER_01: 這個人的各種各樣的
[01:17:18] SPEAKER_01: 更加分佈的一些
[01:17:21] SPEAKER_01: 譬如說你的眼鏡也好
[01:17:22] SPEAKER_01: 甚至你的耳機
[01:17:23] SPEAKER_01: 你的手錶
[01:17:24] SPEAKER_01: 然後你的各種各樣的智能的硬件
[01:17:28] SPEAKER_01: 我想就是它完全是可以有一個
[01:17:30] SPEAKER_01: 共同的一個提供智能服務的中端
[01:17:34] SPEAKER_01: 就有點像家庭裡面的NAS
[01:17:36] SPEAKER_01: 只是說你是一個變形的
[01:17:38] SPEAKER_01: 可以跟著你走的這麼一個NAS
[01:17:40] SPEAKER_01: 最後想討論一下
[01:17:41] SPEAKER_00: 你們關心的更廣泛的一些行業趨勢
[01:17:43] SPEAKER_00: 其實前面一消息
[01:17:44] SPEAKER_00: 也有一些聊到的
[01:17:45] SPEAKER_00: 就是我覺得現在大家
[01:17:46] SPEAKER_00: 也會在討論說這個大園模型
[01:17:48] SPEAKER_00: 在預訓練加強化學習
[01:17:50] SPEAKER_00: 後訓練這個方法
[01:17:52] SPEAKER_00: 之後會有什麼新的方法
[01:17:54] SPEAKER_00: 就下一個不同的方法
[01:17:55] SPEAKER_00: 可能是什麼
[01:17:56] SPEAKER_00: 對 因為我覺得
[01:17:57] SPEAKER_02: 這一個最重要的點
[01:17:58] SPEAKER_02: 就是我們還掐了自主學習
[01:18:00] SPEAKER_02: 或者要自我學習 自我精華
[01:18:01] SPEAKER_02: 或者還是什麼持續學習
[01:18:03] SPEAKER_02: 反正現在名字很多
[01:18:04] SPEAKER_02: 在性學習是這個意思嗎
[01:18:05] SPEAKER_00: 在性學習
[01:18:06] SPEAKER_02: 管結屬於其中的一小部分
[01:18:08] SPEAKER_02: 就是名字其實很多
[01:18:10] SPEAKER_02: 那其實還是那個判斷
[01:18:11] SPEAKER_02: 就是說現在的強化學習
[01:18:14] SPEAKER_02: 已經能夠支持著說
[01:18:15] SPEAKER_02: 我們在任何一個任務上
[01:18:16] SPEAKER_02: 已經能夠去了非常優異的效果
[01:18:18] SPEAKER_02: 但這實際上還不是我們原來所擁有的智能
[01:18:22] SPEAKER_02: 這樣的一個呈現
[01:18:23] SPEAKER_02: 所以其實剛才我們市場的這樣一個場景
[01:18:25] SPEAKER_02: 未來應該是說這個模型
[01:18:28] SPEAKER_02: 它是一個很好的學習者
[01:18:29] SPEAKER_02: 它放在你的中端上
[01:18:31] SPEAKER_02: 或者是放在任何的設備上
[01:18:33] SPEAKER_02: 它可以主電的學習
[01:18:34] SPEAKER_02: 你所需要它學習的那些任務
[01:18:37] SPEAKER_02: 比如說幫你這個譬如說簡波客
[01:18:39] SPEAKER_02: 或者是幫你去寫代碼等等
[01:18:41] SPEAKER_02: 這些都是你的一些專業的技能
[01:18:43] SPEAKER_02: 你可以像帶一個實際生意
[01:18:46] SPEAKER_02: 把他給帶出來
[01:18:46] SPEAKER_02: 然後慢慢的這個幫助你取代你
[01:18:49] SPEAKER_02: 這我覺得是...
[01:18:50] SPEAKER_02: 幫助我取代
[01:18:51] SPEAKER_00: 對 其實這個應該是一個
[01:18:53] SPEAKER_02: AGI 下一步非常重要的一個點
[01:18:56] SPEAKER_02: 我覺得其實現在這個
[01:18:58] SPEAKER_02: 不過強化學習也好
[01:18:59] SPEAKER_02: 包括御徐內
[01:19:00] SPEAKER_02: 其實沒有辦法做到的一個事情
[01:19:02] SPEAKER_02: 這個就是說為什麼現在會有這個事件模型
[01:19:04] SPEAKER_02: 所以它就是想說幫助模型
[01:19:07] SPEAKER_02: 去獲取一個足夠好
[01:19:08] SPEAKER_02: 或去反饋的一個平台
[01:19:10] SPEAKER_02: 還有一些其他的一些
[01:19:12] SPEAKER_02: 各種學習這樣的犯事
[01:19:14] SPEAKER_02: 包括現在那個強化學習之父
[01:19:15] SPEAKER_02: 他也去提到說大模型為什麼
[01:19:17] SPEAKER_02: 他說大模型不是同樣AGI的這種目的
[01:19:19] SPEAKER_02: 核心還是這種Data German的這種學習方式
[01:19:22] SPEAKER_02: 可能還是會阻礙模型
[01:19:24] SPEAKER_02: 就是更高效的學習一些新的技能
[01:19:27] SPEAKER_02: 然後再往後其實還會有
[01:19:28] SPEAKER_02: 比如說這個模型有了自我學習
[01:19:31] SPEAKER_02: 自我精華的能力之後
[01:19:32] SPEAKER_02: 它在你的手上會成為一個
[01:19:34] SPEAKER_02: 很專業的類似的這種播報新聞的
[01:19:37] SPEAKER_02: 然後播報科技前演的這麼一個模型
[01:19:39] SPEAKER_02: 它可能在我的手上
[01:19:40] SPEAKER_02: 它就是一個研究
[01:19:42] SPEAKER_02: 也公開的本身的一個Research
[01:19:43] SPEAKER_02: 那在這樣一個場景下
[01:19:45] SPEAKER_02: 不同的模型它可能再往下一步走
[01:19:47] SPEAKER_02: 其實是相互的和協助
[01:19:50] SPEAKER_02: 對吧 那比如我這個模型是一個AGI的Serture
[01:19:53] SPEAKER_02: 然後會有其他的模型
[01:19:54] SPEAKER_02: 是類似於就是Infart的Research
[01:19:56] SPEAKER_02: 它們怎麼樣能夠讓這個AGI模型跑得更快
[01:19:59] SPEAKER_02: 然後再往後可能是最高階的
[01:20:01] SPEAKER_02: 就是這種創造的能力
[01:20:02] SPEAKER_02: 比如說我們會去講說
[01:20:04] SPEAKER_02: 我們人類會不但的產生足夠落的AINSTOT
[01:20:08] SPEAKER_02: 它能夠根據已有的一些符號體系
[01:20:10] SPEAKER_02: 就是比如說基於牛燉定律這些符號
[01:20:13] SPEAKER_02: 體系推導出相對論
[01:20:14] SPEAKER_02: 但是對於現在的AGI模型來說
[01:20:16] SPEAKER_02: 顯然做不到一點
[01:20:17] SPEAKER_02: 它永遠只能在人類已經定義好的符號
[01:20:20] SPEAKER_02: 接於演符號上做學習
[01:20:22] SPEAKER_02: 它沒有學辦法搞新
[01:20:23] SPEAKER_02: 創造出新的這種符號級的關係
[01:20:25] SPEAKER_02: 甚至創造出一個新的符號
[01:20:27] SPEAKER_02: 然後這個可能假如達到創造能力之後
[01:20:30] SPEAKER_02: 那其實我們會認為
[01:20:32] SPEAKER_02: 現在人類社會上所有需要智能的這些
[01:20:34] SPEAKER_02: 工作可能都會被AGI取來的
[01:20:37] SPEAKER_02: 這是我們大概列的AGI的幾步走的計劃
[01:20:40] SPEAKER_02: 所以總結一下就是自主學習
[01:20:42] SPEAKER_00: 然後是已經完成自主學習
[01:20:44] SPEAKER_00: 這些AI之間的寫作
[01:20:45] SPEAKER_00: 最後就是真正的創新
[01:20:47] SPEAKER_00: 對 我也叫AISertice
[01:20:49] SPEAKER_02: 大家現在創體的概念
[01:20:51] SPEAKER_02: 不過我們提到創新可能的更多會是
[01:20:54] SPEAKER_02: 這種創新的META的能力
[01:20:56] SPEAKER_02: 而不是在某一個
[01:20:58] SPEAKER_02: 比如說具體的螺旋醫
[01:21:00] SPEAKER_02: 像無醫藥等等的接領域
[01:21:02] SPEAKER_02: 它卻做一些智能做這個理論的創新
[01:21:04] SPEAKER_02: 這個也是一個明線
[01:21:05] SPEAKER_01: 它的案線其實就是
[01:21:07] SPEAKER_01: 相當於是在底層的實現的邏輯上
[01:21:10] SPEAKER_01: 我們其實特別喜歡
[01:21:11] SPEAKER_01: 跟信息時代的新片發展其實做類比
[01:21:16] SPEAKER_01: 你會看到就是整個信息革命
[01:21:19] SPEAKER_01: 我們全球是一個信息化的社會
[01:21:21] SPEAKER_01: 但這個信息化其實它的底層的支撐
[01:21:25] SPEAKER_01: 其實就是信片 就是算力
[01:21:27] SPEAKER_01: 但是你就可以看一下
[01:21:29] SPEAKER_01: 這個算力的分布大概是什麼情況
[01:21:31] SPEAKER_01: 就是我們看到一個統計
[01:21:33] SPEAKER_01: 就是二二四年初
[01:21:34] SPEAKER_01: 中國電信研究院的統計
[01:21:36] SPEAKER_01: 它就是說二三年的時候
[01:21:37] SPEAKER_01: 全國的端策的
[01:21:39] SPEAKER_01: 就是手機上的算力加在一起的總量
[01:21:42] SPEAKER_01: 是全國的
[01:21:44] SPEAKER_01: 是數據中心的算力的12倍
[01:21:46] SPEAKER_01: 你大家可以說想就是說
[01:21:48] SPEAKER_01: 雖然雲上的算力看上去很大
[01:21:51] SPEAKER_01: 然後一個算力中心它的算力也很大
[01:21:54] SPEAKER_01: 但是你架不住全國十幾日人
[01:21:57] SPEAKER_01: 可能有幾十一部手機
[01:21:58] SPEAKER_01: 然後它手機上的算力總量
[01:22:01] SPEAKER_01: 其實是非常大的一個規模
[01:22:02] SPEAKER_01: 所以過去的80年
[01:22:04] SPEAKER_01: 相信在全球實現了
[01:22:06] SPEAKER_01: 通過算力支撐的這麼一個信息化的社會
[01:22:09] SPEAKER_01: 那麼這個算力的分布是什麼樣子呢
[01:22:11] SPEAKER_01: 也就是對應的信息的分布是什麼樣子呢
[01:22:14] SPEAKER_01: 其實是分布式的
[01:22:15] SPEAKER_01: 是分布在這些中端上的
[01:22:17] SPEAKER_01: 我們每個人都擁有自己的一個
[01:22:20] SPEAKER_01: 以個人為中心的這麼一個信息
[01:22:23] SPEAKER_01: 這個中端
[01:22:24] SPEAKER_01: 我們手機我們的PC我們的Pi的等等
[01:22:27] SPEAKER_01: 那麼你就可以設想未來的智能社會
[01:22:31] SPEAKER_01: 智能的分布會是什麼樣子呢
[01:22:33] SPEAKER_01: 就是在二三年初的時候
[01:22:34] SPEAKER_01: 曾經全球有過一些巨頭說
[01:22:38] SPEAKER_01: 這個世界上不需要超過幾個大夢想
[01:22:41] SPEAKER_01: 就可以滿足全球的智能化的需求
[01:22:44] SPEAKER_01: 這個跟信息化剛剛開始
[01:22:47] SPEAKER_01: 1943年的時候
[01:22:48] SPEAKER_01: 就是IBM的董事長
[01:22:49] SPEAKER_01: 曾經說全球不需要超過舞台主機
[01:22:51] SPEAKER_01: 就可以滿足全球計算的需求一樣
[01:22:55] SPEAKER_01: 所以就是我們可以設想未來幾年之後
[01:22:58] SPEAKER_01: 智能化它也一定是分布式的
[01:23:00] SPEAKER_01: 原因就是這個世界上
[01:23:02] SPEAKER_01: 本身智能也都是分布在我們每個人的頭腦裡面
[01:23:06] SPEAKER_01: 我們每個人的這個中端上
[01:23:08] SPEAKER_01: 所以就是我們認為未來智能化的社會
[01:23:12] SPEAKER_01: 一定是要在我們的中端的社會上
[01:23:15] SPEAKER_01: 要為每個人每個智能的中端提供
[01:23:19] SPEAKER_01: 這種智能化的證明一個服務
[01:23:21] SPEAKER_01: 這個其實我覺得是未來的一個基本形態
[01:23:24] SPEAKER_01: 那麼在這樣的一個形態的支撐下
[01:23:26] SPEAKER_01: 你就可以想它具備了自主學習能力
[01:23:29] SPEAKER_01: 那就意味著說在中端上
[01:23:31] SPEAKER_01: 我們就可以根據這個用戶個人的數據
[01:23:34] SPEAKER_01: 去形成這個人的專屬的大模型
[01:23:37] SPEAKER_01: 這個大模型就是你的個人的終身的助手
[01:23:41] 它最懂你
[01:23:41] 它是這個世界上最懂你的那個智能大模型
[01:23:44] SPEAKER_01: 那麼在雲上會有各種各樣的這種專家大模型
[01:23:47] SPEAKER_01: 有美團大模型專門懂這個外賣
[01:23:50] SPEAKER_01: 這個有滴滴大模型專門懂
[01:23:53] SPEAKER_01: 如何去規劃這個形成
[01:23:55] SPEAKER_01: 然後有這個抖音大模型懂
[01:23:57] SPEAKER_01: 你的各種各樣的這種娛樂的這種喜好
[01:24:00] SPEAKER_01: 但是就是它一定是一個端的你的個性化大模型
[01:24:04] SPEAKER_01: 和這些雲的專家大模型互相之間的一個系統
[01:24:08] SPEAKER_01: 然後來完成相應的這麼一個工作
[01:24:10] SPEAKER_01: 那麼在這種情況下
[01:24:11] SPEAKER_01: 會形成所謂的智能體的互聯網
[01:24:14] SPEAKER_01: 這個其實我們會對應
[01:24:16] SPEAKER_01: 就是它說的這種寫作的寫作的這個模式
[01:24:18] SPEAKER_01: 那麼這個其實它就是在底層的一個案線
[01:24:22] SPEAKER_01: 也就是說不為公眾所廣為認知的
[01:24:25] SPEAKER_01: 它其實是發生在這些計算設備
[01:24:28] SPEAKER_01: 發生在我們的底層的這麼一個相應的
[01:24:32] SPEAKER_01: 這麼一個邏輯
[01:24:33] SPEAKER_01: 這個邏輯其實我們會覺得
[01:24:35] SPEAKER_01: Dancing Law會發揮一個非常重要的作用
[01:24:37] SPEAKER_01: 因為它就可以幾乎以非常非常低
[01:24:40] SPEAKER_01: 這個用戶所沒有感知的這麼一個成分
[01:24:43] SPEAKER_01: 來開始廣為的服務我們每個用戶
[01:24:46] SPEAKER_01: 在這個未來的設想裡面
[01:24:48] SPEAKER_00: 它的這個經濟模式是怎麼運轉的樣子
[01:24:50] SPEAKER_01: 當然我這是我自己的觀點
[01:24:52] SPEAKER_01: 就是不見得對
[01:24:55] SPEAKER_01: 在我來看就是歷史上工業革命
[01:24:58] SPEAKER_01: 一輪一輪的
[01:24:59] SPEAKER_01: 比如說蒸氣革命 電氣革命
[01:25:01] SPEAKER_01: 其實它大致是屬於工業革命
[01:25:03] SPEAKER_01: 它替代的是人的體力勞動
[01:25:06] SPEAKER_01: 相當於是利用機器
[01:25:07] SPEAKER_01: 然後來去達到替代人的機械化的體力勞動
[01:25:12] SPEAKER_01: 甚至說比人的體力勞動
[01:25:14] SPEAKER_01: 能力還要再強
[01:25:15] SPEAKER_01: 那我們這些輪的智能革命是在幹什麼呢
[01:25:18] SPEAKER_01: 我是覺得它其實是跟上一輪的信息革命
[01:25:22] SPEAKER_01: 再加上這一輪的智能革命
[01:25:24] SPEAKER_01: 其實是在替代我們人的腦力勞動
[01:25:26] SPEAKER_01: 我們所有的知識工作者
[01:25:28] SPEAKER_01: 我們所有的需要這個人類
[01:25:31] SPEAKER_01: 關於這個世界的知識
[01:25:32] SPEAKER_01: 才能完成的那些工作
[01:25:34] SPEAKER_01: 那麼未來只要它是機械化的 重顧化
[01:25:37] SPEAKER_01: 那其實就應該由這個人工智能來去完成
[01:25:40] SPEAKER_01: 那我怎麼賺錢呢
[01:25:41] SPEAKER_00: 我如果是一個個人的話
[01:25:42] SPEAKER_00: 首先我覺得第一步
[01:25:44] SPEAKER_01: 你的個人的智能中端
[01:25:46] SPEAKER_01: 其實是能夠讓你的工作效率變得更高
[01:25:49] SPEAKER_01: 變得更有市場競爭力
[01:25:50] SPEAKER_01: 就像單純說你在你的個人住手的知識下
[01:25:53] SPEAKER_01: 你完成的這個工作
[01:25:55] SPEAKER_01: 水平比別人更高
[01:25:56] SPEAKER_01: 然後完成的這個工作量比別人更多
[01:25:58] SPEAKER_01: 那你就可以能夠得到更多的
[01:26:00] SPEAKER_01: 這麼一個勞動的生活
[01:26:03] SPEAKER_01: 對不對
[01:26:03] SPEAKER_01: 因為本身你是通過知識
[01:26:05] SPEAKER_01: 然後來完成相應的工作
[01:26:07] SPEAKER_01: 然後來去獲得相應的輸入的
[01:26:09] SPEAKER_01: 通過學這個體能想像的
[01:26:10] SPEAKER_00: 我覺得在那個描述的環境
[01:26:12] SPEAKER_00: 你又感覺好像也不超貨幣了
[01:26:14] 也不超過工作了
[01:26:15] SPEAKER_00: 那我就聚一個今年的例子
[01:26:17] SPEAKER_01: 比如說今年的例子就是
[01:26:18] SPEAKER_01: 代碼大模型非常地厲害
[01:26:20] SPEAKER_01: 那不就是相當於是所有的這些程序員
[01:26:24] SPEAKER_01: 其實都要加載上相應的AI
[01:26:26] SPEAKER_01: 然後來輔助著它工作
[01:26:28] SPEAKER_01: 來提高它的工作效率
[01:26:29] SPEAKER_01: 那麼對應的結果就是
[01:26:31] SPEAKER_01: 你有了AI的輔助你的生產效率就變得極高
[01:26:36] SPEAKER_01: 那麼你其實就可以獲得
[01:26:37] SPEAKER_01: 更好的這麼一個輸入
[01:26:39] SPEAKER_01: 假如說你未來說
[01:26:41] SPEAKER_01: 好
[01:26:41] SPEAKER_01: 那大家用一個通用的
[01:26:43] SPEAKER_01: 這個AI的代碼大模型都不足夠了
[01:26:46] SPEAKER_01: 你需要自己去積累出
[01:26:48] SPEAKER_01: 你自己的一個知識庫
[01:26:49] SPEAKER_01: 你自己的這麼一些
[01:26:51] SPEAKER_01: 已經編寫過代碼的這麼一些經驗
[01:26:53] SPEAKER_01: 那麼你通過這些經驗
[01:26:55] SPEAKER_01: 所構造的這個你個人的這個助手
[01:26:57] SPEAKER_01: 能夠更好的幫助你去寫
[01:27:00] SPEAKER_01: 別人寫不了的代碼
[01:27:01] SPEAKER_01: 那你就在這個市場上
[01:27:03] SPEAKER_01: 就會更有競爭力
[01:27:04] SPEAKER_01: 對吧
[01:27:04] SPEAKER_01: 那他變相呢
[01:27:05] SPEAKER_01: 其實是相當於提高了整個
[01:27:07] SPEAKER_01: 這個社會的生產力
[01:27:08] SPEAKER_01: 生產效率
[01:27:09] SPEAKER_01: 那今年還有的發生的時候
[01:27:11] SPEAKER_00: 就是剛剛亞馬遜採了一點四萬人
[01:27:13] SPEAKER_00: 或者其實應該是說
[01:27:14] SPEAKER_02: 可能是一種模式是說未來
[01:27:16] SPEAKER_02: 其實現在來說
[01:27:17] SPEAKER_02: 你你賺錢的方式
[01:27:18] SPEAKER_02: 通過你的知識和技能賺錢
[01:27:20] SPEAKER_02: 但是在未來AI能自舞學習之後
[01:27:22] SPEAKER_02: 你可以設想這個AI
[01:27:24] SPEAKER_02: 本身是為你的知識
[01:27:25] SPEAKER_02: 就是你的知識和技能外化的一個提升
[01:27:28] SPEAKER_02: 所以其實未來可能還是那麼容易
[01:27:30] SPEAKER_02: 就是人機寫成了模式
[01:27:31] SPEAKER_02: 你的AI
[01:27:32] SPEAKER_02: 它的智能水平有多高
[01:27:33] SPEAKER_02: 可能還是取決於你
[01:27:34] SPEAKER_02: 怎麼跟它進行寫統
[01:27:36] SPEAKER_02: 然後你怎麼給它足夠多的經驗
[01:27:38] SPEAKER_02: 然後去交它去帶它
[01:27:40] SPEAKER_02: 我覺得可能下一步
[01:27:41] SPEAKER_02: 第一個改變可能會是這樣
[01:27:43] SPEAKER_02: 所以未來大家給你付費
[01:27:44] SPEAKER_02: 是因為你加你的AI很強
[01:27:47] SPEAKER_01: 所以就是說你設想
[01:27:48] SPEAKER_01: 就是在AI時代還能夠去工作的人
[01:27:51] SPEAKER_01: 那麼它一定是一個高水平的知識工作者
[01:27:54] SPEAKER_01: 因為低水平的知識工作
[01:27:55] SPEAKER_01: 已經被AI所期待了
[01:27:57] SPEAKER_01: 那麼在這種基礎上的話
[01:27:58] SPEAKER_01: 那就是說所有的這種高水平的知識工作者
[01:28:01] SPEAKER_01: 那它一定是由AI來付出的
[01:28:03] SPEAKER_01: 那在這個AI時代
[01:28:04] SPEAKER_00: 一個低水平的工作
[01:28:05] SPEAKER_00: 就如何變成成長為一個高水平的知識工作者
[01:28:08] SPEAKER_00: 你的專業領域之所以還需要你
[01:28:11] SPEAKER_01: 是因為AI肯定做不了
[01:28:13] SPEAKER_01: 肯定有AI做不了的事
[01:28:14] SPEAKER_01: 因為我們是一個人類社會
[01:28:16] SPEAKER_01: 譬如說醫學
[01:28:18] SPEAKER_01: 譬如說心理諮詢
[01:28:19] SPEAKER_01: 譬如說金融
[01:28:20] SPEAKER_01: 譬如說教育
[01:28:21] SPEAKER_01: 所有的這些知識工作
[01:28:23] SPEAKER_01: 特別重要的方向包括法律
[01:28:25] SPEAKER_01: 那不可能離開人
[01:28:27] SPEAKER_01: 所以它一定第一階段
[01:28:28] SPEAKER_01: 一定是AI是付出
[01:28:30] SPEAKER_01: 但是AI在付出的過程中
[01:28:32] SPEAKER_01: 它可以通過你的行為
[01:28:33] SPEAKER_01: 可以通過你的反饋
[01:28:35] SPEAKER_01: 它來不斷地成長
[01:28:37] SPEAKER_01: 它成長到一定的水平
[01:28:39] SPEAKER_01: 那它是不是就可以在某些場景
[01:28:41] SPEAKER_01: 可以發揮一些作用
[01:28:42] SPEAKER_01: 或者是說可能是你一個人開了一家公司
[01:28:45] SPEAKER_02: 然後你的員工是你的AI
[01:28:48] SPEAKER_02: 然後你的成長過程
[01:28:50] SPEAKER_02: 其實就是你這家公司創業發展的過去
[01:28:53] SPEAKER_02: 這種生產力無限大的未來還挺難想像的
[01:28:57] SPEAKER_00: 這你會覺得這個生產就非常過勝了
[01:28:59] SPEAKER_00: 我真的真正充滿了期待
[01:29:01] SPEAKER_01: 原因是什麼呢
[01:29:02] SPEAKER_01: 我會覺得過去的這幾十年
[01:29:04] SPEAKER_01: 我覺得人類的發展其實是被棒的住的
[01:29:08] 為什麼棒的住
[01:29:09] SPEAKER_01: 被我們人類自己的知識給棒的住了
[01:29:11] SPEAKER_01: 你想我們過去的這幾十年
[01:29:13] SPEAKER_01: 這個新西大爆炸
[01:29:14] SPEAKER_01: 知識大爆炸
[01:29:16] 我覺得帶來的結果
[01:29:17] SPEAKER_01: 就是每個人
[01:29:18] SPEAKER_01: 你都只能成為某個特別小的方向上的專家
[01:29:21] SPEAKER_01: 對
[01:29:22] SPEAKER_01: 所以我覺得就是這個社會
[01:29:24] SPEAKER_01: 我們人類歷史上
[01:29:25] SPEAKER_01: 積累的知識總量已經大到了
[01:29:28] SPEAKER_01: 我們任何一個人
[01:29:30] SPEAKER_01: 只能在裡面是一個很小很小的一個拼圖
[01:29:32] SPEAKER_01: 我會覺得如果仍然只是依賴我們人
[01:29:36] SPEAKER_01: 來去完成相應的
[01:29:38] SPEAKER_01: 這麼一個前言探索和創新
[01:29:40] SPEAKER_01: 來更好的繼續認識這個世界
[01:29:42] SPEAKER_01: 改造這個世界
[01:29:43] SPEAKER_01: 我會覺得變得越來越難
[01:29:45] SPEAKER_01: 因為你這一輩子
[01:29:46] SPEAKER_01: 你光學心知識就已經學破了
[01:29:48] SPEAKER_01: 對吧
[01:29:49] SPEAKER_01: 那不是他有一個例子
[01:29:50] SPEAKER_01: 說舉了一個例子
[01:29:52] SPEAKER_01: 說人類知識是一個大的一個球
[01:29:55] SPEAKER_01: 對吧
[01:29:55] SPEAKER_01: 然後任何一個人的學習
[01:29:57] SPEAKER_01: 到了博士階段
[01:29:58] SPEAKER_01: 可能就是在球的邊緣
[01:30:00] SPEAKER_01: 可能是在做那麼一個小的突起
[01:30:03] SPEAKER_01: 那你在往後
[01:30:04] SPEAKER_01: 你可能你窮念一生
[01:30:06] SPEAKER_01: 可能都到達不了那個邊緣了
[01:30:07] SPEAKER_01: 那怎麼辦
[01:30:08] SPEAKER_01: 我是覺得就得有AI的幫助
[01:30:10] SPEAKER_01: 所以在我來看
[01:30:12] SPEAKER_01: 我反而是覺得AI的出現
[01:30:14] SPEAKER_01: 局部上某些方面會帶來我們的
[01:30:17] SPEAKER_01: 這個就是生產歷過生看上去
[01:30:19] SPEAKER_01: 比如說代碼程序員
[01:30:21] SPEAKER_01: 很像大比例的參員或者怎麼樣
[01:30:24] SPEAKER_01: 但是同時我們其實看到
[01:30:25] SPEAKER_01: 人類作為整體
[01:30:27] SPEAKER_01: 其實對我們人類已經積累的知識的掌握
[01:30:30] SPEAKER_01: 其實是完全失控了的
[01:30:32] SPEAKER_01: 這個學術到你說第三階段
[01:30:33] SPEAKER_00: 對吧
[01:30:34] SPEAKER_00: 就是AI
[01:30:34] SPEAKER_00: 他能幫助一些發現
[01:30:36] SPEAKER_00: 我覺得
[01:30:37] SPEAKER_01: 現在其實已經譬如AIFileScience
[01:30:39] SPEAKER_01: 現在國內
[01:30:40] SPEAKER_01: 很多的
[01:30:41] SPEAKER_01: 包括國家機構等等的
[01:30:43] SPEAKER_01: 都在非常權力的布局
[01:30:45] SPEAKER_01: 你看美國剛剛發的那個創始機計畫
[01:30:48] SPEAKER_01: 也是在做這件事情
[01:30:49] SPEAKER_01: 原因就是在於大家都看到了
[01:30:51] SPEAKER_01: 就是AI本身對我們人類
[01:30:54] SPEAKER_01: 這個發現和改造這個世界
[01:30:56] SPEAKER_01: 其實具有非常重要的這個作用
[01:30:58] SPEAKER_01: 你原來可能需要花十幾年
[01:31:00] SPEAKER_01: 幾十年才能完成的工作
[01:31:02] SPEAKER_01: 未來有了AI的幫助
[01:31:03] SPEAKER_01: 你可能用幾個小時就可以去完成
[01:31:06] SPEAKER_01: 這是完全有可能的
[01:31:07] SPEAKER_01: 所以在我來看
[01:31:08] SPEAKER_01: 我會覺得我們在很多很多方向
[01:31:11] SPEAKER_01: 其實對這個世界還完全缺少認知
[01:31:13] SPEAKER_01: 材料能源
[01:31:15] SPEAKER_01: 整個這個宇宙
[01:31:16] SPEAKER_01: 等等各個方面
[01:31:17] SPEAKER_01: 包括AI本身
[01:31:19] SPEAKER_01: 智能本質是什麼
[01:31:20] SPEAKER_01: 不知道
[01:31:21] SPEAKER_01: 包括我們的腦
[01:31:22] SPEAKER_01: 到底是什麼樣的一個機制
[01:31:24] SPEAKER_01: 它的機率是什麼
[01:31:25] SPEAKER_01: 這個時間有太多的位置了
[01:31:27] SPEAKER_01: 所以我是覺得
[01:31:28] SPEAKER_01: 這個智能時代最讓人找米的地方
[01:31:31] SPEAKER_01: 不只是說我們實現了AI
[01:31:33] SPEAKER_01: 而是AI
[01:31:34] SPEAKER_01: 能夠讓我們更快的
[01:31:36] SPEAKER_01: 更好的
[01:31:37] SPEAKER_01: 能夠認識
[01:31:38] SPEAKER_01: 一個更大的世界
[01:31:39] SPEAKER_01: 所以我覺得完全不用擔心生產離過生
[01:31:42] SPEAKER_01: 因為就像劉慈興說的相存教師
[01:31:44] SPEAKER_01: 對吧
[01:31:45] SPEAKER_01: 這麼低頻的這種貸款
[01:31:47] SPEAKER_01: 對吧
[01:31:47] SPEAKER_01: 口口相傳所積累的
[01:31:49] SPEAKER_01: 這麼一個人類的知識
[01:31:50] SPEAKER_01: 到了我們現代
[01:31:51] SPEAKER_01: 我覺得已經難以危機了
[01:31:53] SPEAKER_01: 所以我是覺得現在
[01:31:54] SPEAKER_01: 咱們探討什麼生產離過生
[01:31:56] SPEAKER_01: 什麼程序員都失業
[01:31:58] SPEAKER_01: 我覺得這都是倉海一素
[01:32:00] SPEAKER_01: 你再過十年二十年
[01:32:02] SPEAKER_01: 當我們有了新的材料
[01:32:04] SPEAKER_01: 有了新的能源
[01:32:06] SPEAKER_01: 我們的生命科學
[01:32:07] SPEAKER_01: 我們受命
[01:32:08] SPEAKER_01: 各個方面都有一個非常大的突破
[01:32:11] SPEAKER_01: 當我們的足跡
[01:32:12] SPEAKER_01: 能夠超越地球
[01:32:14] SPEAKER_01: 對吧
[01:32:15] SPEAKER_01: 我們能夠辯部全宇宙
[01:32:16] SPEAKER_01: 那我們還要擔心
[01:32:18] SPEAKER_01: 程序員實業的問題嗎
[01:32:19] SPEAKER_01: 對
[01:32:20] SPEAKER_00: 但我這個擔心不是從紅官層面的
[01:32:22] SPEAKER_00: 它是從每個人的角度的
[01:32:24] SPEAKER_00: 因為你實業對一個人
[01:32:25] SPEAKER_00: 或者說對一個家庭來說
[01:32:26] SPEAKER_00: 肯定是個衝擊嗎
[01:32:27] SPEAKER_00: 但是對我們全人類來講
[01:32:29] SPEAKER_01: 我覺得是一次大的解放
[01:32:30] SPEAKER_01: 關於未來你有什麼擔心的地方嗎
[01:32:32] SPEAKER_00: 你剛說都是很樂觀的部分
[01:32:34] SPEAKER_00: 擔心的地方
[01:32:34] SPEAKER_00: 擔心的地方
[01:32:35] SPEAKER_01: 我擔心的地方是
[01:32:37] SPEAKER_01: 我們人類自己
[01:32:38] SPEAKER_01: 會舒服我們自己的潛進的步伐
[01:32:42] SPEAKER_01: 你這什麼
[01:32:43] SPEAKER_00: 你是質譬如說監管
[01:32:44] SPEAKER_00: 或者說
[01:32:45] 對
[01:32:45] 你看現在已經有很多人
[01:32:47] SPEAKER_01: 會說
[01:32:48] SPEAKER_01: 不要去研究Superintile的真實
[01:32:50] SPEAKER_01: 不要去幹這個
[01:32:51] SPEAKER_01: 不要去幹那個
[01:32:52] SPEAKER_01: 我覺得這個是完全不合理的
[01:32:54] SPEAKER_01: 你就跟說
[01:32:55] SPEAKER_01: 在1940年
[01:32:56] SPEAKER_01: 因為說我們有computer這個職業
[01:32:59] SPEAKER_01: 我們完全可以用算盤來去完成一些計算
[01:33:01] SPEAKER_01: 所以為什麼要研究超級計算機呢
[01:33:04] SPEAKER_01: 為什麼要研究大型計算機呢
[01:33:06] SPEAKER_01: 說這些大型計算機都是用來做軍事的
[01:33:09] SPEAKER_01: 用來去做這個毀滅人類的
[01:33:11] SPEAKER_01: 我們為什麼要去研究它
[01:33:12] SPEAKER_01: 所以你的擔心是其他人對AI未來退估擔心
[01:33:15] SPEAKER_00: 你不覺得嗎
[01:33:16] SPEAKER_01: 如果說回到1940年
[01:33:19] SPEAKER_01: 因為一些人說computer這個東西
[01:33:23] SPEAKER_01: 電子計算機這個東西是用來做軍事的
[01:33:25] SPEAKER_01: 所以我們不應該研究它
[01:33:27] SPEAKER_01: 你覺得是合理的嗎
[01:33:28] SPEAKER_01: 我覺得可以審慎
[01:33:29] SPEAKER_01: 但我是覺得
[01:33:30] SPEAKER_01: 反而是能夠提高我們探索這個世界能力的事情
[01:33:33] SPEAKER_01: 我們一定要支持
[01:33:35] SPEAKER_01: 不然的話
[01:33:35] SPEAKER_01: 我們就永遠的只能夠停留在一個
[01:33:37] SPEAKER_01: 這個盲目的位置之餘
[01:33:39] SPEAKER_01: 不過剩階段
[01:33:40] SPEAKER_00: 我覺得就是擔憂未來的這種擔憂
[01:33:43] SPEAKER_00: 然後迎來更多的監管
[01:33:45] SPEAKER_00: 包括這個什麼約束目前還不是這個業界的主要
[01:33:48] SPEAKER_00: 是的
[01:33:49] SPEAKER_01: 這其實比較少數派的事情
[01:33:51] SPEAKER_00: 對
[01:33:51] SPEAKER_00: 所以我也沒什麼好擔憂的
[01:33:54] SPEAKER_01: 我們就努力往前走就好了
[01:33:56] SPEAKER_00: 最後想問的是
[01:33:57] SPEAKER_00: 未來一年內
[01:33:57] SPEAKER_00: 你們有什麼想驗證的問題嗎
[01:33:59] SPEAKER_00: 可能祭福上的話
[01:34:00] SPEAKER_02: 還是就是
[01:34:01] SPEAKER_02: I-O到底能走多遠
[01:34:03] SPEAKER_02: 然後能走多遠
[01:34:04] SPEAKER_00: 對
[01:34:04] SPEAKER_00: 然後自入學習到底應該以那樣的
[01:34:07] SPEAKER_02: 前往的方式存在
[01:34:08] SPEAKER_02: 我覺得這是一個問題
[01:34:09] SPEAKER_02: 然後可能未來一年
[01:34:11] SPEAKER_02: 我們還得再思考了一個新的問題
[01:34:12] SPEAKER_02: 就是最早到來的那個AGA
[01:34:14] SPEAKER_02: 它的形態會是什麼樣
[01:34:16] SPEAKER_02: 我覺得應該會要在未來一到兩年
[01:34:17] SPEAKER_02: 這內逐漸的呈現出來
[01:34:19] SPEAKER_02: 最早到來的AGA的形態
[01:34:20] SPEAKER_00: 或者是什麼樣子是什麼
[01:34:21] SPEAKER_00: 比如說哪一天
[01:34:22] SPEAKER_02: OPENY 宣稱自己
[01:34:24] SPEAKER_02: 模型進到了AGA這個水平
[01:34:26] SPEAKER_02: 你覺得它會是什麼樣的一個模型
[01:34:27] SPEAKER_02: 或者覺得它是會怎樣的一個系統
[01:34:29] SPEAKER_02: 我覺得這是一個很值得思考的問題
[01:34:31] SPEAKER_02: 因為大家現在覺得這種對話
[01:34:33] SPEAKER_02: 已經很通用了
[01:34:34] SPEAKER_02: 但我覺得還遠不到
[01:34:35] SPEAKER_02: 其實我現在感覺有在思考這些問題
[01:34:37] SPEAKER_02: 比如說大家現在會說
[01:34:39] SPEAKER_02: AGA未來就要做的那時候
[01:34:41] SPEAKER_02: 能做的事情
[01:34:42] SPEAKER_02: 但是其實這件事情我覺得是不對的
[01:34:44] SPEAKER_02: 那其實很簡單一個問題
[01:34:45] SPEAKER_02: 就是自動駕駛
[01:34:46] SPEAKER_02: 一件事情未來可能有大模型來做
[01:34:48] SPEAKER_02: 我覺得顯然不可能
[01:34:50] 它一定會是一個非常專用的想模型
[01:34:52] SPEAKER_02: 做的會比大模型要更好
[01:34:54] SPEAKER_02: 我們會說
[01:34:55] SPEAKER_02: 因為這個大模型做不了
[01:34:56] SPEAKER_02: 比如說人類會做駕駛的事情
[01:34:58] SPEAKER_02: 說它不是AGA
[01:34:59] SPEAKER_02: 我覺得其實肯定也不能這麼定論
[01:35:01] SPEAKER_02: 所以說那AGA到底是什麼
[01:35:03] SPEAKER_02: 所以說剛才那個定義
[01:35:04] SPEAKER_02: 就是人類能做的事情
[01:35:05] SPEAKER_02: 它都能做是錯的
[01:35:07] SPEAKER_02: 其實你就得思考它的形態到底會是什麼樣
[01:35:09] SPEAKER_02: 然後我現在一個感覺就是
[01:35:12] SPEAKER_02: 未來的一個可能AGA的形態
[01:35:14] SPEAKER_02: 是一個生產AI的AI
[01:35:17] SPEAKER_02: 比如說它是一個
[01:35:19] SPEAKER_02: AGA可能就是說
[01:35:20] SPEAKER_02: 之前那個定義可能是
[01:35:21] SPEAKER_02: 它是一個TUC的一個定義
[01:35:23] SPEAKER_02: 但是我覺得AGA應該是一個Tubi的一個定義
[01:35:26] SPEAKER_02: 就是說未來我們現在
[01:35:27] SPEAKER_02: 你可以看到我們會有很多的公司
[01:35:29] SPEAKER_02: 很多的這種需要僱傭人
[01:35:31] SPEAKER_02: 去做很多的生產勞動
[01:35:33] SPEAKER_02: 但是我覺得未來的可能就是
[01:35:35] SPEAKER_02: 會有AGA來幫人家的事情
[01:35:37] SPEAKER_02: 就是我跟他說
[01:35:38] SPEAKER_02: 現在我想要有一個自動駕駛的模型
[01:35:41] SPEAKER_02: 然後跟AGA說
[01:35:42] SPEAKER_02: AGA開始就是去構建這個模型
[01:35:45] SPEAKER_02: 比如說生產數據 收集數據
[01:35:47] SPEAKER_02: 然後去不斷的優化這個模型
[01:35:49] SPEAKER_02: 讓它不斷能夠在那個特定的算量上跑起來
[01:35:52] SPEAKER_02: 這樣的話我們不需要AGA
[01:35:53] SPEAKER_02: 啥都會
[01:35:54] SPEAKER_02: 但它需要會的一個最合理的事情就是生產而已
[01:35:57] SPEAKER_02: 對 但是這個也不是一個這種答案
[01:35:59] SPEAKER_02: 但是我覺得這個問題
[01:36:01] SPEAKER_02: 一定得再不會來一到兩年來去思考
[01:36:03] SPEAKER_02: 這樣才能夠分好
[01:36:04] SPEAKER_02: 就說我們說的什麼自主學習
[01:36:06] SPEAKER_02: 這些都是能力
[01:36:07] SPEAKER_02: 能力它距離應該在寫了一個產品
[01:36:09] SPEAKER_02: 或者什麼樣一個模型上呈現
[01:36:11] SPEAKER_02: 或者是一個什麼用途上去做的
[01:36:13] SPEAKER_00: 對
[01:36:14] SPEAKER_02: 我覺得這是一個很重要的問題
[01:36:16] SPEAKER_01: 因為朝廷說的那個
[01:36:17] SPEAKER_01: 就是你從歷史上來看
[01:36:19] SPEAKER_01: 就是工業革命的一個
[01:36:20] SPEAKER_01: 這個特徵就是用機器制造機器
[01:36:23] SPEAKER_01: 就是你不是用手工制造機器
[01:36:25] SPEAKER_01: 用機器制造機器
[01:36:26] SPEAKER_01: 是機器大生產的一個標誌
[01:36:29] 那其實未來的AI大生產
[01:36:31] SPEAKER_01: 就是你到了智能時代
[01:36:32] SPEAKER_01: 真正的各行各業都能夠把這個AI
[01:36:35] SPEAKER_01: 能夠用起來
[01:36:36] SPEAKER_01: 那其實是用AI制造AI是它的本質
[01:36:38] SPEAKER_01: 那麼在這個裡面其實很重要的一個點
[01:36:40] SPEAKER_01: 其實就是咱們說的這個自主學習
[01:36:43] SPEAKER_01: 因為自主學習就是意味著說
[01:36:44] SPEAKER_01: 這個模型它可以自己去實現一個
[01:36:48] SPEAKER_01: 在某種環境裡面的這種成長
[01:36:50] SPEAKER_01: 其實我覺得是一種
[01:36:51] SPEAKER_01: 用AI制造AI的一個出型了
[01:36:54] SPEAKER_01: 就像這個AI本身它會自我的
[01:36:56] SPEAKER_01: 其聲它自己的這個能力
[01:36:58] SPEAKER_01: 所以其實這件事
[01:36:59] SPEAKER_01: 我覺得還是特別期待這個最早明年
[01:37:02] SPEAKER_01: 最晚後年
[01:37:03] SPEAKER_01: 然後能夠有一個這種學習能力的
[01:37:05] SPEAKER_01: 這麼一個提升
[01:37:06] SPEAKER_01: 那主老師您自己想認真的一個問題是什麼
[01:37:08] SPEAKER_00: 就是我剛才說的這個
[01:37:10] SPEAKER_01: 就是自主學習
[01:37:11] SPEAKER_01: 自主學習
[01:37:13] SPEAKER_01: 明年就會出現自主學習
[01:37:14] SPEAKER_00: 我這麼感覺業界現在對怎麼做這個事
[01:37:17] 並沒有很多信號
[01:37:18] SPEAKER_00: 這才是很有意思的點
[01:37:20] SPEAKER_01: 就是剛才其實朝鮮已經提到了
[01:37:22] SPEAKER_01: 就是Reward的和Nierin
[01:37:23] SPEAKER_01: 就是其實現在已經是在相關的
[01:37:26] SPEAKER_01: 一些特定的領域
[01:37:28] SPEAKER_01: 其實是可以看到
[01:37:29] SPEAKER_01: 就是如何去通過設計
[01:37:32] SPEAKER_01: 特殊的這種Reward的這種機制
[01:37:35] SPEAKER_01: 然後來實現一種持續的這種學習
[01:37:38] SPEAKER_01: 就不只是停留在數學
[01:37:40] SPEAKER_01: 待馬這兩個領域
[01:37:41] SPEAKER_01: 會開始扩展到其他領域
[01:37:43] SPEAKER_01: 最新扩展到了什麼領域
[01:37:44] SPEAKER_00: 從屬於和待馬
[01:37:45] SPEAKER_00: 就是可以通過形式化
[01:37:47] SPEAKER_01: 或者是通過相關的這種方式
[01:37:49] SPEAKER_01: 來提供Reward的這些領域
[01:37:51] SPEAKER_01: 比如最近像物理等等的這些領域
[01:37:53] SPEAKER_01: 其實有非常大的進展
[01:37:56] SPEAKER_01: 何先生還是那幾個學科吧
[01:37:58] SPEAKER_02: 學科
[01:37:58] SPEAKER_02: 對的離課那些進展
[01:38:00] SPEAKER_02: 而且我其實覺得就是
[01:38:01] SPEAKER_02: 現在我們看不到任何苗頭
[01:38:02] SPEAKER_02: 但是你還想記住發展
[01:38:04] SPEAKER_02: 它是一個有個死給領的過程
[01:38:06] SPEAKER_02: 因為你看到
[01:38:07] SPEAKER_02: 這個預生的模型出現
[01:38:08] SPEAKER_02: 到真的廣播一人
[01:38:10] SPEAKER_02: 直到Gb3到CityPT
[01:38:11] SPEAKER_02: 其實有很長的一段死給領的過程
[01:38:13] SPEAKER_02: 當然最近因為大家觀眾的多
[01:38:15] SPEAKER_02: 這個死給領的過程迅速的在縮短
[01:38:17] SPEAKER_02: 但是我覺得明年自我學習
[01:38:19] SPEAKER_02: 已經是已經會成為大家能看到的一個出現
[01:38:21] SPEAKER_02: 比如說它可能沒有那麼飯化
[01:38:23] SPEAKER_02: 但是如果在某個任務上
[01:38:25] SPEAKER_02: 它能夠出現變得更好
[01:38:26] SPEAKER_02: 所以這個明年其實說的是
[01:38:27] SPEAKER_00: 它可能是這個信號出現苗頭出現
[01:38:29] SPEAKER_00: 到不是說被我們被大眾接受的
[01:38:31] SPEAKER_00: 不止啊
[01:38:32] SPEAKER_01: 你想之所以能夠成為Reward的
[01:38:34] SPEAKER_01: Nierry也就是說
[01:38:36] SPEAKER_01: ARO這件事情開始在各行各業
[01:38:38] SPEAKER_01: 大家通過人力
[01:38:40] SPEAKER_01: 然後來幫著去構建這個Reward的
[01:38:42] SPEAKER_01: 道理從這來是比較好的
[01:38:44] SPEAKER_01: 那麼再進一步
[01:38:45] SPEAKER_01: 這個人力越來越少
[01:38:47] SPEAKER_01: 那麼其實就可以形成一個真正的
[01:38:49] SPEAKER_01: 這種各樣自主的這麼一個模式
[01:38:51] SPEAKER_01: 這個董事本來也是一個
[01:38:52] SPEAKER_01: 亮變產生之變的過程
[01:38:54] SPEAKER_01: 你就當然可以想像一下
[01:38:55] SPEAKER_01: 現在的ARO已經開始在非常廣泛的
[01:38:58] SPEAKER_01: 在各個領域來嘗試它的這麼一個應用了
[01:39:01] SPEAKER_01: 然後明年可能會看到數學代碼
[01:39:03] SPEAKER_00: 之外的更多的這個場景的成果
[01:39:05] SPEAKER_00: 對 然後慢慢的再能夠回去到一起
[01:39:08] SPEAKER_01: 它現在先飯花的是環境
[01:39:10] SPEAKER_01: 然後如果我們把這些所有的環境
[01:39:12] SPEAKER_01: 都讓一個模型
[01:39:13] SPEAKER_01: 然後去進行這個學習
[01:39:15] SPEAKER_01: 那是不是它能夠掌握更高層的
[01:39:18] SPEAKER_01: 這種自主判斷這個Reward的能力
[01:39:20] SPEAKER_01: 其實回到我們中間討論過的
[01:39:21] SPEAKER_00: 很多問題
[01:39:22] SPEAKER_00: 對 所以我覺得這個
[01:39:23] SPEAKER_01: 是一個很自然的一個過程
[01:39:24] SPEAKER_01: 好的好的 那今天非常感謝
[01:39:26] SPEAKER_00: 劉柱元老師還有小朋友如我是
[01:39:29] SPEAKER_00: 做客晚點聊
[01:39:30] SPEAKER_00: 然後分享了就是
[01:39:31] SPEAKER_00: 清華還有面臂合作去研究了這個
[01:39:35] SPEAKER_00: 密度法則
[01:39:35] SPEAKER_00: 它皆是一個什麼樣的
[01:39:37] SPEAKER_00: 業界發展的規律
[01:39:38] SPEAKER_00: 以及這樣一個去世可能會幫助
[01:39:40] SPEAKER_00: 大家怎麼去統一接下來的目標
[01:39:42] SPEAKER_00: 在一個就是更高效的方式
[01:39:45] SPEAKER_00: 去獲得更多的智能
[01:39:46] SPEAKER_00: 然後我們也延長了到了就是
[01:39:48] SPEAKER_00: 面臂自己在這個密度法則的
[01:39:50] SPEAKER_00: 支援下的一些模型研發
[01:39:52] SPEAKER_00: 還有業界落地的探索
[01:39:54] SPEAKER_00: 以及就是說這個密度法則
[01:39:56] SPEAKER_00: 對更廣泛的行業有什麼影響
[01:39:58] SPEAKER_00: 那謝謝兩位參加我們的節目
[01:40:00] SPEAKER_00: 各位拜拜
[01:40:01] SPEAKER_02: 謝謝
[01:41:02] SPEAKER_00: 長輸出也變得很重要
[01:41:04] SPEAKER_00: 十個十個月
[01:41:05] SPEAKER_00: 朝軍這一次提到
[01:41:06] SPEAKER_00: 他認為長輸出的重要性
[01:41:08] SPEAKER_00: 還是沒有成為一個行業共識
[01:41:10] SPEAKER_00: 是一個急續優化的方向
[01:41:12] SPEAKER_00: 1037即使對注意力
[01:41:13] SPEAKER_00: 機制發展的一個很好的輸理和科普
[01:41:16] SPEAKER_00: 當時的一些思考也必不過時
[01:41:18] SPEAKER_00: 比如內氣最後
[01:41:19] SPEAKER_00: 我們也討論了Fossign
[01:41:21] SPEAKER_00: 這在今年也成為了一個
[01:41:22] SPEAKER_00: 更明確的發展方向
[01:41:26] 本期節目就到這裡
[01:41:27] SPEAKER_00: 歡迎收聽
[01:41:28] SPEAKER_00: 如果你對今天聊的話
[01:41:29] SPEAKER_00: 有關查好奇會疑問
[01:41:30] SPEAKER_00: 歡迎在評論區分享想法
[01:41:32] SPEAKER_00: 這也會成為我們節目的一部分
[01:41:34] SPEAKER_00: 讓整個討論更完整
[01:41:35] SPEAKER_00: 也可以把我們的節目
[01:41:36] SPEAKER_00: 分享給對這個話題感興趣的朋友
[01:41:38] SPEAKER_00: 歡迎推薦更多你想聽的主題和嘉賓
[01:41:41] SPEAKER_00: 你可以從小宇宙
[01:41:42] SPEAKER_00: 蘋果泡的Costre等曲到
[01:41:43] SPEAKER_00: 關注晚點聊Late Talk
[01:41:45] SPEAKER_00: 也歡迎關注我們的公眾號
[01:41:46] SPEAKER_00: 晚點Late Post
[01:41:47] SPEAKER_00: 下期再見