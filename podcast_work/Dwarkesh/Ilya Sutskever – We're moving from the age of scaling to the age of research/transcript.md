# Ilya Sutskever â€“ We're moving from the age of scaling to the age of research

**Podcast:** Dwarkesh
**Date:** 2025-11-25
**Video ID:** aR20FWCCjAs
**Video URL:** https://www.youtube.com/watch?v=aR20FWCCjAs

---

[00:00:00] You know, it's crazy that all of this is real.
[00:00:04] Yeah, meaning, like all this AI stuff and all this Bay Area, yeah, that it's happened.
[00:00:10] Ilya Sutskever: Like, isn't it straight out of science fiction?
[00:00:13] Ilya Sutskever: Yeah, another thing that's crazy is like how normal this low tape off feels.
[00:00:18] Dwarkesh Patel: The idea that we'd be investing 1% of Gigi in AI, like I feel like it felt like a bigger deal,
[00:00:25] Dwarkesh Patel: you know, but right now it just feels like we get used to things free faster and so out, yeah.
[00:00:29] Ilya Sutskever: But also it's kind of like it's abstract like what does it mean?
[00:00:33] What it means that you see it in the news, yeah, that such and such company announced such and such dollar amount.
[00:00:38] Ilya Sutskever: Right. That's that's all you see.
[00:00:41] Ilya Sutskever: Right. It's not really felt in any other way so far.
[00:00:45] Ilya Sutskever: Yeah, should we actually begin here? I think this is an interesting discussion.
[00:00:47] Dwarkesh Patel: Sure. I think your point about well from the average person's point of view,
[00:00:53] nothing is that different. It will continue being true even into the singularity.
[00:00:57] Dwarkesh Patel: No, I don't think so. Okay, interesting.
[00:00:59] Dwarkesh Patel: So the thing which I was referring to not feeling different is okay.
[00:01:06] Ilya Sutskever: So such and such company announced some difficult to comprehend dollar amount of investment.
[00:01:12] Ilya Sutskever: Right. I don't think anyone knows what to do with that.
[00:01:14] Ilya Sutskever: Yeah.
[00:01:15] Dwarkesh Patel: But I think that the impact of AI is going to be felt.
[00:01:21] The AI is going to be diffused through the economy.
[00:01:23] Ilya Sutskever: There is a very strong economic force for this.
[00:01:27] And I think the impact is going to be felt very strongly.
[00:01:30] When do you expect that impact? I think the models seem smarter than their economic impact would
[00:01:36] Dwarkesh Patel: imply. Yeah, this is one of the very confusing things about the models right now.
[00:01:44] Ilya Sutskever: How to reconcile the fact that they are doing so well on eVALs.
[00:01:52] Ilya Sutskever: And you look at the eVALs and you go those are pretty hard eVALs.
[00:01:56] Ilya Sutskever: They are doing so well.
[00:01:59] But the economic impact seems to be dramatically behind.
[00:02:04] Ilya Sutskever: And it's almost like it's very difficult to make sense of how can the model on the one hand
[00:02:12] Ilya Sutskever: do these amazing things. And on the other hand, like repeat itself twice in some situation in a
[00:02:19] Ilya Sutskever: kind of an example would be let's say you use vibe coding to do something.
[00:02:24] And you go to some place and then you get a bug.
[00:02:26] Ilya Sutskever: And then you tell the model can you please fix the bug?
[00:02:30] And the model says oh my god, you're so right. I have a bug. Let me go fix that.
[00:02:34] Ilya Sutskever: And it reduces the second bug.
[00:02:36] And then you tell it you have this new, the second bug.
[00:02:39] Ilya Sutskever: And it tells you oh my god, how could have done it? You're so right again.
[00:02:42] Ilya Sutskever: And brings back the first bug. And you can alternate between those.
[00:02:45] Ilya Sutskever: Yeah.
[00:02:46] Ilya Sutskever: And it's like how is that possible?
[00:02:48] It's like I'm not sure.
[00:02:51] Ilya Sutskever: But it does suggest that there
[00:02:54] something strange is going on.
[00:02:56] Ilya Sutskever: I have two possible explanations.
[00:02:58] Ilya Sutskever: So here this is the more kind of a
[00:03:01] whimsical explanation is that maybe a rail training makes the models a little bit too
[00:03:05] Ilya Sutskever: single-minded and narrowly focused. A little bit too
[00:03:10] I don't know. Unaware.
[00:03:13] Even though it also makes them aware in some other ways.
[00:03:16] Ilya Sutskever: And because of this they can't do basic things.
[00:03:20] Ilya Sutskever: But there is another explanation which is
[00:03:24] back when people were doing pre-training.
[00:03:27] The question of what data to train on was answered.
[00:03:32] Because that answer was everything.
[00:03:35] Ilya Sutskever: When you do pre-training you need all the data.
[00:03:40] So you don't have to think it's going to be this data or that data.
[00:03:44] Ilya Sutskever: But when people do a rail training they do need to think.
[00:03:48] Ilya Sutskever: They say, okay, we want to have this kind of a rail training for this thing and that kind of
[00:03:52] Ilya Sutskever: a rail training for that thing. And from what I hear, all the companies have teams that
[00:03:58] Ilya Sutskever: just produce new a rail environments and just started to the training mix.
[00:04:02] Ilya Sutskever: And then the questions, well, what are those?
[00:04:03] Ilya Sutskever: There are so many degrees of freedom.
[00:04:05] Ilya Sutskever: There is such a huge variety of a rail environments you could produce.
[00:04:09] Ilya Sutskever: And one of the one thing you could do, and I think that something that is done inadvertently,
[00:04:17] is that people take inspiration from the e-vows.
[00:04:22] Ilya Sutskever: You say, hey, I would love our model to do really well when we release it.
[00:04:25] Ilya Sutskever: I want the e-vows to look great.
[00:04:28] What would be a rail training that could help on this task?
[00:04:32] Ilya Sutskever: Right? I think that is something that happens.
[00:04:35] Ilya Sutskever: And I think it could explain a lot of what's going on.
[00:04:38] Ilya Sutskever: If you combine this with generalization of the models actually being inadequate,
[00:04:44] that has the potential to explain a lot of what we are seeing.
[00:04:47] Ilya Sutskever: This disconnect between e-vows performance and actual real world performance,
[00:04:53] Ilya Sutskever: which is something that we don't today exactly even understand what we mean by that.
[00:05:00] Ilya Sutskever: I like this idea that the real reward hacking is the human researchers who are too focused on the e-vows.
[00:05:08] Dwarkesh Patel: I think there's two ways to understand or to try to think about what you have just pointed out.
[00:05:15] Dwarkesh Patel: One is, look, if it's the case that simply by becoming superhuman at a coding competition,
[00:05:21] Dwarkesh Patel: a model will not automatically become more tasteful and exercise better judgment about how to
[00:05:28] Dwarkesh Patel: improve your code base. Well, then you should expand the suite of environments such that you're not
[00:05:33] Dwarkesh Patel: just testing it on having the best performance in coding competition. It should also be able to make
[00:05:37] Dwarkesh Patel: the best kind of application for X-thing or Y-thing or Z-thing.
[00:05:41] Dwarkesh Patel: And another, maybe this is what you're hinting at, is to say, why should it be the case in the first place
[00:05:47] Dwarkesh Patel: that becoming superhuman at coding competitions doesn't make you a more tasteful programmer more
[00:05:53] Dwarkesh Patel: generally? Maybe the thing to do is not to keep stacking up the amount of environments and the
[00:05:58] Dwarkesh Patel: diversity of environments to figure out approach with, let you learn from one environment and
[00:06:04] Dwarkesh Patel: improve your performance on something else. So I have an analogy, a human analogy, which might be
[00:06:11] Ilya Sutskever: helpful. So even the case, let's take the case of competitive programming since you mentioned that.
[00:06:16] Ilya Sutskever: And suppose you have two students. One of them, work decided they want to be the best competitive
[00:06:23] Ilya Sutskever: programmer so they will practice 10,000 hours for that domain. They will solve all the problems,
[00:06:30] Ilya Sutskever: memorize all the proof techniques and be very, very, you know, be very skilled at quickly and
[00:06:37] Ilya Sutskever: correctly implementing all the algorithms and by doing so they became the best one of the best.
[00:06:44] Student number two thought, oh, competitive programming is cool, maybe they practiced for
[00:06:48] Ilya Sutskever: a hundred hours, much, much less, and they also did really well. Which one do you think is going to
[00:06:53] Ilya Sutskever: do better in their career later on? The second. Right? And I think that's basically what's going on.
[00:06:59] Ilya Sutskever: The models are much more likely for a student, but even more because then we say, okay,
[00:07:03] Ilya Sutskever: so the model should be good competitive programming. So let's get every single competitive programming
[00:07:08] Ilya Sutskever: problem ever. And then let's do some data augmentation. So we have even more competitive programming
[00:07:13] Ilya Sutskever: problems. Yes. And we train on that. And so now I got this great competitive programmer. And with
[00:07:18] Ilya Sutskever: this analogy, I think it's more intuitive. I think it's more intuitive with this analogy that, yeah,
[00:07:23] Ilya Sutskever: okay, so if it's so well trained, okay, it's like all the different algorithms and all the different
[00:07:28] Ilya Sutskever: proof techniques are like right at its fingertips. And it's more intuitive that with this level
[00:07:34] Ilya Sutskever: of preparation, it would not necessarily generalize to other things. But then what is the analogy for
[00:07:41] Dwarkesh Patel: what the second student is doing before they do the hundred hours of fine tuning? I think it's like
[00:07:50] they have it. I think it's the eat factor. Yeah. Right? And like I know, like when I was an
[00:07:55] Ilya Sutskever: undergraduate, I remember there was there was a student like this that studied with me. So I know
[00:08:00] Ilya Sutskever: I know it exists. Yeah. I think it's interesting to distinguish it from whatever pre-training does.
[00:08:05] Dwarkesh Patel: So when we don't understand what you just said about we don't have to choose the data in pre-training
[00:08:10] Dwarkesh Patel: is to say, actually, it's not dissimilar to the 10,000 hours of practice. It's just that you get
[00:08:15] Dwarkesh Patel: that 10,000 hours of practice for free because it's already somewhere in the pre-training distribution.
[00:08:22] Dwarkesh Patel: But it's like maybe you're suggesting actually there's actually not that much
[00:08:25] Dwarkesh Patel: generalization pre-training. There's just so much data in pre-training. But it's not necessarily
[00:08:29] Dwarkesh Patel: generalizing better than RL. But the main strength of pre-training is that there is a so much of it.
[00:08:36] And b, you don't have to think hard about what data to put into pre-training.
[00:08:42] And it's a very kind of natural data and it does include in it a lot of what people do.
[00:08:48] Ilya Sutskever: Yeah. People's thoughts and a lot of the features of, you know, it's like the whole world
[00:08:56] Ilya Sutskever: as projected by people onto text. Yeah. And pre-training tries to capture that using a huge amount
[00:09:02] Ilya Sutskever: of data. It's very, the pre-training is very difficult to reason about because it's so hard to
[00:09:10] Ilya Sutskever: understand the manner in which the model relies on pre-training data.
[00:09:17] And whenever the model makes a mistake, could it be because something by chance is not as
[00:09:23] Ilya Sutskever: supported by the pre-training data? You know, and pre-support by pre-training is maybe a loose term.
[00:09:30] I don't know if I can add anything more useful on this, but I don't think there is a human
[00:09:37] Ilya Sutskever: analog to pre-training. Here's an analogy that people have proposed forward the human analogy
[00:09:42] Dwarkesh Patel: to pre-training is and I'm curious to get your thoughts on why they're potentially wrong.
[00:09:47] Dwarkesh Patel: One is to think about the first 18 or 15 or 13 years of a person's life when they aren't
[00:09:54] Dwarkesh Patel: necessarily economically productive, but they are doing something that is making them
[00:10:01] Dwarkesh Patel: understand the world better and so forth. And the other is to think about evolution as doing
[00:10:07] Dwarkesh Patel: some kind of search for three billion years, which then results in a human lifetime instance.
[00:10:13] Dwarkesh Patel: And then I'm curious if you think either of these are actually analogous pre-training or
[00:10:17] Dwarkesh Patel: how would you think about at least what lifetime human learning is like if not pre-training?
[00:10:22] Dwarkesh Patel: I think there are some similarities between both of these to pre-training and pre-training
[00:10:28] Ilya Sutskever: tries to play the role of both of these, but I think there are some big differences as well.
[00:10:33] Ilya Sutskever: The amount of pre-training data is very, very staggering.
[00:10:38] Ilya Sutskever: Yes.
[00:10:39] Ilya Sutskever: And somehow a human being after even 15 years with the tiny fraction of the pre-training data,
[00:10:48] Ilya Sutskever: they know much less, but whatever they do know, they know much more deeply, somehow.
[00:10:52] Ilya Sutskever: And the mistakes like already at that age, you would not make mistakes that ARIA is make.
[00:10:58] Ilya Sutskever: Yes.
[00:10:59] There is another thing, you might say, could it be something like evolution?
[00:11:02] Ilya Sutskever: And the answer is maybe, but in this case, I think evolution might actually have an edge.
[00:11:07] Ilya Sutskever: There is this, I remember reading about this case where some, you know, that one thing that
[00:11:15] Ilya Sutskever: neuroscientists do, or rather one way in which neuroscientists can learn about the brain,
[00:11:20] Ilya Sutskever: is by studying people with brain damage to different parts of the brain.
[00:11:25] Dwarkesh Patel: And so some people have the most strange symptoms you could imagine.
[00:11:29] Ilya Sutskever: It's actually really, really interesting.
[00:11:32] And there was one case that comes to mind that's relevant.
[00:11:35] I read about this person who had some kind of brain damage
[00:11:40] Ilya Sutskever: that took out, I think, a stroke or an accident,
[00:11:43] Ilya Sutskever: that took out his emotional processing.
[00:11:48] Ilya Sutskever: So he stopped feeling any emotion.
[00:11:50] And as a result of that, you know, he still remained very articulate and he could solve little puzzles
[00:11:58] Ilya Sutskever: and on tests he seemed to be just fine.
[00:12:01] But he felt no emotion, he didn't feel sad, he didn't feel angry, he didn't feel animated.
[00:12:06] And he became somehow extremely bad at making any decisions at all.
[00:12:11] It would take him hours to decide on which socks to wear,
[00:12:14] Ilya Sutskever: and he would make very bad financial decisions.
[00:12:17] Ilya Sutskever: And that's very,
[00:12:21] but what does it say about the role of our built-in emotions
[00:12:29] in making us like a viable agent essentially?
[00:12:33] Ilya Sutskever: And I guess to connect to your question about pre-training.
[00:12:36] Ilya Sutskever: It's like maybe if you're good enough at getting everything out of pre-training,
[00:12:42] Ilya Sutskever: you could get that as well.
[00:12:44] Ilya Sutskever: But that's the kind of thing which seems
[00:12:50] well, it may or may not be possible to get that from pre-training.
[00:12:56] What is that?
[00:12:58] Dwarkesh Patel: Clearly not just directly emotion, it seems like some
[00:13:04] almost value function like thing, which is telling you wish decision to be made.
[00:13:08] Dwarkesh Patel: Like what the end reward for any decision should be.
[00:13:11] Dwarkesh Patel: And you think that doesn't implicitly come from...
[00:13:15] Dwarkesh Patel: I think it's good.
[00:13:16] Dwarkesh Patel: I'm just saying it's not 100% obvious.
[00:13:20] But what is that?
[00:13:21] Dwarkesh Patel: Like how do you think about emotions?
[00:13:23] Dwarkesh Patel: What is the ML analogy for emotions?
[00:13:26] It should be some kind of a value function thing.
[00:13:29] Ilya Sutskever: But I don't think there is a greater
[00:13:30] Ilya Sutskever: ML analogy because right now value functions don't play very prominent role
[00:13:34] Ilya Sutskever: in the things people do.
[00:13:36] Ilya Sutskever: It might be worth defining for the audience what a value function is if you want to do that.
[00:13:39] Dwarkesh Patel: I mean, certainly I'll be very happy to do that.
[00:13:43] Ilya Sutskever: Right?
[00:13:44] Ilya Sutskever: So...
[00:13:48] So when people do reinforcement learning, the very reinforcement learning is done right now.
[00:13:53] How do people train those agents?
[00:13:56] So you have a neural net and you give it a problem.
[00:13:59] Ilya Sutskever: And then you tell the model go solve it.
[00:14:01] Ilya Sutskever: And the model takes maybe thousands, hundreds of thousands of actions
[00:14:06] or thoughts or something and then it produces a solution, a solution is created.
[00:14:09] Ilya Sutskever: And then the score is used to provide a training signal for every single action
[00:14:18] in your trajectory.
[00:14:20] So that means that if you are doing something that goes for a long time,
[00:14:24] Ilya Sutskever: if you're training a task that takes a long time to solve,
[00:14:28] Ilya Sutskever: you will do no learning at all until you solve until you come up with a proposed solution.
[00:14:33] Ilya Sutskever: That's how reinforcement learning is done naively.
[00:14:35] Ilya Sutskever: That's how O1, R1 ostensibly are done.
[00:14:40] The value function says something like, okay, look,
[00:14:44] maybe I could sometimes, not always,
[00:14:47] could tell you if you are doing valor badly.
[00:14:50] Ilya Sutskever: The notion of a value function is more useful in some domains than others.
[00:14:53] Ilya Sutskever: So for example, when you play chess,
[00:14:56] and you lose a piece, you know, I messed up.
[00:14:59] You don't need to play the whole game to know that what I just did was bad and therefore whatever
[00:15:04] Ilya Sutskever: proceeded, it was also bad.
[00:15:08] So the value function lets you short circuit the weight until the very end.
[00:15:14] Ilya Sutskever: Like, let's suppose that you started to pursue some kind of, okay,
[00:15:18] Ilya Sutskever: let's suppose that you are doing some kind of a math thing or a programming thing.
[00:15:22] Ilya Sutskever: And you're trying to explore a particular solution direction.
[00:15:26] And after, let's say after a thousand steps of thinking, you concluded that this direction
[00:15:32] Ilya Sutskever: is unpromising. As soon as you conclude this, you could already get a reward signal
[00:15:39] a thousand times steps previously when you decided to pursue down the path.
[00:15:43] Ilya Sutskever: You say, oh, next time, I shouldn't pursue this path in a similar situation.
[00:15:48] Ilya Sutskever: Long before you actually came up with the proposed solution.
[00:15:52] This was in the deep cigar one paper is that the space of trajectories is so wide
[00:16:00] Dwarkesh Patel: that maybe it's hard to learn a mapping from an intermediate trajectory and value.
[00:16:06] Dwarkesh Patel: And also given that, you know, encoding, for example, you will have the wrong idea
[00:16:10] Dwarkesh Patel: then you'll go back, then you'll change something.
[00:16:12] Dwarkesh Patel: This sounds like such lack of faith in deep learning.
[00:16:15] Ilya Sutskever: Like, I mean, sure, it might be difficult, but nothing deep learning can do.
[00:16:21] Ilya Sutskever: Yeah.
[00:16:22] Ilya Sutskever: So my expectation is that like value function should be useful and
[00:16:30] and I fully, I fully expect that they will be using the future if not already.
[00:16:34] Ilya Sutskever: What was I alluded to with the person whose emotional center got
[00:16:40] Ilya Sutskever: damaged is more that maybe what it suggests is that the value function of humans is
[00:16:49] Ilya Sutskever: modulated by emotions in some important way that's hard coded by evolution.
[00:16:55] And maybe that is important for people to be effective in the world.
[00:17:00] That's the thing I was actually planning on asking you.
[00:17:02] Dwarkesh Patel: There's something really interesting about emotions of the value function, which is that
[00:17:06] it's impressive that they have this much utility while still being rather simple to understand.
[00:17:15] Dwarkesh Patel: So I have two responses.
[00:17:17] Dwarkesh Patel: I do agree that compared to
[00:17:23] the kind of things that we learn and the things we are talking about, the kind of as we are talking
[00:17:27] Ilya Sutskever: about, emotions are relatively simple.
[00:17:30] They might even be so simple that maybe you could map them out in a human understandable way.
[00:17:35] Ilya Sutskever: I think it would be cool to do.
[00:17:39] In terms of utility though, I think there is a thing where,
[00:17:43] Ilya Sutskever: you know, there is this complexity robustness trade-off,
[00:17:48] where complex things can be very useful,
[00:17:52] but simple things are very useful in very broad range of situations.
[00:17:59] So I think one way to interpret what we are seeing is that we've got these emotions that
[00:18:04] Ilya Sutskever: essentially evolved mostly from our mammal ancestors, and then fine tune the little bit while we
[00:18:10] Ilya Sutskever: were hominins just a bit. We do have like a decent amount of social emotions though, which mammals
[00:18:17] Ilya Sutskever: may lack, but they're not very sophisticated, and because they're not sophisticated,
[00:18:23] Ilya Sutskever: they serve us so well in this very different world compared to the one that we've been living in.
[00:18:28] Ilya Sutskever: Actually, they also make mistakes. For example, our emotions, well, I don't know,
[00:18:32] Ilya Sutskever: this hunger count is an emotion, it's debatable, but I think for example, our intuitive feeling of
[00:18:39] Ilya Sutskever: hunger is not succeeding in guiding us correctly in this world with an abundance of food.
[00:18:48] Ilya Sutskever: Yeah. People have been talking about scaling data, scaling parameter, scaling compute.
[00:18:56] Is there a more general way to think about scaling? What are the other scaling axes?
[00:19:00] So the thing, so here is a perspective. Here's a perspective that I think might be true.
[00:19:10] So the way a male used to work is that people would just think of it with stuff and try to
[00:19:20] and try to get interesting results. That's what's been going on in the past.
[00:19:26] Ilya Sutskever: Then the scaling insight arrived, right? Scaling laws, GPT-3, and suddenly everyone realized
[00:19:36] Ilya Sutskever: we should scale. And this is an example of how language affects thought.
[00:19:45] Scaling is just one word, but it's such a powerful word because it informs people what to do.
[00:19:51] Ilya Sutskever: Let's try to scale things. So what are we scaling? And pre-training was a thing to scale.
[00:19:58] Ilya Sutskever: It was a particular scaling recipe. The big breakthrough of pre-training is the realization
[00:20:05] Ilya Sutskever: that this recipe is good. So you say, hey, if you mix some compute with some data into a neural net
[00:20:14] Ilya Sutskever: of a certain size, you will get results. And you will know that it will be better if you just
[00:20:20] Ilya Sutskever: scale the recipe up. And this is also great companies love this because it gives you a very
[00:20:27] Ilya Sutskever: low risk way of investing your resources. It's much harder to invest your resources in research.
[00:20:36] Compare that. If you have research, you need to go for three researchers and research and come
[00:20:41] Ilya Sutskever: up with something. Versus, get more data, get more compute, you know, it'll get something from
[00:20:46] Ilya Sutskever: pre-training. And indeed, it looks like I based on various things, some people say on Twitter,
[00:20:56] Ilya Sutskever: maybe it appears that Gemini have found a way to get more out of pre-training. At some point,
[00:21:02] Ilya Sutskever: the pre-training will run out of data. The data is very clearly finite. And so then, okay,
[00:21:06] Ilya Sutskever: what do you do next? I do some kind of a souped-up pre-training different recipe from the one you've
[00:21:12] Ilya Sutskever: done before, or you're doing a RL, or maybe something else. But now that compute is big,
[00:21:17] Ilya Sutskever: compute is now very big. In some sense, we are back to the age of research. So maybe here's
[00:21:22] Ilya Sutskever: another way to put it. Up until 2020, from 2020, from 2020, it was the age of research.
[00:21:30] Ilya Sutskever: Now, from 2020 to 2025, it was the age of scaling. Or maybe plus minus, let's add
[00:21:36] Ilya Sutskever: the error bars to those years. Because people say, this is amazing, you've got to scale more, keep
[00:21:40] Ilya Sutskever: scaling, the one word, scaling. But now the scale is so big. Like, is the belief really that,
[00:21:48] Ilya Sutskever: oh, it's so big, but if you had a 100x more, everything would be so different. Like, it would be
[00:21:53] different for sure. But like, is the belief that if you just 100x the scale, everything would be
[00:22:00] Ilya Sutskever: transformed? I don't think that's true. So, in respect to the age of research again, just with
[00:22:05] Ilya Sutskever: the computers. That's very interesting. We're going to put it. But let me ask you the question you're
[00:22:11] Dwarkesh Patel: just posed then. What are we scaling? And what would it mean to have a recipe? Because I guess
[00:22:17] Dwarkesh Patel: I'm not aware of a very clean relationship that almost looks like a law of physics, which existed
[00:22:24] Dwarkesh Patel: in pre-training. It does a power law between data or computer parameters and loss. What is the
[00:22:31] Dwarkesh Patel: kind of relationship we should be seeking? And how should we think about what this new recipe
[00:22:37] Dwarkesh Patel: might look like? So, we've already witnessed a transition from one type of scaling to a different
[00:22:46] Ilya Sutskever: type of scaling from pre-training to RL. Now people are scaling RL. Now based on what people say on
[00:22:55] Ilya Sutskever: Twitter, they spend more compute on RL than on pre-training at this point because RL can actually
[00:23:01] Ilya Sutskever: consume quite a bit of compute. You know, you do very, very long roll-outs. So, it takes a lot of
[00:23:07] Ilya Sutskever: compute to produce those roll-outs. And then you get relative to the small amount of learning
[00:23:11] Ilya Sutskever: pillar roll-outs. So, you really can spend a lot of compute. And I could imagine, like, I wouldn't
[00:23:18] at this stage, it's more like, I wouldn't even call it a scaling. I would say, hey, like, what are
[00:23:25] Ilya Sutskever: you doing? And is the thing you're doing the most productive thing you could be doing? Can you
[00:23:31] Ilya Sutskever: find a more productive way of using your compute? We've discussed the value function business earlier.
[00:23:39] Ilya Sutskever: And maybe once people get good at value functions, they will be using their resources more
[00:23:45] Ilya Sutskever: productively. And if you find a whole other way of training models, you could say, is this scaling
[00:23:54] Ilya Sutskever: or is it just using your resources? I think it becomes a little bit ambiguous. In a sense that when
[00:23:58] Ilya Sutskever: people were in the age of research, back then it was like, people say, hey, let's try this in this
[00:24:03] Ilya Sutskever: and this and this. Let's try that and that and that. Oh, look, something interesting is happening.
[00:24:07] And I think there will be a return to that. So, if we're back in the era of research, stepping back,
[00:24:12] Dwarkesh Patel: what is the part of the recipe that we need to think most about? When you say value function,
[00:24:18] Dwarkesh Patel: people are already trying the current recipe, but then having a little limit as a judge and so forth.
[00:24:23] Dwarkesh Patel: You can say that's a value function, but it sounds like you have something much more fundamental
[00:24:26] Dwarkesh Patel: in mind. Do we need to go back to, should we even rethink pre-training at all and not just add
[00:24:33] Dwarkesh Patel: more steps to the end of that process? Yeah. So, the discussion about value function, I think it was
[00:24:40] Ilya Sutskever: interesting. I want to emphasize that I think the value function is something like, it's going to make
[00:24:47] Ilya Sutskever: our realm more efficient. And I think that makes a difference. But I think that anything you can do
[00:24:54] Ilya Sutskever: with a value function, you can do without just more slowly. The thing which I think is the most
[00:25:01] Ilya Sutskever: fundamental is that these models somehow just generalize dramatically worse than people.
[00:25:07] And it's super obvious. That seems like a very fundamental thing.
[00:25:12] Ilya Sutskever: Okay. So, this is the Crocs generalization. And there's two sub-questions.
[00:25:20] There's one which is about sample efficiency, which is why should it take so much more data for
[00:25:24] Dwarkesh Patel: these models to learn than humans? There's a second about even separate from the amount of data it
[00:25:29] Dwarkesh Patel: takes. There's a question of, why is it so hard to teach the thing we want to a model than to a
[00:25:35] Dwarkesh Patel: human, which is to say, for to a human, we don't necessarily need a verifiable reward to be able to
[00:25:43] you're probably mentoring a bunch of researchers right now and you're, you know, talking with them,
[00:25:47] Dwarkesh Patel: you're showing them your code and you're showing them how you think and from that,
[00:25:51] Dwarkesh Patel: they're picking up your way of thinking and how they should do research. You don't have to set
[00:25:55] Dwarkesh Patel: like a verifiable reward for them. That's like, okay, this is the next part of their curriculum. And
[00:25:58] Dwarkesh Patel: now this is the next part of your curriculum. And oh, I was, this training was unstable and we get
[00:26:02] Dwarkesh Patel: a there's not this schleppy bespoke process. So perhaps these two issues are actually related in
[00:26:08] Dwarkesh Patel: some way. But I'd be curious to explore this, this second thing which was more like continuing
[00:26:14] Dwarkesh Patel: or learning and this first thing which feels just like sample efficiency. Yeah. So, you know,
[00:26:20] Ilya Sutskever: you could actually wonder one possible explanation for the human sample efficiency that needs to be
[00:26:27] Ilya Sutskever: considered easy evolution. And evolution has given us a small amount of the most useful information
[00:26:36] Ilya Sutskever: possible. And for things like vision, hearing and locomotion, I think there's a pretty strong case
[00:26:45] Ilya Sutskever: that evolution actually has given us a lot. So for example, human dexterity far exceeds, I mean,
[00:26:54] robots can become dexterous too if you subject them to like a huge amount of training and simulation.
[00:27:00] But to train a robot in the real world to quickly like pick up a new skill like a person does,
[00:27:05] Ilya Sutskever: seems very out of reach. And here you could say, oh yeah, like locomotion, all our ancestors needed
[00:27:12] Ilya Sutskever: a great locomotion squirrels like. So locomotion maybe like you've got like some unbelievable prior.
[00:27:19] Ilya Sutskever: You could make the same case for vision, you know, I believe the Anlequin made the point, oh like
[00:27:24] Ilya Sutskever: come, children learn to drive after 16 hours, after 10 hours of practice, which is true. But our
[00:27:31] Ilya Sutskever: vision is so good. At least for me, when I remember myself being five year old, my I was very excited
[00:27:38] Ilya Sutskever: about cars back then. And I'm pretty sure my car recognition was more than addict but for self-driving
[00:27:45] Ilya Sutskever: already as a five year old. You don't get to see that much data as a five year old. You spend
[00:27:49] Ilya Sutskever: most of your time in your parents' house. So you have very low data diversity. But you could say maybe
[00:27:53] Ilya Sutskever: that's evolution too. But then language and math and coding, probably not. It still seems
[00:28:00] better than models. I mean, obviously models are better than the average human at language and
[00:28:05] Dwarkesh Patel: math and coding. But are they better at the average human at learning? Oh yeah, oh yeah, absolutely.
[00:28:11] Ilya Sutskever: What I meant to say is that language, math and coding and especially math and coding suggest
[00:28:17] Ilya Sutskever: that whatever it is that makes people good at learning is probably not so much a complicated
[00:28:25] Ilya Sutskever: prior but something more, some fundamental thing. Wait, I'm not sure, why should that be the case?
[00:28:32] So consider a skill that people exhibit some kind of great reliability or, you know, M.
[00:28:39] Ilya Sutskever: Yeah. If the skill is one that was very useful to our ancestors for many millions of years,
[00:28:47] Ilya Sutskever: hundreds of millions of years, you could say, you could argue that maybe humans are good at it
[00:28:54] because of evolution because we have a prior and evolutionary prior that's encoded in some very
[00:29:02] Ilya Sutskever: non-obvious way that somehow makes us so good at it. But if people exhibit great
[00:29:10] Ilya Sutskever: ability, reliability, robustness, ability to learn in a domain that really did not exist until recently,
[00:29:19] Ilya Sutskever: then this is more an indication that people might have just better machine learning period.
[00:29:28] But then how should we think about what that is? Is it a matter of,
[00:29:34] yeah, what is the ML analogy for what? There's a couple of interesting things about it. It takes
[00:29:39] Dwarkesh Patel: fewer samples. It's more unsupervised. You don't have to set a, like a child learning to drive a car.
[00:29:45] A child, so no, no, no, learning to drive a car. A teenager learning to drive a car is like not
[00:29:50] Dwarkesh Patel: exactly getting some pre-built verifiable reward. It comes from their interaction with the machine
[00:29:59] Dwarkesh Patel: and with the environment. And yet, it takes much of your samples. It seems more unsupervised.
[00:30:05] Dwarkesh Patel: It seems more robust. Much more robust. The robustness of people is really staggering.
[00:30:12] Yeah. So it's like, okay, and do you have a unified way of thinking about why are all these things
[00:30:16] Dwarkesh Patel: happening at once? What is the ML analogy that could realize something like this?
[00:30:23] Dwarkesh Patel: So this is where one of the things that you've been asking about is how can the teenage driver
[00:30:31] Ilya Sutskever: kind of self-correct and learn from their experience without an external teacher? And the answer
[00:30:37] Ilya Sutskever: is, well, they have their value function. They have a general sense, which is also, by the way,
[00:30:44] Ilya Sutskever: extremely robust in people. Like, whatever it is, the human value function, whatever the human
[00:30:51] Ilya Sutskever: value function is, with a few exceptions around addiction, it's actually very, very robust.
[00:30:58] And so for something like a teenager that's learning to drive, they start to drive
[00:31:04] and they already have a sense of how they're driving. Immediately, how badly they're unconfident,
[00:31:10] Ilya Sutskever: and then they see okay. And then, of course, the learning speed of any teenager is so fast,
[00:31:15] Ilya Sutskever: stuff that 10 hours, you're good to go. Yeah. This seems like humans have some solution, but I'm
[00:31:19] Dwarkesh Patel: curious about like, well, how are they doing it? And like, why is it so hard to, like, how do we
[00:31:23] Dwarkesh Patel: need to reconsensualize the way we're training models to make something like this possible? You know,
[00:31:28] that is a great question to ask. And it's a question I have a lot of opinions about.
[00:31:35] Ilya Sutskever: But unfortunately, we live in a world where not all machine learning ideas are discussed freely,
[00:31:43] Ilya Sutskever: and this is one of them. So there's probably a way to do it. I think it can be done. The fact that
[00:31:51] Ilya Sutskever: people are like that, I think it's a proof that it can be done. There may be another blocker though,
[00:31:57] Ilya Sutskever: which is the responsibility that the human neurons actually do more computing, we think.
[00:32:07] And if that is true, and if that plays an important role, then things might be more difficult.
[00:32:13] But regardless, I do think it points to the existence of some machine learning principle,
[00:32:21] but I have an opinion on, but unfortunately, circumstances make it hard to discuss in detail.
[00:32:27] Ilya Sutskever: Nobody listens to this podcast, Ilia.
[00:32:31] Yeah. So I have to say that prepping for Ilia was pretty tough, because neither I nor anybody else
[00:32:37] Dwarkesh Patel: had any idea what he's working on and what SSI is trying to do. I had no basis to come up with
[00:32:43] Dwarkesh Patel: my questions. And the only thing I could go off, honestly, was trying to think from first
[00:32:48] Dwarkesh Patel: principles about what are the bottlenecks to AGI, because clearly Ilia is working on them in some
[00:32:53] Dwarkesh Patel: way. Part of this question involved thinking about RL scaling, because everybody's asking how well
[00:32:58] Dwarkesh Patel: RL will generalize and how we can make it generalize better. As part of this, I was reading this
[00:33:02] Dwarkesh Patel: paper that came out recently on RL scaling, and it showed that actually the learning curve on RL
[00:33:08] Dwarkesh Patel: looks like a sigmoid. I found this very curious. Why should it be a sigmoid? Where it learns very
[00:33:12] Dwarkesh Patel: little for a long time, and then it quickly learns a lot, and then it asses him totes. This is very
[00:33:17] Dwarkesh Patel: different from the power law you see in pre-training, where the model learns a bunch at the very beginning,
[00:33:22] Dwarkesh Patel: and then less and less over time. And it actually reminded me of a note that I had run down after
[00:33:27] Dwarkesh Patel: I had a conversation with a researcher friend, where he pointed out that the number of samples that
[00:33:32] Dwarkesh Patel: you need to take in order to find the correct answer, scales exponentially with how different your
[00:33:38] Dwarkesh Patel: current probability distribution is from the target probability distribution. And I was thinking
[00:33:42] Dwarkesh Patel: about how these two ideas are related. I had the survey idea that they should be connected, but I
[00:33:46] Dwarkesh Patel: really didn't know how. I don't have a math background, so I couldn't really formalize it.
[00:33:50] Dwarkesh Patel: But I wondered if Gemini3 could help me out here. And so I took a picture of my notebook,
[00:33:54] Dwarkesh Patel: and I took the paper, and I put them both in the context of Gemini3, and I asked it to find the
[00:33:59] Dwarkesh Patel: connection. And I thought a bunch, and then it realized that the correct way to model the
[00:34:05] Dwarkesh Patel: information you gain from a single yes or no outcome in RL is as the entropy of a random binary
[00:34:11] Dwarkesh Patel: variable. It made a graph, which showed how the bits you gain for sample in RL versus supervised
[00:34:17] Dwarkesh Patel: learning scale as a pass rate increases. And as soon as I saw the graph that Gemini3 made,
[00:34:22] Dwarkesh Patel: immediately a ton of things started making sense to me. Then I wanted to see if there was any
[00:34:27] Dwarkesh Patel: empirical basis to this theory. So I asked Gemini to code on my experiment to show whether the
[00:34:33] Dwarkesh Patel: improvement in loss scales in this way with pass rate. I just took the code that Gemini
[00:34:38] Dwarkesh Patel: outputted. I copy-pasted it into a Google call lab notebook, and I was able to run this toy
[00:34:42] Dwarkesh Patel: ML experiment and visualize its results without a single bug. It's interesting because the results look
[00:34:48] Dwarkesh Patel: similar, but not identical to what we should have expected. And so I downloaded this chart,
[00:34:52] Dwarkesh Patel: and I put it into Gemini, and I asked it, what is going on here? And I came up with a hypothesis
[00:34:56] Dwarkesh Patel: that I think is actually correct, which is that we're capping how much supervised learning can
[00:35:01] Dwarkesh Patel: improve in the beginning by having a fixed learning rate. And in fact, we should decrease the
[00:35:05] Dwarkesh Patel: learning rate over time. It actually gives us an intuitive understanding for why in practice,
[00:35:10] Dwarkesh Patel: we have learning rate schedulers that decrease the learning rate over time. I did this entire flow
[00:35:15] Dwarkesh Patel: from coming up with this vague initial question to building a theoretical understanding,
[00:35:20] Dwarkesh Patel: to running some toy ML experiments all with Gemini3. This feels like the first model where it can
[00:35:26] Dwarkesh Patel: actually come up with new connections that I wouldn't have anticipated. It's actually not
[00:35:30] Dwarkesh Patel: become the default place I go to when I want to brainstorm new ways to think about a problem.
[00:35:35] Dwarkesh Patel: If you want to read more about our all-scaling, you can check out the blog post that I wrote with
[00:35:39] Dwarkesh Patel: a little help from Gemini3. And if you want to check out Gemini3Yourself, go to Gemini.google.
[00:35:45] Dwarkesh Patel: I am curious, so if you say we are back in the era of research, you were there from 2012 to 2020.
[00:35:53] Dwarkesh Patel: And do you have... Yeah, what is now the vibe going to be if we go back to the era of research?
[00:36:00] Dwarkesh Patel: For example, even after Alex and I, the amount of compute that was used to run experiments kept
[00:36:07] Dwarkesh Patel: increasing, and the size of frontier systems kept increasing. And do you think now that this
[00:36:15] Dwarkesh Patel: era of research will still require tremendous amounts of compute? Do you think it will require
[00:36:21] Dwarkesh Patel: going back into the archives and reading old papers? What was the vibe of like you were Google and
[00:36:29] Dwarkesh Patel: Open AI and Stanford? These places when there was like more of a vibe of research,
[00:36:34] Dwarkesh Patel: what kind of thing should we be expecting in the community?
[00:36:38] So one consequence of the age of scaling is that there was this
[00:36:47] Ilya Sutskever: scaling sucked out all the air in the room. Yeah. And so because scaling sucked out all the air in the
[00:36:54] Ilya Sutskever: room, everyone started to do the same thing. We got to the point where we are in a world where there
[00:37:04] Ilya Sutskever: are more companies than ideas, but quite a bit. Actually, on that, you know, there is this
[00:37:10] Ilya Sutskever: Silicon Valley saying that says that ideas are cheap, execution is everything. And people say
[00:37:18] that a lot. And there is truth to that. But then I saw someone say on Twitter something like
[00:37:26] Ilya Sutskever: if ideas are so cheap, how come no one's having any ideas? And I think it's true too. I think
[00:37:33] if you think about a research progress in terms of bottlenecks, there are several bottlenecks.
[00:37:42] If you go back to the, if you end them, one of them is ideas and one of them is your ability to
[00:37:47] Ilya Sutskever: bring them to life, which might be compute, but also engineering. So if you go back to the 90s,
[00:37:53] Ilya Sutskever: let's say, you had people who had had pretty good ideas. And if they had much larger computers,
[00:37:58] Ilya Sutskever: maybe they could demonstrate that their ideas were viable, but they could not. So they could only
[00:38:03] Ilya Sutskever: have very, very small demonstration that did not convince anyone. So the bottleneck was compute.
[00:38:09] Then in the age of scaling, computers increased a lot. And of course, there is a question of how
[00:38:16] Ilya Sutskever: much compute is needed, but compute is large. So compute is large enough such that
[00:38:25] it's like not obvious that you need that much more compute to prove some idea. Like I'll give you
[00:38:33] Ilya Sutskever: an analogy, Alex net was built on two GPUs. That was the total amount of compute used for it.
[00:38:40] The transformer was built on 8 to 64 GPUs. No single transformer paper experiment used more than
[00:38:48] Ilya Sutskever: 64 GPUs of 2017, which would be like what, two GPUs of today. So the ResNet, right, many, like even
[00:38:59] Ilya Sutskever: the, the you could argue that the like, oh, one reasoning was not the most compute heavy thing in
[00:39:07] Ilya Sutskever: the world. So they're definitely for research. You need like definitely some amount of compute.
[00:39:16] Ilya Sutskever: But it's far from obvious that you need the absolutely largest amount of compute ever for research.
[00:39:22] Ilya Sutskever: You might argue, and I think it is true, that if you want to build the absolutely best system,
[00:39:28] if you want to build the absolutely best system, then it helps to have much more compute. And
[00:39:33] Ilya Sutskever: especially if everyone is within the same paradigm, then compute becomes one of the big differentiators.
[00:39:42] Yeah, I guess while it was possible to develop these ideas, I'm asking you for the history because
[00:39:47] Dwarkesh Patel: you were actually there. I'm not sure what actually happened, but it sounds like it was possible to
[00:39:51] Dwarkesh Patel: develop these ideas using minimal amounts of compute. But it wasn't the transformer didn't
[00:39:56] Dwarkesh Patel: immediately become famous. It became the thing everybody started doing and then started experimenting
[00:40:00] Dwarkesh Patel: on top of and building on top of because it was validated at higher and higher levels of compute.
[00:40:06] Dwarkesh Patel: Correct. And if you and SSI have 50 different ideas, how will you know which one is the next transformer
[00:40:13] Dwarkesh Patel: and which one is brittle without having the kinds of compute that other frontier labs have?
[00:40:22] So I can comment on that, which is the short comment is that you mentioned SSI,
[00:40:30] Ilya Sutskever: specifically for us, the amount of compute that SSI has for research is really not that small.
[00:40:40] Ilya Sutskever: And I want to explain why, like a simple math can explain why the amount of compute that we have
[00:40:45] Ilya Sutskever: is actually a lot more comparable for research than one might think. Now explain. So,
[00:40:53] Ilya Sutskever: SSI has raised $3 billion, which is like not small, but it's like a lot by any absolute sense.
[00:41:04] Ilya Sutskever: But you could say, but look at the other companies raising much more. But a lot of what their
[00:41:10] Ilya Sutskever: a lot of their compute goes for inference. Like these big numbers, these big loans, it's earmarked for
[00:41:17] Ilya Sutskever: inference. That's number one. Number two, you need if you want to have a product on which you
[00:41:24] Ilya Sutskever: do inference, you need to have a big staff of engineers, of salespeople, a lot of the research needs
[00:41:29] Ilya Sutskever: to be dedicated for producing all kinds of product related features. So then when you look at what's
[00:41:36] Ilya Sutskever: actually left for research, the difference becomes a lot smaller. Now the other thing is that if you are
[00:41:44] Ilya Sutskever: doing something different, do you really need the absolute maximal scale to prove it? I don't
[00:41:51] think it's true at all. I think that in our case, we have sufficient compute to prove to convince
[00:41:59] Ilya Sutskever: ourselves and anyone else that what we're doing is correct. There's been public estimates that
[00:42:04] Dwarkesh Patel: companies like OpenAI spend on the order of $56 billion a year, just so far on experiments.
[00:42:12] Dwarkesh Patel: This is separate from the amount of money they're sending on inference and so forth. So
[00:42:16] Dwarkesh Patel: it seems like they're spending more a year running like research experiments, the new guys
[00:42:21] Dwarkesh Patel: have in total funding. I think it's a question of what you do with it. It's a question of what you
[00:42:25] do with it. Like they have like the more, I think in their case, in the case of others, I think there
[00:42:32] Ilya Sutskever: is a lot more demand on the training compute. There's a lot more different workstreams. There
[00:42:38] Ilya Sutskever: are different modalities. There is just more stuff and so it becomes fragmented. How will SSI
[00:42:45] Ilya Sutskever: make money? My answer to this question is something like maybe just right now, we just focus on
[00:42:54] Ilya Sutskever: the research and then the answer to this question will reveal itself. I think there will be lots of
[00:42:59] Ilya Sutskever: possible answers. Is SSI's plans to author straight shot superintelligence? Maybe. I think that
[00:43:07] Ilya Sutskever: there is merit to it. I think there's a lot of merit because I think that it's very nice to not
[00:43:12] Ilya Sutskever: be affected by the day-to-day market competition. But I think there are two reasons that make
[00:43:22] Ilya Sutskever: causes to change the plan. One is pragmatic if timelines turned out to be long, which they might.
[00:43:30] Ilya Sutskever: And second, I think there is a lot of value in the best and most powerful AI being out there
[00:43:40] Ilya Sutskever: impacting the world. I think this is a meaningful, valuable thing.
[00:43:45] Ilya Sutskever: But then, so why is your default plan to straight shot superintelligence? Because it sounds like
[00:43:50] Dwarkesh Patel: you know, open AI and theropic, all these other companies, their explicit thinking is,
[00:43:54] Dwarkesh Patel: look, we have weaker and weaker intelligences that the public can get used to and prepare for
[00:44:00] Dwarkesh Patel: and why is it potentially better to build a superintelligence directly? So I'll make the case
[00:44:07] Ilya Sutskever: four and against. The case four is that you are, so one of the challenges that people face when
[00:44:15] Ilya Sutskever: they're in the market is that they have to participate in the rat race. And the rat race is quite
[00:44:21] Ilya Sutskever: difficult in that it exposes you to do to difficult trade-offs which you need to make.
[00:44:27] And the reason it is nice to say we'll insulate ourselves from all this and just focus on the
[00:44:33] Ilya Sutskever: research and come out only when we are ready and not before. But the counterpoint is value two.
[00:44:40] Ilya Sutskever: And those opposing forces, the counterpoint is hey, it is useful for the world to see powerful AI.
[00:44:50] Ilya Sutskever: It is useful for the world to see powerful AI because that's the only way you can communicate it.
[00:44:55] Ilya Sutskever: Well, I guess not even just that you can communicate the idea, but communicate the AI, not the idea,
[00:45:01] communicate the AI. What do you mean communicate the AI?
[00:45:03] Dwarkesh Patel: So, okay, so let's suppose you read an essay about AI. And the essay says AI is going to be this
[00:45:09] Ilya Sutskever: and AI is going to be that and it's going to be this. And you read it and you say, okay,
[00:45:13] Ilya Sutskever: this is an interesting essay. Right. Now suppose you see an AI doing this and AI doing that.
[00:45:20] It is incomparable. Like basically I think that there is a big benefit from AI being in the public.
[00:45:29] Ilya Sutskever: And that would be a reason for us to not be quite straight-shot.
[00:45:35] Ilya Sutskever: Yeah. Well, I guess it's not even that, but I do think that is an important part of it.
[00:45:40] Dwarkesh Patel: The other big thing is I can't think of another discipline and human engineering and research where
[00:45:46] Dwarkesh Patel: the end artifact was made safer mostly through just thinking about how to make it safe as opposed to
[00:45:54] Dwarkesh Patel: why are airplane crashes per mile so much lower today than there were decades ago?
[00:45:59] Dwarkesh Patel: Why is it so much harder to find a bug in Linux than it would have been decades ago?
[00:46:04] Dwarkesh Patel: And I think it's mostly because these systems were deployed to the world, you noticed failures.
[00:46:09] Dwarkesh Patel: Those failures were corrected and the systems became more robust. Now, I'm not sure why
[00:46:15] Dwarkesh Patel: AI and superhuman intelligence would be any different, especially given. And I hope we're going to get to this.
[00:46:22] It seems like the harms of superintelligence are not just about having some malevolent
[00:46:28] Dwarkesh Patel: paperclip are out there, but it just like, this is a really powerful thing.
[00:46:32] Dwarkesh Patel: And we don't even know how to conceptualize how people interact with it, what people will do with it.
[00:46:36] Dwarkesh Patel: And having gradual access to it seems like a
[00:46:38] Dwarkesh Patel: better way to maybe spread out the impact of it and help people prefer for it.
[00:46:45] Dwarkesh Patel: Well, I think on this point, even in the stretch of scenario, you would still do a gradual release of it.
[00:46:54] It's how I would imagine it. The gradualism would be an inherent component of any plan.
[00:47:02] Ilya Sutskever: It's just a question of what is the first thing that you get out of the door? That's number one.
[00:47:06] Ilya Sutskever: Number two, I also think, you know, I believe you have advocated for
[00:47:11] Ilya Sutskever: continuing learning more than other people. And I actually think that this is an important
[00:47:17] Ilya Sutskever: and correct thing. And here is why. So one of the things, so I'll give you another example
[00:47:25] Ilya Sutskever: of how thinking, how language affects thinking. And in this case, it will be two words,
[00:47:32] two words that have shaped everyone's thinking I maintain.
[00:47:37] Ilya Sutskever: First word, AGI. Second word, pre-training. Let me explain.
[00:47:44] So the word, the term AGI, why does this term exist? It's a very particular term. Why does it exist?
[00:47:52] Ilya Sutskever: There's a reason. The reason that the term AGI exists is in my opinion, not so much because it's
[00:47:59] Ilya Sutskever: like a very important essential descriptor of some end state of intelligence.
[00:48:06] Ilya Sutskever: But because it is a reaction to a different term that existed in the term is narrow AGI.
[00:48:16] Ilya Sutskever: If you go back to ancient history of game plan AGI, of checker's AGI,
[00:48:22] Ilya Sutskever: chess AGI, computer games AGI, everyone would say, look at this narrow intelligence.
[00:48:27] Ilya Sutskever: Sure, the chess AGI can be a bit cuspere of, but it can't do anything else. It is so narrow
[00:48:32] Ilya Sutskever: artificial narrow intelligence. So in response, as a reaction to this, some people said,
[00:48:39] well, this is not good, it is so narrow. What we need is generally AGI. Generally AGI that can
[00:48:47] Ilya Sutskever: just do all the things. The second and that term just got a lot of traction. The second thing that
[00:48:57] got a lot of traction is pre-training. Specifically, the recipe of pre-training. I think the current,
[00:49:04] Ilya Sutskever: the way people do RL now is maybe undoing the conceptual imprint of pre-training, but pre-training
[00:49:13] Ilya Sutskever: had the property. You do more pre-training and the model gets better at everything, more or less
[00:49:19] Ilya Sutskever: uniformly. Generally AGI. Pre-training gives AGI. But the thing that happened with AGI and pre-training
[00:49:32] Ilya Sutskever: is that in some sense, the overshot the target. Because by the kind, if you think about the term AGI,
[00:49:40] you will realize, and especially in the context of pre-training, you will realize that a human being
[00:49:45] Ilya Sutskever: is not an AGI. Because a human being, yes, there is definitely a foundation of skills,
[00:49:53] a human being, a human being lacks a huge amount of knowledge. Instead, we rely on continued learning.
[00:50:03] We rely on continued learning. And so then when you think about, okay, so let's suppose that we
[00:50:07] Ilya Sutskever: achieve success and we produce a safe super intelligence. The question is, but how do you define it
[00:50:14] Ilya Sutskever: where on the curve of continued learning is going to be? I would produce like a super intelligent
[00:50:20] Ilya Sutskever: 15-year-old that's very eager to go and you say, okay, I'm going to, they don't know very much at all,
[00:50:25] Ilya Sutskever: the great student, very eager. You go and be a programmer, you go and be a doctor,
[00:50:31] go and learn. So you could imagine that the deployment itself will involve some kind of a learning
[00:50:36] Ilya Sutskever: trial and error period. It's a process. As opposed to you drop the finished thing.
[00:50:43] Okay, I see. So you're suggesting that the thing you're pointing out with super intelligence
[00:50:50] Dwarkesh Patel: is not some finished mind, which knows how to do every single job in the economy. Because the way
[00:51:00] Dwarkesh Patel: the original, I think, opening a charter or whatever defines AGI is like, it can do every single
[00:51:05] Dwarkesh Patel: job that every single thing a human can do. You're proposing instead a mind which can learn to
[00:51:12] Dwarkesh Patel: do every single job. Yes. And that is super intelligence. But once you have the learning algorithm,
[00:51:19] it gets deployed into the world the same way a human laborer or my joint in organization.
[00:51:24] Dwarkesh Patel: And it seems like one of these two things might happen, maybe neither of these happens. One,
[00:51:31] Dwarkesh Patel: this super efficient learning algorithm becomes super human, becomes as good as you, and potentially
[00:51:39] Dwarkesh Patel: even better at the task of ML research. And as a result, the algorithm itself becomes more
[00:51:46] Dwarkesh Patel: and more super human. The other is even if that doesn't happen, if you have a single model,
[00:51:51] Dwarkesh Patel: I mean, this is explicit your vision. If you have a single model, where instances of a model,
[00:51:56] Dwarkesh Patel: which are deployed through the economy, doing different jobs, learning how to do those jobs,
[00:52:00] Dwarkesh Patel: continually learning on the job, picking up all the skills that any human could pick up,
[00:52:05] Dwarkesh Patel: but actually picking them all up at the same time and then amalgamating the learnings.
[00:52:10] You basically have a model which functionally becomes super intelligent,
[00:52:14] even without any sort of recursive self-improvement in software. Because you now have one model
[00:52:20] Dwarkesh Patel: that can do every single job in the economy, and humans can't merge our minds in the same way.
[00:52:24] Dwarkesh Patel: And so do you expect some sort of intelligence explosion from broad deployment?
[00:52:28] Dwarkesh Patel: I think that it is likely that we will have rapid economic growth.
[00:52:35] I think the broad deployment, like there are two arguments you could make, which are conflicting.
[00:52:46] One is that look, if indeed you get, once indeed you get to a point where you have an AI that can
[00:52:54] Ilya Sutskever: learn to do things quickly and you have many of them, then they will then, they will be a strong
[00:53:04] Ilya Sutskever: force to deploy them in the economy unless there will be some kind of a regulation that stops it,
[00:53:10] Ilya Sutskever: which by the way there might be. But I think the idea of very rapid economic growth for some time,
[00:53:19] Ilya Sutskever: I think it's very possible from broad deployment. The other question is how rapid it's going to be.
[00:53:25] So I think this is hard to know because on the one hand you have this very efficient worker,
[00:53:30] Ilya Sutskever: on the other hand, the world is just really big and there's a lot of stuff.
[00:53:36] And that stuff moves at a different speed. But then on the other hand, now the AI could,
[00:53:41] Ilya Sutskever: so I think very rapid economic growth is possible. And we will see all kinds of things like
[00:53:47] different countries, different rules and the ones which have the familiar rules,
[00:53:50] Ilya Sutskever: the economic growth will be faster, hard to predict.
[00:53:54] Some people in our audience like to read the transcripts instead of listening to the episode.
[00:53:58] Dwarkesh Patel: And so we put a ton of effort into making the transcripts read like they are standalone essays.
[00:54:04] Dwarkesh Patel: The problem is that if you just transcribe a conversation verbatim using a speech-to-text model,
[00:54:09] Dwarkesh Patel: it'll be full of all kinds of fits and starts and confusing phrasing. We mentioned this problem
[00:54:14] Dwarkesh Patel: to label locks and they asked if they could take a staff. Working with them on this is probably the
[00:54:18] Dwarkesh Patel: reason that I'm most excited to recommend label box to people. It wasn't just, oh hey tell us what
[00:54:23] Dwarkesh Patel: kind of data you need and we'll go get it. They walked us through the entire process from
[00:54:27] Dwarkesh Patel: helping us identify what kind of data we needed in the first place to assembling a team of expert
[00:54:32] Dwarkesh Patel: aligners to generate it. Even after we got all the data back, label box stayed involved. They
[00:54:38] Dwarkesh Patel: helped us choose the right base model and set up Auto QA on the model's output so they could tweak
[00:54:43] Dwarkesh Patel: and refine it. And now we have a new transcribe or tool that we can use for all our episodes moving
[00:54:49] Dwarkesh Patel: forward. This is just one example of how label box meets their customers at the ideas level
[00:54:54] Dwarkesh Patel: and partners with them through their entire journey. If you want to learn more or if you want to
[00:54:58] Dwarkesh Patel: try out the transcribe or tool yourself, go to labelbox.com slash barcash.
[00:55:07] It seems to me that this is a very precarious situation to be in where
[00:55:13] Dwarkesh Patel: looking to limit we know that this should be possible because if you have something that is
[00:55:17] Dwarkesh Patel: as good as a human at learning but which can merge its brains, merge their different instances in
[00:55:23] Dwarkesh Patel: a way that humans can't merge. Already, this seems like a thing that should physically be possible.
[00:55:28] Dwarkesh Patel: Humans are possible, digital computers are possible. You just need both of those combined to produce
[00:55:32] Dwarkesh Patel: this thing. And it also seems like this kind of thing is extremely powerful and economic growth
[00:55:42] Dwarkesh Patel: is one way to put it. I mean, Dyson Spears is a lot of economic growth. But another way to put it
[00:55:46] Dwarkesh Patel: is just like you will have potentially a very short period of time because a human on the job,
[00:55:51] Dwarkesh Patel: and you're hired people to SSI, and six months they're like net productive probably, right? A
[00:55:56] Dwarkesh Patel: human learns really fast. And so this thing is becoming smarter and smarter very fast.
[00:56:00] Dwarkesh Patel: What is how do you think about making that go well? And why is SSI positioned to do that well?
[00:56:05] Dwarkesh Patel: What is SSI's plan there basically is what I'm trying to ask.
[00:56:07] Dwarkesh Patel: Yeah. So one of the ways in which my thinking has been changing,
[00:56:16] Ilya Sutskever: is that I now place more importance on AI being deployed incrementally and in advance.
[00:56:30] Ilya Sutskever: One very difficult thing about AI is that we are talking about systems that don't yet exist.
[00:56:40] And it's hard to imagine them. I think that one of the things that's happening,
[00:56:46] is that in practice, it's very hard to feel the AI. It's very hard to feel the AI.
[00:56:54] We can talk about it, but it's like talking about the long few, like imagine, like having a
[00:57:01] Ilya Sutskever: conversation about how is it like to be old when you're like old and frail and you can have
[00:57:08] Ilya Sutskever: a conversation, you can try to imagine it, but it's just hard and you come back to reality,
[00:57:13] Ilya Sutskever: well that's not the case. And I think that a lot of the issues around
[00:57:21] AGI and its future power stem from the fact that it's very difficult to imagine.
[00:57:30] Ilya Sutskever: Future AI is going to be different. It's going to be powerful. Indeed, the whole problem,
[00:57:37] Ilya Sutskever: what is the problem of AI and AGI? The whole problem is the power.
[00:57:43] The whole problem is the power. When the power is really big, what's going to happen?
[00:57:50] And one of the ways in which I've changed my mind over the past year and so that change of mind may
[00:57:58] Ilya Sutskever: back, may, I'll say, I'll hedge a little bit, may back propagate into the plans of our company,
[00:58:05] Ilya Sutskever: is that, so if it's hard to imagine, what do you do? You got to be showing the thing.
[00:58:14] Ilya Sutskever: You got to be showing the thing. And I maintain that. I think most people who work on AI
[00:58:20] Ilya Sutskever: also can't imagine it because it's too different from what people see on a day-to-day basis.
[00:58:28] I do maintain, here's something which I predict will happen. That's a prediction.
[00:58:34] I maintain that as AI becomes more powerful than people will change their behaviors.
[00:58:45] And we will see all kinds of unprecedented things which are not happening right now.
[00:58:51] And I'll give some examples. I think for better or worse, the frontier companies will play a
[00:58:58] Ilya Sutskever: very important role in what happens as will the government. And the kind of things that I think
[00:59:03] Ilya Sutskever: you'll see, which you see the beginnings of, companies that are fierce competitors starting
[00:59:11] Ilya Sutskever: collaborative to collaborate on AI safety. You may have seen Open AI and Anthropic
[00:59:18] Ilya Sutskever: doing a first small step, but that did not exist. That's actually something which I predicted in one
[00:59:24] Ilya Sutskever: of my talks about three years ago. That's such a thing will happen. I also maintain that as AI
[00:59:30] Ilya Sutskever: continues to become more visibly powerful, there will also be a desire from governments and the
[00:59:39] Ilya Sutskever: public to do something. And I think that this is a very important force of showing the AI. That's
[00:59:47] Ilya Sutskever: number one. Number two, okay, so then the AI has been built. What needs to be done.
[00:59:55] So one thing that I maintain that will happen is that right now people who are working on AI
[01:00:01] maintain that the AI doesn't feel powerful because of its mistakes. I do think that at some
[01:00:07] Ilya Sutskever: point the AI will start to feel powerful actually. And I think when that happens, we will see a big
[01:00:12] Ilya Sutskever: change in the way all AI companies approach safety. They'll become much more paranoid. I think I say
[01:00:22] Ilya Sutskever: this as a prediction that we will see happen. We'll see if I'm right. But I think this is something
[01:00:28] Ilya Sutskever: that will happen because they will see the AI becoming more powerful. Everything that's happening
[01:00:33] Ilya Sutskever: right now I maintain is because people look at today's AI and it's hard to imagine the future AI.
[01:00:41] And there is a third thing which needs to happen. And I think this is this and I'm talking about it
[01:00:47] Ilya Sutskever: in broader terms, not just from the perspective of SSI because you asked me about our company. But
[01:00:54] Ilya Sutskever: the question is, okay, so then what should what should the companies aspire to build? What should
[01:00:58] Dwarkesh Patel: they aspire to build? And there has been one big idea that actually everyone has been locked in
[01:01:05] Ilya Sutskever: locked into which is the self-improving AI. And why did it happen? Because there is fewer ideas
[01:01:13] Ilya Sutskever: than companies. But I maintain that there is something that's better to build. And I think that
[01:01:19] Ilya Sutskever: everyone will actually want that. It's like the AI that's robustly aligned to care about
[01:01:26] Ilya Sutskever: sentient life specifically. I think in particular it will be there's a case to be made that it will be
[01:01:33] Ilya Sutskever: easier to build an AI that cares about sentient life than an AI that cares about human life alone
[01:01:40] because the AI itself will be sentient. And if you think about things like mirror neurons and
[01:01:46] Ilya Sutskever: human empathy for animals, which is, you know, you might argue it's not big enough, but it exists.
[01:01:53] Ilya Sutskever: I think it's an emergent property from the fact that to be model others with the same circuit
[01:01:58] Ilya Sutskever: that we used to model ourselves because that's the most efficient thing to do.
[01:02:03] So even if you got an AI to hear about sentient beings, and it's not actually clear to me that
[01:02:09] Dwarkesh Patel: that's what you should try to do if you solve the alignment, it would still be the case that most
[01:02:14] Dwarkesh Patel: sentient beings will be AI's. There will be trillions, eventually quadrillions of AI's. Humans
[01:02:19] Dwarkesh Patel: will be a very small fraction of sentient beings. So it's not clear to me if the goal is some kind
[01:02:26] Dwarkesh Patel: of human control over this future civilization that this is the best criterion. It's true. I think that
[01:02:38] it's possible it's not the best criterion. I'll say two things. I think that, say number one,
[01:02:45] Ilya Sutskever: I think that if there, so I think that care for sentient life, I think there is merit to it. I think
[01:02:54] Ilya Sutskever: it should be considered. I think that it will be helpful if there was some kind of a short list
[01:03:02] Ilya Sutskever: of ideas that then the companies when they are in the situation could use. That's number two.
[01:03:10] Ilya Sutskever: Number three, I think it would be really material helpful if the power of the most powerful
[01:03:16] Ilya Sutskever: super intelligence was somehow capped because it would address a lot of these concerns.
[01:03:23] The question of how to do it, I'm not sure, but I think that would be material helpful when
[01:03:29] you're talking about really, really powerful systems. Before we continue the alignment discussion,
[01:03:35] Dwarkesh Patel: I want to double click on that. How much room is there at the top? How do you think about
[01:03:39] Dwarkesh Patel: super intelligence? Do you think, I mean, using this learning efficiency idea maybe is just extremely
[01:03:45] Dwarkesh Patel: fast at learning new skills or new knowledge? And does it just have a bigger pool of strategies? Is
[01:03:51] Dwarkesh Patel: there a single cohesive it in the center that's more powerful or bigger? And if so, do you imagine
[01:04:00] Dwarkesh Patel: that this will be sort of godlike in comparison to the rest of human civilization? Or does it just feel
[01:04:04] Dwarkesh Patel: like another agent or another cluster of agents? So this is an area of a different people
[01:04:10] Ilya Sutskever: of different intuitions. I think it will be very powerful for sure. I think that what I think is
[01:04:17] Ilya Sutskever: most likely to happen is that there will be multiple such AIs being created roughly at the same time.
[01:04:26] Ilya Sutskever: I think that if the cluster is big enough, like if the cluster is literally continent sized,
[01:04:36] that thing could be really powerful indeed. Right? If you literally have a continent sized cluster,
[01:04:42] like those, those AIs can be very powerful. And I like, all I can tell you is that if you're talking
[01:04:49] Ilya Sutskever: about extremely powerful AIs, like truly dramatically powerful, then yeah, it would be nice if they could
[01:04:55] Ilya Sutskever: be restrained in some ways or if there was some kind of an agreement or something.
[01:05:03] Because I think that if you are saying hey, like, if you really, like what is the concern of
[01:05:10] Ilya Sutskever: superintelligence? What is one way to explain the concern? If you imagine a system that is
[01:05:15] Ilya Sutskever: sufficiently powerful, like really sufficiently powerful, and you could say, okay, you need to
[01:05:21] Ilya Sutskever: do something sensible, like care for sentient life, let's say, in a very single-minded way.
[01:05:27] We might not like the results. That's really what it is. And so maybe by the way, the answer is that
[01:05:31] Ilya Sutskever: you do not build a single, you do not build an RL agent in the usual sense. And actually, I'll point
[01:05:37] Ilya Sutskever: several things out. I think human beings are a semi-relagient, you know, the pursue a reward,
[01:05:44] Ilya Sutskever: and then the emotions or whatever make a styrofo of the reward we pursue a different reward.
[01:05:49] Ilya Sutskever: The market is like a very short-sighted kind of agent. Evolution is the same. Evolution is very
[01:05:58] Ilya Sutskever: intelligent in some ways, but very dumb in other ways. The government has been designed to be
[01:06:04] Ilya Sutskever: an ever-ending fight between three parts, which has an effect. So I think things like this,
[01:06:11] another thing that makes this discussion difficult is that we are talking about systems that
[01:06:15] Ilya Sutskever: don't exist, that we don't know how to build. That's the other thing. And that's actually my belief.
[01:06:21] Ilya Sutskever: I think what people are doing right now will go some distance and then peter out. It will continue
[01:06:26] Ilya Sutskever: to improve, but it will also not be it. So the it, we don't know how to build. And I think that a lot
[01:06:34] Ilya Sutskever: a lot hinges on understanding reliable generalization. Now, say another thing, which is like,
[01:06:45] you know, one of the things that you could say is that it's your ability to learn human
[01:06:54] Ilya Sutskever: values is fragile, then your ability to optimize them is fragile, you will actually learn to optimize
[01:06:59] Ilya Sutskever: them. And then you can't you say, are these not all instances of unreliable generalization?
[01:07:06] Why is it that human beings appear to generalize so much better? What generalization was much
[01:07:11] Ilya Sutskever: better? What would happen in this case? What would be the effect? But those we can't, we can't,
[01:07:15] Ilya Sutskever: like those questions are right now still answerable. How does one think about what AI going well
[01:07:23] Dwarkesh Patel: looks like? Because I think you've scoped out how AI might evolve. We'll have these sort of
[01:07:27] Dwarkesh Patel: continual learning agents. AI will be very powerful. Maybe they will be many different
[01:07:32] Dwarkesh Patel: AI's. How do you think about lots of continent compute size intelligences going around? How
[01:07:40] Dwarkesh Patel: dangerous is that? How do we make that less dangerous? And how do we do that in a way that
[01:07:48] protects a equilibrium where there might be misaligned AI's out there and bad actors out there?
[01:07:56] So one reason why I liked the AI that cares for sentient life, and we can debate on whether it's
[01:08:01] Ilya Sutskever: good or bad. But if the first end of these dramatic systems actually do care for, you know,
[01:08:14] love humanity or something, you know, care for sentient life, obviously this also needs to be
[01:08:18] Ilya Sutskever: achieved. This needs to be achieved. So if this is achieved by the first end of those systems,
[01:08:27] then I can see it go well, at least for quite some time. And then there is the question of what
[01:08:34] Ilya Sutskever: happens in the long run? What happens in the long run? How do you achieve a long run equilibrium?
[01:08:39] And I think that there, the reason answer as well, and I don't like this answer,
[01:08:46] Ilya Sutskever: but it needs to be considered. In the long run, you might say, okay, so if you have a world where
[01:08:54] Ilya Sutskever: powerfully has exist, in the short term, you could say, okay, you have universal high income,
[01:09:01] you have universal high income, and we all do it well. But we know that what do the Buddhists say?
[01:09:07] Ilya Sutskever: Change is the only constant. And so things change, and there is some kind of government,
[01:09:12] Ilya Sutskever: political structure thing, and it changes because these things have a shelf life.
[01:09:18] You know, some new government thing comes up and it functions and then after some time,
[01:09:22] Ilya Sutskever: it stops functioning. That's something that we see happening all the time. And so I think that for
[01:09:28] Ilya Sutskever: the long run equilibrium, one approach, you could say, okay, so maybe every person will have
[01:09:36] Ilya Sutskever: an AI that will do their bidding. And that's good. And if that could be maintained indefinitely,
[01:09:42] Ilya Sutskever: that's true. But the downside with that is, okay, so then the AI goes and like earns money for
[01:09:50] Ilya Sutskever: for the person and you know, advocates for their needs in like the political sphere,
[01:09:55] Ilya Sutskever: and maybe then writes a little report saying, okay, here's what I've done here is the situation,
[01:09:59] Ilya Sutskever: and the person says, great, keep it up. But the person is no longer a participant.
[01:10:04] Ilya Sutskever: And then you can say that's a precarious place to be in. But so I'm going to preface by saying,
[01:10:13] I don't like this solution, but it is a solution. And the solution is if people become part AI
[01:10:21] Ilya Sutskever: with some kind of neural link plus plus, because what will happen as a result is that now the AI
[01:10:26] Ilya Sutskever: understands something and we understand it too. Because now the understanding is transmitted wholesale.
[01:10:34] Ilya Sutskever: So now if the AI is in some situation, now it's like you are involved in the situation yourself fully.
[01:10:41] And I think this is the answer to the equilibrium. I wonder if the fact that emotions which were
[01:10:49] Dwarkesh Patel: developed millions or in many cases billions of years ago in a totally different environment
[01:10:55] Dwarkesh Patel: are still guiding our actions so strongly is an example of alignment success
[01:11:03] Dwarkesh Patel: to maybe spell out what I mean. The brainstem has these, I don't know if it's more accurate to call
[01:11:11] Dwarkesh Patel: it a value function or reward function, but the brainstem has a directive where it's saying,
[01:11:15] Dwarkesh Patel: mate with somebody who's more successful. The cortex is the part that understands what does
[01:11:19] Dwarkesh Patel: success mean in the modern context. But the brainstem is able to align the cortex and say, however you
[01:11:26] Dwarkesh Patel: recognize success to be, and I'm not smarter than to understand what that is, you're still going to
[01:11:30] Dwarkesh Patel: pursue this directive. I think there is, so I think there's a more general point. I think it's
[01:11:36] Ilya Sutskever: actually really mysterious how the brain encodes high level desires, sorry, how evolution encodes
[01:11:45] Ilya Sutskever: high level desires. Like it's pretty easy to understand how evolution would endow us with the
[01:11:51] Ilya Sutskever: desire for food that smells good. Because smell is a chemical. And so just pursue that chemical. It's
[01:11:58] Ilya Sutskever: very easy to imagine such evolution doing such a thing. But evolution also has endowed us with all
[01:12:06] Ilya Sutskever: these social desires. Like we really care about being seen positively by society, we care about
[01:12:13] Ilya Sutskever: being in a good standing, we like all these social intuitions that we have, I feel strongly that
[01:12:20] Ilya Sutskever: they're baked in. And I don't know how evolution did it because it's a high level concept that's
[01:12:27] Ilya Sutskever: represented in the brain. Like what people think, like let's say you were like, you care about
[01:12:34] some social thing. It's not like a low level signal like smell. It's not something that
[01:12:42] Ilya Sutskever: for which there is a sensor. Like the brain needs to do a lot of processing to piece to get
[01:12:46] Ilya Sutskever: lots of bits of information to understand what's going on socially. And somehow evolution said,
[01:12:52] Ilya Sutskever: that's what you should care about. How did it do it? And it did it quickly too. Because I think all
[01:12:58] Ilya Sutskever: these sophisticated social things that we care about, I think they evolved pretty recently. So
[01:13:04] Ilya Sutskever: evolution had an easy time, hard-coding this high level desire. And I maintain, or at least I'll say
[01:13:11] Ilya Sutskever: I'm unaware of good hypothesis for how it's done. I had some ideas I was kicking around, but none of them,
[01:13:19] Ilya Sutskever: are satisfying. Yeah. And what's especially impressive is it was a desire that you learned in your
[01:13:27] Dwarkesh Patel: lifetime. It kind of makes sense because your brain is intelligent. It makes sense why we would
[01:13:32] Dwarkesh Patel: learn intelligent desires. But your point is that the desire is, maybe this is not your point,
[01:13:38] Dwarkesh Patel: but one way to understand it is, the desire is built into the genome. And the genome is not
[01:13:43] Dwarkesh Patel: intelligent, right? But it's able to, you're somehow able to describe this feature that requires,
[01:13:48] Dwarkesh Patel: like it's not even clear how you define that feature. And you can get it into the, you can build it
[01:13:52] Dwarkesh Patel: into the genes. Yeah, essentially. Or maybe I'll put it differently. If you think about the tools
[01:13:57] Ilya Sutskever: that are available to the genome, it says, okay, here's a recipe for building a brain. And you
[01:14:03] Ilya Sutskever: could say, here is a recipe for connecting the dopamine neurons to like the smell sensor. Yeah.
[01:14:08] Ilya Sutskever: And if the smell is a certain kind of, you know, good smell, you want to eat that. I could imagine
[01:14:13] Ilya Sutskever: the genome doing that. I'm claiming that it is harder to imagine. It's harder to imagine the
[01:14:20] Ilya Sutskever: genome saying you should care about some complicated computation that your entire brain, that like a
[01:14:26] Ilya Sutskever: big chunk of your brain does. That's all I'm claiming. I can tell you like a speculation, I was
[01:14:32] Ilya Sutskever: wondering how it could be done. And let me offer a speculation and I'll explain why the speculation
[01:14:36] Ilya Sutskever: is probably false. So the speculation is, okay, so the brain, it's like the brain has those regions,
[01:14:45] Ilya Sutskever: you know, the brain regions. We have our cortex, right? And it has all those brain regions.
[01:14:51] Ilya Sutskever: And the cortex is uniform, but the brain regions and the neurons in the cortex, they kind of
[01:14:57] Ilya Sutskever: speak to their neighbors mostly. And that's explained by your brain regions. Because if you want to
[01:15:01] do some kind of speech processing, all the neurons that do speech need to talk to each other. And because
[01:15:07] Ilya Sutskever: neurons can only speak to their nearby neighbors for the most part, it has to be a region. All the
[01:15:11] regions are mostly located in the same place from person to person. So maybe evolution hard-coded
[01:15:16] Ilya Sutskever: literally a location on the brain. So it says, oh, like when, when like, you know, the GPS of the
[01:15:25] Ilya Sutskever: brain, GPS coordinates such and such, when that fire is, that's what you should care about. Like,
[01:15:28] Ilya Sutskever: maybe that's what evolution did because that would be within the toolkit of evolution.
[01:15:33] Yeah, although there are examples where, for example, people who are born blind have that
[01:15:38] Dwarkesh Patel: area of their cortex adopted by another sense. And I have no idea, but I'd be surprised if
[01:15:48] Dwarkesh Patel: the desires or the reward functions which require a visual signal no longer worked, you know,
[01:15:56] Dwarkesh Patel: people who have their different areas of their cortex co-opted. For example, if you no longer have
[01:16:01] Dwarkesh Patel: vision, can you still feel the sense that I want people around me to like me and so forth?
[01:16:07] Dwarkesh Patel: Which usually there's also visual cues for. So actually fully agree with that. I think there's
[01:16:11] Ilya Sutskever: an even stronger contra-argument of this theory, which is, like if you think about people, so there are
[01:16:17] Ilya Sutskever: people who get half of their brain removed in childhood. And they still have all their brain
[01:16:24] Ilya Sutskever: regions, but they all somehow move to just one hemisphere, which suggests that the brain regions,
[01:16:30] the location is not fixed. And so that theory is not true. It would have been cool if it was true,
[01:16:35] Ilya Sutskever: but it's not. And so I think that's a mystery, but it's an interesting mystery. Like, the fact is,
[01:16:39] Ilya Sutskever: somehow evolution was able to endow us to care about social stuff very, very reliably. And even
[01:16:47] Ilya Sutskever: people who have like all kinds of strange mental conditions and deficiencies and emotional problems
[01:16:52] Ilya Sutskever: tend to care about this also. AI tools like defects, voice clones, and agents have dramatically
[01:16:59] Dwarkesh Patel: increased the sophistication of fraud and abuse. So it's more important than ever to actually understand
[01:17:05] Dwarkesh Patel: the identity and intent of whoever or whatever is using your platform. That's exactly what Sardine
[01:17:12] Dwarkesh Patel: helps you do. Sardine brings together thousands of device behavior and identity signals to help
[01:17:18] Dwarkesh Patel: you assess risk. Everything from how a user types or moves their mouse or holds their device
[01:17:24] Dwarkesh Patel: to whether they're hiding their true location behind the VPN to whether they're injecting a fake
[01:17:30] Dwarkesh Patel: camera feed during KYC selfie checks. Sardine combines these signals with insights from their
[01:17:36] Dwarkesh Patel: network of almost four billion devices, things like a user's history of fraud or their associations
[01:17:41] Dwarkesh Patel: with other high risk accounts. So you can spot bad actors before they do damage. This would literally
[01:17:48] Dwarkesh Patel: be impossible if you only use data from your own application. Sardine doesn't stop a detection.
[01:17:53] Dwarkesh Patel: They offer suite of agents to streamline onboarding checks and automated investigations.
[01:17:58] Dwarkesh Patel: So as fraudsters use AI to scale their attacks, you can use AI to scale your defenses. Go to
[01:18:05] Dwarkesh Patel: sardine.ai slash twerkesh to learn more and download their guide on AI fraud detection.
[01:18:13] What is SSI planning on doing differently?
[01:18:16] Dwarkesh Patel: So presumably your plan is to be one of the frontier companies when this time arrives.
[01:18:21] And then what is presumably you started SSI because you're like I think I have a way of approaching
[01:18:28] Dwarkesh Patel: how to do this safely in a way that the other companies don't. What is that difference?
[01:18:34] So the way I would describe it as there are some ideas that I think are promising and I want to
[01:18:40] Ilya Sutskever: investigate them and see if they are indeed promising or not. It's really that simple.
[01:18:45] Ilya Sutskever: It's an attempt. I think that if the ideas are now to be correct, these ideas that we discussed
[01:18:51] Ilya Sutskever: around understanding generalization. If these ideas turn out to be correct,
[01:19:00] then I think we will have something worthy. We'll leave it turn out to be correct. We are doing
[01:19:05] Ilya Sutskever: research. We are squarely age of research company. We are making progress. We've actually made
[01:19:10] Ilya Sutskever: quite good progress over the past year, but we need to keep making more progress. Yeah.
[01:19:14] Ilya Sutskever: More research. And that's how I see it. I see it as an attempt to be an attempt to be a voice and
[01:19:24] Ilya Sutskever: a participant. People have asked your co-founder and previously left to go to meta recently
[01:19:35] Dwarkesh Patel: and people have asked, well, if there was a lot of breakthroughs being made, that seems like a
[01:19:40] Dwarkesh Patel: thing that should have been unlikely. I wonder how you respond. Yeah. So for this, I will simply
[01:19:46] Ilya Sutskever: remind a few facts that may have been forgotten. And I think these facts which provide the
[01:19:52] Ilya Sutskever: context, I think they explain the situation. So the context was that we were fundraising at a 32-billion
[01:19:59] Ilya Sutskever: valuation. And then meta came in and offered to acquire us. And I said no, but my former co-founder,
[01:20:13] I can some sense, said yes. And as a result, he also was able to enjoy from a lot of near-charm
[01:20:20] Ilya Sutskever: liquidity. And he was the only person from SSI to join meta. It sounds like SSI's plan has to be
[01:20:26] Dwarkesh Patel: a company that is at the frontier when you get to this very important period in human history
[01:20:33] Dwarkesh Patel: where you have super human intelligence. And you have these ideas about how to make super human
[01:20:37] Dwarkesh Patel: intelligence go well. But other companies will be trying their own ideas. What distinguishes
[01:20:44] Dwarkesh Patel: SSI's approach to making super intelligence go well? The main thing that distinguishes SSI
[01:20:51] Ilya Sutskever: is its technical approach. So we have a different technical approach that I think is worthy.
[01:20:59] And we are pursuing it. I maintain that in the end, there will be a convergence of strategies.
[01:21:05] Ilya Sutskever: So I think there will be a convergence of strategies where at some point, as AI becomes more powerful,
[01:21:14] it's going to become more or less clearer to everyone what the strategy should be. And it should
[01:21:19] be something like, yeah, you need to find some way to talk to each other. And you want your first
[01:21:26] actual, like real super intelligent AI to be aligned and somehow be
[01:21:35] you know, careful sentient life, careful people, democratic, one of those, some combination of
[01:21:42] Ilya Sutskever: and I think this is the condition that everyone should strive for. And that's what SSI's striving for.
[01:21:52] And I think that this time, if not already, all the other companies will realize that they're
[01:21:58] Ilya Sutskever: striving towards the same thing. And we'll see, I think that the world will truly change as
[01:22:02] Ilya Sutskever: AI becomes more powerful. And I think a lot of these forecasts will, like, I think things will be
[01:22:08] Ilya Sutskever: really different. And people will be acting really differently. What is speaking of forecast?
[01:22:12] What are your forecasts to this system you're describing, which can learn as well as a human? And
[01:22:20] Dwarkesh Patel: it's absolutely as a result becomes super human. I think like, five to 20 years. So I just want to
[01:22:29] Dwarkesh Patel: unroll your how you might see the world coming. It's like, we have a couple more years where
[01:22:35] Dwarkesh Patel: these other companies are continuing the current approach and it stalls out and stalls out here
[01:22:40] Dwarkesh Patel: meaning they earn no more than low hundreds of billions in revenue. How do you think about what
[01:22:45] Dwarkesh Patel: stalling all means? Yeah. I think they're, I think it could, I think it could stall out and
[01:22:53] I think stalling out will look like it will all look very similar. Yeah. Among all the different
[01:22:59] Ilya Sutskever: companies, something like this. I'm not sure because I think, I think, I think even with, I think,
[01:23:04] Ilya Sutskever: I think even, I think even with stalling out, I think this company's could make a stupendous,
[01:23:08] Ilya Sutskever: stupendous revenue, maybe not profits because they will be, it will be, they will need to work hard
[01:23:14] Ilya Sutskever: to differentiate each other from themselves, but revenue definitely. But there's something in your
[01:23:20] Dwarkesh Patel: model implies that when the correct solution does emerge, there will be convergence between all the
[01:23:26] Dwarkesh Patel: companies. And I'm curious why you think that's the case. Well, I was talking more about convergence
[01:23:31] Dwarkesh Patel: on their largest strategies. I think eventual convergence on the technical approach is probably
[01:23:35] Ilya Sutskever: going to happen as well. But I was alluding to convergence to their largest strategies. So what,
[01:23:40] Ilya Sutskever: what exactly is the thing that should be done? I just want to better understand how you see the
[01:23:45] Dwarkesh Patel: future and rolling. So currently we have these different companies and you expect their approach to
[01:23:49] Dwarkesh Patel: continue generating revenue. Yes. But not get to this human-like learner. Yes. So now we have these
[01:23:54] Dwarkesh Patel: different forks of companies. We have you, we have thinking machines, there's a bunch of other labs.
[01:23:59] Dwarkesh Patel: Yes. And maybe one of them figures out the correct approach. But then the release of the product
[01:24:04] Dwarkesh Patel: makes it clear to other people how to do this thing. I think it won't be clear how to do it thing,
[01:24:10] Ilya Sutskever: but it can be clear that something different is possible. Right. And that is information.
[01:24:14] Ilya Sutskever: And I think people will do, then be trying to figure out how that's how that works. I do think,
[01:24:20] Ilya Sutskever: though, that one of the things that's that I think, you know, not addressed here, you know,
[01:24:27] Ilya Sutskever: discussed is that with each increase in the AI capabilities, I think there will be some kind of
[01:24:35] Ilya Sutskever: changes, but I don't know exactly which ones in how things are being done. And so like,
[01:24:42] Ilya Sutskever: I think it's going to be important yet I can't spell out what that is exactly. And how, how are the,
[01:24:50] by default, you would expect the company that has the model company that has that model to be
[01:24:54] Dwarkesh Patel: getting all these gains because they have the model that is learning how to do all,
[01:24:58] Dwarkesh Patel: has the skills and knowledge that it's building up in the world. What is the reason to think that
[01:25:03] Dwarkesh Patel: the benefits of that will be widely distributed and not just end up at whatever model company gets
[01:25:08] Dwarkesh Patel: this continuous learning loop going first? Like, I think that empirically what happened. So here,
[01:25:14] here is what I think is going to happen. Number one, I think empirically when let's, let's look at
[01:25:24] Ilya Sutskever: how things have gone so far with the AI's of the past. So one company produced an advance
[01:25:31] and the other company scrambled and produced some, some similar things after some a lot of time
[01:25:39] Ilya Sutskever: and they started to compete in the market and push their, push the prices down. And so I think
[01:25:46] Ilya Sutskever: from the market perspective, I think something similar will happen there as well, even if someone,
[01:25:51] Ilya Sutskever: it's okay, we are talking about the good world, by the way, where, what's the good world?
[01:25:58] Ilya Sutskever: What's the good world? Where we have these powerful human-like learners that are also like,
[01:26:07] Ilya Sutskever: and by the way, maybe there's another thing we haven't discussed on the, on the spec of the
[01:26:12] Ilya Sutskever: super intelligent AI that I think is worth considering is that you make it narrow,
[01:26:18] Ilya Sutskever: it can be useful in narrow at the same time. So you can have lots of narrow super intelligent AI's.
[01:26:23] Ilya Sutskever: But suppose you have many of them and you have some company that's producing a lot of profits
[01:26:33] Ilya Sutskever: from it and then you have another company that comes in and starts to compete and the
[01:26:38] Ilya Sutskever: competitor competition is going to work through specialization. I think what's going to happen is
[01:26:44] Ilya Sutskever: that the way competition, like competition, loves specialization and you see it in the market,
[01:26:52] Ilya Sutskever: you see it in evolution as well. So you're going to have lots of different niches and you're
[01:26:55] Ilya Sutskever: going to have lots of different companies who are occupying different niches in this kind of
[01:27:02] world. We would say, yeah, one AI company is really quite a bit better at some area of really
[01:27:09] Ilya Sutskever: complicated economic activity and a different company is better at another area and the third
[01:27:13] Ilya Sutskever: company is really good at litigation. So it's not really done by what human learning implies,
[01:27:18] Dwarkesh Patel: is that like it can learn. It can, but, but you have accumulated learning, you have a big investment,
[01:27:24] Ilya Sutskever: you spent a lot of compute to become really, really, really good, really phenomenal of this thing.
[01:27:30] Ilya Sutskever: And someone else spent a huge amount of compute and a huge amount of experience to get really,
[01:27:34] Ilya Sutskever: really good at some other thing. Right. You apply a lot of human learning to get there, but now,
[01:27:38] Ilya Sutskever: like you are at this high point where someone else would say, look, like, I don't want to start
[01:27:44] Ilya Sutskever: learning what you've learned. That would require many different companies to begin at the human,
[01:27:49] Dwarkesh Patel: like, continue learning agent at the same time so that they can start their different research
[01:27:55] Dwarkesh Patel: in different branches. But if one company, you know, gets that agent first or gets that learner first,
[01:28:04] it does then seem like, well, you know, they, like, if you just think about every single job in the
[01:28:10] Dwarkesh Patel: economy, you just have an instance learning each one seems tractable for companies. Yeah. That's
[01:28:17] Dwarkesh Patel: that's that's that's a valid argument. My my strong intuition is that it's not how it's going to go.
[01:28:24] My strong intuition is that yeah, like the argument says it will go this way. Yeah.
[01:28:28] Ilya Sutskever: But my strong intuition is that it will not go this way. That this is the
[01:28:33] you know, in in theory, there is no difference between theory and practice and practice
[01:28:37] Ilya Sutskever: the reason. I think that's going to be one of those. A lot of people's models of recursive self
[01:28:41] Dwarkesh Patel: improvement, literally explicitly state, we will have a million Ilias in a server that are coming
[01:28:47] Dwarkesh Patel: up with different ideas. And this will lead to a super intelligence emerging very fast. Do you
[01:28:51] Dwarkesh Patel: have some intuition about how parallelizable the thing you are doing is? How how how what are the
[01:28:57] Dwarkesh Patel: gains from making copies of alia? I don't know. I think I think they'll definitely be a
[01:29:06] Ilya Sutskever: there'll be diminution returns because you want you want people who think differently rather than
[01:29:10] Ilya Sutskever: the same. I think that if they were literal copies of me, I'm not sure how much more incremental
[01:29:15] Ilya Sutskever: value you'd get. I think that but people who think differently, that's what you want. Why is it
[01:29:23] Dwarkesh Patel: that it's been if you look at different models, even released by totally different companies trained
[01:29:28] Dwarkesh Patel: on potentially non overlapping data sets? It's actually crazy how similar LLMs are to each other.
[01:29:35] Dwarkesh Patel: Maybe the data system is not as non overlapping as it seems. But there's there's some sense that
[01:29:41] Dwarkesh Patel: it's like even if an individual human might be less productive than the future AI, maybe there's
[01:29:45] Dwarkesh Patel: something to the fact that human teams have more diversity than teams of the eyes might have. But
[01:29:49] Dwarkesh Patel: how do we elicit meaningful diversity among AI? So I think just raising that temperature just
[01:29:55] Dwarkesh Patel: to result in gibberish. I think you want something more like different scientists have different
[01:29:59] Dwarkesh Patel: different prejudices or different ideas. How do you get that kind of diversity among AI agents?
[01:30:04] Dwarkesh Patel: So the reason there has been no diversity, I believe, is because of pre-training.
[01:30:10] All the pre-trained models are the same. Pretty much because the pre-trained on the same data.
[01:30:16] Ilya Sutskever: Now, our rail and post-training is where some differentiation starts to emerge because different
[01:30:21] Ilya Sutskever: people come up with different RL training. Yeah. And then I've heard you hint in the past about
[01:30:28] Dwarkesh Patel: self-play as a way to either get data or match agents to other agent equivalent intelligence
[01:30:35] Dwarkesh Patel: to kickoff learning. How should we think about why there's no public proposals of this kind of
[01:30:45] Dwarkesh Patel: thinking working with other ones? I would say there are two things to say. I would say that the
[01:30:50] Ilya Sutskever: reason why I thought self-play was interesting is because it offered the way to create models
[01:30:57] Ilya Sutskever: using compute only without data. Right? And if you think that data is the ultimate bottleneck,
[01:31:03] Ilya Sutskever: then using compute only is very interesting. So that's what makes it interesting. Now, the
[01:31:09] Ilya Sutskever: the thing is that self-play, at least the way it was done in the past, when you have agents,
[01:31:18] Ilya Sutskever: which are somehow compete with each other, it's only good for developing a certain set of skills.
[01:31:23] Ilya Sutskever: It is too narrow. It's only good for negotiation, conflict, certain social skills,
[01:31:32] Ilya Sutskever: strategizing that kind of stuff. And so if you care about those skills, then self-play will be useful.
[01:31:38] Ilya Sutskever: Now, actually, I think that self-play did find a home, but just in a different form.
[01:31:46] Ilya Sutskever: In a different form. So things like debate, prove a very fire. You have some kind of an LLM as a
[01:31:54] Ilya Sutskever: judge, which is also incentivized to find mistakes in your work. You could say this is not exactly
[01:31:59] Ilya Sutskever: self-play, but this is a related adversarial setup that people are doing, I believe. And really
[01:32:04] Ilya Sutskever: self-lays an example of a special case of more general like competition between agents.
[01:32:12] Ilya Sutskever: The response, the natural response, the competition is to try to be different. And so if you were to
[01:32:17] Ilya Sutskever: put multiple agents and you tell them, you know, you all need to work on some problem, and you are an
[01:32:22] agent, and you're inspecting what everyone else is working, you're going to say, well, if they
[01:32:27] Ilya Sutskever: already taken this approach, it's not clear I should pursue it. I should pursue something
[01:32:32] Ilya Sutskever: differentiated. And so I think that something like this could also create an incentive for
[01:32:37] Ilya Sutskever: a diversity of approaches. Yeah. Final question. What is research taste? You're obviously
[01:32:46] the person in the world who is considered to have the best taste in doing research in AI. You were
[01:32:55] Dwarkesh Patel: the co-author on many of the biggest, the biggest things that have happened in the history of
[01:33:02] Dwarkesh Patel: deep learning from Alex and that to GPT-3 and so on. What is it that how do you characterize how
[01:33:09] you come up with these ideas? I can answer. So I can comment on this for myself. I think
[01:33:14] Ilya Sutskever: different people do it differently. But one thing that guides me personally is
[01:33:24] an aesthetic of how AI should be by thinking about how people are, but thinking correctly.
[01:33:32] Ilya Sutskever: Like it's very easy to think about how people are incorrectly. But what does it mean to think
[01:33:37] Ilya Sutskever: about people correctly? So I'll give you some examples. The idea of the artificial neuron
[01:33:45] is directly inspired by the brain and it's a great idea. Why? Because you say, sure, the brain has
[01:33:50] Ilya Sutskever: all these different organs. It has the faults, but the faults probably don't matter. Why do we
[01:33:55] Ilya Sutskever: think that the neurons matter? Because there is many of them. It kind of feels right? So you want
[01:34:00] Ilya Sutskever: the neuron. You want some kind of local learning rule that will change the connections. You want
[01:34:04] Ilya Sutskever: some local learning rule that will change the connections between the neurons. It feels plausible
[01:34:10] Ilya Sutskever: that the brain does it. The idea of the distributed representation. The idea that the brain,
[01:34:16] Ilya Sutskever: the brain responds to experience. The neuron that should learn from experience. The brain learns from
[01:34:22] Ilya Sutskever: experience. The neuron that you learn from experience. And you kind of ask yourself, is something
[01:34:28] Ilya Sutskever: fundamental or not fundamental? How things should be? And I think that's been guiding me a
[01:34:34] Ilya Sutskever: fair bit, kind of thinking from multiple angles and looking for almost beauty, beauty, simplicity,
[01:34:41] Ilya Sutskever: ugliness. There's no room for ugliness. It's just beauty, simplicity, elegance, correct
[01:34:46] Ilya Sutskever: inspiration from the brain. And all of those things need to be present at the same time. And the more
[01:34:51] Ilya Sutskever: they are present, the more confident you can be in a top-down belief. And then the top-down belief
[01:34:57] Ilya Sutskever: is the thing that sustains you when the experiments contradict you. Because if you just trust the data
[01:35:03] Ilya Sutskever: all the time, well, sometimes you can be doing a correct thing, but there's a bug. But you don't
[01:35:07] know the reason about. How can you tell that there is a bug? How do you know if you should keep
[01:35:11] Ilya Sutskever: debugging or you conclude it's the wrong direction? Well, is the top-down? Well, how should you
[01:35:16] Ilya Sutskever: can say the things have to be this way? Something like this has to work. Therefore, we gotta keep going.
[01:35:22] Ilya Sutskever: That's the top-down. And it's based on this like multifaceted beauty and inspiration by the brain.
[01:35:29] All right. We'll leave it there. Thank you so much. Thank you so much.
[01:35:33] Oh! All right. Appreciate it. That was great. Yeah. I enjoyed it. Yes. Me too. Hey, everybody.
[01:35:39] I hope you enjoyed that episode. If you did, the most helpful thing you can do is just share it with
[01:35:44] Dwarkesh Patel: other people who you think might enjoy it. It's also helpful if you leave a rating or comment on
[01:35:50] Dwarkesh Patel: whatever platform you're listening on. If you're interested in sponsoring the podcast,
[01:35:54] Dwarkesh Patel: you can reach out at twerkash.com slash advertise. Otherwise, I'll see you on the next one.