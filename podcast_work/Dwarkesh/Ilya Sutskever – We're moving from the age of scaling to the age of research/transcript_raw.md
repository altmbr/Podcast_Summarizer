# Ilya Sutskever â€“ We're moving from the age of scaling to the age of research

**Podcast:** Dwarkesh
**Date:** 2025-11-25
**Video ID:** aR20FWCCjAs
**Video URL:** https://www.youtube.com/watch?v=aR20FWCCjAs

---

[00:00:00] You know, it's crazy that all of this is real.
[00:00:04] Yeah, meaning, like all this AI stuff and all this Bay Area, yeah, that it's happened.
[00:00:10] SPEAKER_01: Like, isn't it straight out of science fiction?
[00:00:13] SPEAKER_01: Yeah, another thing that's crazy is like how normal this low tape off feels.
[00:00:18] SPEAKER_00: The idea that we'd be investing 1% of Gigi in AI, like I feel like it felt like a bigger deal,
[00:00:25] SPEAKER_00: you know, but right now it just feels like we get used to things free faster and so out, yeah.
[00:00:29] SPEAKER_01: But also it's kind of like it's abstract like what does it mean?
[00:00:33] What it means that you see it in the news, yeah, that such and such company announced such and such dollar amount.
[00:00:38] SPEAKER_01: Right. That's that's all you see.
[00:00:41] SPEAKER_01: Right. It's not really felt in any other way so far.
[00:00:45] SPEAKER_01: Yeah, should we actually begin here? I think this is an interesting discussion.
[00:00:47] SPEAKER_00: Sure. I think your point about well from the average person's point of view,
[00:00:53] nothing is that different. It will continue being true even into the singularity.
[00:00:57] SPEAKER_00: No, I don't think so. Okay, interesting.
[00:00:59] SPEAKER_00: So the thing which I was referring to not feeling different is okay.
[00:01:06] SPEAKER_01: So such and such company announced some difficult to comprehend dollar amount of investment.
[00:01:12] SPEAKER_01: Right. I don't think anyone knows what to do with that.
[00:01:14] SPEAKER_01: Yeah.
[00:01:15] SPEAKER_00: But I think that the impact of AI is going to be felt.
[00:01:21] The AI is going to be diffused through the economy.
[00:01:23] SPEAKER_01: There is a very strong economic force for this.
[00:01:27] And I think the impact is going to be felt very strongly.
[00:01:30] When do you expect that impact? I think the models seem smarter than their economic impact would
[00:01:36] SPEAKER_00: imply. Yeah, this is one of the very confusing things about the models right now.
[00:01:44] SPEAKER_01: How to reconcile the fact that they are doing so well on eVALs.
[00:01:52] SPEAKER_01: And you look at the eVALs and you go those are pretty hard eVALs.
[00:01:56] SPEAKER_01: They are doing so well.
[00:01:59] But the economic impact seems to be dramatically behind.
[00:02:04] SPEAKER_01: And it's almost like it's very difficult to make sense of how can the model on the one hand
[00:02:12] SPEAKER_01: do these amazing things. And on the other hand, like repeat itself twice in some situation in a
[00:02:19] SPEAKER_01: kind of an example would be let's say you use vibe coding to do something.
[00:02:24] And you go to some place and then you get a bug.
[00:02:26] SPEAKER_01: And then you tell the model can you please fix the bug?
[00:02:30] And the model says oh my god, you're so right. I have a bug. Let me go fix that.
[00:02:34] SPEAKER_01: And it reduces the second bug.
[00:02:36] And then you tell it you have this new, the second bug.
[00:02:39] SPEAKER_01: And it tells you oh my god, how could have done it? You're so right again.
[00:02:42] SPEAKER_01: And brings back the first bug. And you can alternate between those.
[00:02:45] SPEAKER_01: Yeah.
[00:02:46] SPEAKER_01: And it's like how is that possible?
[00:02:48] It's like I'm not sure.
[00:02:51] SPEAKER_01: But it does suggest that there
[00:02:54] something strange is going on.
[00:02:56] SPEAKER_01: I have two possible explanations.
[00:02:58] SPEAKER_01: So here this is the more kind of a
[00:03:01] whimsical explanation is that maybe a rail training makes the models a little bit too
[00:03:05] SPEAKER_01: single-minded and narrowly focused. A little bit too
[00:03:10] I don't know. Unaware.
[00:03:13] Even though it also makes them aware in some other ways.
[00:03:16] SPEAKER_01: And because of this they can't do basic things.
[00:03:20] SPEAKER_01: But there is another explanation which is
[00:03:24] back when people were doing pre-training.
[00:03:27] The question of what data to train on was answered.
[00:03:32] Because that answer was everything.
[00:03:35] SPEAKER_01: When you do pre-training you need all the data.
[00:03:40] So you don't have to think it's going to be this data or that data.
[00:03:44] SPEAKER_01: But when people do a rail training they do need to think.
[00:03:48] SPEAKER_01: They say, okay, we want to have this kind of a rail training for this thing and that kind of
[00:03:52] SPEAKER_01: a rail training for that thing. And from what I hear, all the companies have teams that
[00:03:58] SPEAKER_01: just produce new a rail environments and just started to the training mix.
[00:04:02] SPEAKER_01: And then the questions, well, what are those?
[00:04:03] SPEAKER_01: There are so many degrees of freedom.
[00:04:05] SPEAKER_01: There is such a huge variety of a rail environments you could produce.
[00:04:09] SPEAKER_01: And one of the one thing you could do, and I think that something that is done inadvertently,
[00:04:17] is that people take inspiration from the e-vows.
[00:04:22] SPEAKER_01: You say, hey, I would love our model to do really well when we release it.
[00:04:25] SPEAKER_01: I want the e-vows to look great.
[00:04:28] What would be a rail training that could help on this task?
[00:04:32] SPEAKER_01: Right? I think that is something that happens.
[00:04:35] SPEAKER_01: And I think it could explain a lot of what's going on.
[00:04:38] SPEAKER_01: If you combine this with generalization of the models actually being inadequate,
[00:04:44] that has the potential to explain a lot of what we are seeing.
[00:04:47] SPEAKER_01: This disconnect between e-vows performance and actual real world performance,
[00:04:53] SPEAKER_01: which is something that we don't today exactly even understand what we mean by that.
[00:05:00] SPEAKER_01: I like this idea that the real reward hacking is the human researchers who are too focused on the e-vows.
[00:05:08] SPEAKER_00: I think there's two ways to understand or to try to think about what you have just pointed out.
[00:05:15] SPEAKER_00: One is, look, if it's the case that simply by becoming superhuman at a coding competition,
[00:05:21] SPEAKER_00: a model will not automatically become more tasteful and exercise better judgment about how to
[00:05:28] SPEAKER_00: improve your code base. Well, then you should expand the suite of environments such that you're not
[00:05:33] SPEAKER_00: just testing it on having the best performance in coding competition. It should also be able to make
[00:05:37] SPEAKER_00: the best kind of application for X-thing or Y-thing or Z-thing.
[00:05:41] SPEAKER_00: And another, maybe this is what you're hinting at, is to say, why should it be the case in the first place
[00:05:47] SPEAKER_00: that becoming superhuman at coding competitions doesn't make you a more tasteful programmer more
[00:05:53] SPEAKER_00: generally? Maybe the thing to do is not to keep stacking up the amount of environments and the
[00:05:58] SPEAKER_00: diversity of environments to figure out approach with, let you learn from one environment and
[00:06:04] SPEAKER_00: improve your performance on something else. So I have an analogy, a human analogy, which might be
[00:06:11] SPEAKER_01: helpful. So even the case, let's take the case of competitive programming since you mentioned that.
[00:06:16] SPEAKER_01: And suppose you have two students. One of them, work decided they want to be the best competitive
[00:06:23] SPEAKER_01: programmer so they will practice 10,000 hours for that domain. They will solve all the problems,
[00:06:30] SPEAKER_01: memorize all the proof techniques and be very, very, you know, be very skilled at quickly and
[00:06:37] SPEAKER_01: correctly implementing all the algorithms and by doing so they became the best one of the best.
[00:06:44] Student number two thought, oh, competitive programming is cool, maybe they practiced for
[00:06:48] SPEAKER_01: a hundred hours, much, much less, and they also did really well. Which one do you think is going to
[00:06:53] SPEAKER_01: do better in their career later on? The second. Right? And I think that's basically what's going on.
[00:06:59] SPEAKER_01: The models are much more likely for a student, but even more because then we say, okay,
[00:07:03] SPEAKER_01: so the model should be good competitive programming. So let's get every single competitive programming
[00:07:08] SPEAKER_01: problem ever. And then let's do some data augmentation. So we have even more competitive programming
[00:07:13] SPEAKER_01: problems. Yes. And we train on that. And so now I got this great competitive programmer. And with
[00:07:18] SPEAKER_01: this analogy, I think it's more intuitive. I think it's more intuitive with this analogy that, yeah,
[00:07:23] SPEAKER_01: okay, so if it's so well trained, okay, it's like all the different algorithms and all the different
[00:07:28] SPEAKER_01: proof techniques are like right at its fingertips. And it's more intuitive that with this level
[00:07:34] SPEAKER_01: of preparation, it would not necessarily generalize to other things. But then what is the analogy for
[00:07:41] SPEAKER_00: what the second student is doing before they do the hundred hours of fine tuning? I think it's like
[00:07:50] they have it. I think it's the eat factor. Yeah. Right? And like I know, like when I was an
[00:07:55] SPEAKER_01: undergraduate, I remember there was there was a student like this that studied with me. So I know
[00:08:00] SPEAKER_01: I know it exists. Yeah. I think it's interesting to distinguish it from whatever pre-training does.
[00:08:05] SPEAKER_00: So when we don't understand what you just said about we don't have to choose the data in pre-training
[00:08:10] SPEAKER_00: is to say, actually, it's not dissimilar to the 10,000 hours of practice. It's just that you get
[00:08:15] SPEAKER_00: that 10,000 hours of practice for free because it's already somewhere in the pre-training distribution.
[00:08:22] SPEAKER_00: But it's like maybe you're suggesting actually there's actually not that much
[00:08:25] SPEAKER_00: generalization pre-training. There's just so much data in pre-training. But it's not necessarily
[00:08:29] SPEAKER_00: generalizing better than RL. But the main strength of pre-training is that there is a so much of it.
[00:08:36] And b, you don't have to think hard about what data to put into pre-training.
[00:08:42] And it's a very kind of natural data and it does include in it a lot of what people do.
[00:08:48] SPEAKER_01: Yeah. People's thoughts and a lot of the features of, you know, it's like the whole world
[00:08:56] SPEAKER_01: as projected by people onto text. Yeah. And pre-training tries to capture that using a huge amount
[00:09:02] SPEAKER_01: of data. It's very, the pre-training is very difficult to reason about because it's so hard to
[00:09:10] SPEAKER_01: understand the manner in which the model relies on pre-training data.
[00:09:17] And whenever the model makes a mistake, could it be because something by chance is not as
[00:09:23] SPEAKER_01: supported by the pre-training data? You know, and pre-support by pre-training is maybe a loose term.
[00:09:30] I don't know if I can add anything more useful on this, but I don't think there is a human
[00:09:37] SPEAKER_01: analog to pre-training. Here's an analogy that people have proposed forward the human analogy
[00:09:42] SPEAKER_00: to pre-training is and I'm curious to get your thoughts on why they're potentially wrong.
[00:09:47] SPEAKER_00: One is to think about the first 18 or 15 or 13 years of a person's life when they aren't
[00:09:54] SPEAKER_00: necessarily economically productive, but they are doing something that is making them
[00:10:01] SPEAKER_00: understand the world better and so forth. And the other is to think about evolution as doing
[00:10:07] SPEAKER_00: some kind of search for three billion years, which then results in a human lifetime instance.
[00:10:13] SPEAKER_00: And then I'm curious if you think either of these are actually analogous pre-training or
[00:10:17] SPEAKER_00: how would you think about at least what lifetime human learning is like if not pre-training?
[00:10:22] SPEAKER_00: I think there are some similarities between both of these to pre-training and pre-training
[00:10:28] SPEAKER_01: tries to play the role of both of these, but I think there are some big differences as well.
[00:10:33] SPEAKER_01: The amount of pre-training data is very, very staggering.
[00:10:38] SPEAKER_01: Yes.
[00:10:39] SPEAKER_01: And somehow a human being after even 15 years with the tiny fraction of the pre-training data,
[00:10:48] SPEAKER_01: they know much less, but whatever they do know, they know much more deeply, somehow.
[00:10:52] SPEAKER_01: And the mistakes like already at that age, you would not make mistakes that ARIA is make.
[00:10:58] SPEAKER_01: Yes.
[00:10:59] There is another thing, you might say, could it be something like evolution?
[00:11:02] SPEAKER_01: And the answer is maybe, but in this case, I think evolution might actually have an edge.
[00:11:07] SPEAKER_01: There is this, I remember reading about this case where some, you know, that one thing that
[00:11:15] SPEAKER_01: neuroscientists do, or rather one way in which neuroscientists can learn about the brain,
[00:11:20] SPEAKER_01: is by studying people with brain damage to different parts of the brain.
[00:11:25] SPEAKER_00: And so some people have the most strange symptoms you could imagine.
[00:11:29] SPEAKER_01: It's actually really, really interesting.
[00:11:32] And there was one case that comes to mind that's relevant.
[00:11:35] I read about this person who had some kind of brain damage
[00:11:40] SPEAKER_01: that took out, I think, a stroke or an accident,
[00:11:43] SPEAKER_01: that took out his emotional processing.
[00:11:48] SPEAKER_01: So he stopped feeling any emotion.
[00:11:50] And as a result of that, you know, he still remained very articulate and he could solve little puzzles
[00:11:58] SPEAKER_01: and on tests he seemed to be just fine.
[00:12:01] But he felt no emotion, he didn't feel sad, he didn't feel angry, he didn't feel animated.
[00:12:06] And he became somehow extremely bad at making any decisions at all.
[00:12:11] It would take him hours to decide on which socks to wear,
[00:12:14] SPEAKER_01: and he would make very bad financial decisions.
[00:12:17] SPEAKER_01: And that's very,
[00:12:21] but what does it say about the role of our built-in emotions
[00:12:29] in making us like a viable agent essentially?
[00:12:33] SPEAKER_01: And I guess to connect to your question about pre-training.
[00:12:36] SPEAKER_01: It's like maybe if you're good enough at getting everything out of pre-training,
[00:12:42] SPEAKER_01: you could get that as well.
[00:12:44] SPEAKER_01: But that's the kind of thing which seems
[00:12:50] well, it may or may not be possible to get that from pre-training.
[00:12:56] What is that?
[00:12:58] SPEAKER_00: Clearly not just directly emotion, it seems like some
[00:13:04] almost value function like thing, which is telling you wish decision to be made.
[00:13:08] SPEAKER_00: Like what the end reward for any decision should be.
[00:13:11] SPEAKER_00: And you think that doesn't implicitly come from...
[00:13:15] SPEAKER_00: I think it's good.
[00:13:16] SPEAKER_00: I'm just saying it's not 100% obvious.
[00:13:20] But what is that?
[00:13:21] SPEAKER_00: Like how do you think about emotions?
[00:13:23] SPEAKER_00: What is the ML analogy for emotions?
[00:13:26] It should be some kind of a value function thing.
[00:13:29] SPEAKER_01: But I don't think there is a greater
[00:13:30] SPEAKER_01: ML analogy because right now value functions don't play very prominent role
[00:13:34] SPEAKER_01: in the things people do.
[00:13:36] SPEAKER_01: It might be worth defining for the audience what a value function is if you want to do that.
[00:13:39] SPEAKER_00: I mean, certainly I'll be very happy to do that.
[00:13:43] SPEAKER_01: Right?
[00:13:44] SPEAKER_01: So...
[00:13:48] So when people do reinforcement learning, the very reinforcement learning is done right now.
[00:13:53] How do people train those agents?
[00:13:56] So you have a neural net and you give it a problem.
[00:13:59] SPEAKER_01: And then you tell the model go solve it.
[00:14:01] SPEAKER_01: And the model takes maybe thousands, hundreds of thousands of actions
[00:14:06] or thoughts or something and then it produces a solution, a solution is created.
[00:14:09] SPEAKER_01: And then the score is used to provide a training signal for every single action
[00:14:18] in your trajectory.
[00:14:20] So that means that if you are doing something that goes for a long time,
[00:14:24] SPEAKER_01: if you're training a task that takes a long time to solve,
[00:14:28] SPEAKER_01: you will do no learning at all until you solve until you come up with a proposed solution.
[00:14:33] SPEAKER_01: That's how reinforcement learning is done naively.
[00:14:35] SPEAKER_01: That's how O1, R1 ostensibly are done.
[00:14:40] The value function says something like, okay, look,
[00:14:44] maybe I could sometimes, not always,
[00:14:47] could tell you if you are doing valor badly.
[00:14:50] SPEAKER_01: The notion of a value function is more useful in some domains than others.
[00:14:53] SPEAKER_01: So for example, when you play chess,
[00:14:56] and you lose a piece, you know, I messed up.
[00:14:59] You don't need to play the whole game to know that what I just did was bad and therefore whatever
[00:15:04] SPEAKER_01: proceeded, it was also bad.
[00:15:08] So the value function lets you short circuit the weight until the very end.
[00:15:14] SPEAKER_01: Like, let's suppose that you started to pursue some kind of, okay,
[00:15:18] SPEAKER_01: let's suppose that you are doing some kind of a math thing or a programming thing.
[00:15:22] SPEAKER_01: And you're trying to explore a particular solution direction.
[00:15:26] And after, let's say after a thousand steps of thinking, you concluded that this direction
[00:15:32] SPEAKER_01: is unpromising. As soon as you conclude this, you could already get a reward signal
[00:15:39] a thousand times steps previously when you decided to pursue down the path.
[00:15:43] SPEAKER_01: You say, oh, next time, I shouldn't pursue this path in a similar situation.
[00:15:48] SPEAKER_01: Long before you actually came up with the proposed solution.
[00:15:52] This was in the deep cigar one paper is that the space of trajectories is so wide
[00:16:00] SPEAKER_00: that maybe it's hard to learn a mapping from an intermediate trajectory and value.
[00:16:06] SPEAKER_00: And also given that, you know, encoding, for example, you will have the wrong idea
[00:16:10] SPEAKER_00: then you'll go back, then you'll change something.
[00:16:12] SPEAKER_00: This sounds like such lack of faith in deep learning.
[00:16:15] SPEAKER_01: Like, I mean, sure, it might be difficult, but nothing deep learning can do.
[00:16:21] SPEAKER_01: Yeah.
[00:16:22] SPEAKER_01: So my expectation is that like value function should be useful and
[00:16:30] and I fully, I fully expect that they will be using the future if not already.
[00:16:34] SPEAKER_01: What was I alluded to with the person whose emotional center got
[00:16:40] SPEAKER_01: damaged is more that maybe what it suggests is that the value function of humans is
[00:16:49] SPEAKER_01: modulated by emotions in some important way that's hard coded by evolution.
[00:16:55] And maybe that is important for people to be effective in the world.
[00:17:00] That's the thing I was actually planning on asking you.
[00:17:02] SPEAKER_00: There's something really interesting about emotions of the value function, which is that
[00:17:06] it's impressive that they have this much utility while still being rather simple to understand.
[00:17:15] SPEAKER_00: So I have two responses.
[00:17:17] SPEAKER_00: I do agree that compared to
[00:17:23] the kind of things that we learn and the things we are talking about, the kind of as we are talking
[00:17:27] SPEAKER_01: about, emotions are relatively simple.
[00:17:30] They might even be so simple that maybe you could map them out in a human understandable way.
[00:17:35] SPEAKER_01: I think it would be cool to do.
[00:17:39] In terms of utility though, I think there is a thing where,
[00:17:43] SPEAKER_01: you know, there is this complexity robustness trade-off,
[00:17:48] where complex things can be very useful,
[00:17:52] but simple things are very useful in very broad range of situations.
[00:17:59] So I think one way to interpret what we are seeing is that we've got these emotions that
[00:18:04] SPEAKER_01: essentially evolved mostly from our mammal ancestors, and then fine tune the little bit while we
[00:18:10] SPEAKER_01: were hominins just a bit. We do have like a decent amount of social emotions though, which mammals
[00:18:17] SPEAKER_01: may lack, but they're not very sophisticated, and because they're not sophisticated,
[00:18:23] SPEAKER_01: they serve us so well in this very different world compared to the one that we've been living in.
[00:18:28] SPEAKER_01: Actually, they also make mistakes. For example, our emotions, well, I don't know,
[00:18:32] SPEAKER_01: this hunger count is an emotion, it's debatable, but I think for example, our intuitive feeling of
[00:18:39] SPEAKER_01: hunger is not succeeding in guiding us correctly in this world with an abundance of food.
[00:18:48] SPEAKER_01: Yeah. People have been talking about scaling data, scaling parameter, scaling compute.
[00:18:56] Is there a more general way to think about scaling? What are the other scaling axes?
[00:19:00] So the thing, so here is a perspective. Here's a perspective that I think might be true.
[00:19:10] So the way a male used to work is that people would just think of it with stuff and try to
[00:19:20] and try to get interesting results. That's what's been going on in the past.
[00:19:26] SPEAKER_01: Then the scaling insight arrived, right? Scaling laws, GPT-3, and suddenly everyone realized
[00:19:36] SPEAKER_01: we should scale. And this is an example of how language affects thought.
[00:19:45] Scaling is just one word, but it's such a powerful word because it informs people what to do.
[00:19:51] SPEAKER_01: Let's try to scale things. So what are we scaling? And pre-training was a thing to scale.
[00:19:58] SPEAKER_01: It was a particular scaling recipe. The big breakthrough of pre-training is the realization
[00:20:05] SPEAKER_01: that this recipe is good. So you say, hey, if you mix some compute with some data into a neural net
[00:20:14] SPEAKER_01: of a certain size, you will get results. And you will know that it will be better if you just
[00:20:20] SPEAKER_01: scale the recipe up. And this is also great companies love this because it gives you a very
[00:20:27] SPEAKER_01: low risk way of investing your resources. It's much harder to invest your resources in research.
[00:20:36] Compare that. If you have research, you need to go for three researchers and research and come
[00:20:41] SPEAKER_01: up with something. Versus, get more data, get more compute, you know, it'll get something from
[00:20:46] SPEAKER_01: pre-training. And indeed, it looks like I based on various things, some people say on Twitter,
[00:20:56] SPEAKER_01: maybe it appears that Gemini have found a way to get more out of pre-training. At some point,
[00:21:02] SPEAKER_01: the pre-training will run out of data. The data is very clearly finite. And so then, okay,
[00:21:06] SPEAKER_01: what do you do next? I do some kind of a souped-up pre-training different recipe from the one you've
[00:21:12] SPEAKER_01: done before, or you're doing a RL, or maybe something else. But now that compute is big,
[00:21:17] SPEAKER_01: compute is now very big. In some sense, we are back to the age of research. So maybe here's
[00:21:22] SPEAKER_01: another way to put it. Up until 2020, from 2020, from 2020, it was the age of research.
[00:21:30] SPEAKER_01: Now, from 2020 to 2025, it was the age of scaling. Or maybe plus minus, let's add
[00:21:36] SPEAKER_01: the error bars to those years. Because people say, this is amazing, you've got to scale more, keep
[00:21:40] SPEAKER_01: scaling, the one word, scaling. But now the scale is so big. Like, is the belief really that,
[00:21:48] SPEAKER_01: oh, it's so big, but if you had a 100x more, everything would be so different. Like, it would be
[00:21:53] different for sure. But like, is the belief that if you just 100x the scale, everything would be
[00:22:00] SPEAKER_01: transformed? I don't think that's true. So, in respect to the age of research again, just with
[00:22:05] SPEAKER_01: the computers. That's very interesting. We're going to put it. But let me ask you the question you're
[00:22:11] SPEAKER_00: just posed then. What are we scaling? And what would it mean to have a recipe? Because I guess
[00:22:17] SPEAKER_00: I'm not aware of a very clean relationship that almost looks like a law of physics, which existed
[00:22:24] SPEAKER_00: in pre-training. It does a power law between data or computer parameters and loss. What is the
[00:22:31] SPEAKER_00: kind of relationship we should be seeking? And how should we think about what this new recipe
[00:22:37] SPEAKER_00: might look like? So, we've already witnessed a transition from one type of scaling to a different
[00:22:46] SPEAKER_01: type of scaling from pre-training to RL. Now people are scaling RL. Now based on what people say on
[00:22:55] SPEAKER_01: Twitter, they spend more compute on RL than on pre-training at this point because RL can actually
[00:23:01] SPEAKER_01: consume quite a bit of compute. You know, you do very, very long roll-outs. So, it takes a lot of
[00:23:07] SPEAKER_01: compute to produce those roll-outs. And then you get relative to the small amount of learning
[00:23:11] SPEAKER_01: pillar roll-outs. So, you really can spend a lot of compute. And I could imagine, like, I wouldn't
[00:23:18] at this stage, it's more like, I wouldn't even call it a scaling. I would say, hey, like, what are
[00:23:25] SPEAKER_01: you doing? And is the thing you're doing the most productive thing you could be doing? Can you
[00:23:31] SPEAKER_01: find a more productive way of using your compute? We've discussed the value function business earlier.
[00:23:39] SPEAKER_01: And maybe once people get good at value functions, they will be using their resources more
[00:23:45] SPEAKER_01: productively. And if you find a whole other way of training models, you could say, is this scaling
[00:23:54] SPEAKER_01: or is it just using your resources? I think it becomes a little bit ambiguous. In a sense that when
[00:23:58] SPEAKER_01: people were in the age of research, back then it was like, people say, hey, let's try this in this
[00:24:03] SPEAKER_01: and this and this. Let's try that and that and that. Oh, look, something interesting is happening.
[00:24:07] And I think there will be a return to that. So, if we're back in the era of research, stepping back,
[00:24:12] SPEAKER_00: what is the part of the recipe that we need to think most about? When you say value function,
[00:24:18] SPEAKER_00: people are already trying the current recipe, but then having a little limit as a judge and so forth.
[00:24:23] SPEAKER_00: You can say that's a value function, but it sounds like you have something much more fundamental
[00:24:26] SPEAKER_00: in mind. Do we need to go back to, should we even rethink pre-training at all and not just add
[00:24:33] SPEAKER_00: more steps to the end of that process? Yeah. So, the discussion about value function, I think it was
[00:24:40] SPEAKER_01: interesting. I want to emphasize that I think the value function is something like, it's going to make
[00:24:47] SPEAKER_01: our realm more efficient. And I think that makes a difference. But I think that anything you can do
[00:24:54] SPEAKER_01: with a value function, you can do without just more slowly. The thing which I think is the most
[00:25:01] SPEAKER_01: fundamental is that these models somehow just generalize dramatically worse than people.
[00:25:07] And it's super obvious. That seems like a very fundamental thing.
[00:25:12] SPEAKER_01: Okay. So, this is the Crocs generalization. And there's two sub-questions.
[00:25:20] There's one which is about sample efficiency, which is why should it take so much more data for
[00:25:24] SPEAKER_00: these models to learn than humans? There's a second about even separate from the amount of data it
[00:25:29] SPEAKER_00: takes. There's a question of, why is it so hard to teach the thing we want to a model than to a
[00:25:35] SPEAKER_00: human, which is to say, for to a human, we don't necessarily need a verifiable reward to be able to
[00:25:43] you're probably mentoring a bunch of researchers right now and you're, you know, talking with them,
[00:25:47] SPEAKER_00: you're showing them your code and you're showing them how you think and from that,
[00:25:51] SPEAKER_00: they're picking up your way of thinking and how they should do research. You don't have to set
[00:25:55] SPEAKER_00: like a verifiable reward for them. That's like, okay, this is the next part of their curriculum. And
[00:25:58] SPEAKER_00: now this is the next part of your curriculum. And oh, I was, this training was unstable and we get
[00:26:02] SPEAKER_00: a there's not this schleppy bespoke process. So perhaps these two issues are actually related in
[00:26:08] SPEAKER_00: some way. But I'd be curious to explore this, this second thing which was more like continuing
[00:26:14] SPEAKER_00: or learning and this first thing which feels just like sample efficiency. Yeah. So, you know,
[00:26:20] SPEAKER_01: you could actually wonder one possible explanation for the human sample efficiency that needs to be
[00:26:27] SPEAKER_01: considered easy evolution. And evolution has given us a small amount of the most useful information
[00:26:36] SPEAKER_01: possible. And for things like vision, hearing and locomotion, I think there's a pretty strong case
[00:26:45] SPEAKER_01: that evolution actually has given us a lot. So for example, human dexterity far exceeds, I mean,
[00:26:54] robots can become dexterous too if you subject them to like a huge amount of training and simulation.
[00:27:00] But to train a robot in the real world to quickly like pick up a new skill like a person does,
[00:27:05] SPEAKER_01: seems very out of reach. And here you could say, oh yeah, like locomotion, all our ancestors needed
[00:27:12] SPEAKER_01: a great locomotion squirrels like. So locomotion maybe like you've got like some unbelievable prior.
[00:27:19] SPEAKER_01: You could make the same case for vision, you know, I believe the Anlequin made the point, oh like
[00:27:24] SPEAKER_01: come, children learn to drive after 16 hours, after 10 hours of practice, which is true. But our
[00:27:31] SPEAKER_01: vision is so good. At least for me, when I remember myself being five year old, my I was very excited
[00:27:38] SPEAKER_01: about cars back then. And I'm pretty sure my car recognition was more than addict but for self-driving
[00:27:45] SPEAKER_01: already as a five year old. You don't get to see that much data as a five year old. You spend
[00:27:49] SPEAKER_01: most of your time in your parents' house. So you have very low data diversity. But you could say maybe
[00:27:53] SPEAKER_01: that's evolution too. But then language and math and coding, probably not. It still seems
[00:28:00] better than models. I mean, obviously models are better than the average human at language and
[00:28:05] SPEAKER_00: math and coding. But are they better at the average human at learning? Oh yeah, oh yeah, absolutely.
[00:28:11] SPEAKER_01: What I meant to say is that language, math and coding and especially math and coding suggest
[00:28:17] SPEAKER_01: that whatever it is that makes people good at learning is probably not so much a complicated
[00:28:25] SPEAKER_01: prior but something more, some fundamental thing. Wait, I'm not sure, why should that be the case?
[00:28:32] So consider a skill that people exhibit some kind of great reliability or, you know, M.
[00:28:39] SPEAKER_01: Yeah. If the skill is one that was very useful to our ancestors for many millions of years,
[00:28:47] SPEAKER_01: hundreds of millions of years, you could say, you could argue that maybe humans are good at it
[00:28:54] because of evolution because we have a prior and evolutionary prior that's encoded in some very
[00:29:02] SPEAKER_01: non-obvious way that somehow makes us so good at it. But if people exhibit great
[00:29:10] SPEAKER_01: ability, reliability, robustness, ability to learn in a domain that really did not exist until recently,
[00:29:19] SPEAKER_01: then this is more an indication that people might have just better machine learning period.
[00:29:28] But then how should we think about what that is? Is it a matter of,
[00:29:34] yeah, what is the ML analogy for what? There's a couple of interesting things about it. It takes
[00:29:39] SPEAKER_00: fewer samples. It's more unsupervised. You don't have to set a, like a child learning to drive a car.
[00:29:45] A child, so no, no, no, learning to drive a car. A teenager learning to drive a car is like not
[00:29:50] SPEAKER_00: exactly getting some pre-built verifiable reward. It comes from their interaction with the machine
[00:29:59] SPEAKER_00: and with the environment. And yet, it takes much of your samples. It seems more unsupervised.
[00:30:05] SPEAKER_00: It seems more robust. Much more robust. The robustness of people is really staggering.
[00:30:12] Yeah. So it's like, okay, and do you have a unified way of thinking about why are all these things
[00:30:16] SPEAKER_00: happening at once? What is the ML analogy that could realize something like this?
[00:30:23] SPEAKER_00: So this is where one of the things that you've been asking about is how can the teenage driver
[00:30:31] SPEAKER_01: kind of self-correct and learn from their experience without an external teacher? And the answer
[00:30:37] SPEAKER_01: is, well, they have their value function. They have a general sense, which is also, by the way,
[00:30:44] SPEAKER_01: extremely robust in people. Like, whatever it is, the human value function, whatever the human
[00:30:51] SPEAKER_01: value function is, with a few exceptions around addiction, it's actually very, very robust.
[00:30:58] And so for something like a teenager that's learning to drive, they start to drive
[00:31:04] and they already have a sense of how they're driving. Immediately, how badly they're unconfident,
[00:31:10] SPEAKER_01: and then they see okay. And then, of course, the learning speed of any teenager is so fast,
[00:31:15] SPEAKER_01: stuff that 10 hours, you're good to go. Yeah. This seems like humans have some solution, but I'm
[00:31:19] SPEAKER_00: curious about like, well, how are they doing it? And like, why is it so hard to, like, how do we
[00:31:23] SPEAKER_00: need to reconsensualize the way we're training models to make something like this possible? You know,
[00:31:28] that is a great question to ask. And it's a question I have a lot of opinions about.
[00:31:35] SPEAKER_01: But unfortunately, we live in a world where not all machine learning ideas are discussed freely,
[00:31:43] SPEAKER_01: and this is one of them. So there's probably a way to do it. I think it can be done. The fact that
[00:31:51] SPEAKER_01: people are like that, I think it's a proof that it can be done. There may be another blocker though,
[00:31:57] SPEAKER_01: which is the responsibility that the human neurons actually do more computing, we think.
[00:32:07] And if that is true, and if that plays an important role, then things might be more difficult.
[00:32:13] But regardless, I do think it points to the existence of some machine learning principle,
[00:32:21] but I have an opinion on, but unfortunately, circumstances make it hard to discuss in detail.
[00:32:27] SPEAKER_01: Nobody listens to this podcast, Ilia.
[00:32:31] Yeah. So I have to say that prepping for Ilia was pretty tough, because neither I nor anybody else
[00:32:37] SPEAKER_00: had any idea what he's working on and what SSI is trying to do. I had no basis to come up with
[00:32:43] SPEAKER_00: my questions. And the only thing I could go off, honestly, was trying to think from first
[00:32:48] SPEAKER_00: principles about what are the bottlenecks to AGI, because clearly Ilia is working on them in some
[00:32:53] SPEAKER_00: way. Part of this question involved thinking about RL scaling, because everybody's asking how well
[00:32:58] SPEAKER_00: RL will generalize and how we can make it generalize better. As part of this, I was reading this
[00:33:02] SPEAKER_00: paper that came out recently on RL scaling, and it showed that actually the learning curve on RL
[00:33:08] SPEAKER_00: looks like a sigmoid. I found this very curious. Why should it be a sigmoid? Where it learns very
[00:33:12] SPEAKER_00: little for a long time, and then it quickly learns a lot, and then it asses him totes. This is very
[00:33:17] SPEAKER_00: different from the power law you see in pre-training, where the model learns a bunch at the very beginning,
[00:33:22] SPEAKER_00: and then less and less over time. And it actually reminded me of a note that I had run down after
[00:33:27] SPEAKER_00: I had a conversation with a researcher friend, where he pointed out that the number of samples that
[00:33:32] SPEAKER_00: you need to take in order to find the correct answer, scales exponentially with how different your
[00:33:38] SPEAKER_00: current probability distribution is from the target probability distribution. And I was thinking
[00:33:42] SPEAKER_00: about how these two ideas are related. I had the survey idea that they should be connected, but I
[00:33:46] SPEAKER_00: really didn't know how. I don't have a math background, so I couldn't really formalize it.
[00:33:50] SPEAKER_00: But I wondered if Gemini3 could help me out here. And so I took a picture of my notebook,
[00:33:54] SPEAKER_00: and I took the paper, and I put them both in the context of Gemini3, and I asked it to find the
[00:33:59] SPEAKER_00: connection. And I thought a bunch, and then it realized that the correct way to model the
[00:34:05] SPEAKER_00: information you gain from a single yes or no outcome in RL is as the entropy of a random binary
[00:34:11] SPEAKER_00: variable. It made a graph, which showed how the bits you gain for sample in RL versus supervised
[00:34:17] SPEAKER_00: learning scale as a pass rate increases. And as soon as I saw the graph that Gemini3 made,
[00:34:22] SPEAKER_00: immediately a ton of things started making sense to me. Then I wanted to see if there was any
[00:34:27] SPEAKER_00: empirical basis to this theory. So I asked Gemini to code on my experiment to show whether the
[00:34:33] SPEAKER_00: improvement in loss scales in this way with pass rate. I just took the code that Gemini
[00:34:38] SPEAKER_00: outputted. I copy-pasted it into a Google call lab notebook, and I was able to run this toy
[00:34:42] SPEAKER_00: ML experiment and visualize its results without a single bug. It's interesting because the results look
[00:34:48] SPEAKER_00: similar, but not identical to what we should have expected. And so I downloaded this chart,
[00:34:52] SPEAKER_00: and I put it into Gemini, and I asked it, what is going on here? And I came up with a hypothesis
[00:34:56] SPEAKER_00: that I think is actually correct, which is that we're capping how much supervised learning can
[00:35:01] SPEAKER_00: improve in the beginning by having a fixed learning rate. And in fact, we should decrease the
[00:35:05] SPEAKER_00: learning rate over time. It actually gives us an intuitive understanding for why in practice,
[00:35:10] SPEAKER_00: we have learning rate schedulers that decrease the learning rate over time. I did this entire flow
[00:35:15] SPEAKER_00: from coming up with this vague initial question to building a theoretical understanding,
[00:35:20] SPEAKER_00: to running some toy ML experiments all with Gemini3. This feels like the first model where it can
[00:35:26] SPEAKER_00: actually come up with new connections that I wouldn't have anticipated. It's actually not
[00:35:30] SPEAKER_00: become the default place I go to when I want to brainstorm new ways to think about a problem.
[00:35:35] SPEAKER_00: If you want to read more about our all-scaling, you can check out the blog post that I wrote with
[00:35:39] SPEAKER_00: a little help from Gemini3. And if you want to check out Gemini3Yourself, go to Gemini.google.
[00:35:45] SPEAKER_00: I am curious, so if you say we are back in the era of research, you were there from 2012 to 2020.
[00:35:53] SPEAKER_00: And do you have... Yeah, what is now the vibe going to be if we go back to the era of research?
[00:36:00] SPEAKER_00: For example, even after Alex and I, the amount of compute that was used to run experiments kept
[00:36:07] SPEAKER_00: increasing, and the size of frontier systems kept increasing. And do you think now that this
[00:36:15] SPEAKER_00: era of research will still require tremendous amounts of compute? Do you think it will require
[00:36:21] SPEAKER_00: going back into the archives and reading old papers? What was the vibe of like you were Google and
[00:36:29] SPEAKER_00: Open AI and Stanford? These places when there was like more of a vibe of research,
[00:36:34] SPEAKER_00: what kind of thing should we be expecting in the community?
[00:36:38] So one consequence of the age of scaling is that there was this
[00:36:47] SPEAKER_01: scaling sucked out all the air in the room. Yeah. And so because scaling sucked out all the air in the
[00:36:54] SPEAKER_01: room, everyone started to do the same thing. We got to the point where we are in a world where there
[00:37:04] SPEAKER_01: are more companies than ideas, but quite a bit. Actually, on that, you know, there is this
[00:37:10] SPEAKER_01: Silicon Valley saying that says that ideas are cheap, execution is everything. And people say
[00:37:18] that a lot. And there is truth to that. But then I saw someone say on Twitter something like
[00:37:26] SPEAKER_01: if ideas are so cheap, how come no one's having any ideas? And I think it's true too. I think
[00:37:33] if you think about a research progress in terms of bottlenecks, there are several bottlenecks.
[00:37:42] If you go back to the, if you end them, one of them is ideas and one of them is your ability to
[00:37:47] SPEAKER_01: bring them to life, which might be compute, but also engineering. So if you go back to the 90s,
[00:37:53] SPEAKER_01: let's say, you had people who had had pretty good ideas. And if they had much larger computers,
[00:37:58] SPEAKER_01: maybe they could demonstrate that their ideas were viable, but they could not. So they could only
[00:38:03] SPEAKER_01: have very, very small demonstration that did not convince anyone. So the bottleneck was compute.
[00:38:09] Then in the age of scaling, computers increased a lot. And of course, there is a question of how
[00:38:16] SPEAKER_01: much compute is needed, but compute is large. So compute is large enough such that
[00:38:25] it's like not obvious that you need that much more compute to prove some idea. Like I'll give you
[00:38:33] SPEAKER_01: an analogy, Alex net was built on two GPUs. That was the total amount of compute used for it.
[00:38:40] The transformer was built on 8 to 64 GPUs. No single transformer paper experiment used more than
[00:38:48] SPEAKER_01: 64 GPUs of 2017, which would be like what, two GPUs of today. So the ResNet, right, many, like even
[00:38:59] SPEAKER_01: the, the you could argue that the like, oh, one reasoning was not the most compute heavy thing in
[00:39:07] SPEAKER_01: the world. So they're definitely for research. You need like definitely some amount of compute.
[00:39:16] SPEAKER_01: But it's far from obvious that you need the absolutely largest amount of compute ever for research.
[00:39:22] SPEAKER_01: You might argue, and I think it is true, that if you want to build the absolutely best system,
[00:39:28] if you want to build the absolutely best system, then it helps to have much more compute. And
[00:39:33] SPEAKER_01: especially if everyone is within the same paradigm, then compute becomes one of the big differentiators.
[00:39:42] Yeah, I guess while it was possible to develop these ideas, I'm asking you for the history because
[00:39:47] SPEAKER_00: you were actually there. I'm not sure what actually happened, but it sounds like it was possible to
[00:39:51] SPEAKER_00: develop these ideas using minimal amounts of compute. But it wasn't the transformer didn't
[00:39:56] SPEAKER_00: immediately become famous. It became the thing everybody started doing and then started experimenting
[00:40:00] SPEAKER_00: on top of and building on top of because it was validated at higher and higher levels of compute.
[00:40:06] SPEAKER_00: Correct. And if you and SSI have 50 different ideas, how will you know which one is the next transformer
[00:40:13] SPEAKER_00: and which one is brittle without having the kinds of compute that other frontier labs have?
[00:40:22] So I can comment on that, which is the short comment is that you mentioned SSI,
[00:40:30] SPEAKER_01: specifically for us, the amount of compute that SSI has for research is really not that small.
[00:40:40] SPEAKER_01: And I want to explain why, like a simple math can explain why the amount of compute that we have
[00:40:45] SPEAKER_01: is actually a lot more comparable for research than one might think. Now explain. So,
[00:40:53] SPEAKER_01: SSI has raised $3 billion, which is like not small, but it's like a lot by any absolute sense.
[00:41:04] SPEAKER_01: But you could say, but look at the other companies raising much more. But a lot of what their
[00:41:10] SPEAKER_01: a lot of their compute goes for inference. Like these big numbers, these big loans, it's earmarked for
[00:41:17] SPEAKER_01: inference. That's number one. Number two, you need if you want to have a product on which you
[00:41:24] SPEAKER_01: do inference, you need to have a big staff of engineers, of salespeople, a lot of the research needs
[00:41:29] SPEAKER_01: to be dedicated for producing all kinds of product related features. So then when you look at what's
[00:41:36] SPEAKER_01: actually left for research, the difference becomes a lot smaller. Now the other thing is that if you are
[00:41:44] SPEAKER_01: doing something different, do you really need the absolute maximal scale to prove it? I don't
[00:41:51] think it's true at all. I think that in our case, we have sufficient compute to prove to convince
[00:41:59] SPEAKER_01: ourselves and anyone else that what we're doing is correct. There's been public estimates that
[00:42:04] SPEAKER_00: companies like OpenAI spend on the order of $56 billion a year, just so far on experiments.
[00:42:12] SPEAKER_00: This is separate from the amount of money they're sending on inference and so forth. So
[00:42:16] SPEAKER_00: it seems like they're spending more a year running like research experiments, the new guys
[00:42:21] SPEAKER_00: have in total funding. I think it's a question of what you do with it. It's a question of what you
[00:42:25] do with it. Like they have like the more, I think in their case, in the case of others, I think there
[00:42:32] SPEAKER_01: is a lot more demand on the training compute. There's a lot more different workstreams. There
[00:42:38] SPEAKER_01: are different modalities. There is just more stuff and so it becomes fragmented. How will SSI
[00:42:45] SPEAKER_01: make money? My answer to this question is something like maybe just right now, we just focus on
[00:42:54] SPEAKER_01: the research and then the answer to this question will reveal itself. I think there will be lots of
[00:42:59] SPEAKER_01: possible answers. Is SSI's plans to author straight shot superintelligence? Maybe. I think that
[00:43:07] SPEAKER_01: there is merit to it. I think there's a lot of merit because I think that it's very nice to not
[00:43:12] SPEAKER_01: be affected by the day-to-day market competition. But I think there are two reasons that make
[00:43:22] SPEAKER_01: causes to change the plan. One is pragmatic if timelines turned out to be long, which they might.
[00:43:30] SPEAKER_01: And second, I think there is a lot of value in the best and most powerful AI being out there
[00:43:40] SPEAKER_01: impacting the world. I think this is a meaningful, valuable thing.
[00:43:45] SPEAKER_01: But then, so why is your default plan to straight shot superintelligence? Because it sounds like
[00:43:50] SPEAKER_00: you know, open AI and theropic, all these other companies, their explicit thinking is,
[00:43:54] SPEAKER_00: look, we have weaker and weaker intelligences that the public can get used to and prepare for
[00:44:00] SPEAKER_00: and why is it potentially better to build a superintelligence directly? So I'll make the case
[00:44:07] SPEAKER_01: four and against. The case four is that you are, so one of the challenges that people face when
[00:44:15] SPEAKER_01: they're in the market is that they have to participate in the rat race. And the rat race is quite
[00:44:21] SPEAKER_01: difficult in that it exposes you to do to difficult trade-offs which you need to make.
[00:44:27] And the reason it is nice to say we'll insulate ourselves from all this and just focus on the
[00:44:33] SPEAKER_01: research and come out only when we are ready and not before. But the counterpoint is value two.
[00:44:40] SPEAKER_01: And those opposing forces, the counterpoint is hey, it is useful for the world to see powerful AI.
[00:44:50] SPEAKER_01: It is useful for the world to see powerful AI because that's the only way you can communicate it.
[00:44:55] SPEAKER_01: Well, I guess not even just that you can communicate the idea, but communicate the AI, not the idea,
[00:45:01] communicate the AI. What do you mean communicate the AI?
[00:45:03] SPEAKER_00: So, okay, so let's suppose you read an essay about AI. And the essay says AI is going to be this
[00:45:09] SPEAKER_01: and AI is going to be that and it's going to be this. And you read it and you say, okay,
[00:45:13] SPEAKER_01: this is an interesting essay. Right. Now suppose you see an AI doing this and AI doing that.
[00:45:20] It is incomparable. Like basically I think that there is a big benefit from AI being in the public.
[00:45:29] SPEAKER_01: And that would be a reason for us to not be quite straight-shot.
[00:45:35] SPEAKER_01: Yeah. Well, I guess it's not even that, but I do think that is an important part of it.
[00:45:40] SPEAKER_00: The other big thing is I can't think of another discipline and human engineering and research where
[00:45:46] SPEAKER_00: the end artifact was made safer mostly through just thinking about how to make it safe as opposed to
[00:45:54] SPEAKER_00: why are airplane crashes per mile so much lower today than there were decades ago?
[00:45:59] SPEAKER_00: Why is it so much harder to find a bug in Linux than it would have been decades ago?
[00:46:04] SPEAKER_00: And I think it's mostly because these systems were deployed to the world, you noticed failures.
[00:46:09] SPEAKER_00: Those failures were corrected and the systems became more robust. Now, I'm not sure why
[00:46:15] SPEAKER_00: AI and superhuman intelligence would be any different, especially given. And I hope we're going to get to this.
[00:46:22] It seems like the harms of superintelligence are not just about having some malevolent
[00:46:28] SPEAKER_00: paperclip are out there, but it just like, this is a really powerful thing.
[00:46:32] SPEAKER_00: And we don't even know how to conceptualize how people interact with it, what people will do with it.
[00:46:36] SPEAKER_00: And having gradual access to it seems like a
[00:46:38] SPEAKER_00: better way to maybe spread out the impact of it and help people prefer for it.
[00:46:45] SPEAKER_00: Well, I think on this point, even in the stretch of scenario, you would still do a gradual release of it.
[00:46:54] It's how I would imagine it. The gradualism would be an inherent component of any plan.
[00:47:02] SPEAKER_01: It's just a question of what is the first thing that you get out of the door? That's number one.
[00:47:06] SPEAKER_01: Number two, I also think, you know, I believe you have advocated for
[00:47:11] SPEAKER_01: continuing learning more than other people. And I actually think that this is an important
[00:47:17] SPEAKER_01: and correct thing. And here is why. So one of the things, so I'll give you another example
[00:47:25] SPEAKER_01: of how thinking, how language affects thinking. And in this case, it will be two words,
[00:47:32] two words that have shaped everyone's thinking I maintain.
[00:47:37] SPEAKER_01: First word, AGI. Second word, pre-training. Let me explain.
[00:47:44] So the word, the term AGI, why does this term exist? It's a very particular term. Why does it exist?
[00:47:52] SPEAKER_01: There's a reason. The reason that the term AGI exists is in my opinion, not so much because it's
[00:47:59] SPEAKER_01: like a very important essential descriptor of some end state of intelligence.
[00:48:06] SPEAKER_01: But because it is a reaction to a different term that existed in the term is narrow AGI.
[00:48:16] SPEAKER_01: If you go back to ancient history of game plan AGI, of checker's AGI,
[00:48:22] SPEAKER_01: chess AGI, computer games AGI, everyone would say, look at this narrow intelligence.
[00:48:27] SPEAKER_01: Sure, the chess AGI can be a bit cuspere of, but it can't do anything else. It is so narrow
[00:48:32] SPEAKER_01: artificial narrow intelligence. So in response, as a reaction to this, some people said,
[00:48:39] well, this is not good, it is so narrow. What we need is generally AGI. Generally AGI that can
[00:48:47] SPEAKER_01: just do all the things. The second and that term just got a lot of traction. The second thing that
[00:48:57] got a lot of traction is pre-training. Specifically, the recipe of pre-training. I think the current,
[00:49:04] SPEAKER_01: the way people do RL now is maybe undoing the conceptual imprint of pre-training, but pre-training
[00:49:13] SPEAKER_01: had the property. You do more pre-training and the model gets better at everything, more or less
[00:49:19] SPEAKER_01: uniformly. Generally AGI. Pre-training gives AGI. But the thing that happened with AGI and pre-training
[00:49:32] SPEAKER_01: is that in some sense, the overshot the target. Because by the kind, if you think about the term AGI,
[00:49:40] you will realize, and especially in the context of pre-training, you will realize that a human being
[00:49:45] SPEAKER_01: is not an AGI. Because a human being, yes, there is definitely a foundation of skills,
[00:49:53] a human being, a human being lacks a huge amount of knowledge. Instead, we rely on continued learning.
[00:50:03] We rely on continued learning. And so then when you think about, okay, so let's suppose that we
[00:50:07] SPEAKER_01: achieve success and we produce a safe super intelligence. The question is, but how do you define it
[00:50:14] SPEAKER_01: where on the curve of continued learning is going to be? I would produce like a super intelligent
[00:50:20] SPEAKER_01: 15-year-old that's very eager to go and you say, okay, I'm going to, they don't know very much at all,
[00:50:25] SPEAKER_01: the great student, very eager. You go and be a programmer, you go and be a doctor,
[00:50:31] go and learn. So you could imagine that the deployment itself will involve some kind of a learning
[00:50:36] SPEAKER_01: trial and error period. It's a process. As opposed to you drop the finished thing.
[00:50:43] Okay, I see. So you're suggesting that the thing you're pointing out with super intelligence
[00:50:50] SPEAKER_00: is not some finished mind, which knows how to do every single job in the economy. Because the way
[00:51:00] SPEAKER_00: the original, I think, opening a charter or whatever defines AGI is like, it can do every single
[00:51:05] SPEAKER_00: job that every single thing a human can do. You're proposing instead a mind which can learn to
[00:51:12] SPEAKER_00: do every single job. Yes. And that is super intelligence. But once you have the learning algorithm,
[00:51:19] it gets deployed into the world the same way a human laborer or my joint in organization.
[00:51:24] SPEAKER_00: And it seems like one of these two things might happen, maybe neither of these happens. One,
[00:51:31] SPEAKER_00: this super efficient learning algorithm becomes super human, becomes as good as you, and potentially
[00:51:39] SPEAKER_00: even better at the task of ML research. And as a result, the algorithm itself becomes more
[00:51:46] SPEAKER_00: and more super human. The other is even if that doesn't happen, if you have a single model,
[00:51:51] SPEAKER_00: I mean, this is explicit your vision. If you have a single model, where instances of a model,
[00:51:56] SPEAKER_00: which are deployed through the economy, doing different jobs, learning how to do those jobs,
[00:52:00] SPEAKER_00: continually learning on the job, picking up all the skills that any human could pick up,
[00:52:05] SPEAKER_00: but actually picking them all up at the same time and then amalgamating the learnings.
[00:52:10] You basically have a model which functionally becomes super intelligent,
[00:52:14] even without any sort of recursive self-improvement in software. Because you now have one model
[00:52:20] SPEAKER_00: that can do every single job in the economy, and humans can't merge our minds in the same way.
[00:52:24] SPEAKER_00: And so do you expect some sort of intelligence explosion from broad deployment?
[00:52:28] SPEAKER_00: I think that it is likely that we will have rapid economic growth.
[00:52:35] I think the broad deployment, like there are two arguments you could make, which are conflicting.
[00:52:46] One is that look, if indeed you get, once indeed you get to a point where you have an AI that can
[00:52:54] SPEAKER_01: learn to do things quickly and you have many of them, then they will then, they will be a strong
[00:53:04] SPEAKER_01: force to deploy them in the economy unless there will be some kind of a regulation that stops it,
[00:53:10] SPEAKER_01: which by the way there might be. But I think the idea of very rapid economic growth for some time,
[00:53:19] SPEAKER_01: I think it's very possible from broad deployment. The other question is how rapid it's going to be.
[00:53:25] So I think this is hard to know because on the one hand you have this very efficient worker,
[00:53:30] SPEAKER_01: on the other hand, the world is just really big and there's a lot of stuff.
[00:53:36] And that stuff moves at a different speed. But then on the other hand, now the AI could,
[00:53:41] SPEAKER_01: so I think very rapid economic growth is possible. And we will see all kinds of things like
[00:53:47] different countries, different rules and the ones which have the familiar rules,
[00:53:50] SPEAKER_01: the economic growth will be faster, hard to predict.
[00:53:54] Some people in our audience like to read the transcripts instead of listening to the episode.
[00:53:58] SPEAKER_00: And so we put a ton of effort into making the transcripts read like they are standalone essays.
[00:54:04] SPEAKER_00: The problem is that if you just transcribe a conversation verbatim using a speech-to-text model,
[00:54:09] SPEAKER_00: it'll be full of all kinds of fits and starts and confusing phrasing. We mentioned this problem
[00:54:14] SPEAKER_00: to label locks and they asked if they could take a staff. Working with them on this is probably the
[00:54:18] SPEAKER_00: reason that I'm most excited to recommend label box to people. It wasn't just, oh hey tell us what
[00:54:23] SPEAKER_00: kind of data you need and we'll go get it. They walked us through the entire process from
[00:54:27] SPEAKER_00: helping us identify what kind of data we needed in the first place to assembling a team of expert
[00:54:32] SPEAKER_00: aligners to generate it. Even after we got all the data back, label box stayed involved. They
[00:54:38] SPEAKER_00: helped us choose the right base model and set up Auto QA on the model's output so they could tweak
[00:54:43] SPEAKER_00: and refine it. And now we have a new transcribe or tool that we can use for all our episodes moving
[00:54:49] SPEAKER_00: forward. This is just one example of how label box meets their customers at the ideas level
[00:54:54] SPEAKER_00: and partners with them through their entire journey. If you want to learn more or if you want to
[00:54:58] SPEAKER_00: try out the transcribe or tool yourself, go to labelbox.com slash barcash.
[00:55:07] It seems to me that this is a very precarious situation to be in where
[00:55:13] SPEAKER_00: looking to limit we know that this should be possible because if you have something that is
[00:55:17] SPEAKER_00: as good as a human at learning but which can merge its brains, merge their different instances in
[00:55:23] SPEAKER_00: a way that humans can't merge. Already, this seems like a thing that should physically be possible.
[00:55:28] SPEAKER_00: Humans are possible, digital computers are possible. You just need both of those combined to produce
[00:55:32] SPEAKER_00: this thing. And it also seems like this kind of thing is extremely powerful and economic growth
[00:55:42] SPEAKER_00: is one way to put it. I mean, Dyson Spears is a lot of economic growth. But another way to put it
[00:55:46] SPEAKER_00: is just like you will have potentially a very short period of time because a human on the job,
[00:55:51] SPEAKER_00: and you're hired people to SSI, and six months they're like net productive probably, right? A
[00:55:56] SPEAKER_00: human learns really fast. And so this thing is becoming smarter and smarter very fast.
[00:56:00] SPEAKER_00: What is how do you think about making that go well? And why is SSI positioned to do that well?
[00:56:05] SPEAKER_00: What is SSI's plan there basically is what I'm trying to ask.
[00:56:07] SPEAKER_00: Yeah. So one of the ways in which my thinking has been changing,
[00:56:16] SPEAKER_01: is that I now place more importance on AI being deployed incrementally and in advance.
[00:56:30] SPEAKER_01: One very difficult thing about AI is that we are talking about systems that don't yet exist.
[00:56:40] And it's hard to imagine them. I think that one of the things that's happening,
[00:56:46] is that in practice, it's very hard to feel the AI. It's very hard to feel the AI.
[00:56:54] We can talk about it, but it's like talking about the long few, like imagine, like having a
[00:57:01] SPEAKER_01: conversation about how is it like to be old when you're like old and frail and you can have
[00:57:08] SPEAKER_01: a conversation, you can try to imagine it, but it's just hard and you come back to reality,
[00:57:13] SPEAKER_01: well that's not the case. And I think that a lot of the issues around
[00:57:21] AGI and its future power stem from the fact that it's very difficult to imagine.
[00:57:30] SPEAKER_01: Future AI is going to be different. It's going to be powerful. Indeed, the whole problem,
[00:57:37] SPEAKER_01: what is the problem of AI and AGI? The whole problem is the power.
[00:57:43] The whole problem is the power. When the power is really big, what's going to happen?
[00:57:50] And one of the ways in which I've changed my mind over the past year and so that change of mind may
[00:57:58] SPEAKER_01: back, may, I'll say, I'll hedge a little bit, may back propagate into the plans of our company,
[00:58:05] SPEAKER_01: is that, so if it's hard to imagine, what do you do? You got to be showing the thing.
[00:58:14] SPEAKER_01: You got to be showing the thing. And I maintain that. I think most people who work on AI
[00:58:20] SPEAKER_01: also can't imagine it because it's too different from what people see on a day-to-day basis.
[00:58:28] I do maintain, here's something which I predict will happen. That's a prediction.
[00:58:34] I maintain that as AI becomes more powerful than people will change their behaviors.
[00:58:45] And we will see all kinds of unprecedented things which are not happening right now.
[00:58:51] And I'll give some examples. I think for better or worse, the frontier companies will play a
[00:58:58] SPEAKER_01: very important role in what happens as will the government. And the kind of things that I think
[00:59:03] SPEAKER_01: you'll see, which you see the beginnings of, companies that are fierce competitors starting
[00:59:11] SPEAKER_01: collaborative to collaborate on AI safety. You may have seen Open AI and Anthropic
[00:59:18] SPEAKER_01: doing a first small step, but that did not exist. That's actually something which I predicted in one
[00:59:24] SPEAKER_01: of my talks about three years ago. That's such a thing will happen. I also maintain that as AI
[00:59:30] SPEAKER_01: continues to become more visibly powerful, there will also be a desire from governments and the
[00:59:39] SPEAKER_01: public to do something. And I think that this is a very important force of showing the AI. That's
[00:59:47] SPEAKER_01: number one. Number two, okay, so then the AI has been built. What needs to be done.
[00:59:55] So one thing that I maintain that will happen is that right now people who are working on AI
[01:00:01] maintain that the AI doesn't feel powerful because of its mistakes. I do think that at some
[01:00:07] SPEAKER_01: point the AI will start to feel powerful actually. And I think when that happens, we will see a big
[01:00:12] SPEAKER_01: change in the way all AI companies approach safety. They'll become much more paranoid. I think I say
[01:00:22] SPEAKER_01: this as a prediction that we will see happen. We'll see if I'm right. But I think this is something
[01:00:28] SPEAKER_01: that will happen because they will see the AI becoming more powerful. Everything that's happening
[01:00:33] SPEAKER_01: right now I maintain is because people look at today's AI and it's hard to imagine the future AI.
[01:00:41] And there is a third thing which needs to happen. And I think this is this and I'm talking about it
[01:00:47] SPEAKER_01: in broader terms, not just from the perspective of SSI because you asked me about our company. But
[01:00:54] SPEAKER_01: the question is, okay, so then what should what should the companies aspire to build? What should
[01:00:58] SPEAKER_00: they aspire to build? And there has been one big idea that actually everyone has been locked in
[01:01:05] SPEAKER_01: locked into which is the self-improving AI. And why did it happen? Because there is fewer ideas
[01:01:13] SPEAKER_01: than companies. But I maintain that there is something that's better to build. And I think that
[01:01:19] SPEAKER_01: everyone will actually want that. It's like the AI that's robustly aligned to care about
[01:01:26] SPEAKER_01: sentient life specifically. I think in particular it will be there's a case to be made that it will be
[01:01:33] SPEAKER_01: easier to build an AI that cares about sentient life than an AI that cares about human life alone
[01:01:40] because the AI itself will be sentient. And if you think about things like mirror neurons and
[01:01:46] SPEAKER_01: human empathy for animals, which is, you know, you might argue it's not big enough, but it exists.
[01:01:53] SPEAKER_01: I think it's an emergent property from the fact that to be model others with the same circuit
[01:01:58] SPEAKER_01: that we used to model ourselves because that's the most efficient thing to do.
[01:02:03] So even if you got an AI to hear about sentient beings, and it's not actually clear to me that
[01:02:09] SPEAKER_00: that's what you should try to do if you solve the alignment, it would still be the case that most
[01:02:14] SPEAKER_00: sentient beings will be AI's. There will be trillions, eventually quadrillions of AI's. Humans
[01:02:19] SPEAKER_00: will be a very small fraction of sentient beings. So it's not clear to me if the goal is some kind
[01:02:26] SPEAKER_00: of human control over this future civilization that this is the best criterion. It's true. I think that
[01:02:38] it's possible it's not the best criterion. I'll say two things. I think that, say number one,
[01:02:45] SPEAKER_01: I think that if there, so I think that care for sentient life, I think there is merit to it. I think
[01:02:54] SPEAKER_01: it should be considered. I think that it will be helpful if there was some kind of a short list
[01:03:02] SPEAKER_01: of ideas that then the companies when they are in the situation could use. That's number two.
[01:03:10] SPEAKER_01: Number three, I think it would be really material helpful if the power of the most powerful
[01:03:16] SPEAKER_01: super intelligence was somehow capped because it would address a lot of these concerns.
[01:03:23] The question of how to do it, I'm not sure, but I think that would be material helpful when
[01:03:29] you're talking about really, really powerful systems. Before we continue the alignment discussion,
[01:03:35] SPEAKER_00: I want to double click on that. How much room is there at the top? How do you think about
[01:03:39] SPEAKER_00: super intelligence? Do you think, I mean, using this learning efficiency idea maybe is just extremely
[01:03:45] SPEAKER_00: fast at learning new skills or new knowledge? And does it just have a bigger pool of strategies? Is
[01:03:51] SPEAKER_00: there a single cohesive it in the center that's more powerful or bigger? And if so, do you imagine
[01:04:00] SPEAKER_00: that this will be sort of godlike in comparison to the rest of human civilization? Or does it just feel
[01:04:04] SPEAKER_00: like another agent or another cluster of agents? So this is an area of a different people
[01:04:10] SPEAKER_01: of different intuitions. I think it will be very powerful for sure. I think that what I think is
[01:04:17] SPEAKER_01: most likely to happen is that there will be multiple such AIs being created roughly at the same time.
[01:04:26] SPEAKER_01: I think that if the cluster is big enough, like if the cluster is literally continent sized,
[01:04:36] that thing could be really powerful indeed. Right? If you literally have a continent sized cluster,
[01:04:42] like those, those AIs can be very powerful. And I like, all I can tell you is that if you're talking
[01:04:49] SPEAKER_01: about extremely powerful AIs, like truly dramatically powerful, then yeah, it would be nice if they could
[01:04:55] SPEAKER_01: be restrained in some ways or if there was some kind of an agreement or something.
[01:05:03] Because I think that if you are saying hey, like, if you really, like what is the concern of
[01:05:10] SPEAKER_01: superintelligence? What is one way to explain the concern? If you imagine a system that is
[01:05:15] SPEAKER_01: sufficiently powerful, like really sufficiently powerful, and you could say, okay, you need to
[01:05:21] SPEAKER_01: do something sensible, like care for sentient life, let's say, in a very single-minded way.
[01:05:27] We might not like the results. That's really what it is. And so maybe by the way, the answer is that
[01:05:31] SPEAKER_01: you do not build a single, you do not build an RL agent in the usual sense. And actually, I'll point
[01:05:37] SPEAKER_01: several things out. I think human beings are a semi-relagient, you know, the pursue a reward,
[01:05:44] SPEAKER_01: and then the emotions or whatever make a styrofo of the reward we pursue a different reward.
[01:05:49] SPEAKER_01: The market is like a very short-sighted kind of agent. Evolution is the same. Evolution is very
[01:05:58] SPEAKER_01: intelligent in some ways, but very dumb in other ways. The government has been designed to be
[01:06:04] SPEAKER_01: an ever-ending fight between three parts, which has an effect. So I think things like this,
[01:06:11] another thing that makes this discussion difficult is that we are talking about systems that
[01:06:15] SPEAKER_01: don't exist, that we don't know how to build. That's the other thing. And that's actually my belief.
[01:06:21] SPEAKER_01: I think what people are doing right now will go some distance and then peter out. It will continue
[01:06:26] SPEAKER_01: to improve, but it will also not be it. So the it, we don't know how to build. And I think that a lot
[01:06:34] SPEAKER_01: a lot hinges on understanding reliable generalization. Now, say another thing, which is like,
[01:06:45] you know, one of the things that you could say is that it's your ability to learn human
[01:06:54] SPEAKER_01: values is fragile, then your ability to optimize them is fragile, you will actually learn to optimize
[01:06:59] SPEAKER_01: them. And then you can't you say, are these not all instances of unreliable generalization?
[01:07:06] Why is it that human beings appear to generalize so much better? What generalization was much
[01:07:11] SPEAKER_01: better? What would happen in this case? What would be the effect? But those we can't, we can't,
[01:07:15] SPEAKER_01: like those questions are right now still answerable. How does one think about what AI going well
[01:07:23] SPEAKER_00: looks like? Because I think you've scoped out how AI might evolve. We'll have these sort of
[01:07:27] SPEAKER_00: continual learning agents. AI will be very powerful. Maybe they will be many different
[01:07:32] SPEAKER_00: AI's. How do you think about lots of continent compute size intelligences going around? How
[01:07:40] SPEAKER_00: dangerous is that? How do we make that less dangerous? And how do we do that in a way that
[01:07:48] protects a equilibrium where there might be misaligned AI's out there and bad actors out there?
[01:07:56] So one reason why I liked the AI that cares for sentient life, and we can debate on whether it's
[01:08:01] SPEAKER_01: good or bad. But if the first end of these dramatic systems actually do care for, you know,
[01:08:14] love humanity or something, you know, care for sentient life, obviously this also needs to be
[01:08:18] SPEAKER_01: achieved. This needs to be achieved. So if this is achieved by the first end of those systems,
[01:08:27] then I can see it go well, at least for quite some time. And then there is the question of what
[01:08:34] SPEAKER_01: happens in the long run? What happens in the long run? How do you achieve a long run equilibrium?
[01:08:39] And I think that there, the reason answer as well, and I don't like this answer,
[01:08:46] SPEAKER_01: but it needs to be considered. In the long run, you might say, okay, so if you have a world where
[01:08:54] SPEAKER_01: powerfully has exist, in the short term, you could say, okay, you have universal high income,
[01:09:01] you have universal high income, and we all do it well. But we know that what do the Buddhists say?
[01:09:07] SPEAKER_01: Change is the only constant. And so things change, and there is some kind of government,
[01:09:12] SPEAKER_01: political structure thing, and it changes because these things have a shelf life.
[01:09:18] You know, some new government thing comes up and it functions and then after some time,
[01:09:22] SPEAKER_01: it stops functioning. That's something that we see happening all the time. And so I think that for
[01:09:28] SPEAKER_01: the long run equilibrium, one approach, you could say, okay, so maybe every person will have
[01:09:36] SPEAKER_01: an AI that will do their bidding. And that's good. And if that could be maintained indefinitely,
[01:09:42] SPEAKER_01: that's true. But the downside with that is, okay, so then the AI goes and like earns money for
[01:09:50] SPEAKER_01: for the person and you know, advocates for their needs in like the political sphere,
[01:09:55] SPEAKER_01: and maybe then writes a little report saying, okay, here's what I've done here is the situation,
[01:09:59] SPEAKER_01: and the person says, great, keep it up. But the person is no longer a participant.
[01:10:04] SPEAKER_01: And then you can say that's a precarious place to be in. But so I'm going to preface by saying,
[01:10:13] I don't like this solution, but it is a solution. And the solution is if people become part AI
[01:10:21] SPEAKER_01: with some kind of neural link plus plus, because what will happen as a result is that now the AI
[01:10:26] SPEAKER_01: understands something and we understand it too. Because now the understanding is transmitted wholesale.
[01:10:34] SPEAKER_01: So now if the AI is in some situation, now it's like you are involved in the situation yourself fully.
[01:10:41] And I think this is the answer to the equilibrium. I wonder if the fact that emotions which were
[01:10:49] SPEAKER_00: developed millions or in many cases billions of years ago in a totally different environment
[01:10:55] SPEAKER_00: are still guiding our actions so strongly is an example of alignment success
[01:11:03] SPEAKER_00: to maybe spell out what I mean. The brainstem has these, I don't know if it's more accurate to call
[01:11:11] SPEAKER_00: it a value function or reward function, but the brainstem has a directive where it's saying,
[01:11:15] SPEAKER_00: mate with somebody who's more successful. The cortex is the part that understands what does
[01:11:19] SPEAKER_00: success mean in the modern context. But the brainstem is able to align the cortex and say, however you
[01:11:26] SPEAKER_00: recognize success to be, and I'm not smarter than to understand what that is, you're still going to
[01:11:30] SPEAKER_00: pursue this directive. I think there is, so I think there's a more general point. I think it's
[01:11:36] SPEAKER_01: actually really mysterious how the brain encodes high level desires, sorry, how evolution encodes
[01:11:45] SPEAKER_01: high level desires. Like it's pretty easy to understand how evolution would endow us with the
[01:11:51] SPEAKER_01: desire for food that smells good. Because smell is a chemical. And so just pursue that chemical. It's
[01:11:58] SPEAKER_01: very easy to imagine such evolution doing such a thing. But evolution also has endowed us with all
[01:12:06] SPEAKER_01: these social desires. Like we really care about being seen positively by society, we care about
[01:12:13] SPEAKER_01: being in a good standing, we like all these social intuitions that we have, I feel strongly that
[01:12:20] SPEAKER_01: they're baked in. And I don't know how evolution did it because it's a high level concept that's
[01:12:27] SPEAKER_01: represented in the brain. Like what people think, like let's say you were like, you care about
[01:12:34] some social thing. It's not like a low level signal like smell. It's not something that
[01:12:42] SPEAKER_01: for which there is a sensor. Like the brain needs to do a lot of processing to piece to get
[01:12:46] SPEAKER_01: lots of bits of information to understand what's going on socially. And somehow evolution said,
[01:12:52] SPEAKER_01: that's what you should care about. How did it do it? And it did it quickly too. Because I think all
[01:12:58] SPEAKER_01: these sophisticated social things that we care about, I think they evolved pretty recently. So
[01:13:04] SPEAKER_01: evolution had an easy time, hard-coding this high level desire. And I maintain, or at least I'll say
[01:13:11] SPEAKER_01: I'm unaware of good hypothesis for how it's done. I had some ideas I was kicking around, but none of them,
[01:13:19] SPEAKER_01: are satisfying. Yeah. And what's especially impressive is it was a desire that you learned in your
[01:13:27] SPEAKER_00: lifetime. It kind of makes sense because your brain is intelligent. It makes sense why we would
[01:13:32] SPEAKER_00: learn intelligent desires. But your point is that the desire is, maybe this is not your point,
[01:13:38] SPEAKER_00: but one way to understand it is, the desire is built into the genome. And the genome is not
[01:13:43] SPEAKER_00: intelligent, right? But it's able to, you're somehow able to describe this feature that requires,
[01:13:48] SPEAKER_00: like it's not even clear how you define that feature. And you can get it into the, you can build it
[01:13:52] SPEAKER_00: into the genes. Yeah, essentially. Or maybe I'll put it differently. If you think about the tools
[01:13:57] SPEAKER_01: that are available to the genome, it says, okay, here's a recipe for building a brain. And you
[01:14:03] SPEAKER_01: could say, here is a recipe for connecting the dopamine neurons to like the smell sensor. Yeah.
[01:14:08] SPEAKER_01: And if the smell is a certain kind of, you know, good smell, you want to eat that. I could imagine
[01:14:13] SPEAKER_01: the genome doing that. I'm claiming that it is harder to imagine. It's harder to imagine the
[01:14:20] SPEAKER_01: genome saying you should care about some complicated computation that your entire brain, that like a
[01:14:26] SPEAKER_01: big chunk of your brain does. That's all I'm claiming. I can tell you like a speculation, I was
[01:14:32] SPEAKER_01: wondering how it could be done. And let me offer a speculation and I'll explain why the speculation
[01:14:36] SPEAKER_01: is probably false. So the speculation is, okay, so the brain, it's like the brain has those regions,
[01:14:45] SPEAKER_01: you know, the brain regions. We have our cortex, right? And it has all those brain regions.
[01:14:51] SPEAKER_01: And the cortex is uniform, but the brain regions and the neurons in the cortex, they kind of
[01:14:57] SPEAKER_01: speak to their neighbors mostly. And that's explained by your brain regions. Because if you want to
[01:15:01] do some kind of speech processing, all the neurons that do speech need to talk to each other. And because
[01:15:07] SPEAKER_01: neurons can only speak to their nearby neighbors for the most part, it has to be a region. All the
[01:15:11] regions are mostly located in the same place from person to person. So maybe evolution hard-coded
[01:15:16] SPEAKER_01: literally a location on the brain. So it says, oh, like when, when like, you know, the GPS of the
[01:15:25] SPEAKER_01: brain, GPS coordinates such and such, when that fire is, that's what you should care about. Like,
[01:15:28] SPEAKER_01: maybe that's what evolution did because that would be within the toolkit of evolution.
[01:15:33] Yeah, although there are examples where, for example, people who are born blind have that
[01:15:38] SPEAKER_00: area of their cortex adopted by another sense. And I have no idea, but I'd be surprised if
[01:15:48] SPEAKER_00: the desires or the reward functions which require a visual signal no longer worked, you know,
[01:15:56] SPEAKER_00: people who have their different areas of their cortex co-opted. For example, if you no longer have
[01:16:01] SPEAKER_00: vision, can you still feel the sense that I want people around me to like me and so forth?
[01:16:07] SPEAKER_00: Which usually there's also visual cues for. So actually fully agree with that. I think there's
[01:16:11] SPEAKER_01: an even stronger contra-argument of this theory, which is, like if you think about people, so there are
[01:16:17] SPEAKER_01: people who get half of their brain removed in childhood. And they still have all their brain
[01:16:24] SPEAKER_01: regions, but they all somehow move to just one hemisphere, which suggests that the brain regions,
[01:16:30] the location is not fixed. And so that theory is not true. It would have been cool if it was true,
[01:16:35] SPEAKER_01: but it's not. And so I think that's a mystery, but it's an interesting mystery. Like, the fact is,
[01:16:39] SPEAKER_01: somehow evolution was able to endow us to care about social stuff very, very reliably. And even
[01:16:47] SPEAKER_01: people who have like all kinds of strange mental conditions and deficiencies and emotional problems
[01:16:52] SPEAKER_01: tend to care about this also. AI tools like defects, voice clones, and agents have dramatically
[01:16:59] SPEAKER_00: increased the sophistication of fraud and abuse. So it's more important than ever to actually understand
[01:17:05] SPEAKER_00: the identity and intent of whoever or whatever is using your platform. That's exactly what Sardine
[01:17:12] SPEAKER_00: helps you do. Sardine brings together thousands of device behavior and identity signals to help
[01:17:18] SPEAKER_00: you assess risk. Everything from how a user types or moves their mouse or holds their device
[01:17:24] SPEAKER_00: to whether they're hiding their true location behind the VPN to whether they're injecting a fake
[01:17:30] SPEAKER_00: camera feed during KYC selfie checks. Sardine combines these signals with insights from their
[01:17:36] SPEAKER_00: network of almost four billion devices, things like a user's history of fraud or their associations
[01:17:41] SPEAKER_00: with other high risk accounts. So you can spot bad actors before they do damage. This would literally
[01:17:48] SPEAKER_00: be impossible if you only use data from your own application. Sardine doesn't stop a detection.
[01:17:53] SPEAKER_00: They offer suite of agents to streamline onboarding checks and automated investigations.
[01:17:58] SPEAKER_00: So as fraudsters use AI to scale their attacks, you can use AI to scale your defenses. Go to
[01:18:05] SPEAKER_00: sardine.ai slash twerkesh to learn more and download their guide on AI fraud detection.
[01:18:13] What is SSI planning on doing differently?
[01:18:16] SPEAKER_00: So presumably your plan is to be one of the frontier companies when this time arrives.
[01:18:21] And then what is presumably you started SSI because you're like I think I have a way of approaching
[01:18:28] SPEAKER_00: how to do this safely in a way that the other companies don't. What is that difference?
[01:18:34] So the way I would describe it as there are some ideas that I think are promising and I want to
[01:18:40] SPEAKER_01: investigate them and see if they are indeed promising or not. It's really that simple.
[01:18:45] SPEAKER_01: It's an attempt. I think that if the ideas are now to be correct, these ideas that we discussed
[01:18:51] SPEAKER_01: around understanding generalization. If these ideas turn out to be correct,
[01:19:00] then I think we will have something worthy. We'll leave it turn out to be correct. We are doing
[01:19:05] SPEAKER_01: research. We are squarely age of research company. We are making progress. We've actually made
[01:19:10] SPEAKER_01: quite good progress over the past year, but we need to keep making more progress. Yeah.
[01:19:14] SPEAKER_01: More research. And that's how I see it. I see it as an attempt to be an attempt to be a voice and
[01:19:24] SPEAKER_01: a participant. People have asked your co-founder and previously left to go to meta recently
[01:19:35] SPEAKER_00: and people have asked, well, if there was a lot of breakthroughs being made, that seems like a
[01:19:40] SPEAKER_00: thing that should have been unlikely. I wonder how you respond. Yeah. So for this, I will simply
[01:19:46] SPEAKER_01: remind a few facts that may have been forgotten. And I think these facts which provide the
[01:19:52] SPEAKER_01: context, I think they explain the situation. So the context was that we were fundraising at a 32-billion
[01:19:59] SPEAKER_01: valuation. And then meta came in and offered to acquire us. And I said no, but my former co-founder,
[01:20:13] I can some sense, said yes. And as a result, he also was able to enjoy from a lot of near-charm
[01:20:20] SPEAKER_01: liquidity. And he was the only person from SSI to join meta. It sounds like SSI's plan has to be
[01:20:26] SPEAKER_00: a company that is at the frontier when you get to this very important period in human history
[01:20:33] SPEAKER_00: where you have super human intelligence. And you have these ideas about how to make super human
[01:20:37] SPEAKER_00: intelligence go well. But other companies will be trying their own ideas. What distinguishes
[01:20:44] SPEAKER_00: SSI's approach to making super intelligence go well? The main thing that distinguishes SSI
[01:20:51] SPEAKER_01: is its technical approach. So we have a different technical approach that I think is worthy.
[01:20:59] And we are pursuing it. I maintain that in the end, there will be a convergence of strategies.
[01:21:05] SPEAKER_01: So I think there will be a convergence of strategies where at some point, as AI becomes more powerful,
[01:21:14] it's going to become more or less clearer to everyone what the strategy should be. And it should
[01:21:19] be something like, yeah, you need to find some way to talk to each other. And you want your first
[01:21:26] actual, like real super intelligent AI to be aligned and somehow be
[01:21:35] you know, careful sentient life, careful people, democratic, one of those, some combination of
[01:21:42] SPEAKER_01: and I think this is the condition that everyone should strive for. And that's what SSI's striving for.
[01:21:52] And I think that this time, if not already, all the other companies will realize that they're
[01:21:58] SPEAKER_01: striving towards the same thing. And we'll see, I think that the world will truly change as
[01:22:02] SPEAKER_01: AI becomes more powerful. And I think a lot of these forecasts will, like, I think things will be
[01:22:08] SPEAKER_01: really different. And people will be acting really differently. What is speaking of forecast?
[01:22:12] What are your forecasts to this system you're describing, which can learn as well as a human? And
[01:22:20] SPEAKER_00: it's absolutely as a result becomes super human. I think like, five to 20 years. So I just want to
[01:22:29] SPEAKER_00: unroll your how you might see the world coming. It's like, we have a couple more years where
[01:22:35] SPEAKER_00: these other companies are continuing the current approach and it stalls out and stalls out here
[01:22:40] SPEAKER_00: meaning they earn no more than low hundreds of billions in revenue. How do you think about what
[01:22:45] SPEAKER_00: stalling all means? Yeah. I think they're, I think it could, I think it could stall out and
[01:22:53] I think stalling out will look like it will all look very similar. Yeah. Among all the different
[01:22:59] SPEAKER_01: companies, something like this. I'm not sure because I think, I think, I think even with, I think,
[01:23:04] SPEAKER_01: I think even, I think even with stalling out, I think this company's could make a stupendous,
[01:23:08] SPEAKER_01: stupendous revenue, maybe not profits because they will be, it will be, they will need to work hard
[01:23:14] SPEAKER_01: to differentiate each other from themselves, but revenue definitely. But there's something in your
[01:23:20] SPEAKER_00: model implies that when the correct solution does emerge, there will be convergence between all the
[01:23:26] SPEAKER_00: companies. And I'm curious why you think that's the case. Well, I was talking more about convergence
[01:23:31] SPEAKER_00: on their largest strategies. I think eventual convergence on the technical approach is probably
[01:23:35] SPEAKER_01: going to happen as well. But I was alluding to convergence to their largest strategies. So what,
[01:23:40] SPEAKER_01: what exactly is the thing that should be done? I just want to better understand how you see the
[01:23:45] SPEAKER_00: future and rolling. So currently we have these different companies and you expect their approach to
[01:23:49] SPEAKER_00: continue generating revenue. Yes. But not get to this human-like learner. Yes. So now we have these
[01:23:54] SPEAKER_00: different forks of companies. We have you, we have thinking machines, there's a bunch of other labs.
[01:23:59] SPEAKER_00: Yes. And maybe one of them figures out the correct approach. But then the release of the product
[01:24:04] SPEAKER_00: makes it clear to other people how to do this thing. I think it won't be clear how to do it thing,
[01:24:10] SPEAKER_01: but it can be clear that something different is possible. Right. And that is information.
[01:24:14] SPEAKER_01: And I think people will do, then be trying to figure out how that's how that works. I do think,
[01:24:20] SPEAKER_01: though, that one of the things that's that I think, you know, not addressed here, you know,
[01:24:27] SPEAKER_01: discussed is that with each increase in the AI capabilities, I think there will be some kind of
[01:24:35] SPEAKER_01: changes, but I don't know exactly which ones in how things are being done. And so like,
[01:24:42] SPEAKER_01: I think it's going to be important yet I can't spell out what that is exactly. And how, how are the,
[01:24:50] by default, you would expect the company that has the model company that has that model to be
[01:24:54] SPEAKER_00: getting all these gains because they have the model that is learning how to do all,
[01:24:58] SPEAKER_00: has the skills and knowledge that it's building up in the world. What is the reason to think that
[01:25:03] SPEAKER_00: the benefits of that will be widely distributed and not just end up at whatever model company gets
[01:25:08] SPEAKER_00: this continuous learning loop going first? Like, I think that empirically what happened. So here,
[01:25:14] here is what I think is going to happen. Number one, I think empirically when let's, let's look at
[01:25:24] SPEAKER_01: how things have gone so far with the AI's of the past. So one company produced an advance
[01:25:31] and the other company scrambled and produced some, some similar things after some a lot of time
[01:25:39] SPEAKER_01: and they started to compete in the market and push their, push the prices down. And so I think
[01:25:46] SPEAKER_01: from the market perspective, I think something similar will happen there as well, even if someone,
[01:25:51] SPEAKER_01: it's okay, we are talking about the good world, by the way, where, what's the good world?
[01:25:58] SPEAKER_01: What's the good world? Where we have these powerful human-like learners that are also like,
[01:26:07] SPEAKER_01: and by the way, maybe there's another thing we haven't discussed on the, on the spec of the
[01:26:12] SPEAKER_01: super intelligent AI that I think is worth considering is that you make it narrow,
[01:26:18] SPEAKER_01: it can be useful in narrow at the same time. So you can have lots of narrow super intelligent AI's.
[01:26:23] SPEAKER_01: But suppose you have many of them and you have some company that's producing a lot of profits
[01:26:33] SPEAKER_01: from it and then you have another company that comes in and starts to compete and the
[01:26:38] SPEAKER_01: competitor competition is going to work through specialization. I think what's going to happen is
[01:26:44] SPEAKER_01: that the way competition, like competition, loves specialization and you see it in the market,
[01:26:52] SPEAKER_01: you see it in evolution as well. So you're going to have lots of different niches and you're
[01:26:55] SPEAKER_01: going to have lots of different companies who are occupying different niches in this kind of
[01:27:02] world. We would say, yeah, one AI company is really quite a bit better at some area of really
[01:27:09] SPEAKER_01: complicated economic activity and a different company is better at another area and the third
[01:27:13] SPEAKER_01: company is really good at litigation. So it's not really done by what human learning implies,
[01:27:18] SPEAKER_00: is that like it can learn. It can, but, but you have accumulated learning, you have a big investment,
[01:27:24] SPEAKER_01: you spent a lot of compute to become really, really, really good, really phenomenal of this thing.
[01:27:30] SPEAKER_01: And someone else spent a huge amount of compute and a huge amount of experience to get really,
[01:27:34] SPEAKER_01: really good at some other thing. Right. You apply a lot of human learning to get there, but now,
[01:27:38] SPEAKER_01: like you are at this high point where someone else would say, look, like, I don't want to start
[01:27:44] SPEAKER_01: learning what you've learned. That would require many different companies to begin at the human,
[01:27:49] SPEAKER_00: like, continue learning agent at the same time so that they can start their different research
[01:27:55] SPEAKER_00: in different branches. But if one company, you know, gets that agent first or gets that learner first,
[01:28:04] it does then seem like, well, you know, they, like, if you just think about every single job in the
[01:28:10] SPEAKER_00: economy, you just have an instance learning each one seems tractable for companies. Yeah. That's
[01:28:17] SPEAKER_00: that's that's that's a valid argument. My my strong intuition is that it's not how it's going to go.
[01:28:24] My strong intuition is that yeah, like the argument says it will go this way. Yeah.
[01:28:28] SPEAKER_01: But my strong intuition is that it will not go this way. That this is the
[01:28:33] you know, in in theory, there is no difference between theory and practice and practice
[01:28:37] SPEAKER_01: the reason. I think that's going to be one of those. A lot of people's models of recursive self
[01:28:41] SPEAKER_00: improvement, literally explicitly state, we will have a million Ilias in a server that are coming
[01:28:47] SPEAKER_00: up with different ideas. And this will lead to a super intelligence emerging very fast. Do you
[01:28:51] SPEAKER_00: have some intuition about how parallelizable the thing you are doing is? How how how what are the
[01:28:57] SPEAKER_00: gains from making copies of alia? I don't know. I think I think they'll definitely be a
[01:29:06] SPEAKER_01: there'll be diminution returns because you want you want people who think differently rather than
[01:29:10] SPEAKER_01: the same. I think that if they were literal copies of me, I'm not sure how much more incremental
[01:29:15] SPEAKER_01: value you'd get. I think that but people who think differently, that's what you want. Why is it
[01:29:23] SPEAKER_00: that it's been if you look at different models, even released by totally different companies trained
[01:29:28] SPEAKER_00: on potentially non overlapping data sets? It's actually crazy how similar LLMs are to each other.
[01:29:35] SPEAKER_00: Maybe the data system is not as non overlapping as it seems. But there's there's some sense that
[01:29:41] SPEAKER_00: it's like even if an individual human might be less productive than the future AI, maybe there's
[01:29:45] SPEAKER_00: something to the fact that human teams have more diversity than teams of the eyes might have. But
[01:29:49] SPEAKER_00: how do we elicit meaningful diversity among AI? So I think just raising that temperature just
[01:29:55] SPEAKER_00: to result in gibberish. I think you want something more like different scientists have different
[01:29:59] SPEAKER_00: different prejudices or different ideas. How do you get that kind of diversity among AI agents?
[01:30:04] SPEAKER_00: So the reason there has been no diversity, I believe, is because of pre-training.
[01:30:10] All the pre-trained models are the same. Pretty much because the pre-trained on the same data.
[01:30:16] SPEAKER_01: Now, our rail and post-training is where some differentiation starts to emerge because different
[01:30:21] SPEAKER_01: people come up with different RL training. Yeah. And then I've heard you hint in the past about
[01:30:28] SPEAKER_00: self-play as a way to either get data or match agents to other agent equivalent intelligence
[01:30:35] SPEAKER_00: to kickoff learning. How should we think about why there's no public proposals of this kind of
[01:30:45] SPEAKER_00: thinking working with other ones? I would say there are two things to say. I would say that the
[01:30:50] SPEAKER_01: reason why I thought self-play was interesting is because it offered the way to create models
[01:30:57] SPEAKER_01: using compute only without data. Right? And if you think that data is the ultimate bottleneck,
[01:31:03] SPEAKER_01: then using compute only is very interesting. So that's what makes it interesting. Now, the
[01:31:09] SPEAKER_01: the thing is that self-play, at least the way it was done in the past, when you have agents,
[01:31:18] SPEAKER_01: which are somehow compete with each other, it's only good for developing a certain set of skills.
[01:31:23] SPEAKER_01: It is too narrow. It's only good for negotiation, conflict, certain social skills,
[01:31:32] SPEAKER_01: strategizing that kind of stuff. And so if you care about those skills, then self-play will be useful.
[01:31:38] SPEAKER_01: Now, actually, I think that self-play did find a home, but just in a different form.
[01:31:46] SPEAKER_01: In a different form. So things like debate, prove a very fire. You have some kind of an LLM as a
[01:31:54] SPEAKER_01: judge, which is also incentivized to find mistakes in your work. You could say this is not exactly
[01:31:59] SPEAKER_01: self-play, but this is a related adversarial setup that people are doing, I believe. And really
[01:32:04] SPEAKER_01: self-lays an example of a special case of more general like competition between agents.
[01:32:12] SPEAKER_01: The response, the natural response, the competition is to try to be different. And so if you were to
[01:32:17] SPEAKER_01: put multiple agents and you tell them, you know, you all need to work on some problem, and you are an
[01:32:22] agent, and you're inspecting what everyone else is working, you're going to say, well, if they
[01:32:27] SPEAKER_01: already taken this approach, it's not clear I should pursue it. I should pursue something
[01:32:32] SPEAKER_01: differentiated. And so I think that something like this could also create an incentive for
[01:32:37] SPEAKER_01: a diversity of approaches. Yeah. Final question. What is research taste? You're obviously
[01:32:46] the person in the world who is considered to have the best taste in doing research in AI. You were
[01:32:55] SPEAKER_00: the co-author on many of the biggest, the biggest things that have happened in the history of
[01:33:02] SPEAKER_00: deep learning from Alex and that to GPT-3 and so on. What is it that how do you characterize how
[01:33:09] you come up with these ideas? I can answer. So I can comment on this for myself. I think
[01:33:14] SPEAKER_01: different people do it differently. But one thing that guides me personally is
[01:33:24] an aesthetic of how AI should be by thinking about how people are, but thinking correctly.
[01:33:32] SPEAKER_01: Like it's very easy to think about how people are incorrectly. But what does it mean to think
[01:33:37] SPEAKER_01: about people correctly? So I'll give you some examples. The idea of the artificial neuron
[01:33:45] is directly inspired by the brain and it's a great idea. Why? Because you say, sure, the brain has
[01:33:50] SPEAKER_01: all these different organs. It has the faults, but the faults probably don't matter. Why do we
[01:33:55] SPEAKER_01: think that the neurons matter? Because there is many of them. It kind of feels right? So you want
[01:34:00] SPEAKER_01: the neuron. You want some kind of local learning rule that will change the connections. You want
[01:34:04] SPEAKER_01: some local learning rule that will change the connections between the neurons. It feels plausible
[01:34:10] SPEAKER_01: that the brain does it. The idea of the distributed representation. The idea that the brain,
[01:34:16] SPEAKER_01: the brain responds to experience. The neuron that should learn from experience. The brain learns from
[01:34:22] SPEAKER_01: experience. The neuron that you learn from experience. And you kind of ask yourself, is something
[01:34:28] SPEAKER_01: fundamental or not fundamental? How things should be? And I think that's been guiding me a
[01:34:34] SPEAKER_01: fair bit, kind of thinking from multiple angles and looking for almost beauty, beauty, simplicity,
[01:34:41] SPEAKER_01: ugliness. There's no room for ugliness. It's just beauty, simplicity, elegance, correct
[01:34:46] SPEAKER_01: inspiration from the brain. And all of those things need to be present at the same time. And the more
[01:34:51] SPEAKER_01: they are present, the more confident you can be in a top-down belief. And then the top-down belief
[01:34:57] SPEAKER_01: is the thing that sustains you when the experiments contradict you. Because if you just trust the data
[01:35:03] SPEAKER_01: all the time, well, sometimes you can be doing a correct thing, but there's a bug. But you don't
[01:35:07] know the reason about. How can you tell that there is a bug? How do you know if you should keep
[01:35:11] SPEAKER_01: debugging or you conclude it's the wrong direction? Well, is the top-down? Well, how should you
[01:35:16] SPEAKER_01: can say the things have to be this way? Something like this has to work. Therefore, we gotta keep going.
[01:35:22] SPEAKER_01: That's the top-down. And it's based on this like multifaceted beauty and inspiration by the brain.
[01:35:29] All right. We'll leave it there. Thank you so much. Thank you so much.
[01:35:33] Oh! All right. Appreciate it. That was great. Yeah. I enjoyed it. Yes. Me too. Hey, everybody.
[01:35:39] I hope you enjoyed that episode. If you did, the most helpful thing you can do is just share it with
[01:35:44] SPEAKER_00: other people who you think might enjoy it. It's also helpful if you leave a rating or comment on
[01:35:50] SPEAKER_00: whatever platform you're listening on. If you're interested in sponsoring the podcast,
[01:35:54] SPEAKER_00: you can reach out at twerkash.com slash advertise. Otherwise, I'll see you on the next one.