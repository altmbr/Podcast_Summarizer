[00:00:00] **Andrej Karpathy:** When personal learning is terrible, it just so happens that everything that we had before
[00:00:04] is much worse.
[00:00:06] I'm actually optimistic.
[00:00:07] I think this will work.
[00:00:08] I think it's tractable.
[00:00:09] I'm only sounding pessimistic because when I go on my Twitter timeline, I see all this
[00:00:13] stuff that makes no sense to me.
[00:00:14] A lot of it is, I think, honestly, just fundraising.
[00:00:17] We're not actually building animals.
[00:00:18] We're building ghosts.
[00:00:20] These lack sort of ethereal spirit entities because they're fully digital and they're
[00:00:23] kind of like minicking humans and it's a different kind of intelligence.
[00:00:26] It's business as usual because we're in an intelligence explosion already and have
[00:00:30] been for decades.
[00:00:31] Everything is gradually being automated.
[00:00:33] Has been for hundreds of years.
[00:00:34] Don't write blog posts.
[00:00:35] Don't do slides.
[00:00:36] Don't do any of that.
[00:00:37] Like build a code, arrange it, get it to work.
[00:00:39] It's the only way to go.
[00:00:40] Otherwise, you're missing knowledge.
[00:00:41] If you have a perfect AI tutor, maybe you can get extremely far.
[00:00:44] The geniuses of today, I bear a discussion in the surface of what a human mind can do, I
[00:00:47] think.
[00:00:48] **Dwarkesh Patel:** Today, I'm speaking with Andre Carpati.
[00:00:51] Andre, why do you say that this will be the decade of agents and not the year of agents?
[00:00:55] **Andrej Karpathy:** Well, first of all, thank you for having me here.
[00:00:57] I'm excited to be here.
[00:01:00] The quote that you just mentioned, it's the decade of agents.
[00:01:02] That's actually a reaction to an existing pre-existing quote, I should say, where I think
[00:01:06] a lot of some of the labs, I'm not actually sure who said this, but they were alluding
[00:01:09] to this being the year of agents with respect to LLMs and how they were going to evolve.
[00:01:14] I think I was triggered by that because I feel like there's some over-predictions going
[00:01:19] on in the industry.
[00:01:21] In my mind, this is really a lot more accurately described as the decade of agents.
[00:01:25] We have some very early agents that are actually extremely impressive and that I use daily,
[00:01:30] cloud and codecs and so on, but I still feel like there's so much work to be done.
[00:01:34] I think my reaction is, we'll be working with these things for a decade.
[00:01:38] They're going to get better and it's going to be wonderful, but I think I was just reacting
[00:01:42] to the timeline, I suppose, of the implication.
[00:01:47] **Dwarkesh Patel:** What do you think we'll take a decade to accomplish?
[00:01:49] Where are the bottlenecks?
[00:01:51] **Andrej Karpathy:** Actually, make it work.
[00:01:52] In my mind, when you're talking about an agent, I guess, or what the labs have in mind
[00:01:56] and what maybe I have in mind as well, you should think of it almost like an employee
[00:01:59] or like an intern that you would hire to work with you.
[00:02:02] For example, you work with some employees here.
[00:02:04] When would you prefer to have an agent like cloud or codecs do that work?
[00:02:08] Currently, of course, they can't.
[00:02:09] What would it take for them to be able to do that?
[00:02:11] Why don't you do it today?
[00:02:13] The reason you don't do it today is because they just don't work.
[00:02:16] They don't have enough intelligence.
[00:02:17] They're not multimodal enough.
[00:02:18] They can do computer use and all this kind of stuff.
[00:02:20] They don't do a lot of the things that you've alluded to earlier.
[00:02:23] They don't have continual learning.
[00:02:24] You can't just tell them something and they'll remember it.
[00:02:27] They're just cognitively lacking and it's just not working.
[00:02:30] I just think that it will take about a decade to work through all of those issues.
[00:02:32] **Dwarkesh Patel:** Interesting.
[00:02:33] As a professional podcaster, a viewer of AI from afar, it's easy to identify for me.
[00:02:43] Here's what's lacking.
[00:02:44] Continual learning is lacking or multimodality is lacking.
[00:02:47] I don't really have a good way of trying to put a timeline on it.
[00:02:52] If somebody's like, how long will continue learning take?
[00:02:55] There's no prior I have, but this is a project that's just like five years, 10 years, 50 years.
[00:03:01] Why a decade?
[00:03:02] Why not one year?
[00:03:03] Why not 50 years?
[00:03:04] **Andrej Karpathy:** I guess this is where you get into a bit of my own intuition a little bit and also
[00:03:10] doing a bit of an extrapolation with respect to my own experience in the field.
[00:03:14] I guess I've been in AI for almost two decades.
[00:03:17] It's going to be maybe 15 years or so, not that long.
[00:03:20] You had Richard Sutton here who was all around, of course, for much longer, but I do have
[00:03:23] about 15 years of experience of people making predictions of seeing how they actually turned
[00:03:27] out.
[00:03:28] Also, I was in the industry for a while and I was in research and I worked in the industry
[00:03:31] for a while.
[00:03:32] I guess I have just a general intuition that I have left from that.
[00:03:37] I feel like the problems are tractable.
[00:03:40] They're surmountable, but they're still difficult.
[00:03:44] If I just average it out, it just feels like a decade, I guess, to me.
[00:03:47] **Dwarkesh Patel:** This is actually quite interesting.
[00:03:48] I want to hear not only the history, but what people in the room felt was about to happen
[00:03:54] at various different breakthrough moments.
[00:03:58] What were the ways in which their feelings were either overly pessimistic or overly optimistic?
[00:04:03] Should we just go through each other one by one?
[00:04:05] **Andrej Karpathy:** That's a giant question because, of course, you're talking about 15 years of stuff that
[00:04:08] happened.
[00:04:09] AI is actually so wonderful because there have been a number of seismic shifts that were
[00:04:13] like the entire feel has suddenly looked a different way.
[00:04:16] I guess I've maybe lived through two or three of those and I still think there will continue
[00:04:20] to be some because they come with some almost surprising irregularity.
[00:04:25] When my career began, of course, when I started to work on deep learning, when I became interested
[00:04:29] in deep learning, this was just by chance of being right next to Jeff Hinton at the University
[00:04:33] of Toronto and Jeff Hinton, of course, is the Godfather figure of AI.
[00:04:37] He was training all these neural networks and I thought it was incredible and interesting,
[00:04:40] but this was not the main thing that everyone in AI was doing by far.
[00:04:43] This was a niche little subject on the side.
[00:04:46] That's maybe the first dramatic seismic shift that came with the Alexander and so on.
[00:04:51] I would say, like, Alexander's re-oriented everyone and everyone started to train neural
[00:04:54] networks, but it was still very per-task, per-specific task, so maybe I have an image
[00:05:00] classifier or I have a neural machine translator or something like that.
[00:05:04] People became very slowly actually interested in basically agents, I would say, and people
[00:05:09] started to think, maybe we have a check mark next to the visual cortex or something like
[00:05:12] that, but what about the other parts of the brain and how can we get an actual full
[00:05:16] agent or an entity that can actually interact in the world?
[00:05:19] I would say the Atari deep reinforcement learning shift in 2013 or so was part of that
[00:05:25] early effort of agents, in my mind, because it was an attempt to try to get agents that
[00:05:29] not just perceive the world, but also take actions and interact and get rewards from environments.
[00:05:34] At the time, this was Atari Games.
[00:05:36] I kind of feel like that was a misstep, actually, and it was a misstep that actually, even the
[00:05:41] early OpenAI that I was a part of, of course, kind of adopted, because at that time, the
[00:05:45] side-guised was reinforcement learning environments, games, game playing, beat games, get lots of
[00:05:51] different types of games, and OpenAI was doing a lot of that.
[00:05:54] So that was maybe like another prominent part of, I would say, AI, where, maybe before
[00:05:59] two or three or four years, everyone was doing reinforcement learning in games.
[00:06:03] And basically, that was a little bit of a misstep.
[00:06:06] And what I was trying to do at OpenAI, actually, is like, I was always a little bit suspicious
[00:06:09] of games as being like this thing that would actually lead to AGI, because in my mind,
[00:06:13] you want something like an accountant or something that's actually interacting with the
[00:06:16] real world.
[00:06:17] And I just didn't see how games kind of like add up to it.
[00:06:20] And so my project at OpenAI, for example, was within the scope of the universe project
[00:06:26] on an agent that was using keyboard and mouse to operate web pages.
[00:06:30] And I really wanted to have something that, like, interacts with, you know, the actual
[00:06:33] digital world that can do knowledge work.
[00:06:35] And it just so turns out that this was extremely early, way too early.
[00:06:39] So early that the wish didn't have been working on that, you know, because if you're just
[00:06:43] stumbling your way around and keyboard mashing and masclicking and trying to get rewards
[00:06:48] in these environments, your reward is two sparse and you just won't learn, and you're
[00:06:52] going to burn a forest computing, and you're never actually going to get something off
[00:06:55] the ground.
[00:06:56] So what you're missing is this power of representation in the neural network.
[00:07:01] And so for example, today, people are training those computer using agents, but they're doing
[00:07:03] it on top of a large language model.
[00:07:05] And so you actually have to get the language model first, you have to get the representations
[00:07:08] first, and you do that by all the pre-training and all the LLM stuff.
[00:07:11] So I kind of feel like maybe loosely speaking, it was like, people keep maybe trying to get
[00:07:17] the full thing to early a few times, where people like really try to go after agents too
[00:07:21] early, I would say.
[00:07:22] It was Atari and Universe, and even my own experience.
[00:07:26] And you actually have to do some things first before you sort of get to those agents.
[00:07:30] And maybe now the agents are a lot more competent, but maybe we're still missing sort
[00:07:33] of some parts of that stack.
[00:07:36] But I would say maybe those are like the three major buckets of what people were doing,
[00:07:40] training neural nets, per tasks, trying to the first round of agents, and then maybe
[00:07:45] the LLM's and actually seeking the representation power of the neural networks before you tack
[00:07:49] on everything else on top.
[00:07:51] **Dwarkesh Patel:** Interesting.
[00:07:52] I guess if they were to steal men, the sort of a sudden perspective would be that humans
[00:07:56] actually can just take on everything at once, right?
[00:07:58] Even animals can take on everything at once, right?
[00:08:00] Animals are maybe a better example because they don't even have the scaffold of language.
[00:08:03] They just get thrown out into the world, and they just have to make sense of everything
[00:08:07] without any labels, and the vision for AGI then should just be something which just
[00:08:12] looks at sensory data, looks like the computer screen, and it just figures out what's going
[00:08:17] on from scratch.
[00:08:18] I mean, if a human was put in a similar situation, that would be trained from scratch,
[00:08:21] but I mean, this is like a human growing up, where an animal growing up.
[00:08:24] So why shouldn't that be the vision for AGI rather than like this thing where we're doing
[00:08:27] millions of years of training?
[00:08:29] **Andrej Karpathy:** I think that's a really good question, and I think, I mean, so Sutton was in your podcast,
[00:08:34] and I saw the podcast, and I had to write up about that podcast almost that gets into
[00:08:38] a little bit of how I see things, and I kind of feel like I'm very careful to make analogies
[00:08:43] to animals because they came about by very different optimization processes.
[00:08:48] Animals are evolved, and they actually come with a huge amount of hardware that's built
[00:08:51] in.
[00:08:52] And when, for example, my example in the post was the zebra, the zebra gets born, and
[00:08:57] a few minutes later, it's running around and following its mother.
[00:08:59] That's an extremely complicated thing to do.
[00:09:02] That's not reinforcement learning, that's something that's baked in, and evolution obviously
[00:09:05] has some way of encoding the weights of our neural nets in ATCGs, and I have no idea
[00:09:10] how that works, but it apparently works.
[00:09:12] So I kind of feel like brains just were king from a very different process, and I am
[00:09:18] very hesitant to take inspiration from it because we're not actually running that process.
[00:09:22] So in my post, I kind of said, we're not actually building animals.
[00:09:25] We're building ghosts, or spirits, or whatever people want to call it.
[00:09:30] Because we're not doing training by evolution, we're doing training by basically imitation
[00:09:36] of humans, and the data that they've put on the internet.
[00:09:39] And so you end up with these, like, sort of ethereal spirit entities because they're
[00:09:43] fully digital, and they're kind of like mimicking humans, and it's a different kind of intelligence.
[00:09:46] Like if you imagine a space of intelligence is, we're starting off at a different point
[00:09:50] almost.
[00:09:51] We're not really building animals, but I think it's also possible to make them a bit
[00:09:54] more animal-like over time, and I think we should be doing that.
[00:09:56] And so I kind of feel like, sort of just, I guess, one more point is, I do feel like
[00:09:59] Sutton basically has a very, like his framework is, like we want to build animals, and I actually
[00:10:05] think that would be wonderful.
[00:10:06] If we can get that to work, that would be amazing.
[00:10:07] If there was a single algorithm that you can just, you know, run on the internet, and
[00:10:12] it learns everything, that would be incredible, I almost suspect that I'm not actually sure
[00:10:17] that it exists, and that's certainly actually not what animals do.
[00:10:21] Because animals have this outer loop of evolution, and a lot of what looks like learning
[00:10:26] is actually a lot more maturation of the brain.
[00:10:28] And I think that there's actually a very little reinforcement learning for animals.
[00:10:32] And I think a lot of the reinforcement learning is actually like more like motor tasks.
[00:10:35] It's not intelligence tasks.
[00:10:36] So I actually kind of think humans don't actually like really use RL, roughly speaking
[00:10:40] is what I would say.
[00:10:41] **Dwarkesh Patel:** The last sentence, a lot of that intelligence is not motor tasks.
[00:10:43] It's white, sorry.
[00:10:44] **Andrej Karpathy:** A lot of the reinforcement learning in my perspective would be things that are a lot
[00:10:46] more like motor-like, like simple kind of like task throwing hoop or stuff like that.
[00:10:53] But I don't think that humans use reinforcement learning for a lot of intelligence tasks,
[00:10:57] like problem solving and so on.
[00:10:59] **Dwarkesh Patel:** Interesting.
[00:11:00] That doesn't mean we shouldn't do that for research, but I just feel like that's what
[00:11:04] animals do or don't.
[00:11:05] **Andrej Karpathy:** I'm going to take a second to digest that because there's a lot of different ideas.
[00:11:10] One clarification question I can ask to understand the perspective.
[00:11:14] So I think you suggest that look, evolution is doing the kind of thing that pre-training
[00:11:18] does in the sense of building something which can then understand the world.
[00:11:24] The difference I guess is that evolution has to be titrated in the case of humans through
[00:11:29] three gigabytes of DNA.
[00:11:31] And so that's very unlike the weights of a model.
[00:11:37] I mean, literally the weights of the model are a brain which obviously is not encoded
[00:11:40] in the sperm and the egg or does not exist in the sperm and the egg.
[00:11:44] So it has to be grown and also the information for every single synapse in the brain simply
[00:11:50] cannot exist in the three gigabytes that exist in the DNA.
[00:11:52] Evolution seems closer to finding the algorithm which then does the lifetime learning.
[00:11:58] Now, maybe the lifetime learning is not analogous to RL to your point.
[00:12:03] Is that compatible with the thing you were saying or would you disagree with that?
[00:12:05] **Andrej Karpathy:** I think so. I would agree with you that there's some miraculous compression going on
[00:12:08] because obviously the weights of the neural net are not stored in ATCGs.
[00:12:11] There's some kind of a dramatic compression and there's some kind of like learning algorithms
[00:12:15] encoded that take over and do some of the learning online.
[00:12:18] So I definitely agree with you on that.
[00:12:20] Basically, I would say I'm a lot more kind of like practically minded.
[00:12:23] I don't come at it from a perspective of like let's build animals.
[00:12:26] I come from a perspective of like let's build useful things.
[00:12:28] So I have a hard hat on and I'm just observing that like we're not going to do evolution
[00:12:32] because I don't know how to do that, but it does turn out we can build these ghost
[00:12:36] spirit like entities by imitating internet documents.
[00:12:39] This works and it's actually kind of like it's a way to bring you up to something that has a
[00:12:44] lot of sort of built in knowledge and intelligence in some way, similar to maybe what evolution has
[00:12:49] done. So that's why I kind of call pre-training this kind of like crappy evolution.
[00:12:53] It's like the practically possible version with art technology and what we have available to us
[00:12:58] to get to a starting point where we can actually do things like reinforcement learning and so on.
[00:13:03] **Dwarkesh Patel:** Just to steal man the other perspective because after doing this on an interview and thinking
[00:13:06] about it a bit, it has an important point here. Evolution does not give us the knowledge really,
[00:13:11] right? It gives us the algorithm to find the knowledge and that seems different for pre-training.
[00:13:15] So if perhaps the perspective is that pre-training helps build the kind of entity which can
[00:13:20] learn better, teaches meta learning and therefore it is a similar to like finding an algorithm.
[00:13:26] But if if it's like evolution gives us knowledge, pre-training gives us knowledge or that
[00:13:29] analogy seems to break down. **Andrej Karpathy:** So it's subtle and I think you're right to push back on it. But basically
[00:13:34] the thing that pre-training is doing, so you're basically getting the next token predictor
[00:13:37] on over the internet and you're training that into a neural net. It's doing two things actually
[00:13:41] there are kind of like unrelated. Number one, it's picking up all this knowledge as I call it.
[00:13:45] Number two, it's actually becoming intelligent. By observing the algorithmic patterns in the
[00:13:50] internet, it actually kind of like boots up all these little circuits and algorithms inside the
[00:13:54] neural net to do things like in context learning and all this kind of stuff. And actually you don't
[00:13:58] actually need or want the knowledge. I actually think that's probably actually holding back the
[00:14:02] neural networks overall because it's actually like getting them to rely on the knowledge all too much
[00:14:06] sometimes. For example, I kind of feel like agents one thing they're not very good at is going
[00:14:10] off the data manifold of what exists on the internet. If they had less knowledge or less memory,
[00:14:15] actually maybe they would be better. And so what I think we have to do kind of going forward and this
[00:14:19] would be part of the research paradigms is actually think we need to start, we need to figure out ways to
[00:14:24] remove some of the knowledge and to keep what I call this cognitive as this cognitive core.
[00:14:28] Is this like intelligent entity that is kind of stripped from knowledge but contains the algorithms
[00:14:33] and contains the magic, you know, of intelligence and problem solving and the strategies of it
[00:14:38] and all this kind of stuff. **Dwarkesh Patel:** There's so much interesting stuff there. Okay, so let's start with
[00:14:42] in context learning. This is an obvious point, but I think it's worth just like saying it explicitly
[00:14:47] and meditating on it. The situation in which these models seem the most intelligent in which they
[00:14:52] are like, I talked to them and I'm like, wow, there's really something on the other end that's
[00:14:56] responding to me thinking about things. If it like makes a mistake, it's like, oh, wait, that's
[00:14:59] actually the wrong way to think about it. I'm packing up. All that is happening in context. That's
[00:15:03] where I feel like the real intelligence you can like visibly see. And that in context learning
[00:15:08] process is developed by gradient descent on pre-training, right? Like it's spontaneously meta-learns
[00:15:14] in context learning. But the in context learning itself is not gradient descent. In the same way that
[00:15:20] our lifetime intelligence as humans to be able to do things is conditioned by evolution. But our
[00:15:26] actual learning during our lifetime is like happening through some other process. **Andrej Karpathy:** Actually, don't
[00:15:31] fully agree with that, but you should continue with that. **Dwarkesh Patel:** Actually, then I'm curious to understand
[00:15:35] how that analogy breaks down. **Andrej Karpathy:** I think I'm hesitant to say that in context learning is not doing
[00:15:39] gradient descent because I mean, it's not doing explicit gradient descent, but I still think that
[00:15:44] so in context learning, basically, it's pattern completion within a token window, right? And it
[00:15:49] just turns out that there's a huge amount of patterns on the internet. And so you write the model
[00:15:52] kind of like learns to complete the pattern. And that's inside the weights. The weights of the
[00:15:56] neural network are trying to discover patterns and complete the pattern. And there's some kind of
[00:16:00] adaptation that happens inside the neural network, right? Which is kind of magical and just falls
[00:16:04] out from internet, just because there's a lot of patterns. I will say that there have been some
[00:16:10] papers that I thought were interesting that actually look at the mechanisms behind in context
[00:16:13] learning. And I do think it's possible that in context learning actually runs a small gradient
[00:16:16] descent loop internally in the layers of the neural network. And so I recall one paper in particular
[00:16:21] where they were doing linear regression actually using in context learning. So basically your inputs
[00:16:27] into the neural network are x, y pairs, x, y, x, y that happened to be on the line. And then you do x
[00:16:34] and you expect the y. And the neural network when you train it in this way actually does do, does do
[00:16:39] linear regression. And normally when you would run linear regression, you have a small gradient
[00:16:44] descent optimizer that basically looks at x, y, looks at an error, calculus, the gradient of
[00:16:48] the weights and does the update a few times. It just turns out that when they looked at the weights
[00:16:52] of that in context learning algorithm, they actually found some analogies to gradient descent
[00:16:58] and mechanics. In fact, I think even the paper one was stronger because they actually hard coded
[00:17:02] the weights of the neural network to do gradient descent through attention and all the
[00:17:08] internals of the neural network. So I guess that's just my only pushback is that who knows how in
[00:17:13] context learning works, but actually think that it's probably doing a little bit of some kind of
[00:17:17] funky gradient descent internally. And then I think that that's possible. So I guess I was only
[00:17:22] pushing back on you're saying it's not doing in context learning who knows what it's doing, but
[00:17:25] it's probably maybe doing something similar to it, but we don't know. **Dwarkesh Patel:** So then it's worth thinking
[00:17:29] about, okay, if both of them are implementing gradient descent, if in context learning and
[00:17:33] pre-training are both implementing, something like gradient descent, why does it feel like in context
[00:17:39] learning actually we're getting to this like continual learning, real intelligence like thing,
[00:17:44] whereas you don't get the analogous feeling just from pre-training, at least you could argue that.
[00:17:49] And so if it's the same algorithm, what could be different? Well, one way you can think about it is
[00:17:52] how much information does the model store per information it receives from training? And if you
[00:18:00] look at pre-training, if I think if you look at Lama 3, for example, I think it's trained on
[00:18:05] 15 trillion tokens and if you look at the 70B model, that would be the equivalent of 0.07
[00:18:12] bits per token in that it sees in pre-training in terms of like the information in the weights of
[00:18:16] the model compared to the tokens it reads, whereas if you look at the KV cache and how it grows per
[00:18:22] additional token in context learning, it's like 320 kilobytes. So that's a 35 million fold difference
[00:18:28] in how much information per token is assimilated by the model. I wonder if that's relevant at all.
[00:18:34] **Andrej Karpathy:** I think I kind of agree. I mean, the way I usually put this is that anything that happens during
[00:18:39] the training of the neural network, the knowledge is only kind of like a hazy recollection of what
[00:18:43] happened in training in a training time. And that's because the compression is dramatic. You're
[00:18:47] taking 15 trillion tokens and you're compressing it to just your final network of a few balloon
[00:18:50] parameters. So obviously it's a massive amount of compression going on. So I kind of refer to it as
[00:18:55] like a hazy recollection of the internet documents, whereas anything that happens in the context window
[00:18:59] of the neural network, you're plugging all the tokens and building up all the KV cache representation,
[00:19:03] it's very directly accessible to the neural net. So I compare the KV cache and the stuff that
[00:19:08] happens at test time to like more like a working memory, like all the stuff that's in the in the
[00:19:13] in the context window is very directly accessible to the neural net. So there's always like these
[00:19:18] almost surprising analogies between LLMs and humans. And I find them kind of surprising because
[00:19:22] we're not trying to build a human brain, of course, just directly. We're just finding that this
[00:19:26] works and we're doing it. But I do think that anything that's in the weights, it's kind of like a
[00:19:30] hazy recollection of what you read a year ago. Anything that you give it as a context at test time
[00:19:36] is directly in the working memory. And I think that's a very powerful analogy to things for things.
[00:19:40] So when you, for example, go to an LLM and you ask it about some book and what happened in it,
[00:19:44] like on the claims book or something like that, the LLM will often give you some stuff which is
[00:19:48] roughly correct. But if you give it the full chapter and ask it questions, you're going to get
[00:19:52] much better results because it's now loaded in the working memory of the model. So I basically agree
[00:19:57] with you a very long way of saying that I kind of agree. **Dwarkesh Patel:** And that's why stepping back,
[00:20:00] what is it the part about human intelligence that we like have a most feel to replicate with these
[00:20:06] models? **Andrej Karpathy:** I almost feel like just just a lot of it. So maybe one way to think about it, I don't
[00:20:15] know if this is the best way, but I almost kind of feel like, again, making these analogies in
[00:20:20] perfect as they are, we've stumbled by with the transformer neural network, which is extremely
[00:20:25] powerful. Very general. You can train transformers on audio or video or text or whatever you want. And
[00:20:31] it just learns patterns and they're very powerful and it works really well. That to me almost
[00:20:36] indicates that this is kind of like some piece of cortical tissue. It's something like that because
[00:20:40] the cortex is famously very plastic as well. You can rewire, you know, parts of brains and there was
[00:20:46] the slightly gruesome experiments with rewiring like visual cortex to the auditory cortex and
[00:20:51] this animal like learn file, et cetera. So I think that this is kind of like a cortical tissue.
[00:20:56] I think when we're doing reasoning and planning inside the neural networks, so basically doing
[00:21:01] a reasoning traces for thinking models, that's kind of like the prefrontal cortex. And then
[00:21:08] I think we maybe those are like blue check marks, but I still think there's many brain parts and
[00:21:13] nuclei that are not explored. So maybe for example, there's a basic anglia doing their
[00:21:16] better reinforcement learning when we find two in the models on reinforcement learning. But
[00:21:20] you know, whereas like the hippocampus, not obvious what that would be. Some parts are probably not
[00:21:24] important. Maybe the cerebellum is like not important to cognition. It's thoughts of maybe we can
[00:21:27] skip some of it. But I still think there's, for example, the amygdala, all the emotions and instincts.
[00:21:33] And there's probably like a bunch of other nuclei in the brain that are very ancient that I don't
[00:21:36] think we've like really replicated. I don't actually know that we should be pursuing, you know,
[00:21:40] the building of an analog of human brain. Again, an engineer, mostly at heart. But
[00:21:46] I still feel like maybe another way to answer the question is, you're not going to hire this thing
[00:21:51] as an intern and it's missing a lot of it's because it comes with a lot of these cognitive deficits
[00:21:55] that we all intuitively feel when we talk to the models. And so it's just like not fully there yet.
[00:22:00] You can look at it as like not all the brain parts are checked off yet.
[00:22:04] **Dwarkesh Patel:** This is maybe relevant to the question of thinking about how fast these issues will be solved.
[00:22:09] So sometimes people will say about continual learning. Look, actually you could already,
[00:22:15] you could easily replicate this capability. Just as in context learning emerged spontaneously as a
[00:22:20] result of pre-training, continual learning over longer horizons will emerge spontaneously if the
[00:22:26] model is incentivized to recollect information over longer horizons or horizons longer than one
[00:22:32] session. So if there's some like outer loop RL, which has many sessions within that outer loop,
[00:22:42] then like this continual learning where it uses like fine tunes itself or it writes to an
[00:22:46] external memory or something will just sort of like emerge spontaneously. Do you think, do you think
[00:22:50] things are anything that are plausible? I don't know really a prior of it. How plausible is that,
[00:22:54] how likely is that to happen? **Andrej Karpathy:** I don't know that I fully resonate with that because I feel like these
[00:22:57] models when you boot them up and they have zero tokens in the window, they're always like restarting
[00:23:01] from scratch where they were. So I don't actually know in that world view what it looks like because
[00:23:08] again, maybe some analogies to humans just because I think it's roughly concrete and kind of
[00:23:12] interesting to think through. I feel like when I'm awake, I'm building up a context window of
[00:23:16] stuff that's happening during the day. But I feel like when I go to sleep, something magical happens
[00:23:20] where I don't actually think that the context window stays around. I think there's some process
[00:23:24] of distillation into weights of my brain and this happens during sleep and all this kind of stuff.
[00:23:29] We don't have an equivalent of that in larger language models and that's to me more adjacent to
[00:23:35] when you talk about continual learning and so on, as absent. These models don't really have
[00:23:39] this distillation phase of taking what happened, analyzing it, obsessively thinking through it,
[00:23:47] basically doing some kind of a synthetic data generation process and distilling it back into the
[00:23:50] weights and maybe having a specific neural net per person, maybe it's a lura, it's not a full weight
[00:23:59] neural network that's just some of the small sparse subset of the weights are changed.
[00:24:04] But basically, we do want to create ways of creating these individuals that have very long
[00:24:09] contexts. It's not only remaining in the context window because the context windows grow very,
[00:24:13] very long. Maybe we have some very elaborate sparse attention over it. But I still think that
[00:24:18] humans obviously have some process for distilling some of that knowledge into the weights. We're
[00:24:22] missing it. And I do also think that humans have some kind of a very elaborate sparse attention
[00:24:28] scheme, which I think we're starting to see some early hints of. So DeepSeq V3.2 just came out and
[00:24:35] I saw that they have like a sparse attention as an example. And this is one way to have very,
[00:24:39] very long context windows. So I almost feel like we are redoing a lot of the cognitive tricks that
[00:24:45] evolution came up with through a very different process. But we're, I think, converging a similar
[00:24:48] architecture cognitively. **Dwarkesh Patel:** Interesting. In 10 years, do you think it'll still be something like a
[00:24:53] transformer, but with a much more modified attention and more sparse MLPs and so forth?
[00:24:58] **Andrej Karpathy:** Well, the way I like to think about it is, okay, let's translation invariance in time, right? So
[00:25:02] 10 years ago, where were we? 2015, we had a convolutional neural networks primarily. Resurone
[00:25:08] networks just came out. So remarkably similar, I guess, but quite a bit different still. I mean,
[00:25:13] transformer was not around. All these more modern tweaks on a transformer were not around. So
[00:25:21] maybe some of the things that we can bet on, I think, in 10 years by translational sort of
[00:25:26] equivalence is we're still training giant neural networks forward backward pass and update
[00:25:31] through gradient descent. But maybe it looks a little bit different and it's just everything
[00:25:37] is much bigger. Actually, recently, I also went back all the way to 1989, which was kind of a fun
[00:25:43] exercise for me a few years ago, because I was reproducing Jan LaCoon's 1989
[00:25:48] convolutional network, which was the first neural network I'm aware of trained via
[00:25:51] gradient descent, like modern neural network trained gradient descent on a digital recognition.
[00:25:57] And I was just interested in, okay, how can I modernize this? How much of this is algorithms? How
[00:26:01] much of this is data? How much of this progress is compute and systems? And I was able to very quickly
[00:26:05] like half the learning rate, just knowing by time travel by 33 years. So if I time travel by
[00:26:11] algorithms to 33 years, I could adjust with Jan LaCoon data in 1989 and I could basically have
[00:26:16] the learning half the error. But to get further gains, I had to add a lot more data. I had to
[00:26:21] like 10X the training set. And then I had to actually add more computational optimizations,
[00:26:26] had to basically train for much longer with dropout and other regularization techniques.
[00:26:30] And so it's almost like all these things have to improve simultaneously. So you know,
[00:26:35] we're probably going to have a lot more data. We're probably going to have a lot better hardware,
[00:26:37] probably going to have a lot better kernels and software. We're probably going to have better
[00:26:40] algorithms. And all of those is almost like no one of them is winning too much. All of them are
[00:26:45] surprisingly equal. And this has kind of been the trend for a while. So I guess to answer maybe
[00:26:51] your question, I expect differences. Algorithmically to what's happening today. But I do also expect
[00:26:58] that some of the things that have stuck around for a very long time will probably still be there.
[00:27:01] It's probably still a giant neural network trained with gradient descent. That would be my guess.
[00:27:04] **Dwarkesh Patel:** It's surprising that all of those things together only have half the error, which is like 30 years
[00:27:13] of progress. **Andrej Karpathy:** Maybe half is like, if you have the error, that actually means that half is a lot.
[00:27:18] Yeah. **Dwarkesh Patel:** But it's I guess what we're shocking to me is everything needs to improve across the board.
[00:27:24] **Andrej Karpathy:** Architecture, optimize a loss function and also has improved across the board forever. So I kind of
[00:27:28] expect all those changes to be alive and well. **Dwarkesh Patel:** Yeah, actually, I was about to ask you a very
[00:27:32] similar question about NanoChat because since you just coded up recently, every single sort of step
[00:27:38] in the process of building a chatbot is like freshening around. And I'm curious. If you had
[00:27:44] similar thoughts about like, oh, there was no one thing that was relevant to going from GPT-2
[00:27:50] to NanoChat. What are sort of like surprising takeaways from the experience of building
[00:27:56] AnChat? **Andrej Karpathy:** So NanoChat is a kind of repository I released. Was it yesterday or day before? I can't
[00:28:01] remember. We can see this liberation though and into the. Well, it's just trying to be a,
[00:28:09] it's trying to be the simplest, complete repository that covers the whole pipeline into end of
[00:28:13] building a chatbot clone. And so, you know, you have all of the steps, not just any individual step,
[00:28:19] which is a bunch of, I worked on all the individual steps sort of in the past and released small
[00:28:22] pieces of code that kind of show you how that's done in algorithmic sense in like simple code.
[00:28:29] But this kind of handles a lot of the entire pipeline. I think in terms of learning, it's not,
[00:28:33] it's not so much, I don't know that I actually found something that I learned from it,
[00:28:37] necessarily. I kind of already had in my mind as like how you build it. And this is just a process
[00:28:41] of mechanically building it and making it clean enough so that people can actually learn from it.
[00:28:48] And that they find it useful. **Dwarkesh Patel:** Yeah. What is the best way for somebody to learn from it?
[00:28:54] Is it just like delete all the code and try to remove them from scratch, try to add modifications
[00:28:58] to it? **Andrej Karpathy:** Yeah, I think that's a great question. I would probably say, so basically it's about
[00:29:02] a thousand lines of code that takes you through the entire pipeline. I would probably put it on the
[00:29:06] right monitor, like if you have two monitors, you put it on the right. And you want to build it from
[00:29:10] scratch, you build it from start, you're not allowed to copy paste, you're allowed to reference,
[00:29:15] you're not allowed to copy paste. Maybe that's how I would do it. But I also think the repository
[00:29:19] by itself, it is like a pretty large beast. I mean, it's, you know, it's a ritz. When you write
[00:29:24] this code, you don't go from top to bottom. You go from trunks and you grow the trunks. And
[00:29:28] that information is absent, like you wouldn't know where to start. And so I think it's not just a
[00:29:32] file repository that's needed. It's like the building of the repository, which is a complicated trunk
[00:29:37] growing process. Right. So that part is not there yet. I would love to actually like add that
[00:29:41] probably later this week or something in some way. Like either it's a, it's probably a video or
[00:29:46] something like that. But, but maybe roughly speaking with that's what I would try to do is build
[00:29:51] the stuff yourself. But don't allow yourself copy paste. **Dwarkesh Patel:** Yeah. I do think that there's two types
[00:29:56] of knowledge, almost like there's the high level surface knowledge. But the thing is that when
[00:29:59] you actually build something from scratch, you're forced to come to terms with what you don't
[00:30:03] actually understand. And you don't know that you don't understand it. **Andrej Karpathy:** Interesting. **Dwarkesh Patel:** And it always
[00:30:06] leads to a deeper understanding. And it's like just the only way to build this, like if I can't build
[00:30:12] it, I don't understand it. Is that a fine man code, I believe, or something along those lines?
[00:30:16] **Andrej Karpathy:** I 100% I've always believed this very strongly because there's all these like micro things that
[00:30:22] are just not properly arranged and you don't really have the knowledge. You just think you have
[00:30:25] the knowledge. So don't write blog posts. Don't do slides. Don't do any of that. I can build a code,
[00:30:29] arrange it, get it to work. So the only way to go, otherwise you're missing knowledge.
[00:30:33] **Dwarkesh Patel:** You treated out that coding models were actually a very little help to you in assembling this
[00:30:37] repository. And I'm curious why that was. **Andrej Karpathy:** Yeah. So the repository, I guess I built it over a period of
[00:30:44] a bit more than a month. And I would say there's like three major classes of how people
[00:30:48] interact with code right now. Some people completely reject all of LLMs and they are just
[00:30:53] writing by scratch. I think this is probably not their, their I think to do anymore.
[00:30:58] The intermediate part, which is where I am, is you still write a lot of things from scratch,
[00:31:02] but you use the autocomplete. That's basically available now from these models. So when you start
[00:31:07] writing out, it will be piece of it. It will, they were all complete from you and you can just tap
[00:31:11] through. And most of the time it's correct. Sometimes it's not and you edit it. But you're still
[00:31:14] very much the sort of architect of what you're writing. And then there's the, you know, vibe coding,
[00:31:20] you know, high, please implement this or that, you know, enter and then let the model do it. And
[00:31:25] that's the agents. I do feel like the agents work in very specific settings. And I would use them
[00:31:32] in specific settings. But again, these are all tools available to you and you have to like learn what
[00:31:36] they, what they're good at and what they're not good at and when to use them. So the agents are
[00:31:39] actually pretty good, for example, if you're doing boilerplate stuff, boilerplate code that's like
[00:31:43] just, you know, just copy paste stuff, they're very good at that. They're very good at stuff that
[00:31:47] occurs very often in the internet because there's lots of examples of it in the training sets of
[00:31:52] these models. So, so there's like features of things that where the models will do very well.
[00:31:58] I would say nanochet is not an example of this because it's a fairly unique repository. There's
[00:32:03] not that much code, I think, in a way that I've structured it. And, and it's not boilerplate code.
[00:32:08] It's like actually like intellectual intense code almost and everything has to be very
[00:32:11] precisely arranged. And the models were always trying to, they kept trying to, I mean,
[00:32:16] they have so many cognitive deficits, right? So one example, they keep trying to, they keep
[00:32:19] misunderstanding the code because they, they have too much memory from all the typical ways of
[00:32:25] doing things on the internet that I just wasn't adopting. So the models, for example, I mean,
[00:32:30] I don't know if I want to get into the full details, but they keep, they keep, they keep thinking
[00:32:34] I'm writing normal code and I'm not. **Dwarkesh Patel:** Maybe one example, maybe one example. **Andrej Karpathy:** So the way to synchronize,
[00:32:40] so we have eight GPUs that are all doing forward efforts. The way to synchronize gradients between
[00:32:44] them is to use distributed data parallel container of PyTorch, which automatically does all the,
[00:32:49] as you're doing the back where it will start communicating and synchronizing gradients,
[00:32:52] I didn't use DDP because I didn't want to use it because it's not necessary. So I threw it out.
[00:32:57] And I basically wrote my own synchronization routine that's inside the step of the optimizer.
[00:33:02] And so the models were trying to get me to use the DDP container. Yeah. And they were very concerned
[00:33:06] about, okay, this gets way too technical, but I wasn't using that container because I don't need it
[00:33:11] and I have a custom implementation of something like it. And they just couldn't internalize it,
[00:33:15] you had your own. **Dwarkesh Patel:** Yeah, they couldn't, they couldn't, they couldn't get passed out. **Andrej Karpathy:** And then,
[00:33:19] they kept trying to like mess up the style, like they're way too over defensive. They make all
[00:33:23] these tri-catch statements. They keep trying to make a production code base. And I have a bunch of
[00:33:28] assumptions in my code and it's okay. And, and it's just like, I don't need all this extra stuff
[00:33:34] in there. And so I just kind of feel like they're bloating the code base. They're bloating the
[00:33:36] complexity. They keep misunderstanding. They're using deprecated APIs a bunch of times. So it's total
[00:33:42] mess. And it's just, it's just not net useful. I can go in and I can clean it up, but it's not net
[00:33:47] and useful. I also feel like it's kind of annoying to have to like type out what I want in English,
[00:33:52] because it's just too much typing. Like, if I just navigate to the part of the code that I want,
[00:33:56] and I go where I know the code has to appear and I start typing out the first three letters,
[00:34:00] auto-complete gets it and just gives you the code. And so I think it's, this is a very high
[00:34:04] information bandwidth to specify what you want. If you point to the code where you want it and you
[00:34:08] type out the first few pieces, and the model will complete it. So I guess what I mean is,
[00:34:14] I think these models are good in certain parts of the stack. Actually, use the models a little bit
[00:34:19] in, there are two examples where I actually use the models that I think are illustrative.
[00:34:24] One was when I generated the report, and that's actually more boilerplatey. So actually, if I
[00:34:28] coded partially some of that stuff, that was fine. Because it's not like mission-critical stuff,
[00:34:33] and it works fine. And then the other part is when I was rewriting the tokenizer in Rust,
[00:34:37] I'm actually not as good at Rust because I'm fairly new to Rust. So I was doing, there's a bit of
[00:34:42] vibe coding going on in when I was writing some of the Rust code. But I had Python implementation
[00:34:47] that I fully understand, and I'm just making sure I'm making more efficient version of it,
[00:34:50] and I have tests. So I feel safer doing that stuff. And so basically they lower or like the
[00:34:56] increased accessibility to languages or paradigms that you might not be as familiar with. So I
[00:35:02] think they're very helpful there as well. Because there's a ton of Rust code out there,
[00:35:06] the models are actually pretty good at it. I happen to not know that much about it. So the models
[00:35:09] are very useful there. **Dwarkesh Patel:** The reason I think this question is so interesting is because the main story
[00:35:15] people have about AI exploding and getting to super intelligence pretty rapidly is AI automating,
[00:35:22] AI engineering and AI research. And so they'll look at the fact that you can have cloud code and
[00:35:27] make entire application, crowd applications from scratch and be like, if you had this
[00:35:30] incubability inside of open AI and deep mind and everything, well, just imagine the level of like
[00:35:34] just, you know, a thousand of you or a million of you in parallel finding little architectural
[00:35:40] tweaks. And so it's quite interesting to hear you say that this is the thing they're sort of
[00:35:44] asymmetrically worse at. And it's like quite relevant to forecasting whether the AI 2027 type
[00:35:50] explosion is likely to happen anytime soon. **Andrej Karpathy:** I think that's a good way of putting it. And I think
[00:35:55] you're getting at some of my, like why my timelines are a bit longer. You're right. I think,
[00:36:00] yeah, they're not very good at code that has never been written before. Maybe it's like one way
[00:36:04] to put it, which is like what we're trying to achieve when we're building these models.
[00:36:07] **Dwarkesh Patel:** Very naive question, but the architectural tweaks that you're adding to NANO chat,
[00:36:14] they're in a paper somewhere, right? They might even be in a repo somewhere. So it's,
[00:36:18] is it, is it surprising that they aren't able to integrate that into whenever you're like
[00:36:25] ad rope embeddings or something, they do that in the wrong way?
[00:36:29] **Andrej Karpathy:** It's, it's tough. I think they kind of know, they kind of know, but they don't fully know,
[00:36:32] and they don't know how to fully integrate it into the repo in your style and your code and your
[00:36:36] place and some of the custom things that you're doing. And how fits with all the assumptions of
[00:36:40] the repository and all this kind of stuff. So I think they do have some knowledge, but
[00:36:44] they haven't gotten to the place where they can actually integrate it, make sense of it, and so on.
[00:36:50] I do think that a lot of the stuff, by the way, continues to improve. So I think currently,
[00:36:54] probably state-of-the-art model that I go to is the GPT-5 Pro. And that's a very, very powerful
[00:36:59] model. So if I actually have 20 minutes, I will copy paste my entire repo and I go to GPT-5 Pro,
[00:37:03] the Oracle for like some questions. And often it's not too bad. And surprisingly good compared to
[00:37:08] what existed a year ago. But I do think that overall, the models are, they're not there. And I
[00:37:14] kind of feel like the industry, it's over, it's making too big of a jump, and it's trying to pretend
[00:37:23] like this is amazing, and it's not. It's slop. And I think they're not coming to terms with it,
[00:37:27] and maybe they're trying to fundraise or something like that. I'm not sure what's going on, but
[00:37:31] we're at this intermediate stage. The models are amazing. They still leave a lot of work.
[00:37:35] For now, autocomplete is my sweet spot. But sometimes for sometimes of code, I will go to a
[00:37:40] null-imagined. **Dwarkesh Patel:** Yeah. Actually, this is also, here's another reason, but this is really interesting.
[00:37:45] Through the history of programming, there's been many productivity improvements,
[00:37:52] compilers, linting, better programming languages, etc. Which have increased program of productivity,
[00:37:57] but have not led to an explosion. So that's like one, that sounds very much like autocomplete
[00:38:02] tab. And this other category is just like automation of the programmer. And it's interesting,
[00:38:08] you're seeing more in the category of the historical analogies of like, you know, better compilers
[00:38:12] or something. **Andrej Karpathy:** I mean, because this gets said, one other kind of thought is like, I do feel like
[00:38:17] I have a hard time differentiating where AI begins and stops, because I do see AI as fundamentally
[00:38:22] an extension of computing in some pretty fundamental way. And I feel like I see a continuum of
[00:38:27] this kind of like recursive self-improvement or like of speeding up programmers all the way from
[00:38:32] beginning, like even like, I would say like code editors, syntax highlighting, syntax or like
[00:38:40] checking even of the types, like data type checking. All these kinds of tools that we've built for
[00:38:46] each other, even search engines, like why aren't search engines part of AI? Like, I don't know,
[00:38:50] like ranking is kind of AI, right? At some point Google was like, even early on, they were thinking
[00:38:55] of themselves as an AI company doing Google search engine, which I think is totally fair. And so
[00:38:59] I kind of see it as a lot more of a continuum than I think other people do. And I don't,
[00:39:02] it's hard for me to draw the line. And I kind of feel like, okay, we're now getting a much
[00:39:05] better autocomplete. And now we're also getting some agents, which are kind of like these loopy
[00:39:09] things, but they kind of go off rails sometimes. And what's going on is that the human is progressively
[00:39:15] doing a bit less and less of the low level stuff. For example, we're not writing the assembly code
[00:39:19] because we have compilers, right? Like compilers will take my high level language and see and write
[00:39:22] the assembly code. So we're abstracting ourselves very, very slowly. And there's this what I call
[00:39:27] autonomy slider of like more and more stuff is automated off the stuff that can be automated at
[00:39:31] any point in time. And we're doing a bit less and less than raising ourselves in the layer
[00:39:35] abstraction over the automation. **Dwarkesh Patel:** One of the big problems with RL is that it's incredibly
[00:39:40] information sparse. The toolbox can help you with this by increasing the amount of information
[00:39:45] that your agent gets to learn from with every single episode. For example, one of their
[00:39:50] customers wanted to train a coding agent. So label box augmented an IDE with a bunch of extra
[00:39:56] data collection tools and staffed a team of expert software engineers from their aligner network
[00:40:01] to generate trajectories that were optimized for training. Now, obviously, these engineers evaluated
[00:40:06] these interactions on a pass field basis, but they also rated every single response on a bunch of
[00:40:12] different dimensions like readability and performance. And they wrote down their thought processes
[00:40:17] for every single rating that they gave. So you're basically showing every single step an engineer
[00:40:22] takes at every single thought that they have while they're doing their job. And this is just
[00:40:27] something you could never get from usage data alone. And so label box packaged up all these evaluations
[00:40:33] and included all the agent trajectories and the corrective human edits for the customer to train on.
[00:40:39] This is just one example. So go check out how label box can get you high quality frontier data
[00:40:44] across domains, modalities, and training parents. Reach out at labelbox.com slash the war cache.
[00:40:53] Let's talk about RL a bit. You two do some very interesting things about this. Conceptually,
[00:40:59] how should we think about the way that humans are able to build a rich world model just from
[00:41:06] interacting with our environment? And in ways that seems almost irrespective of the final reward
[00:41:11] at the end of the episode. If somebody started a start a business and at the end of 10 years,
[00:41:17] she finds out whether the business succeeded or failed. We say that she's earned a bunch of wisdom
[00:41:21] and experience. But it's not because like the log probs of every single thing that happened over
[00:41:25] the last 10 years are up weighted or down weighted. It's something much more deliberate and rich
[00:41:29] is happening. What is the ML analogy? And how does that compare to what we're doing with other ones
[00:41:34] right now? **Andrej Karpathy:** Yeah, maybe the way I would put it is humans don't use reinforcement learning is maybe what
[00:41:38] as I've said it all. I think they do something different which is yeah, you experience. So
[00:41:42] reinforcement learning is a lot worse than I think the average person thinks.
[00:41:47] reinforcement learning is terrible. It just so happens that everything that we had before
[00:41:52] it is much worse. Because previously we're just imitating people so it has all these issues.
[00:41:59] So in reinforcement learning, say you're working with you're solving a math problem because it's
[00:42:03] very simple. You're giving a math problem and you're trying to find a solution. Now in reinforcement
[00:42:08] learning, you will try it lots of things in parallel first. So you're given a problem. You try hundreds
[00:42:15] of different attempts and these attempts can be complex right? They can be like, oh, let me try this,
[00:42:19] let me try that. This didn't work. That didn't work, et cetera. And then maybe you get an answer.
[00:42:23] And I checked the back of the book and you see, okay, the correct answer is this. And then you can
[00:42:28] see that, okay, this one, this one and that one got the correct answer, but these other 97 of them
[00:42:32] didn't. So literally what reinforcement learning does is it goes to the ones that worked really well.
[00:42:36] And every single thing you did along the way, every single token gets up weighted of like do more
[00:42:41] of this. The problem with that is I mean, people will say that your estimator has high variance,
[00:42:46] but what I mean, it's just noisy. It's noisy. So basically, it kind of almost assumes that every
[00:42:52] single little piece of the solution that you made that ride the dry answer was correct thing to do,
[00:42:55] which is not true. Like you may have gone down the wrong alleys until you ride the right solution.
[00:43:00] Every single one of those incorrect things you did, as long as you got to the correct solution,
[00:43:03] will be up weighted as do more of this. It's terrible. Yeah, it's noise. You've done all this work
[00:43:08] only to find a single, at the end, you get a single number of like, oh, you did correct. And
[00:43:14] based on that, you weigh that entire trajectory is like up weight or down weight. And so the way
[00:43:19] I like to put it is you're sucking supervision through straw because you've done all this work that
[00:43:23] could be a minute to roll out. And you're like sucking the bits of supervision of the final reward
[00:43:28] through straw. And you're like putting it, you're like, you're basically like,
[00:43:33] yeah, you're broadcasting that across the entire trajectory and using that to up weight or down
[00:43:36] with that trajectory. It's too crazy. A human would never do this. Number one, a human would never
[00:43:40] do hundreds of roll outs. Number two, when a person sort of finds a solution, they will have a
[00:43:46] pretty complicated process of review of like, okay, I think these parts that I did well, these parts,
[00:43:50] I did not do that well. I should probably do this or that. And they think through things.
[00:43:55] There's nothing in current LLM that does this. There's no equivalent of it. But I do see papers
[00:44:00] popping out that are trying to do this because it's obvious to everyone in the field. So I kind of see
[00:44:04] as like the first imitation learning actually, by the way, was extremely surprising and miraculous
[00:44:08] and amazing that we can fine tune by imitation of humans. And that was incredible because in the
[00:44:13] beginning, all we had was base models. Base models are autocomplete. And it wasn't obvious to me at
[00:44:18] the time. And I had to learn this and the paper that like blew my mind was instruct GPT because it
[00:44:24] pointed out that, hey, you can trade the pre-train model, which is autocomplete. And if you just
[00:44:28] fine tune it on text that looks like conversations, the model will very rapidly adapt to become
[00:44:33] a very conversational. And it keeps all the knowledge from pre-train. And this blew my mind because
[00:44:37] I didn't understand that it's just like stylistically can adjust so quickly and become an assistant
[00:44:42] to a user through just a few loops of fine tuning on that kind of data. It was very miraculous to
[00:44:47] me that that worked. So incredible. And that was like two years, three years of work. And now came
[00:44:53] RL. And RL allows you to do a bit better than just imitation learning, right? Because you can have
[00:44:58] these reward functions and you can hill climb on the reward functions. And so some problems have
[00:45:03] just correct answers. You can hill climb on that without getting expert trajectories to imitate.
[00:45:07] So that's amazing. And the model can also discover solutions that the human might never come up with.
[00:45:12] So this is incredible. And yet it's still stupid. So I think we need more. And so I saw a paper from
[00:45:19] Google yesterday that tried to have this reflect and review page idea in mind. **Dwarkesh Patel:** What was the
[00:45:26] memory bank paper or something? I don't know. **Andrej Karpathy:** I've actually seen a few papers along these lines.
[00:45:30] So I expect there to be some kind of a major update to how we do algorithms for other
[00:45:34] lambs coming in that realm. And then I think we need three or four or five more.
[00:45:41] Something like that. **Dwarkesh Patel:** But you're still going to come up with the evocative evocative phrases,
[00:45:46] sucking supervision through a straw. It's like so good.
[00:45:50] Why hasn't, so you're saying like you're problem with outcome based reward is that you have
[00:45:56] this huge trajectory. And then at the end, you're trying to learn every single possible thing about
[00:46:01] what you should do and what you should learn about the world from that one final bit. Why hasn't,
[00:46:07] given the fact that this is obvious, why hasn't processed space supervision as an alternative,
[00:46:11] been a successful way to make models more capable? What has been preventing us from using this
[00:46:15] alternative paradigm? **Andrej Karpathy:** So process based supervision just refers to the fact that we're not going to have
[00:46:19] a reward function only at the very end of after you've made 10 minutes of work and not going to tell you
[00:46:22] you did well or not. Well, I'm going to tell you at every single step of the way how well you're doing.
[00:46:27] And this is basically the reason we don't have that is it's not tricky how you do that properly.
[00:46:31] Because you have partial solutions and you don't know how to assign credit.
[00:46:34] So when you get the right answer, it's just an equality match to the answer, very simple to
[00:46:39] implement. If you're doing basically process supervision, how do you assign an automatical way,
[00:46:45] partial credit assignment? It's not obvious how you do it. Lots of labs, I think, are trying to do it
[00:46:49] with these LLM judges. So basically you get LLMs to try to do it. So you prompt an LLM,
[00:46:53] hey, look at a partial solution of a student. How well do you think they're doing if the answer is
[00:46:56] this and they try to tune the prompt? The reason that I think this is kind of tricky is quite subtle.
[00:47:02] And it's the fact that anytime you use an LLM to assign a reward, those LLMs are giant things with
[00:47:07] billions of parameters and they're gameable. And if you're reinforcement learning with respect to
[00:47:11] them, you will find adversarial examples for your LLM judges, almost guaranteed. You can't do this
[00:47:16] for too long. You do maybe 10 steps or 20 steps, maybe it will work, but you can't do 100 or 1000
[00:47:20] because it's not obvious because I know I understand it's not obvious, but basically the model will find
[00:47:26] little cracks, it will find all these like spurious things in the nooks and crannies of the giant
[00:47:32] model and find a way to cheat it. So one example that's prominently in my mind is I think this,
[00:47:38] I think this was probably public, but basically if you're using an LLM judge for a reward,
[00:47:43] so you just give it a solution from a student and ask it if the student will or not. We were training
[00:47:47] with reinforcement learning against that reward function and it worked really well and then
[00:47:53] suddenly the reward became extremely large, like it was massive jump and it did perfect. And you're
[00:47:57] looking at it like, wow, this means the student is perfect and all these problems is fully solved
[00:48:02] math. But actually what's happening is that when you look at the completions that you're
[00:48:06] getting from the model, they are complete nonsense, they start out okay and then they change to
[00:48:09] the, so it's just like, okay, let's take two plus three and we do this and this and then
[00:48:15] and you're looking at it's like, this is crazy, how is it getting reward of one or 100%
[00:48:20] and you look at the LLM judge and turns out that the the the the the is an adversarial examples
[00:48:23] for the model and it assigns 100% probability to it. And it's just because this is an out of sample
[00:48:28] example to the LLM, it's never seen you during training and you're in pure generalization land,
[00:48:33] it's never seen you during training and in the pure generalization land, you can find these
[00:48:37] examples that break it. You're basically training the LLM to be a prompt injection model. **Dwarkesh Patel:** Not
[00:48:44] even that prompt injection is way too fancy **Andrej Karpathy:** or you're finding adversarial examples that are called
[00:48:48] these are nonsensical solutions that are obviously wrong but the model things are amazing.
[00:48:55] **Dwarkesh Patel:** So today's thing you think this is the bottleneck to making RL more functional, then that will
[00:48:59] require making LLM's better judges if you want to do this in an automated way. And then so is it
[00:49:05] just going to be like some sort of GAN like approach or you had to train models to be more robust?
[00:49:08] **Andrej Karpathy:** Yeah, to I think the labs are probably doing all that like, okay, so the obvious thing is like
[00:49:12] that the the should not get 100% reward. Okay, well take the the the the the the the
[00:49:16] but in the training set of the LLM judge and say this is not 100% this is 0%. You can do this.
[00:49:20] But every time you do this, you get a new LLM and it still has adversarial examples. There's
[00:49:24] infinity adversarial examples and I think probably if you iterate this a few times, it'll probably
[00:49:29] be harder and harder to find adversarial examples. But I'm not 100% sure because this thing has
[00:49:32] a trillion parameters or whatnot. So I bet you the the labs are trying. I don't actually I still
[00:49:40] think I still think we need other ideas. **Dwarkesh Patel:** Interesting. Do you do you have some shape of what the other
[00:49:47] idea is going to be? **Andrej Karpathy:** So like this this idea of like every review review solution and compass
[00:49:54] synthetic examples such that when you train on them, you get you get better and like metallurinate
[00:49:59] and someone. And I think there's some papers that I'm starting to see pop out. I only am at a
[00:50:02] stage of like reading abstracts because a lot of these papers, you know, they're just ideas.
[00:50:06] Someone has to actually like make it work on a frontier LLM lab scale in full generality.
[00:50:12] Because when you see these papers, they pop up and it's just like a little bit of noisy, you know,
[00:50:15] it's cool ideas. But I haven't actually seen anyone convincingly show that this is possible.
[00:50:21] That said, the LLM labs are fairly closed. Also, who knows what they're doing that? But yeah.
[00:50:26] **Dwarkesh Patel:** So I guess I guess I see a very not easy, but like I can conceptualize how you would be able to train
[00:50:33] on synthetic examples or synthetic problems that you have made for yourself. But there seems to be
[00:50:37] another thing humans do. Maybe sleep is this. Maybe daydreaming is this, which is not necessarily come
[00:50:42] up with fake problems, but just like reflect. And I'm not sure what the ML analogy for daydreaming
[00:50:49] or sleeping, but just reflecting. I haven't come up with any problem. I mean, obviously the
[00:50:53] very basic analogies be like fine tuning on reflection bits. But I feel like in practice,
[00:50:58] that probably wouldn't work that well. So I don't know if you have some take on what the analogy
[00:51:02] of like this thing is. **Andrej Karpathy:** Yeah, I do think that we're missing some aspects there. So as an example,
[00:51:08] when you're reading a book, I almost feel like currently when LLM's are reading a book,
[00:51:13] what that means is we stretch out the sequence of text and the model is predicting the next token
[00:51:17] and it's getting some knowledge from that. That's not really what humans do, right? So when you're
[00:51:20] reading a book, I almost don't even feel like the book is like exposition. I'm supposed to be
[00:51:24] attending to and training on. The book is a set of prompts for me to do synthetic data generation
[00:51:30] or for you to get into a book club and talk about it with your friends. And by manipulating that
[00:51:34] information that you actually gained that knowledge. And I think we have no equivalent of that,
[00:51:38] again, with LLM's. They don't really do that, but I'd love to see during pre-training some kind of
[00:51:42] a stage that thinks through the material and tries to reconcile it with what it already knows.
[00:51:47] And thanks through for like some amount of time and guess that to work. And so there's no
[00:51:52] equivalence of any of this. This is all research. There's some subtle, very subtle that I think
[00:51:56] I'm very hard to understand reasons why it's not trivial. So if I can just do one,
[00:52:01] why can we just synthetically generate and train on it? Well, because every synthetic example,
[00:52:05] like if I just give synthetic generation of the model thinking about a book, you look at it and
[00:52:09] you like, this looks great. Why can't I train on it? Well, you could try, but the model will actually
[00:52:13] get much worse if you continue trying. And that's because all of the samples you get from models
[00:52:17] are silently collapsed. They're silently. This is not obvious if you look at any individual
[00:52:22] example of it. They occupy a very tiny manifold of the possible space of sort of thoughts about
[00:52:27] content. So the LLM's, when they come off, they're what we call collapsed. They have a collapse
[00:52:32] data distribution. If you sample one easy way to say it is go through chatchipity and ask it,
[00:52:37] tell me a joke. It only has like three jokes. It's not giving you the whole breath of possible
[00:52:41] jokes. It's giving you like, it knows like three jokes. They're silently collapsed. So basically,
[00:52:46] you're not getting the richness and diversity and the entropy from these models as you would get
[00:52:51] from humans. So humans are a lot more sort of noisier, but at least they're not biased. They're not
[00:52:56] in a statistical sense. They're not silently collapsed. They maintain a huge amount of entropy.
[00:53:00] So how do you get synthetic generation to work despite the collapse and while maintaining the
[00:53:05] entropy is a research problem? **Dwarkesh Patel:** Just to make sure I understood, the reason that the collapse is
[00:53:10] relevant to synthetic data generation is because you want to be able to come up with synthetic
[00:53:15] problems or reflections, which are not already in your data distribution. **Andrej Karpathy:** I guess what I'm saying is
[00:53:22] say we have a chapter of a book and I ask an alum to think about it. It will give you something
[00:53:27] that looks very reasonable. But if I ask it 10 times, you'll notice that all of them are the same.
[00:53:31] You can't just leave scaling, scaling quote unquote reflection on the same amount of
[00:53:38] prompt information and then get returns from that. So any individual sample will look okay,
[00:53:43] but the distribution of it is quite terrible. It's quite terrible in such a way that if you continue
[00:53:48] training on too much of your own stuff, you actually collapse. I actually think that there's no
[00:53:52] fundamental solutions to this possibly. I also think humans collapse over time. I think these
[00:53:58] analogies are surprisingly good, but humans collapse during the course of their lives. This is why
[00:54:02] children have completely, they haven't overfitted and they will say stuff that will shock you
[00:54:07] because it's kind of, you can see where they're coming from, but it's just not the thing people
[00:54:11] say and because they're not yet collapsed. But we're collapsed. We end up revisiting the same thoughts.
[00:54:17] We end up saying more and more of the same stuff and the learning rates go down and the collapse
[00:54:22] continues to get worse and then everything deteriorates. **Dwarkesh Patel:** Have you seen this amazing paper that
[00:54:29] dreaming is a way of preventing this kind of overfitting and collapse that the reason
[00:54:35] dreaming is evolutionary adaptive is to put you in weird situations that are like very unlike
[00:54:42] your day to day reality so that to prevent this kind of overfitting. **Andrej Karpathy:** It's an interesting idea. I
[00:54:45] mean, I do think that when you're generating things in your head and then you're attending to it,
[00:54:49] you're kind of like training on your own samples, you're training on your synthetic data,
[00:54:52] and if you do it for too long, you go off rails and you collapse way too much. So you always have to
[00:54:57] like seek entropy in your life. So talking to other people is a great source of entropy and
[00:55:04] things like that. So maybe the brain has also built some internal mechanisms for increasing the
[00:55:08] amount of entropy in that process, but yeah, maybe that's an interesting idea.
[00:55:14] **Dwarkesh Patel:** This is a very ill-formed thought. So I'll just put it out and let you react to it. The best learners
[00:55:19] that we are aware of, which are children, are extremely bad at recollecting information. In fact,
[00:55:25] at the very earliest stages of childhood, you will forget everything. You're just an amnesiac
[00:55:30] about everything that happens before a certain year date, but you're like extremely good at picking
[00:55:33] up new languages and learning from the world, and maybe there's some element of like being able to
[00:55:37] see the forest for the trees. Whereas if you compare it to the opposite end of the spectrum, you have
[00:55:42] LLM pre-training, which these models will literally want to regurgitate word for word. What is the
[00:55:48] next thing? We get PDF page, but their ability to learn abstract concepts really quickly in the way
[00:55:53] a child can is much more limited, and then adults are somewhere in between where they don't have
[00:55:58] the flexibility of childhood learning, but adults can memorize facts and information in a way that
[00:56:04] is hard for kids. I don't know if there's something interesting about that. **Andrej Karpathy:** I think there's
[00:56:08] something very interesting. I do think that humans actually, they do kind of like have a lot more
[00:56:14] of an element compared to all of them of seeing the forest for the trees, and we're not actually
[00:56:18] that good at memorization, which is actually a feature. Because we're not that good at memorization,
[00:56:24] we actually are kind of like forced to find patterns, like in a more general sense. I think LLM
[00:56:32] and in comparison are extremely good at memorization. They will recite passages from all these
[00:56:37] training sources. You can give them completely nonsensical data. You can hash some amount of text
[00:56:42] or something like that. You get a completely random sequence. If you train on it, even just I think
[00:56:45] a single iteration or two, it can suddenly regurgitate the entire thing. You'll memorize it. There's
[00:56:49] no way a person can read a single sequence of random numbers and recite it to you. That's a feature
[00:56:56] not a bug almost, because it forces you to only learn the generalizable components,
[00:57:00] whereas LLMs are distracted by all the memory that they have of the preaching documents,
[00:57:05] and it's probably very distracting to them in a certain sense. That's why when I talk about the
[00:57:10] cognitive core, I actually want to remove the memory, which is what we talked about. I'd love to
[00:57:14] have them have less the memory so that they have to look things up. They only maintain the algorithms
[00:57:19] for thought and the idea of an experiment and all this cognitive glue of acting. And this is also
[00:57:27] relevant to preventing Marla from collapse. **Dwarkesh Patel:** Let me think. I'm not sure. **Andrej Karpathy:** I think it's almost like
[00:57:36] a separate axis. The models are way too good at memorization and somehow we should remove that.
[00:57:42] And I think people are much worse, but it's a good thing.
[00:57:46] **Dwarkesh Patel:** What is a solution to model collapse? I mean, there's very naive things you could attempt is just like
[00:57:52] the distribution over load just should be wider or something. There's many naive things you could
[00:57:56] try. What ends up being the problem with the naive approaches? **Andrej Karpathy:** Yeah, I think that's a great question.
[00:58:01] I mean, you can imagine having a regularization for entropy and things like that. I guess they just
[00:58:05] don't work as well empirically, because right now, the models are collapsed, but I will say
[00:58:10] most of the tasks that we want of them don't actually demand the diversity. It's probably the
[00:58:17] answer of what's going on. And so it's just that the frontier labs are trying to make the models
[00:58:21] useful. And I kind of just feel like the diversity of the outputs is not so much, number one,
[00:58:26] it's much harder to work with an evaluate and all this kind of stuff, but maybe it's not what's
[00:58:29] actually capturing most of the value. **Dwarkesh Patel:** Right, it's actively penalized, right? If you're like super
[00:58:35] creative in RL, it's like not good. **Andrej Karpathy:** Yeah, or like maybe if you're doing a lot of writing, help
[00:58:39] promote elements and stuff like that, I think it's probably bad because the models will give you
[00:58:42] these like silently all the same stuff, you know, so they're not, they won't explore lots of
[00:58:48] different ways of answering a question, right? But I kind of feel like maybe the diversity is just not
[00:58:52] as big of a, yeah, maybe like, yeah, not as many applications need it so the models don't have it,
[00:58:57] but then it's actually a problem, it's in theory generation time, etc. So we're actually shooting
[00:59:00] ourselves in the foot by not allowing this entropy to maintain in the model. And I think possibly
[00:59:05] the labs should try harder. **Dwarkesh Patel:** And then I think you hinted that it's a, it's a very fundamental problem,
[00:59:10] it won't be easy to solve. And yeah, what's your intuition for that? **Andrej Karpathy:** I don't actually know if it's
[00:59:15] super fundamental. I don't actually know if I intended to say that. I do think that
[00:59:22] I haven't done these experiments, but I do think that you could probably regularize the entropy
[00:59:25] to be to be higher. So you're encouraging the model to give you more and more solutions,
[00:59:30] but you don't want it to start deviating too much from the training data. It's going to start
[00:59:33] making up its own language. It's going to start using words that are extremely rare. You know,
[00:59:37] so it's going to drift too much from the distribution. So I think controlling the distribution is just
[00:59:41] like a tricky, it's just like someone just has to. It's probably not trivial in that sense.
[00:59:47] **Dwarkesh Patel:** How many bits should the optimal core of intelligence end up being if you just had to make a guess?
[00:59:54] The thing we put on the von Neumann Pro, how big does it have to be?
[00:59:59] **Andrej Karpathy:** So it's really interesting in the history of the field, because at one point everything was very
[01:00:04] scaling-pilled in terms of like, oh, we're going to make much bigger models, trillions of
[01:00:07] parameter models. And actually what the models have done in size is they've gone up. And that was
[01:00:11] actually kind of like actually even come down to see if the models are smaller. And even then,
[01:00:17] I actually think they memorized way too much. So I think I had a prediction a while back that I
[01:00:22] almost feel like we can get cognitive course. There are very good at even like a billion
[01:00:26] billion parameters. It should be already like, like if you talk to a billion parameter model,
[01:00:30] I think in 20 years, you can actually have a very productive conversation, it thinks,
[01:00:35] and it's a lot more like a human. But if you ask it some factual question, might have to look it up,
[01:00:39] but it knows that it doesn't know and it might have to look it up and they will just do all the
[01:00:42] reasonable things. **Dwarkesh Patel:** That's actually surprising that you think it will take a billion, because already
[01:00:46] we have a billion parameter models or a couple of billion parameter models that are like very
[01:00:50] intelligent. Also, our models are like a trillion parameters, right? **Andrej Karpathy:** But they remember so much stuff,
[01:00:54] like, yeah. **Dwarkesh Patel:** But I'm surprised that in 10 years, given the pace, okay, we have a GPT OSS 20B
[01:01:04] that's way better than GPT-4 original, which was a trillion plus parameters. So given that trend,
[01:01:10] I'm actually surprised you think in 10 years, the cognitive course is still a billion parameters.
[01:01:14] I would, yeah, I'm surprised you're not actually going to be like tens of millions or millions.
[01:01:19] **Andrej Karpathy:** No, because I basically think that the training data is, so here's the issue, the training data
[01:01:23] is the internet, which is really terrible. So there's a huge amount of gains to be made because
[01:01:27] the internet is terrible. Like, if you actually, and even the internet, when you and I think of the
[01:01:30] internet, you're thinking of like, oh, Wall Street Journal, or that's not what this is. When you're
[01:01:35] actually looking at a preaching data set in the Frontier Lab and you look at a random internet
[01:01:38] document, it's total garbage. Like, I don't even know how this works at all. It's some like stock ticker
[01:01:44] symbols. It's a huge amount of slop and garbage from like all the corners of the internet. It's not
[01:01:50] like your Wall Street Journal article. That's extremely rare. So I almost feel like because the
[01:01:55] internet is so terrible, we actually have to sort of build really big models to compress all that.
[01:02:00] Most of that compression is memory work instead of like cognitive work. But what we really want
[01:02:04] is the cognitive part, actually, delete the memory. And then, so I guess what I'm saying is like,
[01:02:09] we need intelligent models to help us refine even the pre-training set to just narrow it down to
[01:02:14] the cognitive components. And then I think get away with a much smaller model because this is
[01:02:18] my much better data set and you could train it on it. But probably it's not trained directly
[01:02:21] on it. It's probably distilled for a much better model still. **Dwarkesh Patel:** But why is it distilled version still,
[01:02:26] a billion? I guess the thing I'm curious about. **Andrej Karpathy:** I just feel like distillation work is
[01:02:29] extremely well. So almost every small model, if you have a small model, it's almost certainly
[01:02:33] distilled. Why would you train on? **Dwarkesh Patel:** Why is a distillation not in 10 years not getting below one billion?
[01:02:39] **Andrej Karpathy:** Oh, you think it should be smaller? And then a billion? **Dwarkesh Patel:** I mean, come on, right?
[01:02:43] **Andrej Karpathy:** I don't know. At some point, it should take at least a billion knobs to do something interesting.
[01:02:49] **Dwarkesh Patel:** You just think it should be even smaller? **Andrej Karpathy:** Yeah, I mean, just like if you look at the trend
[01:02:52] over the last few years, just finding a little hanging fruit and going from like trillion plus
[01:02:56] models that are like literally two orders of magnitude smaller in a matter of two years and
[01:03:01] having better performance. Yeah, yeah. **Dwarkesh Patel:** It means that you think that the sort of like core of
[01:03:05] intelligence might be even way, way smaller. Like plenty of room at the bottom to prepare first
[01:03:11] Feynman. **Andrej Karpathy:** I mean, I almost feel like I'm already contrarian by talking about a billion in the
[01:03:14] parameter cognitive core and you're out doing me. I think, um, yeah, maybe we could get a little bit
[01:03:19] smaller. I mean, I still think that there should be enough, yeah, maybe it can be smaller. I do think
[01:03:23] that practically speaking, you want the model to have some knowledge. You don't want it to be looking
[01:03:27] up everything. Yeah. Um, because then you can't like think in your head, you're looking up way too
[01:03:30] much stuff all the time. So I do think it needs to be some basic curriculum needs to be there for
[01:03:35] knowledge. Uh, but it doesn't have a certain knowledge, you know? **Dwarkesh Patel:** So we're discussing what like
[01:03:39] plausibly could be the cognitive core. There's a separate question, which is, what will actually be
[01:03:44] the size of frontier models over time? And I'm curious to have predictions. So we had increasing
[01:03:50] scale up to maybe 4.5 and now we're seeing decreasing slash plateauing scale. There's many reasons
[01: