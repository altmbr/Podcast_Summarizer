[00:00:00] When personal learning is terrible, it just so happens that everything that we had before
[00:00:04] is much worse.
[00:00:06] I'm actually optimistic.
[00:00:07] I think this will work.
[00:00:08] I think it's tractable.
[00:00:09] I'm only sounding pessimistic because when I go on my Twitter timeline, I see all this
[00:00:13] stuff that makes no sense to me.
[00:00:14] A lot of it is, I think, honestly, just fundraising.
[00:00:17] We're not actually building animals.
[00:00:18] We're building ghosts.
[00:00:20] These lack sort of ethereal spirit entities because they're fully digital and they're
[00:00:23] kind of like minicking humans and it's a different kind of intelligence.
[00:00:26] It's business as usual because we're in an intelligence explosion already and have
[00:00:30] been for decades.
[00:00:31] Everything is gradually being automated.
[00:00:33] Has been for hundreds of years.
[00:00:34] Don't write blog posts.
[00:00:35] Don't do slides.
[00:00:36] Don't do any of that.
[00:00:37] Like build a code, arrange it, get it to work.
[00:00:39] It's the only way to go.
[00:00:40] Otherwise, you're missing knowledge.
[00:00:41] If you have a perfect AI tutor, maybe you can get extremely far.
[00:00:44] The geniuses of today, I bear a discussion in the surface of what a human mind can do, I
[00:00:47] think.
[00:00:48] Today, I'm speaking with Andre Carpati.
[00:00:51] Andre, why do you say that this will be the decade of agents and not the year of agents?
[00:00:55] Well, first of all, thank you for having me here.
[00:00:57] I'm excited to be here.
[00:01:00] The quote that you just mentioned, it's the decade of agents.
[00:01:02] That's actually a reaction to an existing pre-existing quote, I should say, where I think
[00:01:06] a lot of some of the labs, I'm not actually sure who said this, but they were alluding
[00:01:09] to this being the year of agents with respect to LLMs and how they were going to evolve.
[00:01:14] I think I was triggered by that because I feel like there's some over-predictions going
[00:01:19] on in the industry.
[00:01:21] In my mind, this is really a lot more accurately described as the decade of agents.
[00:01:25] We have some very early agents that are actually extremely impressive and that I use daily,
[00:01:30] cloud and codecs and so on, but I still feel like there's so much work to be done.
[00:01:34] I think my reaction is, we'll be working with these things for a decade.
[00:01:38] They're going to get better and it's going to be wonderful, but I think I was just reacting
[00:01:42] to the timeline, I suppose, of the implication.
[00:01:47] What do you think we'll take a decade to accomplish?
[00:01:49] Where are the bottlenecks?
[00:01:51] Actually, make it work.
[00:01:52] In my mind, when you're talking about an agent, I guess, or what the labs have in mind
[00:01:56] and what maybe I have in mind as well, you should think of it almost like an employee
[00:01:59] or like an intern that you would hire to work with you.
[00:02:02] For example, you work with some employees here.
[00:02:04] When would you prefer to have an agent like cloud or codecs do that work?
[00:02:08] Currently, of course, they can't.
[00:02:09] What would it take for them to be able to do that?
[00:02:11] Why don't you do it today?
[00:02:13] The reason you don't do it today is because they just don't work.
[00:02:16] They don't have enough intelligence.
[00:02:17] They're not multimodal enough.
[00:02:18] They can do computer use and all this kind of stuff.
[00:02:20] They don't do a lot of the things that you've alluded to earlier.
[00:02:23] They don't have continual learning.
[00:02:24] You can't just tell them something and they'll remember it.
[00:02:27] They're just cognitively lacking and it's just not working.
[00:02:30] I just think that it will take about a decade to work through all of those issues.
[00:02:32] Interesting.
[00:02:33] As a professional podcaster, a viewer of AI from afar, it's easy to identify for me.
[00:02:43] Here's what's lacking.
[00:02:44] Continual learning is lacking or multimodality is lacking.
[00:02:47] I don't really have a good way of trying to put a timeline on it.
[00:02:52] If somebody's like, how long will continue learning take?
[00:02:55] There's no prior I have, but this is a project that's just like five years, 10 years, 50 years.
[00:03:01] Why a decade?
[00:03:02] Why not one year?
[00:03:03] Why not 50 years?
[00:03:04] I guess this is where you get into a bit of my own intuition a little bit and also
[00:03:10] doing a bit of an extrapolation with respect to my own experience in the field.
[00:03:14] I guess I've been in AI for almost two decades.
[00:03:17] It's going to be maybe 15 years or so, not that long.
[00:03:20] You had Richard Sutton here who was all around, of course, for much longer, but I do have
[00:03:23] about 15 years of experience of people making predictions of seeing how they actually turned
[00:03:27] out.
[00:03:28] Also, I was in the industry for a while and I was in research and I worked in the industry
[00:03:31] for a while.
[00:03:32] I guess I have just a general intuition that I have left from that.
[00:03:37] I feel like the problems are tractable.
[00:03:40] They're surmountable, but they're still difficult.
[00:03:44] If I just average it out, it just feels like a decade, I guess, to me.
[00:03:47] This is actually quite interesting.
[00:03:48] I want to hear not only the history, but what people in the room felt was about to happen
[00:03:54] at various different breakthrough moments.
[00:03:58] What were the ways in which their feelings were either overly pessimistic or overly optimistic?
[00:04:03] Should we just go through each other one by one?
[00:04:05] That's a giant question because, of course, you're talking about 15 years of stuff that
[00:04:08] happened.
[00:04:09] AI is actually so wonderful because there have been a number of seismic shifts that were
[00:04:13] like the entire feel has suddenly looked a different way.
[00:04:16] I guess I've maybe lived through two or three of those and I still think there will continue
[00:04:20] to be some because they come with some almost surprising irregularity.
[00:04:25] When my career began, of course, when I started to work on deep learning, when I became interested
[00:04:29] in deep learning, this was just by chance of being right next to Jeff Hinton at the University
[00:04:33] of Toronto and Jeff Hinton, of course, is the Godfather figure of AI.
[00:04:37] He was training all these neural networks and I thought it was incredible and interesting,
[00:04:40] but this was not the main thing that everyone in AI was doing by far.
[00:04:43] This was a niche little subject on the side.
[00:04:46] That's maybe the first dramatic seismic shift that came with the Alexander and so on.
[00:04:51] I would say, like, Alexander's re-oriented everyone and everyone started to train neural
[00:04:54] networks, but it was still very per-task, per-specific task, so maybe I have an image
[00:05:00] classifier or I have a neural machine translator or something like that.
[00:05:04] People became very slowly actually interested in basically agents, I would say, and people
[00:05:09] started to think, maybe we have a check mark next to the visual cortex or something like
[00:05:12] that, but what about the other parts of the brain and how can we get an actual full
[00:05:16] agent or an entity that can actually interact in the world?
[00:05:19] I would say the Atari deep reinforcement learning shift in 2013 or so was part of that
[00:05:25] early effort of agents, in my mind, because it was an attempt to try to get agents that
[00:05:29] not just perceive the world, but also take actions and interact and get rewards from environments.
[00:05:34] At the time, this was Atari Games.
[00:05:36] I kind of feel like that was a misstep, actually, and it was a misstep that actually, even the
[00:05:41] early OpenAI that I was a part of, of course, kind of adopted, because at that time, the
[00:05:45] side-guised was reinforcement learning environments, games, game playing, beat games, get lots of
[00:05:51] different types of games, and OpenAI was doing a lot of that.
[00:05:54] So that was maybe like another prominent part of, I would say, AI, where, maybe before
[00:05:59] two or three or four years, everyone was doing reinforcement learning in games.
[00:06:03] And basically, that was a little bit of a misstep.
[00:06:06] And what I was trying to do at OpenAI, actually, is like, I was always a little bit suspicious
[00:06:09] of games as being like this thing that would actually lead to AGI, because in my mind,
[00:06:13] you want something like an accountant or something that's actually interacting with the
[00:06:16] real world.
[00:06:17] And I just didn't see how games kind of like add up to it.
[00:06:20] And so my project at OpenAI, for example, was within the scope of the universe project
[00:06:26] on an agent that was using keyboard and mouse to operate web pages.
[00:06:30] And I really wanted to have something that, like, interacts with, you know, the actual
[00:06:33] digital world that can do knowledge work.
[00:06:35] And it just so turns out that this was extremely early, way too early.
[00:06:39] So early that the wish didn't have been working on that, you know, because if you're just
[00:06:43] stumbling your way around and keyboard mashing and masclicking and trying to get rewards
[00:06:48] in these environments, your reward is two sparse and you just won't learn, and you're
[00:06:52] going to burn a forest computing, and you're never actually going to get something off
[00:06:55] the ground.
[00:06:56] So what you're missing is this power of representation in the neural network.
[00:07:01] And so for example, today, people are training those computer using agents, but they're doing
[00:07:03] it on top of a large language model.
[00:07:05] And so you actually have to get the language model first, you have to get the representations
[00:07:08] first, and you do that by all the pre-training and all the LLM stuff.
[00:07:11] So I kind of feel like maybe loosely speaking, it was like, people keep maybe trying to get
[00:07:17] the full thing to early a few times, where people like really try to go after agents too
[00:07:21] early, I would say.
[00:07:22] It was Atari and Universe, and even my own experience.
[00:07:26] And you actually have to do some things first before you sort of get to those agents.
[00:07:30] And maybe now the agents are a lot more competent, but maybe we're still missing sort
[00:07:33] of some parts of that stack.
[00:07:36] But I would say maybe those are like the three major buckets of what people were doing,
[00:07:40] training neural nets, per tasks, trying to the first round of agents, and then maybe
[00:07:45] the LLM's and actually seeking the representation power of the neural networks before you tack
[00:07:49] on everything else on top.
[00:07:51] Interesting.
[00:07:52] I guess if they were to steal men, the sort of a sudden perspective would be that humans
[00:07:56] actually can just take on everything at once, right?
[00:07:58] Even animals can take on everything at once, right?
[00:08:00] Animals are maybe a better example because they don't even have the scaffold of language.
[00:08:03] They just get thrown out into the world, and they just have to make sense of everything
[00:08:07] without any labels, and the vision for AGI then should just be something which just
[00:08:12] looks at sensory data, looks like the computer screen, and it just figures out what's going
[00:08:17] on from scratch.
[00:08:18] I mean, if a human was put in a similar situation, that would be trained from scratch,
[00:08:21] but I mean, this is like a human growing up, where an animal growing up.
[00:08:24] So why shouldn't that be the vision for AGI rather than like this thing where we're doing
[00:08:27] millions of years of training?
[00:08:29] I think that's a really good question, and I think, I mean, so Sutton was in your podcast,
[00:08:34] and I saw the podcast, and I had to write up about that podcast almost that gets into
[00:08:38] a little bit of how I see things, and I kind of feel like I'm very careful to make analogies
[00:08:43] to animals because they came about by very different optimization processes.
[00:08:48] Animals are evolved, and they actually come with a huge amount of hardware that's built
[00:08:51] in.
[00:08:52] And when, for example, my example in the post was the zebra, the zebra gets born, and
[00:08:57] a few minutes later, it's running around and following its mother.
[00:08:59] That's an extremely complicated thing to do.
[00:09:02] That's not reinforcement learning, that's something that's baked in, and evolution obviously
[00:09:05] has some way of encoding the weights of our neural nets in ATCGs, and I have no idea
[00:09:10] how that works, but it apparently works.
[00:09:12] So I kind of feel like brains just were king from a very different process, and I am
[00:09:18] very hesitant to take inspiration from it because we're not actually running that process.
[00:09:22] So in my post, I kind of said, we're not actually building animals.
[00:09:25] We're building ghosts, or spirits, or whatever people want to call it.
[00:09:30] Because we're not doing training by evolution, we're doing training by basically imitation
[00:09:36] of humans, and the data that they've put on the internet.
[00:09:39] And so you end up with these, like, sort of ethereal spirit entities because they're
[00:09:43] fully digital, and they're kind of like mimicking humans, and it's a different kind of intelligence.
[00:09:46] Like if you imagine a space of intelligence is, we're starting off at a different point
[00:09:50] almost.
[00:09:51] We're not really building animals, but I think it's also possible to make them a bit
[00:09:54] more animal-like over time, and I think we should be doing that.
[00:09:56] And so I kind of feel like, sort of just, I guess, one more point is, I do feel like
[00:09:59] Sutton basically has a very, like his framework is, like we want to build animals, and I actually
[00:10:05] think that would be wonderful.
[00:10:06] If we can get that to work, that would be amazing.
[00:10:07] If there was a single algorithm that you can just, you know, run on the internet, and
[00:10:12] it learns everything, that would be incredible, I almost suspect that I'm not actually sure
[00:10:17] that it exists, and that's certainly actually not what animals do.
[00:10:21] Because animals have this outer loop of evolution, and a lot of what looks like learning
[00:10:26] is actually a lot more maturation of the brain.
[00:10:28] And I think that there's actually a very little reinforcement learning for animals.
[00:10:32] And I think a lot of the reinforcement learning is actually like more like motor tasks.
[00:10:35] It's not intelligence tasks.
[00:10:36] So I actually kind of think humans don't actually like really use RL, roughly speaking
[00:10:40] is what I would say.
[00:10:41] The last sentence, a lot of that intelligence is not motor tasks.
[00:10:43] It's white, sorry.
[00:10:44] A lot of the reinforcement learning in my perspective would be things that are a lot
[00:10:46] more like motor-like, like simple kind of like task throwing hoop or stuff like that.
[00:10:53] But I don't think that humans use reinforcement learning for a lot of intelligence tasks,
[00:10:57] like problem solving and so on.
[00:10:59] Interesting.
[00:11:00] That doesn't mean we shouldn't do that for research, but I just feel like that's what
[00:11:04] animals do or don't.
[00:11:05] I'm going to take a second to digest that because there's a lot of different ideas.
[00:11:10] One clarification question I can ask to understand the perspective.
[00:11:14] So I think you suggest that look, evolution is doing the kind of thing that pre-training
[00:11:18] does in the sense of building something which can then understand the world.
[00:11:24] The difference I guess is that evolution has to be titrated in the case of humans through
[00:11:29] three gigabytes of DNA.
[00:11:31] And so that's very unlike the weights of a model.
[00:11:37] I mean, literally the weights of the model are a brain which obviously is not encoded
[00:11:40] in the sperm and the egg or does not exist in the sperm and the egg.
[00:11:44] So it has to be grown and also the information for every single synapse in the brain simply
[00:11:50] cannot exist in the three gigabytes that exist in the DNA.
[00:11:52] Evolution seems closer to finding the algorithm which then does the lifetime learning.
[00:11:58] Now, maybe the lifetime learning is not analogous to RL to your point.
[00:12:03] Is that compatible with the thing you were saying or would you disagree with that?
[00:12:05] I think so. I would agree with you that there's some miraculous compression going on
[00:12:08] because obviously the weights of the neural net are not stored in ATCGs.
[00:12:11] There's some kind of a dramatic compression and there's some kind of like learning algorithms
[00:12:15] encoded that take over and do some of the learning online.
[00:12:18] So I definitely agree with you on that.
[00:12:20] Basically, I would say I'm a lot more kind of like practically minded.
[00:12:23] I don't come at it from a perspective of like let's build animals.
[00:12:26] I come from a perspective of like let's build useful things.
[00:12:28] So I have a hard hat on and I'm just observing that like we're not going to do evolution
[00:12:32] because I don't know how to do that, but it does turn out we can build these ghost
[00:12:36] spirit like entities by imitating internet documents.
[00:12:39] This works and it's actually kind of like it's a way to bring you up to something that has a
[00:12:44] lot of sort of built in knowledge and intelligence in some way, similar to maybe what evolution has
[00:12:49] done. So that's why I kind of call pre-training this kind of like crappy evolution.
[00:12:53] It's like the practically possible version with art technology and what we have available to us
[00:12:58] to get to a starting point where we can actually do things like reinforcement learning and so on.
[00:13:03] Just to steal man the other perspective because after doing this on an interview and thinking
[00:13:06] about it a bit, it has an important point here. Evolution does not give us the knowledge really,
[00:13:11] right? It gives us the algorithm to find the knowledge and that seems different for pre-training.
[00:13:15] So if perhaps the perspective is that pre-training helps build the kind of entity which can
[00:13:20] learn better, teaches meta learning and therefore it is a similar to like finding an algorithm.
[00:13:26] But if if it's like evolution gives us knowledge, pre-training gives us knowledge or that
[00:13:29] analogy seems to break down. So it's subtle and I think you're right to push back on it. But basically
[00:13:34] the thing that pre-training is doing, so you're basically getting the next token predictor
[00:13:37] on over the internet and you're training that into a neural net. It's doing two things actually
[00:13:41] there are kind of like unrelated. Number one, it's picking up all this knowledge as I call it.
[00:13:45] Number two, it's actually becoming intelligent. By observing the algorithmic patterns in the
[00:13:50] internet, it actually kind of like boots up all these little circuits and algorithms inside the
[00:13:54] neural net to do things like in context learning and all this kind of stuff. And actually you don't
[00:13:58] actually need or want the knowledge. I actually think that's probably actually holding back the
[00:14:02] neural networks overall because it's actually like getting them to rely on the knowledge all too much
[00:14:06] sometimes. For example, I kind of feel like agents one thing they're not very good at is going
[00:14:10] off the data manifold of what exists on the internet. If they had less knowledge or less memory,
[00:14:15] actually maybe they would be better. And so what I think we have to do kind of going forward and this
[00:14:19] would be part of the research paradigms is actually think we need to start, we need to figure out ways to
[00:14:24] remove some of the knowledge and to keep what I call this cognitive as this cognitive core.
[00:14:28] Is this like intelligent entity that is kind of stripped from knowledge but contains the algorithms
[00:14:33] and contains the magic, you know, of intelligence and problem solving and the strategies of it
[00:14:38] and all this kind of stuff. There's so much interesting stuff there. Okay, so let's start with
[00:14:42] in context learning. This is an obvious point, but I think it's worth just like saying it explicitly
[00:14:47] and meditating on it. The situation in which these models seem the most intelligent in which they
[00:14:52] are like, I talked to them and I'm like, wow, there's really something on the other end that's
[00:14:56] responding to me thinking about things. If it like makes a mistake, it's like, oh, wait, that's
[00:14:59] actually the wrong way to think about it. I'm packing up. All that is happening in context. That's
[00:15:03] where I feel like the real intelligence you can like visibly see. And that in context learning
[00:15:08] process is developed by gradient descent on pre-training, right? Like it's spontaneously meta-learns
[00:15:14] in context learning. But the in context learning itself is not gradient descent. In the same way that
[00:15:20] our lifetime intelligence as humans to be able to do things is conditioned by evolution. But our
[00:15:26] actual learning during our lifetime is like happening through some other process. Actually, don't
[00:15:31] fully agree with that, but you should continue with that. Actually, then I'm curious to understand
[00:15:35] how that analogy breaks down. I think I'm hesitant to say that in context learning is not doing
[00:15:39] gradient descent because I mean, it's not doing explicit gradient descent, but I still think that
[00:15:44] so in context learning, basically, it's pattern completion within a token window, right? And it
[00:15:49] just turns out that there's a huge amount of patterns on the internet. And so you write the model
[00:15:52] kind of like learns to complete the pattern. And that's inside the weights. The weights of the
[00:15:56] neural network are trying to discover patterns and complete the pattern. And there's some kind of
[00:16:00] adaptation that happens inside the neural network, right? Which is kind of magical and just falls
[00:16:04] out from internet, just because there's a lot of patterns. I will say that there have been some
[00:16:10] papers that I thought were interesting that actually look at the mechanisms behind in context
[00:16:13] learning. And I do think it's possible that in context learning actually runs a small gradient
[00:16:16] descent loop internally in the layers of the neural network. And so I recall one paper in particular
[00:16:21] where they were doing linear regression actually using in context learning. So basically your inputs
[00:16:27] into the neural network are x, y pairs, x, y, x, y that happened to be on the line. And then you do x
[00:16:34] and you expect the y. And the neural network when you train it in this way actually does do, does do
[00:16:39] linear regression. And normally when you would run linear regression, you have a small gradient
[00:16:44] descent optimizer that basically looks at x, y, looks at an error, calculus, the gradient of
[00:16:48] the weights and does the update a few times. It just turns out that when they looked at the weights
[00:16:52] of that in context learning algorithm, they actually found some analogies to gradient descent
[00:16:58] and mechanics. In fact, I think even the paper one was stronger because they actually hard coded
[00:17:02] the weights of the neural network to do gradient descent through attention and all the
[00:17:08] internals of the neural network. So I guess that's just my only pushback is that who knows how in
[00:17:13] context learning works, but actually think that it's probably doing a little bit of some kind of
[00:17:17] funky gradient descent internally. And then I think that that's possible. So I guess I was only
[00:17:22] pushing back on you're saying it's not doing in context learning who knows what it's doing, but
[00:17:25] it's probably maybe doing something similar to it, but we don't know. So then it's worth thinking
[00:17:29] about, okay, if both of them are implementing gradient descent, if in context learning and
[00:17:33] pre-training are both implementing, something like gradient descent, why does it feel like in context
[00:17:39] learning actually we're getting to this like continual learning, real intelligence like thing,
[00:17:44] whereas you don't get the analogous feeling just from pre-training, at least you could argue that.
[00:17:49] And so if it's the same algorithm, what could be different? Well, one way you can think about it is
[00:17:52] how much information does the model store per information it receives from training? And if you
[00:18:00] look at pre-training, if I think if you look at Lama 3, for example, I think it's trained on
[00:18:05] 15 trillion tokens and if you look at the 70B model, that would be the equivalent of 0.07
[00:18:12] bits per token in that it sees in pre-training in terms of like the information in the weights of
[00:18:16] the model compared to the tokens it reads, whereas if you look at the KV cache and how it grows per
[00:18:22] additional token in context learning, it's like 320 kilobytes. So that's a 35 million fold difference
[00:18:28] in how much information per token is assimilated by the model. I wonder if that's relevant at all.
[00:18:34] I think I kind of agree. I mean, the way I usually put this is that anything that happens during
[00:18:39] the training of the neural network, the knowledge is only kind of like a hazy recollection of what
[00:18:43] happened in training in a training time. And that's because the compression is dramatic. You're
[00:18:47] taking 15 trillion tokens and you're compressing it to just your final network of a few balloon
[00:18:50] parameters. So obviously it's a massive amount of compression going on. So I kind of refer to it as
[00:18:55] like a hazy recollection of the internet documents, whereas anything that happens in the context window
[00:18:59] of the neural network, you're plugging all the tokens and building up all the KV cache representation,
[00:19:03] it's very directly accessible to the neural net. So I compare the KV cache and the stuff that
[00:19:08] happens at test time to like more like a working memory, like all the stuff that's in the in the
[00:19:13] in the context window is very directly accessible to the neural net. So there's always like these
[00:19:18] almost surprising analogies between LLMs and humans. And I find them kind of surprising because
[00:19:22] we're not trying to build a human brain, of course, just directly. We're just finding that this
[00:19:26] works and we're doing it. But I do think that anything that's in the weights, it's kind of like a
[00:19:30] hazy recollection of what you read a year ago. Anything that you give it as a context at test time
[00:19:36] is directly in the working memory. And I think that's a very powerful analogy to things for things.
[00:19:40] So when you, for example, go to an LLM and you ask it about some book and what happened in it,
[00:19:44] like on the claims book or something like that, the LLM will often give you some stuff which is
[00:19:48] roughly correct. But if you give it the full chapter and ask it questions, you're going to get
[00:19:52] much better results because it's now loaded in the working memory of the model. So I basically agree
[00:19:57] with you a very long way of saying that I kind of agree. And that's why stepping back,
[00:20:00] what is it the part about human intelligence that we like have a most feel to replicate with these
[00:20:06] models? I almost feel like just just a lot of it. So maybe one way to think about it, I don't
[00:20:15] know if this is the best way, but I almost kind of feel like, again, making these analogies in
[00:20:20] perfect as they are, we've stumbled by with the transformer neural network, which is extremely
[00:20:25] powerful. Very general. You can train transformers on audio or video or text or whatever you want. And
[00:20:31] it just learns patterns and they're very powerful and it works really well. That to me almost
[00:20:36] indicates that this is kind of like some piece of cortical tissue. It's something like that because
[00:20:40] the cortex is famously very plastic as well. You can rewire, you know, parts of brains and there was
[00:20:46] the slightly gruesome experiments with rewiring like visual cortex to the auditory cortex and
[00:20:51] this animal like learn file, et cetera. So I think that this is kind of like a cortical tissue.
[00:20:56] I think when we're doing reasoning and planning inside the neural networks, so basically doing
[00:21:01] a reasoning traces for thinking models, that's kind of like the prefrontal cortex. And then
[00:21:08] I think we maybe those are like blue check marks, but I still think there's many brain parts and
[00:21:13] nuclei that are not explored. So maybe for example, there's a basic anglia doing their
[00:21:16] better reinforcement learning when we find two in the models on reinforcement learning. But
[00:21:20] you know, whereas like the hippocampus, not obvious what that would be. Some parts are probably not
[00:21:24] important. Maybe the cerebellum is like not important to cognition. It's thoughts of maybe we can
[00:21:27] skip some of it. But I still think there's, for example, the amygdala, all the emotions and instincts.
[00:21:33] And there's probably like a bunch of other nuclei in the brain that are very ancient that I don't
[00:21:36] think we've like really replicated. I don't actually know that we should be pursuing, you know,
[00:21:40] the building of an analog of human brain. Again, an engineer, mostly at heart. But
[00:21:46] I still feel like maybe another way to answer the question is, you're not going to hire this thing
[00:21:51] as an intern and it's missing a lot of it's because it comes with a lot of these cognitive deficits
[00:21:55] that we all intuitively feel when we talk to the models. And so it's just like not fully there yet.
[00:22:00] You can look at it as like not all the brain parts are checked off yet.
[00:22:04] This is maybe relevant to the question of thinking about how fast these issues will be solved.
[00:22:09] So sometimes people will say about continual learning. Look, actually you could already,
[00:22:15] you could easily replicate this capability. Just as in context learning emerged spontaneously as a
[00:22:20] result of pre-training, continual learning over longer horizons will emerge spontaneously if the
[00:22:26] model is incentivized to recollect information over longer horizons or horizons longer than one
[00:22:32] session. So if there's some like outer loop RL, which has many sessions within that outer loop,
[00:22:42] then like this continual learning where it uses like fine tunes itself or it writes to an
[00:22:46] external memory or something will just sort of like emerge spontaneously. Do you think, do you think
[00:22:50] things are anything that are plausible? I don't know really a prior of it. How plausible is that,
[00:22:54] how likely is that to happen? I don't know that I fully resonate with that because I feel like these
[00:22:57] models when you boot them up and they have zero tokens in the window, they're always like restarting
[00:23:01] from scratch where they were. So I don't actually know in that world view what it looks like because
[00:23:08] again, maybe some analogies to humans just because I think it's roughly concrete and kind of
[00:23:12] interesting to think through. I feel like when I'm awake, I'm building up a context window of
[00:23:16] stuff that's happening during the day. But I feel like when I go to sleep, something magical happens
[00:23:20] where I don't actually think that the context window stays around. I think there's some process
[00:23:24] of distillation into weights of my brain and this happens during sleep and all this kind of stuff.
[00:23:29] We don't have an equivalent of that in larger language models and that's to me more adjacent to
[00:23:35] when you talk about continual learning and so on, as absent. These models don't really have
[00:23:39] this distillation phase of taking what happened, analyzing it, obsessively thinking through it,
[00:23:47] basically doing some kind of a synthetic data generation process and distilling it back into the
[00:23:50] weights and maybe having a specific neural net per person, maybe it's a lura, it's not a full weight
[00:23:59] neural network that's just some of the small sparse subset of the weights are changed.
[00:24:04] But basically, we do want to create ways of creating these individuals that have very long
[00:24:09] contexts. It's not only remaining in the context window because the context windows grow very,
[00:24:13] very long. Maybe we have some very elaborate sparse attention over it. But I still think that
[00:24:18] humans obviously have some process for distilling some of that knowledge into the weights. We're
[00:24:22] missing it. And I do also think that humans have some kind of a very elaborate sparse attention
[00:24:28] scheme, which I think we're starting to see some early hints of. So DeepSeq V3.2 just came out and
[00:24:35] I saw that they have like a sparse attention as an example. And this is one way to have very,
[00:24:39] very long context windows. So I almost feel like we are redoing a lot of the cognitive tricks that
[00:24:45] evolution came up with through a very different process. But we're, I think, converging a similar
[00:24:48] architecture cognitively. Interesting. In 10 years, do you think it'll still be something like a
[00:24:53] transformer, but with a much more modified attention and more sparse MLPs and so forth?
[00:24:58] Well, the way I like to think about it is, okay, let's translation invariance in time, right? So
[00:25:02] 10 years ago, where were we? 2015, we had a convolutional neural networks primarily. Resurone
[00:25:08] networks just came out. So remarkably similar, I guess, but quite a bit different still. I mean,
[00:25:13] transformer was not around. All these more modern tweaks on a transformer were not around. So
[00:25:21] maybe some of the things that we can bet on, I think, in 10 years by translational sort of
[00:25:26] equivalence is we're still training giant neural networks forward backward pass and update
[00:25:31] through gradient descent. But maybe it looks a little bit different and it's just everything
[00:25:37] is much bigger. Actually, recently, I also went back all the way to 1989, which was kind of a fun
[00:25:43] exercise for me a few years ago, because I was reproducing Jan LaCoon's 1989
[00:25:48] convolutional network, which was the first neural network I'm aware of trained via
[00:25:51] gradient descent, like modern neural network trained gradient descent on a digital recognition.
[00:25:57] And I was just interested in, okay, how can I modernize this? How much of this is algorithms? How
[00:26:01] much of this is data? How much of this progress is compute and systems? And I was able to very quickly
[00:26:05] like half the learning rate, just knowing by time travel by 33 years. So if I time travel by
[00:26:11] algorithms to 33 years, I could adjust with Jan LaCoon data in 1989 and I could basically have
[00:26:16] the learning half the error. But to get further gains, I had to add a lot more data. I had to
[00:26:21] like 10X the training set. And then I had to actually add more computational optimizations,
[00:26:26] had to basically train for much longer with dropout and other regularization techniques.
[00:26:30] And so it's almost like all these things have to improve simultaneously. So you know,
[00:26:35] we're probably going to have a lot more data. We're probably going to have a lot better hardware,
[00:26:37] probably going to have a lot better kernels and software. We're probably going to have better
[00:26:40] algorithms. And all of those is almost like no one of them is winning too much. All of them are
[00:26:45] surprisingly equal. And this has kind of been the trend for a while. So I guess to answer maybe
[00:26:51] your question, I expect differences. Algorithmically to what's happening today. But I do also expect
[00:26:58] that some of the things that have stuck around for a very long time will probably still be there.
[00:27:01] It's probably still a giant neural network trained with gradient descent. That would be my guess.
[00:27:04] It's surprising that all of those things together only have half the error, which is like 30 years
[00:27:13] of progress. Maybe half is like, if you have the error, that actually means that half is a lot.
[00:27:18] Yeah. But it's I guess what we're shocking to me is everything needs to improve across the board.
[00:27:24] Architecture, optimize a loss function and also has improved across the board forever. So I kind of
[00:27:28] expect all those changes to be alive and well. Yeah, actually, I was about to ask you a very
[00:27:32] similar question about NanoChat because since you just coded up recently, every single sort of step
[00:27:38] in the process of building a chatbot is like freshening around. And I'm curious. If you had
[00:27:44] similar thoughts about like, oh, there was no one thing that was relevant to going from GPT-2
[00:27:50] to NanoChat. What are sort of like surprising takeaways from the experience of building
[00:27:56] AnChat? So NanoChat is a kind of repository I released. Was it yesterday or day before? I can't
[00:28:01] remember. We can see this liberation though and into the. Well, it's just trying to be a,
[00:28:09] it's trying to be the simplest, complete repository that covers the whole pipeline into end of
[00:28:13] building a chatbot clone. And so, you know, you have all of the steps, not just any individual step,
[00:28:19] which is a bunch of, I worked on all the individual steps sort of in the past and released small
[00:28:22] pieces of code that kind of show you how that's done in algorithmic sense in like simple code.
[00:28:29] But this kind of handles a lot of the entire pipeline. I think in terms of learning, it's not,
[00:28:33] it's not so much, I don't know that I actually found something that I learned from it,
[00:28:37] necessarily. I kind of already had in my mind as like how you build it. And this is just a process
[00:28:41] of mechanically building it and making it clean enough so that people can actually learn from it.
[00:28:48] And that they find it useful. Yeah. What is the best way for somebody to learn from it?
[00:28:54] Is it just like delete all the code and try to remove them from scratch, try to add modifications
[00:28:58] to it? Yeah, I think that's a great question. I would probably say, so basically it's about
[00:29:02] a thousand lines of code that takes you through the entire pipeline. I would probably put it on the
[00:29:06] right monitor, like if you have two monitors, you put it on the right. And you want to build it from
[00:29:10] scratch, you build it from start, you're not allowed to copy paste, you're allowed to reference,
[00:29:15] you're not allowed to copy paste. Maybe that's how I would do it. But I also think the repository
[00:29:19] by itself, it is like a pretty large beast. I mean, it's, you know, it's a ritz. When you write
[00:29:24] this code, you don't go from top to bottom. You go from trunks and you grow the trunks. And
[00:29:28] that information is absent, like you wouldn't know where to start. And so I think it's not just a
[00:29:32] file repository that's needed. It's like the building of the repository, which is a complicated trunk
[00:29:37] growing process. Right. So that part is not there yet. I would love to actually like add that
[00:29:41] probably later this week or something in some way. Like either it's a, it's probably a video or
[00:29:46] something like that. But, but maybe roughly speaking with that's what I would try to do is build
[00:29:51] the stuff yourself. But don't allow yourself copy paste. Yeah. I do think that there's two types
[00:29:56] of knowledge, almost like there's the high level surface knowledge. But the thing is that when
[00:29:59] you actually build something from scratch, you're forced to come to terms with what you don't
[00:30:03] actually understand. And you don't know that you don't understand it. Interesting. And it always
[00:30:06] leads to a deeper understanding. And it's like just the only way to build this, like if I can't build
[00:30:12] it, I don't understand it. Is that a fine man code, I believe, or something along those lines?
[00:30:16] I 100% I've always believed this very strongly because there's all these like micro things that
[00:30:22] are just not properly arranged and you don't really have the knowledge. You just think you have
[00:30:25] the knowledge. So don't write blog posts. Don't do slides. Don't do any of that. I can build a code,
[00:30:29] arrange it, get it to work. So the only way to go, otherwise you're missing knowledge.
[00:30:33] You treated out that coding models were actually a very little help to you in assembling this
[00:30:37] repository. And I'm curious why that was. Yeah. So the repository, I guess I built it over a period of
[00:30:44] a bit more than a month. And I would say there's like three major classes of how people
[00:30:48] interact with code right now. Some people completely reject all of LLMs and they are just
[00:30:53] writing by scratch. I think this is probably not their, their I think to do anymore.
[00:30:58] The intermediate part, which is where I am, is you still write a lot of things from scratch,
[00:31:02] but you use the autocomplete. That's basically available now from these models. So when you start
[00:31:07] writing out, it will be piece of it. It will, they were all complete from you and you can just tap
[00:31:11] through. And most of the time it's correct. Sometimes it's not and you edit it. But you're still
[00:31:14] very much the sort of architect of what you're writing. And then there's the, you know, vibe coding,
[00:31:20] you know, high, please implement this or that, you know, enter and then let the model do it. And
[00:31:25] that's the agents. I do feel like the agents work in very specific settings. And I would use them
[00:31:32] in specific settings. But again, these are all tools available to you and you have to like learn what
[00:31:36] they, what they're good at and what they're not good at and when to use them. So the agents are
[00:31:39] actually pretty good, for example, if you're doing boilerplate stuff, boilerplate code that's like
[00:31:43] just, you know, just copy paste stuff, they're very good at that. They're very good at stuff that
[00:31:47] occurs very often in the internet because there's lots of examples of it in the training sets of
[00:31:52] these models. So, so there's like features of things that where the models will do very well.
[00:31:58] I would say nanochet is not an example of this because it's a fairly unique repository. There's
[00:32:03] not that much code, I think, in a way that I've structured it. And, and it's not boilerplate code.
[00:32:08] It's like actually like intellectual intense code almost and everything has to be very
[00:32:11] precisely arranged. And the models were always trying to, they kept trying to, I mean,
[00:32:16] they have so many cognitive deficits, right? So one example, they keep trying to, they keep
[00:32:19] misunderstanding the code because they, they have too much memory from all the typical ways of
[00:32:25] doing things on the internet that I just wasn't adopting. So the models, for example, I mean,
[00:32:30] I don't know if I want to get into the full details, but they keep, they keep, they keep thinking
[00:32:34] I'm writing normal code and I'm not. Maybe one example, maybe one example. So the way to synchronize,
[00:32:40] so we have eight GPUs that are all doing forward efforts. The way to synchronize gradients between
[00:32:44] them is to use distributed data parallel container of PyTorch, which automatically does all the,
[00:32:49] as you're doing the back where it will start communicating and synchronizing gradients,
[00:32:52] I didn't use DDP because I didn't want to use it because it's not necessary. So I threw it out.
[00:32:57] And I basically wrote my own synchronization routine that's inside the step of the optimizer.
[00:33:02] And so the models were trying to get me to use the DDP container. Yeah. And they were very concerned
[00:33:06] about, okay, this gets way too technical, but I wasn't using that container because I don't need it
[00:33:11] and I have a custom implementation of something like it. And they just couldn't internalize it,
[00:33:15] you had your own. Yeah, they couldn't, they couldn't, they couldn't get passed out. And then,
[00:33:19] they kept trying to like mess up the style, like they're way too over defensive. They make all
[00:33:23] these tri-catch statements. They keep trying to make a production code base. And I have a bunch of
[00:33:28] assumptions in my code and it's okay. And, and it's just like, I don't need all this extra stuff
[00:33:34] in there. And so I just kind of feel like they're bloating the code base. They're bloating the
[00:33:36] complexity. They keep misunderstanding. They're using deprecated APIs a bunch of times. So it's total
[00:33:42] mess. And it's just, it's just not net useful. I can go in and I can clean it up, but it's not net
[00:33:47] and useful. I also feel like it's kind of annoying to have to like type out what I want in English,
[00:33:52] because it's just too much typing. Like, if I just navigate to the part of the code that I want,
[00:33:56] and I go where I know the code has to appear and I start typing out the first three letters,
[00:34:00] auto-complete gets it and just gives you the code. And so I think it's, this is a very high
[00:34:04] information bandwidth to specify what you want. If you point to the code where you want it and you
[00:34:08] type out the first few pieces, and the model will complete it. So I guess what I mean is,
[00:34:14] I think these models are good in certain parts of the stack. Actually, use the models a little bit
[00:34:19] in, there are two examples where I actually use the models that I think are illustrative.
[00:34:24] One was when I generated the report, and that's actually more boilerplatey. So actually, if I
[00:34:28] coded partially some of that stuff, that was fine. Because it's not like mission-critical stuff,
[00:34:33] and it works fine. And then the other part is when I was rewriting the tokenizer in Rust,
[00:34:37] I'm actually not as good at Rust because I'm fairly new to Rust. So I was doing, there's a bit of
[00:34:42] vibe coding going on in when I was writing some of the Rust code. But I had Python implementation
[00:34:47] that I fully understand, and I'm just making sure I'm making more efficient version of it,
[00:34:50] and I have tests. So I feel safer doing that stuff. And so basically they lower or like the
[00:34:56] increased accessibility to languages or paradigms that you might not be as familiar with. So I
[00:35:02] think they're very helpful there as well. Because there's a ton of Rust code out there,
[00:35:06] the models are actually pretty good at it. I happen to not know that much about it. So the models
[00:35:09] are very useful there. The reason I think this question is so interesting is because the main story
[00:35:15] people have about AI exploding and getting to super intelligence pretty rapidly is AI automating,
[00:35:22] AI engineering and AI research. And so they'll look at the fact that you can have cloud code and
[00:35:27] make entire application, crowd applications from scratch and be like, if you had this
[00:35:30] incubability inside of open AI and deep mind and everything, well, just imagine the level of like
[00:35:34] just, you know, a thousand of you or a million of you in parallel finding little architectural
[00:35:40] tweaks. And so it's quite interesting to hear you say that this is the thing they're sort of
[00:35:44] asymmetrically worse at. And it's like quite relevant to forecasting whether the AI 2027 type
[00:35:50] explosion is likely to happen anytime soon. I think that's a good way of putting it. And I think
[00:35:55] you're getting at some of my, like why my timelines are a bit longer. You're right. I think,
[00:36:00] yeah, they're not very good at code that has never been written before. Maybe it's like one way
[00:36:04] to put it, which is like what we're trying to achieve when we're building these models.
[00:36:07] Very naive question, but the architectural tweaks that you're adding to NANO chat,
[00:36:14] they're in a paper somewhere, right? They might even be in a repo somewhere. So it's,
[00:36:18] is it, is it surprising that they aren't able to integrate that into whenever you're like
[00:36:25] ad rope embeddings or something, they do that in the wrong way?
[00:36:29] It's, it's tough. I think they kind of know, they kind of know, but they don't fully know,
[00:36:32] and they don't know how to fully integrate it into the repo in your style and your code and your
[00:36:36] place and some of the custom things that you're doing. And how fits with all the assumptions of
[00:36:40] the repository and all this kind of stuff. So I think they do have some knowledge, but
[00:36:44] they haven't gotten to the place where they can actually integrate it, make sense of it, and so on.
[00:36:50] I do think that a lot of the stuff, by the way, continues to improve. So I think currently,
[00:36:54] probably state-of-the-art model that I go to is the GPT-5 Pro. And that's a very, very powerful
[00:36:59] model. So if I actually have 20 minutes, I will copy paste my entire repo and I go to GPT-5 Pro,
[00:37:03] the Oracle for like some questions. And often it's not too bad. And surprisingly good compared to
[00:37:08] what existed a year ago. But I do think that overall, the models are, they're not there. And I
[00:37:14] kind of feel like the industry, it's over, it's making too big of a jump, and it's trying to pretend
[00:37:23] like this is amazing, and it's not. It's slop. And I think they're not coming to terms with it,
[00:37:27] and maybe they're trying to fundraise or something like that. I'm not sure what's going on, but
[00:37:31] we're at this intermediate stage. The models are amazing. They still leave a lot of work.
[00:37:35] For now, autocomplete is my sweet spot. But sometimes for sometimes of code, I will go to a
[00:37:40] null-imagined. Yeah. Actually, this is also, here's another reason, but this is really interesting.
[00:37:45] Through the history of programming, there's been many productivity improvements,
[00:37:52] compilers, linting, better programming languages, etc. Which have increased program of productivity,
[00:37:57] but have not led to an explosion. So that's like one, that sounds very much like autocomplete
[00:38:02] tab. And this other category is just like automation of the programmer. And it's interesting,
[00:38:08] you're seeing more in the category of the historical analogies of like, you know, better compilers
[00:38:12] or something. I mean, because this gets said, one other kind of thought is like, I do feel like
[00:38:17] I have a hard time differentiating where AI begins and stops, because I do see AI as fundamentally
[00:38:22] an extension of computing in some pretty fundamental way. And I feel like I see a continuum of
[00:38:27] this kind of like recursive self-improvement or like of speeding up programmers all the way from
[00:38:32] beginning, like even like, I would say like code editors, syntax highlighting, syntax or like
[00:38:40] checking even of the types, like data type checking. All these kinds of tools that we've built for
[00:38:46] each other, even search engines, like why aren't search engines part of AI? Like, I don't know,
[00:38:50] like ranking is kind of AI, right? At some point Google was like, even early on, they were thinking
[00:38:55] of themselves as an AI company doing Google search engine, which I think is totally fair. And so
[00:38:59] I kind of see it as a lot more of a continuum than I think other people do. And I don't,
[00:39:02] it's hard for me to draw the line. And I kind of feel like, okay, we're now getting a much
[00:39:05] better autocomplete. And now we're also getting some agents, which are kind of like these loopy
[00:39:09] things, but they kind of go off rails sometimes. And what's going on is that the human is progressively
[00:39:15] doing a bit less and less of the low level stuff. For example, we're not writing the assembly code
[00:39:19] because we have compilers, right? Like compilers will take my high level language and see and write
[00:39:22] the assembly code. So we're abstracting ourselves very, very slowly. And there's this what I call
[00:39:27] autonomy slider of like more and more stuff is automated off the stuff that can be automated at
[00:39:31] any point in time. And we're doing a bit less and less than raising ourselves in the layer
[00:39:35] abstraction over the automation. One of the big problems with RL is that it's incredibly
[00:39:40] information sparse. The toolbox can help you with this by increasing the amount of information
[00:39:45] that your agent gets to learn from with every single episode. For example, one of their
[00:39:50] customers wanted to train a coding agent. So label box augmented an IDE with a bunch of extra
[00:39:56] data collection tools and staffed a team of expert software engineers from their aligner network
[00:40:01] to generate trajectories that were optimized for training. Now, obviously, these engineers evaluated
[00:40:06] these interactions on a pass field basis, but they also rated every single response on a bunch of
[00:40:12] different dimensions like readability and performance. And they wrote down their thought processes
[00:40:17] for every single rating that they gave. So you're basically showing every single step an engineer
[00:40:22] takes at every single thought that they have while they're doing their job. And this is just
[00:40:27] something you could never get from usage data alone. And so label box packaged up all these evaluations
[00:40:33] and included all the agent trajectories and the corrective human edits for the customer to train on.
[00:40:39] This is just one example. So go check out how label box can get you high quality frontier data
[00:40:44] across domains, modalities, and training parents. Reach out at labelbox.com slash the war cache.
[00:40:53] Let's talk about RL a bit. You two do some very interesting things about this. Conceptually,
[00:40:59] how should we think about the way that humans are able to build a rich world model just from
[00:41:06] interacting with our environment? And in ways that seems almost irrespective of the final reward
[00:41:11] at the end of the episode. If somebody started a start a business and at the end of 10 years,
[00:41:17] she finds out whether the business succeeded or failed. We say that she's earned a bunch of wisdom
[00:41:21] and experience. But it's not because like the log probs of every single thing that happened over
[00:41:25] the last 10 years are up weighted or down weighted. It's something much more deliberate and rich
[00:41:29] is happening. What is the ML analogy? And how does that compare to what we're doing with other ones
[00:41:34] right now? Yeah, maybe the way I would put it is humans don't use reinforcement learning is maybe what
[00:41:38] as I've said it all. I think they do something different which is yeah, you experience. So
[00:41:42] reinforcement learning is a lot worse than I think the average person thinks.
[00:41:47] reinforcement learning is terrible. It just so happens that everything that we had before
[00:41:52] it is much worse. Because previously we're just imitating people so it has all these issues.
[00:41:59] So in reinforcement learning, say you're working with you're solving a math problem because it's
[00:42:03] very simple. You're giving a math problem and you're trying to find a solution. Now in reinforcement
[00:42:08] learning, you will try it lots of things in parallel first. So you're given a problem. You try hundreds
[00:42:15] of different attempts and these attempts can be complex right? They can be like, oh, let me try this,
[00:42:19] let me try that. This didn't work. That didn't work, et cetera. And then maybe you get an answer.
[00:42:23] And I checked the back of the book and you see, okay, the correct answer is this. And then you can
[00:42:28] see that, okay, this one, this one and that one got the correct answer, but these other 97 of them
[00:42:32] didn't. So literally what reinforcement learning does is it goes to the ones that worked really well.
[00:42:36] And every single thing you did along the way, every single token gets up weighted of like do more
[00:42:41] of this. The problem with that is I mean, people will say that your estimator has high variance,
[00:42:46] but what I mean, it's just noisy. It's noisy. So basically, it kind of almost assumes that every
[00:42:52] single little piece of the solution that you made that ride the dry answer was correct thing to do,
[00:42:55] which is not true. Like you may have gone down the wrong alleys until you ride the right solution.
[00:43:00] Every single one of those incorrect things you did, as long as you got to the correct solution,
[00:43:03] will be up weighted as do more of this. It's terrible. Yeah, it's noise. You've done all this work
[00:43:08] only to find a single, at the end, you get a single number of like, oh, you did correct. And
[00:43:14] based on that, you weigh that entire trajectory is like up weight or down weight. And so the way
[00:43:19] I like to put it is you're sucking supervision through straw because you've done all this work that
[00:43:23] could be a minute to roll out. And you're like sucking the bits of supervision of the final reward
[00:43:28] through straw. And you're like putting it, you're like, you're basically like,
[00:43:33] yeah, you're broadcasting that across the entire trajectory and using that to up weight or down
[00:43:36] with that trajectory. It's too crazy. A human would never do this. Number one, a human would never
[00:43:40] do hundreds of roll outs. Number two, when a person sort of finds a solution, they will have a
[00:43:46] pretty complicated process of review of like, okay, I think these parts that I did well, these parts,
[00:43:50] I did not do that well. I should probably do this or that. And they think through things.
[00:43:55] There's nothing in current LLM that does this. There's no equivalent of it. But I do see papers
[00:44:00] popping out that are trying to do this because it's obvious to everyone in the field. So I kind of see
[00:44:04] as like the first imitation learning actually, by the way, was extremely surprising and miraculous
[00:44:08] and amazing that we can fine tune by imitation of humans. And that was incredible because in the
[00:44:13] beginning, all we had was base models. Base models are autocomplete. And it wasn't obvious to me at
[00:44:18] the time. And I had to learn this and the paper that like blew my mind was instruct GPT because it
[00:44:24] pointed out that, hey, you can trade the pre-train model, which is autocomplete. And if you just
[00:44:28] fine tune it on text that looks like conversations, the model will very rapidly adapt to become
[00:44:33] a very conversational. And it keeps all the knowledge from pre-train. And this blew my mind because
[00:44:37] I didn't understand that it's just like stylistically can adjust so quickly and become an assistant
[00:44:42] to a user through just a few loops of fine tuning on that kind of data. It was very miraculous to
[00:44:47] me that that worked. So incredible. And that was like two years, three years of work. And now came
[00:44:53] RL. And RL allows you to do a bit better than just imitation learning, right? Because you can have
[00:44:58] these reward functions and you can hill climb on the reward functions. And so some problems have
[00:45:03] just correct answers. You can hill climb on that without getting expert trajectories to imitate.
[00:45:07] So that's amazing. And the model can also discover solutions that the human might never come up with.
[00:45:12] So this is incredible. And yet it's still stupid. So I think we need more. And so I saw a paper from
[00:45:19] Google yesterday that tried to have this reflect and review page idea in mind. What was the
[00:45:26] memory bank paper or something? I don't know. I've actually seen a few papers along these lines.
[00:45:30] So I expect there to be some kind of a major update to how we do algorithms for other
[00:45:34] lambs coming in that realm. And then I think we need three or four or five more.
[00:45:41] Something like that. But you're still going to come up with the evocative evocative phrases,
[00:45:46] sucking supervision through a straw. It's like so good.
[00:45:50] Why hasn't, so you're saying like you're problem with outcome based reward is that you have
[00:45:56] this huge trajectory. And then at the end, you're trying to learn every single possible thing about
[00:46:01] what you should do and what you should learn about the world from that one final bit. Why hasn't,
[00:46:07] given the fact that this is obvious, why hasn't processed space supervision as an alternative,
[00:46:11] been a successful way to make models more capable? What has been preventing us from using this
[00:46:15] alternative paradigm? So process based supervision just refers to the fact that we're not going to have
[00:46:19] a reward function only at the very end of after you've made 10 minutes of work and not going to tell you
[00:46:22] you did well or not. Well, I'm going to tell you at every single step of the way how well you're doing.
[00:46:27] And this is basically the reason we don't have that is it's not tricky how you do that properly.
[00:46:31] Because you have partial solutions and you don't know how to assign credit.
[00:46:34] So when you get the right answer, it's just an equality match to the answer, very simple to
[00:46:39] implement. If you're doing basically process supervision, how do you assign an automatical way,
[00:46:45] partial credit assignment? It's not obvious how you do it. Lots of labs, I think, are trying to do it
[00:46:49] with these LLM judges. So basically you get LLMs to try to do it. So you prompt an LLM,
[00:46:53] hey, look at a partial solution of a student. How well do you think they're doing if the answer is
[00:46:56] this and they try to tune the prompt? The reason that I think this is kind of tricky is quite subtle.
[00:47:02] And it's the fact that anytime you use an LLM to assign a reward, those LLMs are giant things with
[00:47:07] billions of parameters and they're gameable. And if you're reinforcement learning with respect to
[00:47:11] them, you will find adversarial examples for your LLM judges, almost guaranteed. You can't do this
[00:47:16] for too long. You do maybe 10 steps or 20 steps, maybe it will work, but you can't do 100 or 1000
[00:47:20] because it's not obvious because I know I understand it's not obvious, but basically the model will find
[00:47:26] little cracks, it will find all these like spurious things in the nooks and crannies of the giant
[00:47:32] model and find a way to cheat it. So one example that's prominently in my mind is I think this,
[00:47:38] I think this was probably public, but basically if you're using an LLM judge for a reward,
[00:47:43] so you just give it a solution from a student and ask it if the student will or not. We were training
[00:47:47] with reinforcement learning against that reward function and it worked really well and then
[00:47:53] suddenly the reward became extremely large, like it was massive jump and it did perfect. And you're
[00:47:57] looking at it like, wow, this means the student is perfect and all these problems is fully solved
[00:48:02] math. But actually what's happening is that when you look at the completions that you're
[00:48:06] getting from the model, they are complete nonsense, they start out okay and then they change to
[00:48:09] the, so it's just like, okay, let's take two plus three and we do this and this and then
[00:48:15] and you're looking at it's like, this is crazy, how is it getting reward of one or 100%
[00:48:20] and you look at the LLM judge and turns out that the the the the the is an adversarial examples
[00:48:23] for the model and it assigns 100% probability to it. And it's just because this is an out of sample
[00:48:28] example to the LLM, it's never seen you during training and you're in pure generalization land,
[00:48:33] it's never seen you during training and in the pure generalization land, you can find these
[00:48:37] examples that break it. You're basically training the LLM to be a prompt injection model. Not
[00:48:44] even that prompt injection is way too fancy or you're finding adversarial examples that are called
[00:48:48] these are nonsensical solutions that are obviously wrong but the model things are amazing.
[00:48:55] So today's thing you think this is the bottleneck to making RL more functional, then that will
[00:48:59] require making LLM's better judges if you want to do this in an automated way. And then so is it
[00:49:05] just going to be like some sort of GAN like approach or you had to train models to be more robust?
[00:49:08] Yeah, to I think the labs are probably doing all that like, okay, so the obvious thing is like
[00:49:12] that the the should not get 100% reward. Okay, well take the the the the the the the
[00:49:16] but in the training set of the LLM judge and say this is not 100% this is 0%. You can do this.
[00:49:20] But every time you do this, you get a new LLM and it still has adversarial examples. There's
[00:49:24] infinity adversarial examples and I think probably if you iterate this a few times, it'll probably
[00:49:29] be harder and harder to find adversarial examples. But I'm not 100% sure because this thing has
[00:49:32] a trillion parameters or whatnot. So I bet you the the labs are trying. I don't actually I still
[00:49:40] think I still think we need other ideas. Interesting. Do you do you have some shape of what the other
[00:49:47] idea is going to be? So like this this idea of like every review review solution and compass
[00:49:54] synthetic examples such that when you train on them, you get you get better and like metallurinate
[00:49:59] and someone. And I think there's some papers that I'm starting to see pop out. I only am at a
[00:50:02] stage of like reading abstracts because a lot of these papers, you know, they're just ideas.
[00:50:06] Someone has to actually like make it work on a frontier LLM lab scale in full generality.
[00:50:12] Because when you see these papers, they pop up and it's just like a little bit of noisy, you know,
[00:50:15] it's cool ideas. But I haven't actually seen anyone convincingly show that this is possible.
[00:50:21] That said, the LLM labs are fairly closed. Also, who knows what they're doing that? But yeah.
[00:50:26] So I guess I guess I see a very not easy, but like I can conceptualize how you would be able to train
[00:50:33] on synthetic examples or synthetic problems that you have made for yourself. But there seems to be
[00:50:37] another thing humans do. Maybe sleep is this. Maybe daydreaming is this, which is not necessarily come
[00:50:42] up with fake problems, but just like reflect. And I'm not sure what the ML analogy for daydreaming
[00:50:49] or sleeping, but just reflecting. I haven't come up with any problem. I mean, obviously the
[00:50:53] very basic analogies be like fine tuning on reflection bits. But I feel like in practice,
[00:50:58] that probably wouldn't work that well. So I don't know if you have some take on what the analogy
[00:51:02] of like this thing is. Yeah, I do think that we're missing some aspects there. So as an example,
[00:51:08] when you're reading a book, I almost feel like currently when LLM's are reading a book,
[00:51:13] what that means is we stretch out the sequence of text and the model is predicting the next token
[00:51:17] and it's getting some knowledge from that. That's not really what humans do, right? So when you're
[00:51:20] reading a book, I almost don't even feel like the book is like exposition. I'm supposed to be
[00:51:24] attending to and training on. The book is a set of prompts for me to do synthetic data generation
[00:51:30] or for you to get into a book club and talk about it with your friends. And by manipulating that
[00:51:34] information that you actually gained that knowledge. And I think we have no equivalent of that,
[00:51:38] again, with LLM's. They don't really do that, but I'd love to see during pre-training some kind of
[00:51:42] a stage that thinks through the material and tries to reconcile it with what it already knows.
[00:51:47] And thanks through for like some amount of time and guess that to work. And so there's no
[00:51:52] equivalence of any of this. This is all research. There's some subtle, very subtle that I think
[00:51:56] I'm very hard to understand reasons why it's not trivial. So if I can just do one,
[00:52:01] why can we just synthetically generate and train on it? Well, because every synthetic example,
[00:52:05] like if I just give synthetic generation of the model thinking about a book, you look at it and
[00:52:09] you like, this looks great. Why can't I train on it? Well, you could try, but the model will actually
[00:52:13] get much worse if you continue trying. And that's because all of the samples you get from models
[00:52:17] are silently collapsed. They're silently. This is not obvious if you look at any individual
[00:52:22] example of it. They occupy a very tiny manifold of the possible space of sort of thoughts about
[00:52:27] content. So the LLM's, when they come off, they're what we call collapsed. They have a collapse
[00:52:32] data distribution. If you sample one easy way to say it is go through chatchipity and ask it,
[00:52:37] tell me a joke. It only has like three jokes. It's not giving you the whole breath of possible
[00:52:41] jokes. It's giving you like, it knows like three jokes. They're silently collapsed. So basically,
[00:52:46] you're not getting the richness and diversity and the entropy from these models as you would get
[00:52:51] from humans. So humans are a lot more sort of noisier, but at least they're not biased. They're not
[00:52:56] in a statistical sense. They're not silently collapsed. They maintain a huge amount of entropy.
[00:53:00] So how do you get synthetic generation to work despite the collapse and while maintaining the
[00:53:05] entropy is a research problem? Just to make sure I understood, the reason that the collapse is
[00:53:10] relevant to synthetic data generation is because you want to be able to come up with synthetic
[00:53:15] problems or reflections, which are not already in your data distribution. I guess what I'm saying is
[00:53:22] say we have a chapter of a book and I ask an alum to think about it. It will give you something
[00:53:27] that looks very reasonable. But if I ask it 10 times, you'll notice that all of them are the same.
[00:53:31] You can't just leave scaling, scaling quote unquote reflection on the same amount of
[00:53:38] prompt information and then get returns from that. So any individual sample will look okay,
[00:53:43] but the distribution of it is quite terrible. It's quite terrible in such a way that if you continue
[00:53:48] training on too much of your own stuff, you actually collapse. I actually think that there's no
[00:53:52] fundamental solutions to this possibly. I also think humans collapse over time. I think these
[00:53:58] analogies are surprisingly good, but humans collapse during the course of their lives. This is why
[00:54:02] children have completely, they haven't overfitted and they will say stuff that will shock you
[00:54:07] because it's kind of, you can see where they're coming from, but it's just not the thing people
[00:54:11] say and because they're not yet collapsed. But we're collapsed. We end up revisiting the same thoughts.
[00:54:17] We end up saying more and more of the same stuff and the learning rates go down and the collapse
[00:54:22] continues to get worse and then everything deteriorates. Have you seen this amazing paper that
[00:54:29] dreaming is a way of preventing this kind of overfitting and collapse that the reason
[00:54:35] dreaming is evolutionary adaptive is to put you in weird situations that are like very unlike
[00:54:42] your day to day reality so that to prevent this kind of overfitting. It's an interesting idea. I
[00:54:45] mean, I do think that when you're generating things in your head and then you're attending to it,
[00:54:49] you're kind of like training on your own samples, you're training on your synthetic data,
[00:54:52] and if you do it for too long, you go off rails and you collapse way too much. So you always have to
[00:54:57] like seek entropy in your life. So talking to other people is a great source of entropy and
[00:55:04] things like that. So maybe the brain has also built some internal mechanisms for increasing the
[00:55:08] amount of entropy in that process, but yeah, maybe that's an interesting idea.
[00:55:14] This is a very ill-formed thought. So I'll just put it out and let you react to it. The best learners
[00:55:19] that we are aware of, which are children, are extremely bad at recollecting information. In fact,
[00:55:25] at the very earliest stages of childhood, you will forget everything. You're just an amnesiac
[00:55:30] about everything that happens before a certain year date, but you're like extremely good at picking
[00:55:33] up new languages and learning from the world, and maybe there's some element of like being able to
[00:55:37] see the forest for the trees. Whereas if you compare it to the opposite end of the spectrum, you have
[00:55:42] LLM pre-training, which these models will literally want to regurgitate word for word. What is the
[00:55:48] next thing? We get PDF page, but their ability to learn abstract concepts really quickly in the way
[00:55:53] a child can is much more limited, and then adults are somewhere in between where they don't have
[00:55:58] the flexibility of childhood learning, but adults can memorize facts and information in a way that
[00:56:04] is hard for kids. I don't know if there's something interesting about that. I think there's
[00:56:08] something very interesting. I do think that humans actually, they do kind of like have a lot more
[00:56:14] of an element compared to all of them of seeing the forest for the trees, and we're not actually
[00:56:18] that good at memorization, which is actually a feature. Because we're not that good at memorization,
[00:56:24] we actually are kind of like forced to find patterns, like in a more general sense. I think LLM
[00:56:32] and in comparison are extremely good at memorization. They will recite passages from all these
[00:56:37] training sources. You can give them completely nonsensical data. You can hash some amount of text
[00:56:42] or something like that. You get a completely random sequence. If you train on it, even just I think
[00:56:45] a single iteration or two, it can suddenly regurgitate the entire thing. You'll memorize it. There's
[00:56:49] no way a person can read a single sequence of random numbers and recite it to you. That's a feature
[00:56:56] not a bug almost, because it forces you to only learn the generalizable components,
[00:57:00] whereas LLMs are distracted by all the memory that they have of the preaching documents,
[00:57:05] and it's probably very distracting to them in a certain sense. That's why when I talk about the
[00:57:10] cognitive core, I actually want to remove the memory, which is what we talked about. I'd love to
[00:57:14] have them have less the memory so that they have to look things up. They only maintain the algorithms
[00:57:19] for thought and the idea of an experiment and all this cognitive glue of acting. And this is also
[00:57:27] relevant to preventing Marla from collapse. Let me think. I'm not sure. I think it's almost like
[00:57:36] a separate axis. The models are way too good at memorization and somehow we should remove that.
[00:57:42] And I think people are much worse, but it's a good thing.
[00:57:46] What is a solution to model collapse? I mean, there's very naive things you could attempt is just like
[00:57:52] the distribution over load just should be wider or something. There's many naive things you could
[00:57:56] try. What ends up being the problem with the naive approaches? Yeah, I think that's a great question.
[00:58:01] I mean, you can imagine having a regularization for entropy and things like that. I guess they just
[00:58:05] don't work as well empirically, because right now, the models are collapsed, but I will say
[00:58:10] most of the tasks that we want of them don't actually demand the diversity. It's probably the
[00:58:17] answer of what's going on. And so it's just that the frontier labs are trying to make the models
[00:58:21] useful. And I kind of just feel like the diversity of the outputs is not so much, number one,
[00:58:26] it's much harder to work with an evaluate and all this kind of stuff, but maybe it's not what's
[00:58:29] actually capturing most of the value. Right, it's actively penalized, right? If you're like super
[00:58:35] creative in RL, it's like not good. Yeah, or like maybe if you're doing a lot of writing, help
[00:58:39] promote elements and stuff like that, I think it's probably bad because the models will give you
[00:58:42] these like silently all the same stuff, you know, so they're not, they won't explore lots of
[00:58:48] different ways of answering a question, right? But I kind of feel like maybe the diversity is just not
[00:58:52] as big of a, yeah, maybe like, yeah, not as many applications need it so the models don't have it,
[00:58:57] but then it's actually a problem, it's in theory generation time, etc. So we're actually shooting
[00:59:00] ourselves in the foot by not allowing this entropy to maintain in the model. And I think possibly
[00:59:05] the labs should try harder. And then I think you hinted that it's a, it's a very fundamental problem,
[00:59:10] it won't be easy to solve. And yeah, what's your intuition for that? I don't actually know if it's
[00:59:15] super fundamental. I don't actually know if I intended to say that. I do think that
[00:59:22] I haven't done these experiments, but I do think that you could probably regularize the entropy
[00:59:25] to be to be higher. So you're encouraging the model to give you more and more solutions,
[00:59:30] but you don't want it to start deviating too much from the training data. It's going to start
[00:59:33] making up its own language. It's going to start using words that are extremely rare. You know,
[00:59:37] so it's going to drift too much from the distribution. So I think controlling the distribution is just
[00:59:41] like a tricky, it's just like someone just has to. It's probably not trivial in that sense.
[00:59:47] How many bits should the optimal core of intelligence end up being if you just had to make a guess?
[00:59:54] The thing we put on the von Neumann Pro, how big does it have to be?
[00:59:59] So it's really interesting in the history of the field, because at one point everything was very
[01:00:04] scaling-pilled in terms of like, oh, we're going to make much bigger models, trillions of
[01:00:07] parameter models. And actually what the models have done in size is they've gone up. And that was
[01:00:11] actually kind of like actually even come down to see if the models are smaller. And even then,
[01:00:17] I actually think they memorized way too much. So I think I had a prediction a while back that I
[01:00:22] almost feel like we can get cognitive course. There are very good at even like a billion
[01:00:26] billion parameters. It should be already like, like if you talk to a billion parameter model,
[01:00:30] I think in 20 years, you can actually have a very productive conversation, it thinks,
[01:00:35] and it's a lot more like a human. But if you ask it some factual question, might have to look it up,
[01:00:39] but it knows that it doesn't know and it might have to look it up and they will just do all the
[01:00:42] reasonable things. That's actually surprising that you think it will take a billion, because already
[01:00:46] we have a billion parameter models or a couple of billion parameter models that are like very
[01:00:50] intelligent. Also, our models are like a trillion parameters, right? But they remember so much stuff,
[01:00:54] like, yeah. But I'm surprised that in 10 years, given the pace, okay, we have a GPT OSS 20B
[01:01:04] that's way better than GPT-4 original, which was a trillion plus parameters. So given that trend,
[01:01:10] I'm actually surprised you think in 10 years, the cognitive course is still a billion parameters.
[01:01:14] I would, yeah, I'm surprised you're not actually going to be like tens of millions or millions.
[01:01:19] No, because I basically think that the training data is, so here's the issue, the training data
[01:01:23] is the internet, which is really terrible. So there's a huge amount of gains to be made because
[01:01:27] the internet is terrible. Like, if you actually, and even the internet, when you and I think of the
[01:01:30] internet, you're thinking of like, oh, Wall Street Journal, or that's not what this is. When you're
[01:01:35] actually looking at a preaching data set in the Frontier Lab and you look at a random internet
[01:01:38] document, it's total garbage. Like, I don't even know how this works at all. It's some like stock ticker
[01:01:44] symbols. It's a huge amount of slop and garbage from like all the corners of the internet. It's not
[01:01:50] like your Wall Street Journal article. That's extremely rare. So I almost feel like because the
[01:01:55] internet is so terrible, we actually have to sort of build really big models to compress all that.
[01:02:00] Most of that compression is memory work instead of like cognitive work. But what we really want
[01:02:04] is the cognitive part, actually, delete the memory. And then, so I guess what I'm saying is like,
[01:02:09] we need intelligent models to help us refine even the pre-training set to just narrow it down to
[01:02:14] the cognitive components. And then I think get away with a much smaller model because this is
[01:02:18] my much better data set and you could train it on it. But probably it's not trained directly
[01:02:21] on it. It's probably distilled for a much better model still. But why is it distilled version still,
[01:02:26] a billion? I guess the thing I'm curious about. I just feel like distillation work is
[01:02:29] extremely well. So almost every small model, if you have a small model, it's almost certainly
[01:02:33] distilled. Why would you train on? Why is a distillation not in 10 years not getting below one billion?
[01:02:39] Oh, you think it should be smaller? And then a billion? I mean, come on, right?
[01:02:43] I don't know. At some point, it should take at least a billion knobs to do something interesting.
[01:02:49] You just think it should be even smaller? Yeah, I mean, just like if you look at the trend
[01:02:52] over the last few years, just finding a little hanging fruit and going from like trillion plus
[01:02:56] models that are like literally two orders of magnitude smaller in a matter of two years and
[01:03:01] having better performance. Yeah, yeah. It means that you think that the sort of like core of
[01:03:05] intelligence might be even way, way smaller. Like plenty of room at the bottom to prepare first
[01:03:11] Feynman. I mean, I almost feel like I'm already contrarian by talking about a billion in the
[01:03:14] parameter cognitive core and you're out doing me. I think, um, yeah, maybe we could get a little bit
[01:03:19] smaller. I mean, I still think that there should be enough, yeah, maybe it can be smaller. I do think
[01:03:23] that practically speaking, you want the model to have some knowledge. You don't want it to be looking
[01:03:27] up everything. Yeah. Um, because then you can't like think in your head, you're looking up way too
[01:03:30] much stuff all the time. So I do think it needs to be some basic curriculum needs to be there for
[01:03:35] knowledge. Uh, but it doesn't have a certain knowledge, you know? So we're discussing what like
[01:03:39] plausibly could be the cognitive core. There's a separate question, which is, what will actually be
[01:03:44] the size of frontier models over time? And I'm curious to have predictions. So we had increasing
[01:03:50] scale up to maybe 4.5 and now we're seeing decreasing slash plateauing scale. There's many reasons
[01:03:55] that could be going on. But the other prediction about going forward will scale, will the bigger
[01:03:59] models be bigger? Will they be smaller? Will they be the same? Um, yeah, I don't know that I have
[01:04:04] a super strong prediction. I do think that the labs are just being practical. They have a flops
[01:04:08] budget and a cost budget. And it just turns out that pre-training is not where you want to put
[01:04:12] most of your flops or your cost. So that's why the models have gotten smaller because they are a
[01:04:16] bit smaller, the pre-training stages, smaller, etc. But they make it up in the reinforcement learning
[01:04:20] and all this kind of stuff, mid-training and all this kind of stuff that follows. So they're just
[01:04:24] being practical in terms of all the stages and how you get the most bang for the buck. Um, so I guess
[01:04:27] like forecasting that trend, I think, uh, is quite hard. I do still expect that there's so much
[01:04:32] longing for it. That's my basic, it's my basic expectation. Um, and so I have a very wide
[01:04:39] distribution here. Um, do you say they're looking for it to be similar in kind to the kinds of
[01:04:44] things that have been happening over the last two to five years? Like, just in terms of like,
[01:04:49] if I look at nano chat versus an energy PT and then the architectural tweaks you made,
[01:04:53] is that basically like the flavor of things you continue to keep happening? Or is there?
[01:04:57] You're not expecting any giant furniture. I expect the data says to get much, much better
[01:05:02] because when you look at the average data sets, they're extremely terrible. Like so bad that I don't
[01:05:05] even know how anything works to be honest. Like, look at the average example in the training set.
[01:05:10] It like factual mistakes, errors, non-sensical things. Um, somehow when you do it at scale,
[01:05:16] the noise washes away and you're left with some of the signal. Um, so data sets will improve a ton.
[01:05:21] It's just everything gets better. So, um, our hardware, um, our older kernels, um,
[01:05:27] older kernels for running the hardware and maximizing what you get with the hardware.
[01:05:30] You know, so NVIDIA is slowly tuning the actual hardware itself,
[01:05:33] tensor, tensor course and so on. All that needs to happen and we'll continue to happen.
[01:05:36] All the kernels will get better and utilize the chip to the max extent. All the algorithms will
[01:05:40] probably improve over optimization architecture and, um, just all of the modeling components of how
[01:05:45] everything is done and what the algorithms are that we're even training with. So I do, I do kind of
[01:05:49] expect like a, just very just everything, nothing dominates everything plus 20%. Right.
[01:05:57] This is like roughly what I've seen. Okay. This is my general manager, Max.
[01:06:01] Good to be here here every day. And you haven't here since you were onboarded about six months ago.
[01:06:05] But when I was a month ago. All right. Uh, time passes so fast. But when I onboarded you,
[01:06:11] I was in France and so we basically didn't get the chance to talk at all almost. And you basically
[01:06:16] just gave me one logins. I gave you access to my Mercury platform, which is the banking platform
[01:06:22] that I was using at the time to run the podcast. And so I logged into Mercury assuming that
[01:06:26] that would just be the first of many steps. But I realized that was how you were running
[01:06:30] the entire business, even down to a lot of our editors or international contractors. And so you
[01:06:35] would just figure out how to set up these recurring payments to set up basic payroll. I mean,
[01:06:39] we're currently made the experience of all of these things I was doing before. So seamless that
[01:06:43] it didn't even occur to me until he pointed it out that this is not the natural way to set up
[01:06:46] payroll or invoicing or any of these other things. Yeah. I was surprised, but I was like, it's worked
[01:06:51] so far. That's right. So maybe I'll trust it. And then now I can't think of doing anything else.
[01:06:55] All right. You heard them. Visit mercury.com to apply online in minutes. Cool. Thanks, Max.
[01:07:01] Thanks for having me. Dude, you're great at this. I'm so nervous. But thank you. Mercury is a
[01:07:06] financial technology company, not a bank. Banking services provided through choice financial group,
[01:07:10] column A and evolve bank interest members, FDIC. People have proposed different ways of
[01:07:16] charting how much progress you've made towards full AGI. Because if you get come up with some line,
[01:07:23] then you can see where that line intersects with AGI and where that would happen on the
[01:07:26] X axis. And so people have proposed, oh, it's like the education level, like we had a high
[01:07:30] schooler and then they went to college with RL and they're going to get a PhD. I don't like that one.
[01:07:35] Or then they propose horizon length. So maybe they can do tasks to take a minute.
[01:07:40] They can do those autonomously. Then they can autonomous. They do tasks to take an hour,
[01:07:43] a human an hour, a human a week, et cetera. How do you think about what is the relevant
[01:07:50] Y axis here? How should we think about how AI is making progress?
[01:07:54] So I guess I have two answers to that. Number one, I'm almost tempted to reject the question entirely.
[01:07:58] Because again, I see this as an extension of computing. Have we talked about how to chart progress
[01:08:02] in computing? Or how do you chart progress in computing since 1970s or whatever? What is the
[01:08:06] X axis? So I kind of feel like the whole question is funny from that perspective a little bit.
[01:08:11] But I will say, I guess, when people talk about AI and the original AGI and how we spoke about it,
[01:08:16] when we when opening has started. AGI was a system you can go to that can do any task that is
[01:08:23] economically valuable, any economically valuable task at human performance or better.
[01:08:29] So that was the definition. And I was pretty happy with that at the time. And I kind of feel like
[01:08:32] I've stuck to that definition forever. And then people have made up all kinds of other definitions.
[01:08:37] But I like, I feel like I like that definition. Now number one, the first concession that people
[01:08:42] make all the time is they just take out all the physical stuff because we're just talking about
[01:08:46] digital knowledge work. I feel like that's a pretty major concession compared to the original
[01:08:50] definition, which was like any task a human can do. I can lift things, etc. Like AI can't do that,
[01:08:55] obviously. So, okay, but we'll take it. What fraction of the economy are we taking away by saying,
[01:09:00] oh, only knowledge work? I don't actually know the numbers. I feel like it's about 10 to 20%
[01:09:05] if I had to guess is only knowledge work. Like someone could work from home and perform tasks,
[01:09:11] something like that. I still think it's a really large market. Like, yeah, what is the size of
[01:09:17] the economy and what is 10 to 20% like we're still talking about a few trillion dollars of even in
[01:09:21] the US of market share almost or like work. So still a very massive bucket. So, but I guess like
[01:09:28] going back to the definition, I guess what I would be looking for is to what extent is that
[01:09:32] definition true. So are there jobs or lots of tasks if we think of tasks as, you know, not jobs
[01:09:39] but tasks kind of difficult because the problem is like society will refactor based on the tasks that
[01:09:45] make up jobs compared to what's based on what's so automatable or not. But today, what jobs are
[01:09:50] replaceable by AI. So a good example recently was Jeff Hinton's prediction that radiologists would
[01:09:56] not be a job anymore and this turned out to be very wrong in a bunch of ways, right? So radiologists
[01:10:01] are alive and well and growing even though computer vision is really, really good at recognizing all
[01:10:05] the different things that they have to recognize in images. And it's just messy, complicated job with
[01:10:10] a lot of surfaces and dealing with patients and all this kind of stuff in the context of it.
[01:10:14] So I guess I don't actually know that by that definition, AI has made a huge amount of
[01:10:19] dent yet. But some of the jobs maybe that I would be looking for have some features that I think
[01:10:24] make it very amenable to automation earlier than later. As an example, call center employees often
[01:10:28] come up and I think rightly so because call center employees have a number of simplifying
[01:10:34] properties with respect to what's automatable today. Their jobs are pretty simple.
[01:10:39] It's a sequence of tasks and every task looks similar. Like you take a phone call with a person,
[01:10:43] it's 10 minutes of interaction or whatever it is, probably a bit longer, in my experience a lot
[01:10:47] longer. And you complete some task and some scheme and you change some database entries around
[01:10:53] or something like that. So you keep repeating something over and over again and that's your job.
[01:10:57] So basically you do want to bring in the task horizon, how long it takes to perform a task.
[01:11:02] And then you want to also remove context like you're not dealing with different parts of services
[01:11:06] of companies or other customers is just the database you and a person you're serving. And so it's
[01:11:11] more closed, it's more understandable and it's purely digital. So I would be looking for those
[01:11:16] things. But even there, I'm not actually looking at full automation yet. I'm looking for an autonomy
[01:11:20] slider and I almost expect that we are not going to instantly replace people. We're going to be swapping
[01:11:26] in AI's that do 80% of the volume. They delegate 20% of volume to humans and humans are supervising
[01:11:32] teams of five AI's doing the call center work that's more wrote. So I would be looking for new
[01:11:37] interfaces or new companies that provide some kind of a later that allows you to manage some of
[01:11:44] these AI's. They're not yet perfect. And then I would expect that across the economy and a lot of
[01:11:49] jobs are a lot harder than call center employee. And when you're with radiologists, I'm totally
[01:11:54] speculative. I have no idea how with the actual workload of radiologists involves. But one analogy
[01:11:59] that might be applicable is when Wayne was the first being ruled out, there'd be a person sitting
[01:12:06] in the front seat. And you just had to have them there to make sure that if something went really
[01:12:10] wrong, they're their demander. And I think even today, people are still watching to make sure
[01:12:14] things are going well. Robotaxi, who was just deployed actually still has a person inside it.
[01:12:18] And we could be in a similar situation where if you automate 99% of a job that last 1%
[01:12:25] of the human has to do is incredibly valuable because it's bottlenecking everything else.
[01:12:29] And if it was the case with like with radiologists where the person sitting in the front of the
[01:12:33] Uber or the front of the Waymo has to be specially trained for years in order to be able to provide
[01:12:37] the last 1%. Their wages should go off tremendously because they're like the one thing bottlenecking
[01:12:42] wide deployment. So radiologists, I think their wages have gone up for similar reasons. If you're
[01:12:46] like the last bottleneck, you should, you're like, and you're not fungible, which like, you know,
[01:12:50] a Waymo driver might be fungible with other things. So you might see this thing where like your
[01:12:54] wages go like, and then to get 90% and then like just like that. I mean the last 1% is gone.
[01:13:00] I see. And I wonder if we're similar things with radiology or salaries of call center workers
[01:13:05] or anything like that. Yeah, I think that's an interesting question. I don't think we're currently
[01:13:10] seeing that with radiology or and I don't have like in my understanding, but I think radiology is
[01:13:16] not a good example basically. I don't know why Jeff Hinton picked on radiology because I think it's
[01:13:21] an extremely messy, messy, complicated profession. So I would be a lot more interested in what's
[01:13:26] happening with call center employees today, for example, because I would expect a lot of the
[01:13:29] road stuff to be automatable today. And I don't have a first level access to it, but maybe I would
[01:13:34] be looking for trends of what's happening with the call center employees. Maybe some of the things
[01:13:38] I would also expect is maybe they are swapping an AI, but then I would still wait for a year or two
[01:13:44] because I would potentially expect them to pull back and actually rehire some of the people.
[01:13:49] I think there's been evidence that that's already been happening in the
[01:13:51] generally and companies that have been adopting AI, which I think is quite surprising.
[01:13:54] Yeah, I also find we're really surprising. Okay, AGI, right? Like a thing which should do everything
[01:14:02] and okay, we'll take out physical work. So the thing which should be able to do all knowledge work
[01:14:06] and what you would have naively anticipated that the way this regression happened is like
[01:14:10] you take a little task that a consultant is doing, you take that out of the bucket, you take
[01:14:16] a little task that an accountant is doing, you take that out of the bucket, and then you're just
[01:14:21] doing this across all knowledge work. But instead, if we do believe we're on the path of AGI with
[01:14:26] the current paradigm, the progression is very much not like that. At least it just does not seem
[01:14:31] like consultants and accounts and whatever are getting like huge productivity improvement. It's very
[01:14:34] much like programmers are like getting more and more chills of the way of their work. If you're
[01:14:40] looking at the revenues of these companies, discounting just like normal chat revenue, which I think is
[01:14:44] like, I don't know, that's similar to like Google or something. Just looking at API revenues,
[01:14:49] it's like dominated by coding, right? So this thing, which is general quote-unquote,
[01:14:54] it should be able to do any knowledge work. It's just overwhelmingly doing only coding.
[01:14:58] And it's a surprising way that you would expect like the AGI to be deployed.
[01:15:02] So I think there's an interesting point here because I do believe coding is like the perfect first
[01:15:07] thing for these LLMs and agents. And that's because coding has always fundamentally worked around
[01:15:16] text. It's computer terminals and text and everything is based around text and LLMs, the way they're
[01:15:21] trained on the internet, law of text. And so they're perfect text processors and there's all this
[01:15:26] data out there and it's just perfect fit. And also we have a lot of infrastructure pre-built for
[01:15:31] handling code and text. So for example, we have a Visual Studio code or your favorite IDE,
[01:15:39] showing you code. And then agent can plug into that. So for example, if an agent has a diff
[01:15:44] where it made some change, we suddenly have all this code already that shows all the differences
[01:15:48] to a code base using a diff. So it's almost like we pre-built a lot of the infrastructure for code.
[01:15:55] Now contrast that with some of the things that don't enjoy that at all. So as an example,
[01:16:00] there's people trying to build automation not for coding, but for example for slides.
[01:16:03] Like I saw a company doing slides. That's much, much harder. And the reason it's much, much harder
[01:16:07] is because slides are not text. Slides are little graphics and they're arranged spatially. And
[01:16:12] there's visual component to it. And slides don't have this pre-built infrastructure. For example,
[01:16:19] if an agent is to make a different change to your slides, how does a thing show you the diff?
[01:16:24] How do you see the diff? There's nothing that shows diffs for slides. So I want us to build it.
[01:16:29] So it's just some of these things are not amenable to AIs as they are, which is text processors.
[01:16:35] And code surprisingly is. I actually am not sure if that alone explains it because
[01:16:42] I personally have tried to get LLMs to be useful in domains which are just pure language and
[01:16:49] language out. Like rewriting transcripts, like coming up with clips based on transcripts, etc.
[01:16:56] And you might say, well, it's very plausible that like, I didn't do every single possible thing I
[01:17:00] could do. I put a bunch of good examples in context, but maybe I should have done some fine
[01:17:05] tuning, whatever. So our mutual friend, Andy Matushak, told me that he actually tried 50 billion
[01:17:11] things to try to get models to be good at writing space repetition prompts. Again, very much
[01:17:17] language in, language out. The kind of thing that should be death center in the repertoire of
[01:17:21] these all a lot. And he tried in context learning, obviously with a few short examples. He tried,
[01:17:26] I think he told me like a bunch of things like supervised fine tuning and like, retrieval,
[01:17:32] whatever. And he just could not get them to make hearts through satisfaction. So I find it
[01:17:37] striking that even in language out domains, it's actually very hard to get a lot of economic
[01:17:43] value out of these models separate from coding. And I don't know what explains it.
[01:17:46] Yeah, I think I think that makes sense. I mean, I would say, yeah, I'm not saying that anything
[01:17:52] text is trivial, right? I do think that code is like, it's pretty structured.
[01:17:58] Text is maybe a lot more flowery and there's a lot more like like entropy and text, I would say.
[01:18:04] I don't know how I also put it. And also, I mean, code is hard. And so people sort of feel
[01:18:10] quite empowered by LLMs, even from like simple, simple kind of knowledge. I basically, I don't
[01:18:16] actually know that I have a very good answer. I mean, obviously, like text makes it
[01:18:20] much much easier, maybe he's maybe why I put it, but it doesn't mean that all text is trivial.
[01:18:25] How do you think about super intelligence? Do you expect it to feel qualitatively different from
[01:18:30] normal humans or human companies? I guess I think I see it as like a progression of automation
[01:18:37] in society, right? And again, like extrapolating the trend of computing, I just I feel like there
[01:18:42] will be a gradual automation of a lot of things and super intelligence will be sort of like the
[01:18:45] extrapolation of that. So I do think we expect more and more autonomous entities over time that
[01:18:49] are doing a lot of the digital work and then eventually even the physical work, probably some
[01:18:54] amount of time later. But basically, I see it as just automation, roughly speaking.
[01:19:00] I guess automation includes the things humans can already do and super intelligence
[01:19:03] applies things humans. Well, but some of the things that people do is invent new things, which I
[01:19:07] would just put into the automation of that makes sense. Yeah. But I guess maybe less abstractly
[01:19:14] and more sort of like qualitatively. Do you expect something to feel like, okay, this because
[01:19:20] this thing can either think so fast or has so many copies or the copies can merge back in them
[01:19:27] themselves or is, quote unquote, much smarter. Any number of advantages in the I might have,
[01:19:34] it will qualitative the civilization in which these AIs as well just feel qualitatively different
[01:19:39] from human civilization. I mean, it is fundamentally automation, but I mean, it will be like
[01:19:42] extremely foreign. I do think it will look really strange because like you mentioned, we can
[01:19:48] run all of this on the computer cluster, et cetera, and much faster and all this thing. I mean,
[01:19:52] maybe some of the scenarios, for example, that I start to get like nervous about with respect,
[01:19:57] with respect to when the world looks like that is this kind of like gradual loss of control and
[01:20:00] understanding of what's happening. And I think that's actually the most likely outcome probably
[01:20:04] is that there will be a gradual loss of understanding of and will gradually layer all the stuff
[01:20:09] everywhere. And there will be a few or a few of people who understand it. And then there will be
[01:20:12] a sort of this like scenario of a gradual loss of control and understanding of what's happening.
[01:20:16] That to me seems most likely outcome of how the stuff will go down. Let me prove on that a bit.
[01:20:21] It's not clear to me that loss of control and loss of understanding are the same things. A board
[01:20:27] of directors at like whatever, TSMC, Intel, name around somebody, they're just like prestigious 80
[01:20:36] year olds. They have very little understanding and maybe they don't practically actually have control.
[01:20:40] But actually, maybe their example is the president of the United States.
[01:20:45] President has a lot of fucking power. I'm not trying to make a good statement about the current
[01:20:51] operant, but maybe I am, but like the actual level of understanding is very different from the
[01:20:55] level of control. Yeah, I think that's fair. That's a good pushback. I think like I guess I expect
[01:21:01] loss of both. Yeah. How come I mean, loss of understanding is obvious, but why loss of control?
[01:21:08] So so we're really far into territory of I don't know what this looks like, but if I was to write
[01:21:14] sci-fi novels, they would look along the lines of not even a single like entity or something like
[01:21:20] that. So that should sort of like take over everything. But actually like multiple competing entities
[01:21:24] that gradually become more and more autonomous and some of them go rogue and the others like
[01:21:29] fight them off and all this kind of stuff. And it's like this hot pot of completely autonomous
[01:21:34] activity that we've delegated to. I kind of feel like it would have that flavor.
[01:21:42] It is not the fact that they are smarter than us that is resulting in the loss of control.
[01:21:45] It is the fact that they are competing with each other and whatever arises out of that competition
[01:21:52] at least to the loss of control. I mean, I basically expect there to be I mean, a lot of these
[01:21:59] things, I mean, they will be tools to people and the people could some of the population is like
[01:22:04] they're acting on behalf of people or something like that. So maybe those people are in control,
[01:22:07] but maybe it's a loss of control overall for society in the sense that of like outcomes we want
[01:22:12] or something like that. Where you have entities acting on behalf of individuals,
[01:22:16] there are still kind of roughly seen as out of control. Yeah, yeah. This is a question I should
[01:22:20] have asked earlier. So we were talking about how currently it feels like when you're doing AI
[01:22:24] engineering or AI research, these models are more like in the category of compiler rather than
[01:22:30] in the category of a replacement. Yeah. At some point, if you have quote unquote AGI, it should
[01:22:34] be able to do what you do. And do you feel like having a million copies of you in parallel
[01:22:40] results in some huge speed up of AI progress? Basically, if that does happen, would you do expect
[01:22:45] to see an intelligence explosion or even once we have a true agent, not talking about other
[01:22:49] elements today, but really like what I mean is I do, but it's business as usual because we're in
[01:22:56] an intelligence explosion already and have been for decades. And when you look at Jeep, it's basically
[01:23:00] the GDP curve that is an exponential weighted sum over so many aspects of the industry. Everything
[01:23:05] is gradually being automated has been for hundreds of years. Industry revolution is automation and
[01:23:10] some of the physical components and the tool building and all this kind of stuff. Compilers are
[01:23:13] early software automation, et cetera. So I kind of feel like we've been recursively self-improving and
[01:23:19] exploding for a long time. Maybe another way to see it is, I mean, Earth was a pretty, I mean,
[01:23:25] if you don't look at the biomechanics and so on, it was a pretty boring place, I think, and look
[01:23:29] very similar if you just look from space and Earth is spinning and then like we're in the middle of
[01:23:33] this like firecracker event, but we're seeing it in slow motion. But I definitely feel like
[01:23:40] this is this has already happened for a very long time. And again, like I don't see AI as like a
[01:23:44] distinct technology with respect to what has already been happening for a long time.
[01:23:48] So it is, you think it's going to continue with this hyper exponential trend. And that's why,
[01:23:52] like this is, this was very interesting to me because I was trying to find AI in the GDP for a while.
[01:23:57] I thought that GDP should go up. But then I looked at some of the other technologies that I thought
[01:24:02] were very transformative, like maybe computers or mobile phones or et cetera, you can't find them in
[01:24:07] GDP. GDP is the same exponential. And it's just that even for example, the early iPhone didn't have
[01:24:12] the app store and it didn't have a lot of the bells and whistles that the more an iPhone has.
[01:24:15] And so even though we think of 2008 was it when iPhone came out as like some major seismic change,
[01:24:20] it's actually not. Everything is like so spread out and so slowly diffuses that everything ends up
[01:24:25] being averaged up into the same exponential. And it's the exact same thing with computers. You can't
[01:24:28] find them in a GDP is like, oh, we have computers about. It's not what happened because it's such a
[01:24:32] slow progression. And with AI, we're going to see the exact same thing. It's just more automation.
[01:24:36] It allows us to write different kinds of programs that we couldn't write before. But AI is still
[01:24:40] fundamentally a program. And it's a new kind of computer and a new kind of computing system.
[01:24:46] But it has all these problems. It's going to diffuse over over time and it's still going to add up
[01:24:50] to the same exponential. And we're still going to get an exponential that's going to get extremely
[01:24:55] vertical. And it's going to be very foreign to live in that kind of an environment.
[01:24:59] Are you saying that like what will happen is if you go if you look at the trend before the
[01:25:03] industrial revolution to currently, you have a hyper exponential where you go from like 0% growth
[01:25:09] to then 10,000 years ago, 0.0, 2% growth. And then currently we're at 2% growth. So that's
[01:25:14] a hyper exponential. And you're saying if you're charting AI on there, then it's like AI takes you to
[01:25:17] 20% growth or 200% growth. Or you could be saying, if you look at the last 300 years, what have
[01:25:23] you been seeing is you have technology after technology, computers, electrification and steam,
[01:25:28] steam engines, railways, et cetera. But the rate of growth is exact same. It's 2%. So are you saying
[01:25:33] the rate of growth will do I actually expect this, the rate of growth has also stayed roughly
[01:25:39] constant, right? For only the last 200, 300 years. But over the course of human history, it's like
[01:25:43] exploded, right? It's like gone from like 0% basically to like faster, faster, faster,
[01:25:47] industrial explosion. Yeah. 2%. Basically, I guess what I'm saying is for a while, I tried to find
[01:25:52] AI or look for AI in like the GDP curve. And I've kind of convinced myself that this is false.
[01:25:56] And that even when people talk about recursive self-improvement and labs and stuff like that,
[01:25:59] I even don't, this is business as usual. Of course, it's going to recursively self-improve
[01:26:03] and it's been recursively self-improvement. Like LLMs allow the engineers to work much more
[01:26:08] efficiently to build the next round of the LLM. And a lot more of the components are being
[01:26:12] automated and tuned and et cetera. So all the engineers having access to Google search is sort of
[01:26:18] part of it. All the engineers having an ID, all of them having autocomplete or having cloth code,
[01:26:22] et cetera. It's all just part of the same speed up of the whole thing. So it's just so smooth.
[01:26:29] But just to clarify, you're saying that the rate of growth will not change.
[01:26:35] The intelligence explosion will show up as like it just enabled us to continue staying on the 2%
[01:26:39] growth trajectory just to get in and help us stay on the 2% growth trajectory. My expectation is
[01:26:43] that it stays the same pattern. Yeah. I mean, just to throw that opposite argument against you,
[01:26:50] my expectation is that it blows up because I think true AI, and I'm not talking about
[01:26:56] LLM coding bots. I'm talking about like actual, this is like a replacement of a human
[01:27:01] in a server is qualitatively different from these other productivity improving technologies
[01:27:08] because it's labor itself, right? I think we're living a very labor constrained world.
[01:27:12] Like we talked to any startup founder and a person who can just be like, okay, what do you need
[01:27:16] more of? You just need really talented people. And if you just have billions of extra people who are
[01:27:21] inventing stuff, integrating themselves, making companies, bottoms, start to finish, that feels
[01:27:27] qualitatively different from just like a single technology. It's sort of like just asking if you
[01:27:31] like, if you get 10 billion extra people on the planet. I mean, maybe a counterpoint. I mean,
[01:27:34] number one, I'm actually pretty pretty willing to be a convinced one way or another on this point.
[01:27:39] But I will say, for example, computing is labor. Computing was labor. Computers like a lot of jobs
[01:27:44] disappears because computers are automating a bunch of digital information processing that you
[01:27:48] now don't need a human for. And so computers are labor. And that has played out. And, you know,
[01:27:55] self-driving as an example is also like computers doing labor. So like, I guess that's already been
[01:28:00] playing out. So still business as usual. Yeah. I guess you have a machine which is spitting out
[01:28:04] more things like that at potentially faster pace. And so we historically, we have examples of
[01:28:09] the growth regime changing where like you went from, you know, 0.2% growth to 2% growth. So it's
[01:28:15] very plausible to me that like a machine, which is then spitting out the next self-driving car
[01:28:21] and the next internet and whatever. I mean, I kind of, yeah, I see where it's coming from. At the
[01:28:26] same time, I do feel like people make this assumption of like, okay, we have God in the box,
[01:28:31] and I can do everything. And it just won't just won't look like that. It's going to be,
[01:28:34] it's going to be able to do some of the things. It's going to fail at some other things. It's going
[01:28:37] to be gradually put into society and basically end up with the same pattern as my prediction.
[01:28:41] Yeah. Because because this assumption of suddenly having a completely intelligent,
[01:28:45] fully flexible, fully general human in a box and we can dispense it at arbitrary problems in
[01:28:50] society, I don't think that we will have this like discrete change. And so I think we'll arrive
[01:28:58] at the same at the same kind of a gradual diffusion of this across the industry.
[01:29:02] Hmm. I think what often ends up being misleading in these conversations is people, I don't like to
[01:29:09] use a word intelligence in this context because intelligence implies you think like, oh, super,
[01:29:14] a super intelligence will be sitting, there'll be a single super intelligence sitting in a server
[01:29:17] and it'll like divine how to come up with new technologies and inventions that causes this
[01:29:21] explosion. And that's not what I'm imagining when I'm imagining 20% growth. I'm imagining that
[01:29:27] there's billions of, you know, basically like very smart human like minds potentially or that's
[01:29:33] all that's required. But the fact that there's hundreds of millions of them, billions of them,
[01:29:38] each individually making new products, figuring out how to integrate themselves into the economy,
[01:29:43] just the way it's like a highly experienced smart immigrant came to the country. You wouldn't
[01:29:47] need to like figure out how we integrate them into economy. They figured out they could start a
[01:29:49] company, they could like make inventions, you know, or like just increase productivity in the world.
[01:29:55] And we have examples even in the current regime of places that have had 10, 20% economic growth.
[01:30:01] You know, if you just have a lot of people and less capital in comparison to the people,
[01:30:05] you can have Hong Kong or Shenzhen or whatever, just had decades of 10% plus growth.
[01:30:12] And I think there's a lot of really smart people who are ready to like make use of the resources and
[01:30:17] do this like period of catch up because we've had this discontinuity. And I think yeah,
[01:30:21] maybe similar. So I think I think I understand, but I still think that you're presupposing some
[01:30:26] discrete jump, there's some unlock that we're waiting to claim. And suddenly we're going to have
[01:30:31] geniuses and data centers. And I still think you're presupposing some discrete jump that I think
[01:30:36] has basically no historical precedent that I can't find in any of the statistics. And that I think
[01:30:41] probably won't happen. I mean, the initial revolution is such a jump, right? You went from like zero
[01:30:44] percent growth, 0.2 percent growth to 2 percent growth. I'm just saying like you'll see another jump
[01:30:48] like that. I'm a little bit suspicious. I would have to look at it. I'm a little bit suspicious.
[01:30:52] And I would have to take a look. For example, like maybe some of the logs are not very good from
[01:30:56] before the industrial revolution or something like that. So I'm a little bit suspicious of it,
[01:31:01] but yeah, maybe you're right. I don't have strong opinions. Maybe you're saying that this was a
[01:31:05] singular event that was extremely magical. And you're saying that maybe there's going to be another
[01:31:08] event that's going to be just like that, extremely magical. It will break paradigm and so on.
[01:31:12] I actually don't think the, I mean, the crucial thing with the industrial revolution was that it was
[01:31:16] not magical, right? Like if you just zoomed in, where you would see in 1770 or 1870 is not that
[01:31:25] they're like with some key invention. Yeah, exactly. But at the same time, you did move the
[01:31:30] economy to a regime where the progress was much faster and the exponential 10x. And I expect
[01:31:36] to some of the thing for me, where it's not like there's going to be a single moment where we made
[01:31:41] the crucial invention that's being unlocked. Like maybe there's a new energy source. There's
[01:31:45] there's some unlock in this case, some kind of a cognitive capacity. And there's an overhang
[01:31:49] cognitive cognitive work to do. That's right. And you were expecting that overhang to be filled by
[01:31:53] this new technology went across to the threshold. Yeah. And I mean, I mean, maybe when we think
[01:31:57] about it is through history, a lot of growth, I mean, growth comes because people come up with ideas.
[01:32:03] And then people are like out there doing stuff to execute those ideas and make valuable output.
[01:32:08] And through most of this time, population isn't exploding. That has been driving growth. For the
[01:32:13] last few years, people have argued that growth is stagnated. Population and frontier countries
[01:32:17] are also stagnated. I think we go back on the hyper exponential growth and population and output.
[01:32:22] Right. It's our exponential growth in population that causes hyper exponential growth and output.
[01:32:26] Yeah. I mean, yeah, it's really hard to tell. Yeah. I understand that viewpoint. I don't intuitively
[01:32:32] feel that viewpoint. So we just got to access to Google's VO 3.1. And it's been really cool to play
[01:32:39] around with. The first thing we did was run a bunch of problems through both VO3 and 3.1 to see
[01:32:46] what's changing in the new version. So here's VO3. Hi, I'm Max and I got stuck in a local
[01:32:52] minimum again. It's okay, Max. We've all been there. Took me three epochs to get out.
[01:32:57] And here's VO3.1. Hi, I'm Max and I got stuck in a local minimum again.
[01:33:03] It's okay, Max. We've all been there. Took me three epochs to get out.
[01:33:07] Rebrand One's output is just consistently more coherent and the audio is noticeably higher quality.
[01:33:12] We've been using VO4 a while now, actually. We released an essay earlier this year about AI firms
[01:33:18] fully animated by VO2. And it's been amazing to see how fast these models are improving.
[01:33:22] This update makes VO even more useful in terms of animating our ideas and our explainers.
[01:33:28] You can try VO right now in the Gemini app with pro and ultra subscriptions. You can also
[01:33:34] access it through the Gemini API or through Google Flow. You recommend a Nick Lane's book to me
[01:33:40] and then on that basis, I also find it super interesting and I interviewed him. And so I actually
[01:33:45] have some questions about thinking about intelligence and evolutionary history. Now that you
[01:33:50] over the last 20 years are doing AI research, you maybe have a more tangible sense of what
[01:33:54] intelligence is, what it takes to develop it. Are you more or less surprised as a result
[01:34:01] that evolution just sort of spontaneously stumbled upon it?
[01:34:05] I love Nick Lane's books by the way, so yeah, I was just listening to his podcast
[01:34:12] one way up here. With respect to intelligence and its evolution, I do came it came fairly,
[01:34:18] I mean, it's very, very recent, right? I am surprised that it evolved. I find it fascinating to
[01:34:23] think about all the worlds out there, like say there's a thousand planets like Earth and what they
[01:34:26] look like. I think Nick Lane was here talking about some of the early parts, right? Like okay,
[01:34:30] he expects basically very similar life forms. Roughly speaking in bacteria like things and most
[01:34:35] of them. And then there's a few breaks in there. I would expect that the evolution of intelligence
[01:34:41] intuitively feels to me like it should be fairly rare event and there have been animals for,
[01:34:45] I guess maybe you should base it on how long something has existed. So for example,
[01:34:49] bacteria, I've been around for two billion years and nothing happened. Then going to your care,
[01:34:52] it's probably pretty hard because this bacteria actually came up quite early in Earth's evolution
[01:34:58] or history. And so I guess how long have we had animals, maybe a couple hundred million years,
[01:35:04] like multicellular animals that run around crawl, etc. Which is maybe 10% of Earth's lifespan
[01:35:11] or something like that. So maybe on that time scale is actually not too tricky. I still feel like
[01:35:17] it's still surprising to me, I think intuitively that it evolved. I would maybe expect just a lot of
[01:35:21] animal-like life forms doing animal-like things. The fact that you can get something that creates
[01:35:26] culture and knowledge and accumulates it is surprising to me. Okay, so there's actually a
[01:35:32] couple of interesting follow-ups. If you buy the Sun Perspective that actually the crux of
[01:35:39] intelligence is animal intelligence, what the quote he said is, if you got to the squirrel,
[01:35:43] you'd be most of the way to AGI. Then we got to squirrel intelligence, I guess right after the
[01:35:48] Cambrian explosion, 600 million years ago. It seems like what instigated that was the oxygenation
[01:35:54] event 600 million years ago. But immediately the sort of like intelligence algorithm was there
[01:35:58] to make the squirrel intelligence. So it's suggestive that animal intelligence was like that.
[01:36:06] As soon as you had the oxygen in the environment, you had the ecuriaut, you could just get the algorithm.
[01:36:12] Maybe there was an accident in the devolution smell, the bond is so fast, but I don't know if
[01:36:16] that suggests it's actually quite at the end going to be quite simple. Yes, it's basically so hard
[01:36:21] to tell with any of this stuff. I guess you can base it a little bit on how long something
[01:36:25] is as exit, or how long it feels like something else in bottlenecked. So Nuclein is very good
[01:36:28] about describing this very apparent bottleneck in bacteria in archaea. For two billion years,
[01:36:33] nothing happened. Extreme diversity of biochemistry and yet nothing that grows to become animals.
[01:36:41] Two billion years. I don't know that we've seen exactly that kind of an equivalent with animals
[01:36:46] and intelligence to your point, right? But I guess maybe we could also look at it with respect to
[01:36:51] how many times we think intelligence has individually sprung up. That's a really good thing to investigate.
[01:36:57] Maybe one thought on that is I almost feel like, well, there's the hominid intelligence. I would say
[01:37:03] the bird intelligence, right? Ravens, etc., are extremely clever. But their brain parts are
[01:37:09] actually quite distinct and we don't have that much existence. So maybe that's a slight event
[01:37:15] of there's a slight indication of maybe intelligence springing up a few times. So in that case,
[01:37:19] you'd maybe expect it more frequently or some longer. Yeah. A former guest,
[01:37:23] Gwern, and also Carl Schulman have made a really interesting point about that, which is
[01:37:28] their perspective is that the scalable algorithm which humans have and primates have are
[01:37:35] rose in birds as well. And maybe other times as well. But humans found a evolutionary niche which
[01:37:43] rewarded marginal increases in intelligence. And also how to scale a little brain algorithm that
[01:37:49] could achieve those increases in intelligence. And so for example, a bird had a bigger brain,
[01:37:55] which is like collapsed out of the air. So it's very smart for the size of its brain, but it's
[01:37:59] like it's not an niche which rewards the brain getting bigger. Yeah. Maybe similar with some really
[01:38:05] smart dolphins, etc. Exactly. Yeah. Whereas humans, you know, like we have hands that like reward
[01:38:10] being able to learn how to do tools, we can externalize digestion and more energy to the brain.
[01:38:14] And that kicks off the flywheel. Yeah. And just stuff to work with. I mean, I'm guessing it would
[01:38:19] be harder to if I was dolphin. Yeah. I mean, how do you do? You can't have fire, for example,
[01:38:24] and stuff like that. I mean, they're probably like the universe of things you can do in water,
[01:38:28] like inside water is probably lower than what you can do on land. Yeah. Just chemically.
[01:38:32] Right. Yeah. I do agree with this with this viewpoint of these niches and what's what's being
[01:38:36] incentivized. I still find it's kind of miraculous that I don't I would maybe expect
[01:38:42] the things to get stuck on like animals with bigger muscles, you know? Yeah. Like going through
[01:38:47] intelligence is actually a really fascinating breaking point. The way where I'm putting it is,
[01:38:52] the reason it was so hard is it's a very tight line between being in a situation where
[01:38:57] something is so important to learn that it's not just worth distilling the exact right
[01:39:03] circuits. Yeah. Directly back into your DNA versus it's not important enough to learn at all.
[01:39:09] Yeah. It has to be something which is like you have to to incentivize building the algorithm to
[01:39:15] learn in lifetime. Yeah. Exactly. You have to incentivize some kind of adaptability. You actually
[01:39:19] want something that you actually want environments that are unpredictable. So evolution can't bake your
[01:39:23] algorithms into your weights. A lot of a lot of animals are basically a pre-baked in the sense.
[01:39:28] And so humans have to figure it out that test time when they get born. And so maybe there was you
[01:39:33] actually want these kinds of environments that actually change really rapidly or something like
[01:39:36] that where you can't foresee what will work well. And so you actually put all that intelligence,
[01:39:41] you create intelligence to figure it out at test time. So Quentin Pope had this interesting
[01:39:46] blog post where he's saying the reasoning doesn't expect a sharp takeoff is so humans had the
[01:39:52] sharp takeoff where 60,000 years ago we seem to have had the kind of architectures that we have
[01:39:57] today. And 10,000 years ago, agricultural evolution, modernity, dot, dot, dot, what was happening
[01:40:03] in that 50,000 years? Well, you had to build this sort of like cultural scaffold where you can
[01:40:08] accumulate knowledge over generations. This is an ability that exists for free in the way we do AI
[01:40:15] training where if you retrain a model, it can still, I mean, in many cases, they're literally
[01:40:20] distilled, but they can be trained on each other. You know, they can be trained on the pre-state
[01:40:23] pre-training corpus. They don't literally have to start from scratch. So there's a sense in which
[01:40:28] the thing which it took humans a long time to get this cultural loop going just comes for free with
[01:40:34] the way we do all of them training. Um, yes and no, because all of them don't really have the
[01:40:38] equivalent of culture. And maybe we're giving them way too much and incentivizing not to create it
[01:40:42] or something like that. But I guess like the invention of culture and have written record
[01:40:45] on and of like passing down notes between each other, I don't think there's an equivalent of that
[01:40:49] with all of them right now. So, elements don't really have culture right now. And it's kind of like
[01:40:53] one of the, I think, impediments, I would say. Can you give me some sense of what LLM culture
[01:40:59] might look like? So in the simplest case, it would be a giant scratch pad that the LLM can edit.
[01:41:03] And as it's reading stuff or as it's helping out work with work, it's editing the scratch pad
[01:41:07] for itself. Why can't an LLM write a book for the other LLMs? That would be cool. Yeah.
[01:41:12] Like, why can't other LLMs read this LLM's book and be inspired by it or shocked by it or
[01:41:18] something like that? There's no equivalents for any of this stuff. Interesting. When would you
[01:41:21] expect that kind of thing to start happening? And more general question about like multi-agent
[01:41:25] systems and this sort of like independent AI civilization and culture? I think there's two
[01:41:31] powerful ideas in the realm of multi-agent that have both not been like really claimed or
[01:41:36] or so on. The first one I would say is culture and LLM's basically a growing repertoire of knowledge
[01:41:42] for their own purposes. The second one looks a lot more like the powerful idea of self-play in my
[01:41:47] mind is extremely powerful. So evolution actually is a lot of competition basically driving intelligence
[01:41:53] and evolution. And for an alpha-go, more algorithmically, like alpha-go is playing against itself and
[01:42:01] that's how it learns to get really good at go. And there's no equivalent of self-playing LLMs,
[01:42:05] but I would expect that to also exist but no one has done it yet. Like, why can't an LLM,
[01:42:09] for example, create a bunch of problems that another LLM is learning to solve? And then the
[01:42:14] LLM is always trying to like serve more and more difficult problems, stuff like that, you know? So like,
[01:42:19] I think there's a bunch of ways to actually organize it. And I think it's realm of research.
[01:42:23] But I think I haven't seen anything that convincingly like claims both of those, like multi-agent
[01:42:28] improvements. I still think we're mostly in the realm of a single individual agent, but I think
[01:42:33] I also think that will change. And in the realm of culture also, I would bucket also organizations.
[01:42:39] And we haven't seen anything like that convincingly either. So that's why we're still early.
[01:42:44] And can you identify the key bottleneck that's preventing this kind of collaboration between LLMs?
[01:42:50] Maybe like the way I would put it is somehow remarkably, again, some of these analogies work,
[01:42:55] and they shouldn't, but somehow remarkably they do. A lot of the smaller models, or the smaller models,
[01:43:00] somehow remarkably resemble like a kindergarten student, or then like a elementary school student,
[01:43:05] or high school student, etc. And somehow we still haven't like graduated enough where the stuff
[01:43:09] can take over. Like it's still mostly like my cloth code or codex. They still kind of feel like
[01:43:15] this elementary student. I know that they can take PhD quizzes, but they still cognitively feel like
[01:43:21] a kindergarten or a non-tree school student. So I don't think they can create culture because
[01:43:24] they're still kids, you know, like they're savant kids. They have absurd, they have perfect memory
[01:43:31] of all this stuff, etc. And they can convincingly create all kinds of slop that looks really good.
[01:43:36] But I still think they don't really know what they're doing and they don't really have the
[01:43:38] cognition across all these little check boxes that we still have to collect. Yeah. So you've
[01:43:43] talked about how you were at Tesla leading self-driving from 2017 to 2022. And then you first
[01:43:51] hand saw this progress from we went from cool demos to now thousands of cars out there actually
[01:43:58] autonomously doing drives. Why did that take a decade? Like what was happening through that time?
[01:44:02] Yeah. So I would say one thing I will almost instantly also push back on is this is not even
[01:44:07] you're done. So in a bunch of ways that I'm going to get to. I do think that self-driving is
[01:44:13] very interesting because it's definitely like where I get a lot of my intuitions because I spent
[01:44:17] five years on it. And it has this entire history where actually the first demos of self-driving
[01:44:23] go all the way to 1980s. You can see a demo from CMU in 1986. There's a truck that's driving
[01:44:28] itself on roads. But okay, fast forward, I think when I was joining Tesla, I had a very early
[01:44:36] demo of a Waymo. And it basically gave me a perfect drive in 2014 or something like that.
[01:44:43] So perfect Waymo drive a decade ago. It took us around Palo Alto and so on because I had a friend
[01:44:48] who worked there. And I thought it was like very close and then still took a long time. And I do
[01:44:53] think that some there's for some kinds of tasks and jobs and so on. There's a very large demo to
[01:45:00] product gap where the demo is very easy, but the product is very hard. And it's especially the case
[01:45:06] in cases like self-driving where the cost of failure is too high. Many industries, tasks and jobs
[01:45:13] maybe don't have that property, but when you do have that property that definitely increases
[01:45:16] the timelines. I do think that for example in software engineering, I do actually think that
[01:45:20] that property does exist. I think for a lot of vibe coding, it doesn't. But I think if you're
[01:45:25] writing actual production grade code, I think that property should exist because any kind of mistake
[01:45:29] actually leads to a security vulnerability or something like that. And millions and hundreds of
[01:45:32] millions of people's personal social security numbers, et cetera, get leaked or something like that.
[01:45:37] And so I do think that it is a case that in software, people should be careful.
[01:45:42] Kind of like in self-driving. Like in self-driving, if you if things go wrong, you might get injury.
[01:45:46] In I guess there's worse outcomes, but I guess in software, I almost feel like it's almost
[01:45:52] unbounded how terrible something could be. So I do think that they share that property.
[01:45:58] And then I think basically what takes them a long amount of time and the way to think about it
[01:46:02] is that it's a march of nines. And every single nine is a constant amount of work.
[01:46:08] So every single nine is the same amount of work. So when you get a demo and something works 90%
[01:46:12] of the time, that's just that's just the first nine. And then you need a second nine and third nine,
[01:46:17] fourth nine, fifth nine. And while I was at Tesla for was it five years or so, I think we went through
[01:46:21] maybe three nines, two nines, I don't know what it is. But like multiple nines of iteration,
[01:46:25] there's still more nines to go. And so that's why these things take takes a long.
[01:46:30] And so it's definitely formative for me, like seeing something that was a demo, I'm very
[01:46:35] unimpressed by demos. So whenever I see demos of anything, I'm extremely unimpressed by that.
[01:46:41] It works better if you can, if it's a demo that someone cooked up and is just showing you,
[01:46:45] it's worst. If you can interact with it, it's a bit better. But even then you're not done,
[01:46:48] you need actual product. It's going to face all these challenges in when it comes in contact
[01:46:52] with reality and all these different pockets of behavior that need patching. And so I think we're
[01:46:56] going to see all this stuff play out. It's a march of nines. Each nine is constant. demos are
[01:47:00] encouraging. Still a huge amount of work to do. I do think it is a kind of a critical safety domain
[01:47:07] unless you're doing vibe coding, which is all nice and fun and so on. And so that's why I think
[01:47:12] this also enforce my timelines from that perspective. That's very interesting to hear you say that
[01:47:18] the sort of safety guarantees you need from software are actually not dissimilar to self driving
[01:47:22] because what we will often say is that self driving took so long because the cost of failure
[01:47:28] is so high. Like a human makes a mistake on average every 400,000 miles or every seven years.
[01:47:33] And if you had to release a coding agent that couldn't make a mistake for at least seven years,
[01:47:39] it would be much harder to deploy. But I guess your point is that if you made a catastrophic
[01:47:44] coding mistake, like breaking some important system every seven years. And in fact, in terms of
[01:47:49] sort of wall clock time, it would be much less than seven years because you're like constantly
[01:47:53] outputting code like that. So it's like per tokens or in terms of tokens, it would be seven years,
[01:47:58] but in terms of wall clock time. It's a much harder problem. I mean, self driving is just one of
[01:48:02] thousands of things that people do. It's almost like a single vertical asset post, whereas when we're
[01:48:07] talking about general software engineering, it's even more there's more surface area. There's another
[01:48:11] objection people make to that analogy, which is that with self driving, what took a big fraction
[01:48:19] of that time was solving the problem of building basic, having basic perception that's robust and
[01:48:25] building representations and having a model that has some common sense so it can generalize to
[01:48:31] when I see something that's slightly out of distribution. If somebody's waving down the road
[01:48:36] this way, you don't need to train for it. The thing will have some understanding of how to respond
[01:48:41] to something like that. And these are things we're getting for free with LLMs or VLMs today. So we
[01:48:46] don't have to solve these very basic representation problems. And so now deploying AIs across different
[01:48:51] domains will sort of be like deploying a self driving car with current models to a different city,
[01:48:55] which is hard, but not like a 10 year long task. Yeah, basically I'm not 100% sure if I fully agree
[01:49:00] with that. I don't know that we're how much we're getting for free. And I still think there's
[01:49:03] like a lot of gaps in understanding in what we are getting. And we're definitely getting more
[01:49:08] generalizable intelligence in a single entity. Whereas self driving is a very special purpose
[01:49:13] task that requires in some sense, building a special purpose task is maybe even harder in a
[01:49:17] certain sense because it doesn't like fall out from a more general thing that you're doing at
[01:49:20] scale. That makes sense. But I still think that the analogy doesn't, I still don't know if it
[01:49:27] fully resonates because like the LLMs are still pretty fallible and I still think that they have a
[01:49:31] lot of gaps and that still needs to be filled in. And I don't think that we're getting like magical
[01:49:35] generalization completely out of the box sort of in a certain sense. And the other aspect that I
[01:49:40] want to also actually a return to when I was in the in the beginning was self driving cars are
[01:49:45] no wonder down still. So even though so the deployment still are pretty minimal, right? So even
[01:49:51] Waymo and so on has very few cars and they're doing that roughly speaking because they're not
[01:49:54] economical, right? Because they built something that that lives in the future. And so they they
[01:50:00] had to like pull back future, but they had to make it un-economical. So they have all these like
[01:50:05] you know, there's all these costs, not just marginal costs for those cars and their operation
[01:50:09] and maintenance, but also the capex of the entire thing. So making the economical is still going
[01:50:14] to be a slog I think for them. And then also I think when you look at these cars and there's no
[01:50:19] one driving, I also think it's a little bit deceiving because there are actually very elaborate
[01:50:24] tele-operation centers of people actually kind of like in a loop with these cars. And I don't have
[01:50:29] the full extent of it, but I think there's more human in a loop that you might expect. And there's
[01:50:34] people somewhere out there basically beaming in from the sky. And I don't actually know they're
[01:50:39] fully in the loop with the driving. I think some of the times they are, but they're certainly involved
[01:50:43] and there are people. And in some sense we haven't actually removed the person we've like moved
[01:50:46] them to somewhere where you can't see them. I still think there will be some work as you mentioned
[01:50:50] going from environment to environment. And so I think like there's still challenges to make
[01:50:54] self-driving real. But I do agree that it's definitely across a threshold where it kind of feels
[01:50:59] real unless it's like really tele-operated. For example, Waymo can't go to all the different parts
[01:51:04] of the city. My suspicion is it's like parts of city where you don't get a good signal.
[01:51:09] Anyway, so basically I don't actually know anything about the stack. I mean, I'm just making
[01:51:13] up, making outside. Surely you let self-driving for five years of Tesla. Sorry, I don't know anything
[01:51:18] about the specifics of Waymo. I actually, by the way, I love Waymo and I take it all the time.
[01:51:22] So I don't want to say like, I just think that people again are sometimes a little bit too naive
[01:51:28] but some of the progress. And I still think there's a huge amount of work. And I think Tesla took
[01:51:32] in my mind a lot more scalable approach. And I think the team is doing extremely well and it's
[01:51:35] going to, and I'm kind of like on the record for predicting how this will go, which is like
[01:51:41] Waymo had like early start because you can package up so many sensors. But I do think Tesla is
[01:51:45] taking the more scalable strategy and it's going to look a lot more like that. So I think this will
[01:51:49] have to still play out and hasn't. But basically like, I don't want to talk about self-driving or
[01:51:53] something that took a decade because it didn't take it. It didn't take it.
[01:51:57] Of that makes sense. Because one, the start is at 1980, not 10 years ago, and then two,
[01:52:03] the end is not here yet. Yeah, the end is not near yet because when we're talking about self-driving,
[01:52:08] usually in my mind, it's self-driving at scale. People don't have to get a driver's license, etc.
[01:52:13] I'm curious to bounce two other ways in which the analogy might be different. And the reason
[01:52:19] I'm especially curious about this is because I think the question of how fast AI is deployed,
[01:52:24] how valuable it is when it's early on, is like potentially the most important question in the
[01:52:29] world right now, right? Like if you're trying to model what the year or 20 or 30 looks like,
[01:52:32] this is the question you want to have some understanding of. So another thing you might think is
[01:52:38] one, you have this latency requirement with self-driving where you have, I have no idea what
[01:52:42] the actual models are, but I assume like tens of millions of parameters or something, which is not
[01:52:46] the necessary constraint for knowledge work with LLM's. Or maybe it might be with a computer
[01:52:52] use and stuff. But anyways, the other big one is maybe more importantly, on this cat-backed question,
[01:52:59] yes, there is additional cost to serving up an additional copy of a model. But the sort of
[01:53:07] apex of a session is quite low and you can amortize the cost of AI into the training run itself,
[01:53:15] depending on how inference scaling goes and stuff. But it's certainly not as much as like building
[01:53:19] a whole new car to serve another instance of a model. So it's just the economics of deploying more
[01:53:26] widely are much more favorable. I think that's right. I think if you're sticking the realm of bits,
[01:53:31] bits are like a million times easier than anything that touches the physical world. I definitely
[01:53:36] grant that. Bits are completely changeable, arbitrarily, reshuffleable at a very rapid speed.
[01:53:42] So you would expect a lot more faster adaptation also in the industry and so on.
[01:53:48] And then what was the first one? The Lindsay requirements. I think that's roughly right. I
[01:53:54] also think that if we are talking about knowledge work at scale, there will be some latency requirements
[01:53:59] practically speaking because we're going to have to create a huge amount of compute to serve that.
[01:54:05] And then I think the last aspect that I very briefly want to also talk about is all the rest
[01:54:09] of it, just all the rest of it. So what does society think about it? What is the legal
[01:54:16] how is it working legally? How is it working insurance-wise? What are those layers of it?
[01:54:22] An aspect of it? What happens with what is the equivalent of people putting a cone on a
[01:54:26] Waymo? There's going to be equivalence of all that. And so I do think that I almost feel like
[01:54:32] self-strapping is a very nice analogy that you can borrow things from. What is the equivalent of
[01:54:37] a cone on the car? What is the equivalent of a teleoperating worker who's hidden away? Almost
[01:54:43] like all the aspects of it. Do you have any opinions on whether this implies that the current
[01:54:47] day I build out, which would like 10x the amount of available compute in the world in a year or two
[01:54:54] and maybe like 100 more than 100x at the end of the decade? If the use of AI will be lower
[01:55:00] than some people in the early predict, does that mean that we're over building compute or is that
[01:55:06] a separate question? Kind of like what happened with railroads and all this kind of stuff.
[01:55:09] What's the railroads? Sorry. There is like a historical precedent or was it with telecommunication
[01:55:15] industry like prepaving the internet that only came like a decade later and creating like a
[01:55:20] whole bubble in the telecommunications industry in the late 90s kind of thing?
[01:55:25] So I don't know. I mean I understand I'm sounding very pessimistic here. I'm only doing that.
[01:55:31] I'm actually optimistic. I think this will work. I think it's tractable. I'm only sounding
[01:55:35] pessimistic because when I go on my Twitter timeline, I see all this stuff that makes no sense to me.
[01:55:42] And I think there's a lot of reasons for why that exists and I think a lot of it is I think
[01:55:45] honestly just stuff fundraising. It's just incentive structures. A lot of it may be fundraising.
[01:55:50] A lot of it is just attention, you know, converting attention to money on the internet, you know,
[01:55:55] stuff like that. So I think there's a lot of that going on and I think I'm only reacting to that.
[01:56:03] But I'm still like overall very bullish on technology. I think we're going to work through all
[01:56:06] this stuff. And I think there's been a rapid amount of progress. I don't actually know that
[01:56:10] there's overbuilding. I think that we're going to be able to go up what in my understanding is
[01:56:15] being built because I do think that for example, Cloud Code or OpenAI Codex and stuff like that,
[01:56:20] they didn't even exist a year ago, right? Is that right? I think it's roughly right.
[01:56:24] This is miraculous technology that didn't exist. I think there's going to be a huge amount of
[01:56:29] demand as we see the demand in charge of it. He already has someone. So yeah, I don't actually
[01:56:34] know that there's overbuilding. But I guess I'm just reacting to some of the very fast timelines
[01:56:40] that people continue to say incorrectly. And I've heard many, many times over the course of my
[01:56:44] 15 years in AI, where very reputable people keep getting this wrong all the time.
[01:56:50] And I think I want us to be properly calibrated. And I think some of this also, it does have like
[01:56:54] geopolitical ramifications and things like that when like some of these questions. And I think
[01:56:59] I don't want people to make mistakes on that sphere of things. So I do want us to be grounded in
[01:57:05] reality of what technology is and isn't. So let's talk about education in Eureka and stuff. One
[01:57:11] thing you could do is start another AI lab and then try to solve those problems. Yeah,
[01:57:19] you're curious what you up to now. Yeah. And then yeah, why not AI research itself?
[01:57:24] I guess maybe like the way I would put it is, I feel some amount of like the
[01:57:28] terminism around the things that AI labs are doing. And I feel like I could help out there,
[01:57:34] but I don't know that I would like uniquely, I don't know that I would like uniquely improve it.
[01:57:41] But I think like my personal big fear is that a lot of the stuff happens on the side of humanity
[01:57:46] and that humanity gets disempowered by it. And I kind of like, I care not just about all the
[01:57:51] licensed fears that we're going to build and that AI is going to build in a fully autonomous way.
[01:57:55] I care about what happens to humans. And I want humans to be well off in this future. And I feel
[01:58:00] like that's where I can a lot more uniquely at value than like an incremental improvement in the
[01:58:05] frontier lab. And so I guess I'm most afraid of something maybe like the pictures in movies like
[01:58:11] Wally or Ideocracy or something like that where humanity is sort of on the side of this stuff.
[01:58:16] And I want humans to be much, much better in this future. And so I guess to me,
[01:58:22] this is kind of like through education that you can actually achieve this.
[01:58:24] And so what are you working on there?
[01:58:26] Yeah. So Eureka is trying to build, I think maybe the easiest way I can describe it as we're
[01:58:30] trying to build the Starfleet Academy. I don't know if you watch Star Trek.
[01:58:34] I haven't. Yeah. Okay. Starfleet Academy is this like elite institution for frontier
[01:58:40] technology building spaceships and graduating cadets to be like in the palace of these spaces,
[01:58:44] no, not. So I just imagine like an elite institution for technical knowledge. And
[01:58:50] and basically a kind of school that's very up to date and very like a premier institution.
[01:58:56] A category of questions I have for you is just explaining how one teaches technical or scientific
[01:59:03] content. Well, because you are one of the world masters at it. And then I'm curious both about
[01:59:09] how you think about it for content you already put out there on YouTube. But also to the extent
[01:59:13] it's any different, how you think about it for Eureka. Yeah. Well, the respect to Eureka, I think
[01:59:17] like one thing that is very fascinating to me about education is like I do think education will
[01:59:21] pretty fundamentally change with AIs on the side. And I think it has to be rewired and changed
[01:59:26] to some extent. I still think that we're pretty early. I think there's going to be a lot of people
[01:59:31] who are going to try to do the obvious things, which is like, oh, have an LLM and ask it questions
[01:59:36] and get, you know, do all the basic things that you would do via prompting right now. I think
[01:59:39] it's helpful, but it still feels to me a bit like slop, like slop. I'd like to do it properly,
[01:59:43] and I think the capability is not there for what I would want. What I'd want is like an actual
[01:59:48] tutor experience. Maybe a prominent example in my mind is I was recently learning Korean.
[01:59:55] It's language learning. And I went through a phase where I was learning Korean by myself
[01:59:59] on the internet. I went through a phase where I was actually part of a small class in Korea,
[02:00:04] taking a Korean with a bunch of other people, which was really funny. But we had a teacher and
[02:00:07] like 10 people or so taking Korean. And then I switched to a 101 tutor. And I guess what was
[02:00:14] fascinating to me is I think I had a really good tutor. But I mean, just thinking through like what
[02:00:20] this tutor was doing for me and how incredible that experience was and how high the bar is for
[02:00:25] like what I actually want to build eventually. Because I mean, she was extremely, so she instantly
[02:00:30] from a very short conversation understood like where I am as a student, what I know and don't know.
[02:00:35] And she was able to like probe exactly like the kinds of questions or things to understand my
[02:00:39] world model. No LLM will do that for you. 100% right now not even close, right. But a tutor will
[02:00:44] do that if they're good. Once she understands, she actually like really served me all the things that
[02:00:50] I needed at my current sliver of capability. I need to be always appropriately challenged. I can't
[02:00:54] be faced with something too hard or too trivial. And a tutor is really good at serving you just
[02:00:59] to write stuff. And so basically I felt like I was the only constraint to learning like my own.
[02:01:04] I was the only constraint. I was always given the perfect information. I'm the only constraint.
[02:01:08] And I felt good because I'm the only impediment that exists. It's not that I can't find knowledge
[02:01:12] or there's not properly explained or etc. Like it's just my ability to memorize and so on.
[02:01:16] And this is what I want for people. How do you automate that? So very good question. At the current
[02:01:21] capability, you don't. But I do think that with as and that's why I think it's not actually the right
[02:01:27] right time to actually build this kind of an AI tutor. I still think it's a useful product
[02:01:31] and lots of people will build it. But I still feel like the bar is so high and the capability is not
[02:01:37] there. But I mean, even today, I would say ChargeBT needs an extremely valuable educational product.
[02:01:45] But I think for me was so fascinating to see how high the bar is. And when I was with her,
[02:01:49] I almost felt like there's no way I can build this. But you're building it right?
[02:01:54] Anyone who's had a really good tutor is like, how are you going to build this?
[02:01:59] So I guess I'm waiting for that capability. I do think that in a lot of ways in the industry,
[02:02:03] for example, I did some AI consulting for computer vision. A lot of my times, the value that I
[02:02:08] brought to the company was telling them not to use AI. It wasn't like, I was the AI expert and they
[02:02:12] described the problem. I said, don't use AI. This was my value. And I feel like it's in the same
[02:02:18] in education right now where I kind of feel like for what I have in mind, it's not yet the time,
[02:02:22] but the time will come. But for now, I'm building something that looks maybe a bit more conventional
[02:02:27] that has a physical and digital component and so on. But I think there's obvious, there's obvious,
[02:02:32] it's obvious how this should look like in the future. Do these things you're really going to say,
[02:02:35] what is the thing you hope will be released this year or next year? Also, I'm building the first course
[02:02:42] and I want to have a really, really good course. State of the art, obvious state of the art
[02:02:47] destination you go to learn AI in this case, because that's just what I'm familiar with. So I think
[02:02:51] it's a really good first product to get to be really good. And so that's what I'm building. And
[02:02:55] Nanachad, which you briefly mentioned, is a capstone project of LLM101N, which is a class that I'm
[02:02:59] building. So that's a really big piece of it. But now I have to build out a lot of the intermediate
[02:03:05] and then I have to actually like hire a small team of, you know, TAs and so on and actually like
[02:03:09] build the entire course. And maybe one more thing that I would say is like many times when
[02:03:14] people think about education, they think about sort of like the more, what I would say is like kind
[02:03:18] of a softer component of like diffusing knowledge or like, but I actually have something very
[02:03:23] hard and technical in mind. And so in my mind, education is kind of like the very difficult
[02:03:26] technical like process of building ramps to knowledge. So in my mind, Nanachad is a ramp to
[02:03:33] knowledge, because it's a very simple, it's like the super simplified, full stack thing. If you
[02:03:38] give this artifact to someone and they like look through it, they're learning a ton of stuff.
[02:03:42] And so it's giving you a lot of what I call eureka's per second, which is like understanding per
[02:03:47] second. That's what I want. Lots of eureka's per second. And so to me, this is a technical problem
[02:03:52] of how do we build these ramps to knowledge. And so I almost think of eureka as almost like a
[02:03:57] it's not like maybe that different, maybe through some of the future from tier labs or some of the
[02:04:01] work that's going to be going on because I want to figure out how to build these frontier,
[02:04:05] these ramps very efficiently so that people are never stuck. And everything is always not too
[02:04:10] hard or not too trivial. And you can't, you have just the right material to actually progress.
[02:04:16] Yeah, so you're imagining the short term that instead of a tutor being able to like probe your
[02:04:21] understanding, if you have enough self awareness to be able to probe yourself,
[02:04:25] you're never going to be stuck. You can like find the right answer between talking to the TA
[02:04:30] or talking to an L and looking at the reference implementation. It sounds like
[02:04:34] automation or AI is actually not as significant. Like so far, it's actually the big alpha here
[02:04:41] is your ability to explain AI, the hardified in the source material of the class, right?
[02:04:49] That's like fundamentally what the course is. I mean, I think you always have to be
[02:04:52] calibrated to what the capability exists in the industry. And I think a lot of people are going
[02:04:56] to pursue like, oh, just ask, charge of PT, et cetera. But I think like right now, for example,
[02:05:00] if you go to charge between you say, oh, teach me AI, there's no way it's going to give you some
[02:05:04] slop, right? Like when AI is never going to write nanachat right now, but nanachat is a really
[02:05:10] useful, I think, intermediate point. So I still, I'm collaborating with AI to create all this
[02:05:15] material. So AI is still fundamentally very helpful. Earlier on, I built a CS231 and Stanford,
[02:05:20] which was one of the earlier, actually, started. I think it was the first deep learning class
[02:05:24] at Stanford, which became very popular. And the difference in building out to 31N and L11N
[02:05:30] now is quite stark, because I feel really empowered by the elements as they exist right now,
[02:05:36] but I'm very much in the loop. So they're helping me build a little materials, I go much faster,
[02:05:40] they're doing a lot of the boring stuff, et cetera. So I feel like I'm developing the course
[02:05:44] much faster and those LLM, if used in it, but it's not yet at a place where I can creatively
[02:05:49] create the content. I'm still there to do that. So like, I think the trickiness is always
[02:05:53] calibrating yourself to what exists. And so when you imagine what is available through Ureka
[02:05:57] in a couple of years, it seems like the big bottleneck is going to be finding corpothies and
[02:06:03] field after field who can convert their understanding into these ramps, right? So I think it would change
[02:06:09] over time. So I think right now, it would be hiring faculty to help work hand in hand with AI and
[02:06:16] a team of people probably to build a state of the art courses. And then I think over time it can,
[02:06:22] maybe some of the TAs can actually become AI's, because some of the TAs like, okay, you just take
[02:06:26] all the course materials and then I think you could serve a very good like automated TA for the
[02:06:31] students when they have more basic questions or something like that, right? But I think you'll need
[02:06:35] faculty for the overall architecture of course and making sure that it fits. And so I kind of see
[02:06:40] a progression of how this will evolve. And maybe at some future point, you know, I'm not even that
[02:06:44] useful in AI is doing most of the design much better than I could. But I still think that that's
[02:06:48] going to take some time to play up. But are you imagining that like people who have expertise in
[02:06:54] other fields are then contributing courses or do you feel like it's actually quite essential to
[02:06:58] the vision that you, given your understanding of how you want to teach are the one designing the
[02:07:05] content? Like I don't know, Sal Khan is like narrating all the videos on Khan Academy. Are you
[02:07:09] imagining something like that or? Oh, no, I will hire faculty. I think because there are
[02:07:12] domains in which I'm not an expert. And I think that's the only way to offer the state of the art
[02:07:18] experience for the student ultimately. So yeah, I do expect that I would hire faculty. But I will
[02:07:24] probably stick around in AI for some time. But I do have something I think more conventional in mind
[02:07:29] for the current capability. I think that what people would probably anticipate. And when I'm building
[02:07:34] Starfleet Academy, I do probably imagine a physical institution and maybe a tier below that a digital
[02:07:39] offering that is not the state and not the state of the art experience you would get when someone
[02:07:44] comes in physically full-time. And we work through material from Starfleet and make sure you
[02:07:48] understand it. That's the physical offering. The digital offering is a bunch of stuff on the
[02:07:53] internet and maybe some LLM assistant and some bit more gimmicky in a tier below. But at least
[02:07:57] it's accessible to like 8 billion people. So yeah, I think you're basically inventing college
[02:08:04] from first principles for the tools that are available today. And then just like for just
[02:08:11] selecting for people who have the motivation and the interest of actually really engaging out
[02:08:16] material. Yeah. And I think there's going to have to be a lot of not just education, but also
[02:08:20] re-education. And I would love to help out there. Because I think the jobs will probably change quite
[02:08:25] a bit. And so for example, today a lot of people are trying to upskill in AI specifically. So I think
[02:08:30] it's a really good course to teach in this in this respect. And yeah, I think the motivation wise
[02:08:35] before AGI motivation is very simple to solve because people want to make money. And this is how
[02:08:41] you make money in history today. I think post AGI is a lot more interesting, possibly because yeah,
[02:08:47] if everything is automated and there's nothing to do for anyone, why would anyone go to a school,
[02:08:51] etc. So I think I guess like I often say that pre AGI education is useful post AGI education
[02:08:59] is fun. And in a similar way as people, for example, people go to gym today. But we don't need
[02:09:06] their physical strength to manipulate heavy objects because we have machines to do that. They still
[02:09:11] go to gym. Why do they go to gym? Well, because it's fun. It's healthy. It's and it's and you look
[02:09:16] hot when you have a six back. I don't know. I guess like so it's I guess what I'm saying is it's
[02:09:22] attractive for people to do that is in a certain like very deep psychological evolutionary sense
[02:09:27] for humanity. And so I kind of think that education will kind of play out in the same way,
[02:09:31] like you'll go to school, like you go to gym. And you'll and I think that right now I think not
[02:09:36] that many people learn because learning is hard. You bounce from material because and some people
[02:09:42] overcome that barrier, but for most people it's hard. But I do think that we should it's a technical
[02:09:46] problem to solve. It's a technical problem to do what my tutor did for me when I was learning
[02:09:51] Korean. I think it's tractable and buildable and someone should build it. And I think it's
[02:09:54] going to make learning anything like trivial and desirable and people will do it for fun because
[02:09:59] it's trivial. If I had a tutor like that for any arbitrary piece of like knowledge, I think
[02:10:04] it's going to be so much easier to to learn anything and people will do it. And they'll do it for
[02:10:07] the same reason they go to gym. I mean, that sounds different from using this supposed AGI. You're
[02:10:14] using this to basically as entertainment or as like a self-betterment. But it sounded like you
[02:10:22] had a vision also that this education is relevant to keeping humanity in control of AI. I say
[02:10:27] and they sound different and I'm curious is it like it's entertaining for some people but then
[02:10:31] empowerment for some others? How do you think about that? I think this so I do definitely feel like
[02:10:34] people will be I do think like eventually it's a bit of a losing game if that makes sense. I
[02:10:40] do think that it is in long term. Yeah. Long term which I think is longer than I think maybe
[02:10:45] most people in the history. It's a losing game. I do think that people can go so far and that we
[02:10:50] barely scratch the surface so much a person can go. And that's just because people are bouncing off
[02:10:54] of material that's too easy or too hard. And I actually kind of feel that people will be able to
[02:11:00] go much further. Like anyone speaks five languages because why not? Because it's so trivial.
[02:11:04] Anyone knows all the basic curriculum of hundreds of others, etc.
[02:11:09] No, no, that I'm understanding the vision. That's very interesting. Like I think it actually has
[02:11:14] a perfect analog in gym culture. I don't think a hundred years ago anybody would be like
[02:11:19] ripped. Like nobody would have, you know, be able to like just spontaneously bench two
[02:11:23] plates or three plays or something. It's actually very common now. And you're because this idea of
[02:11:29] systematically training and lifting weights in the gym or systematically training to be able to run
[02:11:33] a marathon, which is a capability spontaneously you would not have or most humans would not have.
[02:11:38] And you're imagining similar things were learning across many different domains,
[02:11:43] which were intensely deeply faster. Yeah, exactly. And I kind of feel like I am betting a
[02:11:47] little bit implicitly on some of the timelessness of human nature. And I think it will be desirable
[02:11:53] to do all these things. And I think people will look up to it. And as they have for millennia,
[02:12:01] because, and I think this will continue to be true. And actually also maybe there's some evidence
[02:12:06] of that historically. Because if you look at, for example, aristocrats or you look at maybe
[02:12:09] ancient Greece or something like that, whenever you had little pocket environments that were
[02:12:12] post-AGI in a certain sense, I do feel like people have spent a lot of their time flourishing in
[02:12:17] a certain way, either physically or comfortably. And so I think I feel okay about the prospects of
[02:12:23] that. And I think if this is false and I'm wrong, and we end up in like, you know,
[02:12:29] Wally or Idiocracy future, then I think it's very, I don't even care. There's like licensed fears.
[02:12:34] This is terrible outcome. Yeah. Like I actually really do care about humanity. Like everyone has to
[02:12:40] just be superhuman in a certain sense. I guess it's still a world in which that is not enabling us to
[02:12:48] it's like the culture world, right? Like you're not fundamentally going to be able to like
[02:12:52] transform the trajectory of, yeah, technology or, yeah, influence decisions by your own
[02:12:59] labor or cognition alone. Maybe you can influence decisions because the AI is like for your approval.
[02:13:04] But you're not like, it's not because I have like, I can, because I've invented something,
[02:13:08] or I've like come up with a new design. I'm like really influencing the future.
[02:13:12] Yeah, maybe I don't actually think that I think there will be transition in a period where we
[02:13:16] are going to be able to be in the loop and, you know, advanced things if we actually understand
[02:13:20] a lot of stuff. I do think that long term that probably goes away, right? But maybe it's going
[02:13:25] into even a sport. But right now you have power lifters who go extreme on this ration.
[02:13:30] So what is powerlifting in a cognitive era? Maybe it's people who are really trying to make
[02:13:35] Olympics out of knowing stuff. Yeah. And if you have a perfect AI tutor, maybe you can get
[02:13:42] extremely far. I almost feel like we're just barely the geniuses of today. I bear discussion
[02:13:48] to surface of what a human mind can do, I think. Yeah, I love this vision. I also, it's like,
[02:13:55] I feel like the person who have like most product market fit with is like me because like my job
[02:13:59] involves having to learn different subjects every week. And I am, I am like very excited if you can.
[02:14:08] I'm similar for that matter. I mean, I, you know, a lot of people, for example,
[02:14:11] hate school. I want to get out of it. I was, I was actually, I really liked school. I love
[02:14:14] learning things, etc. I wanted to stay in school. I stayed all the way until PhD. And then they
[02:14:18] wouldn't let me stay longer. So I went to the industry. But I mean, I basically, it's roughly speaking,
[02:14:23] I love, I love learning, even for the sake of learning. But I also love learning because it's
[02:14:28] a form of empowerment and being useful and productive. I think you also made a point that
[02:14:32] was subtle. So just to spell it out, I think what's happened so far with online courses is that
[02:14:37] why haven't they already enabled us to enable everything we've meant to know everything.
[02:14:44] And I think they're just so motivation-laden because there's not obvious on ramps. And it's
[02:14:49] so easy to get stuck. And if you had, instead of this, this thing, basically like a really good
[02:14:58] human tutor, it would just be such an unlike from a motivation perspective. I think so. Because it
[02:15:02] feels bad to bounce from material. It feels bad. You get negative reward from a sinking amount of
[02:15:08] time and something and this doesn't pen out or like being completely bored because which are
[02:15:12] getting us too easy or too hard. So I think, yeah, I think when you actually do it properly,
[02:15:17] learning feels good. And I think it's a technical problem to get there. And I think for a while,
[02:15:22] it's going to be AI plus human collab. And at some point, maybe it's just AI.
[02:15:25] I don't know. Can I ask some of my questions about teaching? Well, if you had to give advice to
[02:15:31] another educator in another field that you're curious about to make the kinds of YouTube tutorials
[02:15:38] you've made, maybe especially interesting to talk about domains where you can't just like,
[02:15:43] you can't test somebody's technical understanding by having them code something up or something.
[02:15:47] What advice would you give them? So I think that's a pretty broad topic. I do feel like there's
[02:15:51] basically, I almost feel like there are 10, 20 tips and tricks that I kind of semi-consciously
[02:15:55] probably do. But I guess like on a high level, I always try to, I think a lot of this comes
[02:16:03] from my physics background. I really, really did enjoy my physics background. I have a whole rant
[02:16:06] when I think how everyone should learn physics in early school education. Because I think
[02:16:12] early school education is not about crimulating knowledge or memory for tasks later in the
[02:16:16] industry. It's about booting up a brain. And I think physics uniquely boots up the brain the best.
[02:16:21] Because some of the things that they get you to do in your brain during physics is extremely
[02:16:25] valuable later. The idea of building models and abstractions and understanding that there are,
[02:16:29] there's a first order of approximation that describes most of the system. But then there's a second
[02:16:33] order, third order, first order terms that may or may not be present. And the idea that you're
[02:16:38] observing like a very noisy system, but actually there's like these fundamental frequencies that you
[02:16:41] can abstract away. Like when a physicist walks into the class and they say, oh,
[02:16:46] assume there's a spherical cow and dot, dot, dot. And everyone laughs at that, but actually this
[02:16:50] brilliant, brilliant thinking that's very journalizable across the industry. Because yeah, cow
[02:16:54] can be approximate as a sphere, I guess, in much of a ways. There's a really good book, for example,
[02:17:00] scale. It's basically from a physicist talking about biology. And maybe this is also a book I
[02:17:05] recommend reading. But you can actually get a lot of really interesting approximations and chart
[02:17:09] scaling loss of animals. And you're going to get their heartbeats and things like that. And they
[02:17:13] actually line up and with the size of the animal and things like that, you can talk about an animal
[02:17:18] as a volume and you can actually drive a lot of, you can talk about the heat dissipation off that
[02:17:22] because your, your heat dissipation grows as the surface area, which is growing a square,
[02:17:27] but your heat creation or generation is growing as a cube. And so I just feel like physicists have
[02:17:33] all the right cognitive tools to approach problem solving in the world. So I think because of that
[02:17:36] training, I always try to find the first order terms or the second order terms of everything.
[02:17:41] When I'm observing a system or thing, I have a tangle of a web of ideas or knowledge in my world
[02:17:46] in my mind. And I'm trying to find what is the, what is the thing that actually matters? What is
[02:17:49] the first order component? How can I simplify it? How can I have a simple thing that actually shows
[02:17:53] that thing, right? It shows, shows an action. And then I can tackle in the other terms. Yeah.
[02:17:57] Maybe, maybe an example from my, from one of my repos that I think illustrates it well is
[02:18:01] called micrograd. I don't know if you're familiar with this, but so micrograd is 100 lines of
[02:18:06] code that shows back propagation. It can, you can create neural networks out of simple operations,
[02:18:11] like plus and times, et cetera, Lego blocks of neural networks. And you build up a computational
[02:18:15] graph and you do a forward pass and a backward pass to get the gradients. Now this is at the heart
[02:18:20] of all neural network learning. So micrograd is a 100 lines of pretty interpretable Python code.
[02:18:25] And it can do forward and backward arbitrary neural networks, but not efficiently. So micrograd,
[02:18:29] these 100 lines of Python are everything you need to understand how neural networks train.
[02:18:33] Everything else is just efficiency. Yeah. Everything else is efficiency. And there's a huge amount
[02:18:38] of work to do efficiency, you know, you need your tensors, you lay them out, you stride them, you
[02:18:42] make sure your kernels are orchestrating memory movement correctly, et cetera. It's also just
[02:18:45] efficiency, roughly speaking. Yeah. But the core intellectual sort of piece of neural network
[02:18:49] training is micrograd 100 lines. You can easily understand it. You're changing. It's a recursive
[02:18:53] application of chain rule to drive the gradient, which allows you to optimize any arbitrary
[02:18:56] differential function. So it's a, I love finding these like, you know, the smaller the terms.
[02:19:03] And serving them on a, on a platter and discovering them. And I feel like education is like
[02:19:08] the most intellectual, interesting thing because you have a tangle of understanding and you're
[02:19:12] trying to lay it out in a way that creates a ramp where everything only depends on the thing
[02:19:18] before it. And I find that this like, you know, untangling of knowledge is just so intellectually
[02:19:22] interesting as a cognitive task. Yeah. And so I love doing it personally, but I just find
[02:19:27] to have fascination with trying to lay things out in a certain way. Maybe that helps me.
[02:19:30] It also just makes a learning experience so much more motivated. Your tutorial on the transformer
[02:19:38] begins with bi-grams, literally like a lookup table from, here's the word right now, or here's
[02:19:44] the previous word, here's the next word. And it's literally just a lookup table. Yeah, that's the
[02:19:47] essence of it. Yeah. I mean, such a brilliant way. Like, okay, start with the lookup table and then
[02:19:52] go to a transformer and each piece is motivated. Why would you add that? Why would you add the next
[02:19:56] thing? You couldn't memorize the sort of attention formula, which is like having an understanding of why
[02:20:01] every single piece is relevant, what problem it solves. Yeah, you're presenting the pain before
[02:20:05] you present a solution. And how clever is that? And you want to take the student through that
[02:20:08] progression. So there's a lot of like other small things like that that I think make it
[02:20:13] make it nice and engaging and interesting. And you know, always prompting the student, there's
[02:20:17] there's a lot of small things like that that I think are, you know, important and a lot of good
[02:20:20] educators will do. Like, how would you solve this? Like, I'm not going to present a solution before
[02:20:25] you're going to guess. Yeah, that would be wasteful. That would be that's that's a little bit of a
[02:20:31] I don't want to swear, but like it's a it's a it's a dick move towards you to present you with
[02:20:34] the solution before I give you a shot to try to, um, right, to come up with it yourself. Yeah.
[02:20:39] And because because if you try to come with yourself, you get I guess you get a better understanding
[02:20:43] of like, what is the action space? Yeah. And then what is the sort of like objective? Then like,
[02:20:50] why does only this action fulfill that objective? Right? Yeah. Well, you have a chance to like try
[02:20:54] yourself and you've you've you've given appreciation when I give you the solution. And you
[02:20:58] maximizes the amount of knowledge per you fact added. That's right. Yeah. Yeah. Why do you think
[02:21:03] by default people who are genuine experts in their field are often bad at explaining it to
[02:21:12] somebody ramping up? Well, it's the curse of knowledge and expertise. Yeah. This is a real phenomenon
[02:21:17] and I actually suffered from it myself as much as I tried to not not suffer from it. But you take
[02:21:21] certain things for granted and you can't put yourself in issues of new of people who are just
[02:21:25] starting out. And this is pervasive and happens to me as well. One thing that I actually think
[02:21:29] is extremely helpful as an example, someone was trying to show me a paper in biology recently.
[02:21:34] And I just had instantly so many terrible questions. So what I did was I used chat GPT to ask
[02:21:39] the questions with the with the paper in context window. And then it worked through some of the
[02:21:44] simple things. And then I actually shared the thread to the person who shared it, who actually
[02:21:48] wrote that paper, worked on that work. And I almost feel like it was like if they can see the
[02:21:54] dumb questions I had, it might help them explain better in the future and stuff like that.
[02:21:58] Because so for example, for my material, I would love if people shared their dumb conversations
[02:22:04] with chat GPT about the stuff that I've created because it really helps me put myself again in the
[02:22:08] shoes of someone who's starting out. Another trick like that that I just works astoundingly well.
[02:22:15] If somebody writes a paper or a blog post or an announcement, it is in a hundred percent of
[02:22:22] cases true that just the narration or the transcription of how they would explain it to you
[02:22:29] over lunch is way more not only understandable, but actually also more accurate and scientific.
[02:22:39] In the sense that people have a bias to explain things in the most abstract,
[02:22:44] jargon-filled way possible and to clear their throat for four paragraphs before they explain
[02:22:48] the central idea. But there's something about communicating one on one with a person which
[02:22:54] compels you to just say the thing. Just say the thing. Yeah. Actually, I saw that tweet. I thought
[02:23:00] it was really good. I shared it with a bunch of people actually. I think it was really good.
[02:23:03] And I noticed this many, many times. Maybe the most prominent example is I remember back in my
[02:23:08] PhD days doing research, et cetera. You read someone's paper and you work to understand what
[02:23:14] is doing, et cetera. And then you catch them, you're having beers at the conference later,
[02:23:17] and you ask them, so this paper, what is the paper about? They will just tell you these three
[02:23:23] sentences that perfectly captured the essence of that paper and told you to give you the idea,
[02:23:26] and you didn't have to read the paper. When you're sitting at the table with a beer or something
[02:23:31] like that, the paper is just, oh, you take this idea, you take that idea and try this experiment,
[02:23:36] and you try this thing. And they have a way of just putting it conversationally.
[02:23:39] Right. And just like, perfectly, like, why isn't that the answer? Exactly.
[02:23:46] This is coming from the perspective of how somebody who's trying to explain an idea should
[02:23:49] formulate it better. What is your advice as a student to other students, where if you don't have a
[02:23:56] carpathy who is doing the exposition of an idea, if you're reading a paper from somebody or
[02:24:01] reading a book, what strategies do you employ to learn material you're interested in,
[02:24:07] in fields you're not an expert at? I don't actually know that I have
[02:24:12] like unique tips and tricks, to be honest. Basically, it's kind of a painful process,
[02:24:18] but you know, like, redraft one. I think like one thing that has always helped me quite a bit is
[02:24:26] I had a small tweet about this actually. So like, learning things on demand is pretty nice,
[02:24:30] learning depthwise. I do feel like you need a bit of alternation of learning depthwise on
[02:24:34] demand. You're trying to achieve a certain project that you're going to get a reward from.
[02:24:37] And learning breadthwise, which is just, oh, let's do whatever 101. And here's all the things you
[02:24:41] might need, which is a lot of school. There's a lot of breadthwise learning. Like, oh, trust me,
[02:24:45] you'll need this later, you know, that kind of a stuff. Like, okay, I trust you. I'll learn it
[02:24:49] because I guess I need it. But I love the kind of learning where you actually get a reward.
[02:24:53] I don't do something and you learn it on demand. The other thing that I've found is extremely
[02:24:57] helpful is maybe this is an aspect where education is a bit more selfless because
[02:25:03] explaining things to people is a beautiful way to learn something more deeply. This happens to me
[02:25:08] all the time. I think it probably happens to other people too because I realize if I don't really
[02:25:12] understand something, I can't explain it, you know, and I'm trying and I'm like, actually,
[02:25:17] actually, I don't understand this. And it's so annoying to come to terms with that. And then
[02:25:21] you can go back and make sure you understood it. And so it fills these gaps of your understanding.
[02:25:25] It forces you to come to terms with them and to reconcile them. I love to
[02:25:30] re-explain and things like that. And I think people should be doing that more as well. I think
[02:25:33] that forces you to manipulate the knowledge and make sure that you, you know, what you're talking
[02:25:36] about when you're explaining it. Oh, yeah. I think that's an excellent note to close on. Yeah.
[02:25:39] Andre, that was great. Yeah. Thank you. Thanks.
[02:25:42] Good time. Hey, everybody. I hope you enjoyed that episode. If you did, the most helpful thing you
[02:25:47] can do is just share it with other people who you think might enjoy it. It's also helpful if you
[02:25:52] leave a rating or a comment on whatever platform you're listening on. If you're interested
[02:25:57] in sponsoring the podcast, you're going to reach out at twerkesh.com slash advertise. Otherwise,
[02:26:05] I'll see you on the next one.