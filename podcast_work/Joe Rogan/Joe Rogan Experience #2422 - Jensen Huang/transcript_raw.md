# Joe Rogan Experience #2422 - Jensen Huang

**Podcast:** Joe Rogan
**Date:** 2025-12-03
**Video ID:** 3hptKYix4X8
**Video URL:** https://www.youtube.com/watch?v=3hptKYix4X8

---

[00:00:00] The Joe Rogan experience dream by nature.
[00:00:07] SPEAKER_00: Rogan podcast by night all day.
[00:00:09] SPEAKER_00: All right.
[00:00:11] Nice job.
[00:00:13] SPEAKER_02: Good to see you again.
[00:00:15] SPEAKER_02: We were just talking about was that the first time we ever spoke or did was the first
[00:00:18] SPEAKER_02: time we spoke at space.
[00:00:20] SPEAKER_02: Space ex space exer first time when you were giving Elon that crazy a i chip.
[00:00:24] SPEAKER_02: All right.
[00:00:25] SPEAKER_02: Did you export.
[00:00:26] SPEAKER_02: Yeah.
[00:00:27] SPEAKER_02: Ooh.
[00:00:28] SPEAKER_02: That was a big moment.
[00:00:29] SPEAKER_02: was like watching these wizards of tech,
[00:00:32] SPEAKER_02: like exchange information and you're giving him
[00:00:36] SPEAKER_02: this crazy device, you know?
[00:00:38] SPEAKER_02: And then the other time was,
[00:00:40] SPEAKER_02: I was shooting arrows in my backyard
[00:00:42] SPEAKER_02: and randomly get this call from Trump
[00:00:45] SPEAKER_02: and he's hanging out with you.
[00:00:46] SPEAKER_02: President Trump called and I called you.
[00:00:47] SPEAKER_02: Yeah, we were talking about you.
[00:00:49] SPEAKER_02: He's just talking about you.
[00:00:52] SPEAKER_01: He was talking about the UFC thing
[00:00:54] SPEAKER_01: he was gonna do in his front yard.
[00:00:55] SPEAKER_01: Yeah.
[00:00:56] SPEAKER_02: And he pulls out, he's,
[00:00:57] SPEAKER_01: Johnson, look at this design.
[00:00:59] He's so proud of it.
[00:01:01] And I go, you're gonna have a fight
[00:01:04] in the front lawn in the White House?
[00:01:05] SPEAKER_01: He goes, yeah, yeah, you're gonna come.
[00:01:07] SPEAKER_01: This is gonna be awesome.
[00:01:08] SPEAKER_01: And he's showing his design and how beautiful it is.
[00:01:11] SPEAKER_01: And he goes, and somehow your name comes up.
[00:01:15] SPEAKER_01: He goes, do you know Joe?
[00:01:17] SPEAKER_01: And I said, yeah, I'm gonna be on his podcast.
[00:01:20] He's, let's call him.
[00:01:21] SPEAKER_01: He's like a kid.
[00:01:24] SPEAKER_01: I know, let's call him.
[00:01:25] SPEAKER_01: It's so, he like,
[00:01:27] SPEAKER_02: Blind your old kid.
[00:01:28] SPEAKER_02: He's somewhere in trouble.
[00:01:30] Yeah, he's an odd guy.
[00:01:32] Just very different.
[00:01:34] SPEAKER_02: You know, like what you'd expect from him,
[00:01:37] SPEAKER_02: very different than what people think of him.
[00:01:39] SPEAKER_02: And also just very different as a president.
[00:01:41] SPEAKER_02: A guy who just calls your text you out of the blue.
[00:01:44] SPEAKER_02: Also, when you text you,
[00:01:45] SPEAKER_02: you have an android, so it won't go through with you.
[00:01:47] SPEAKER_02: But my iPhone, he makes the text go big.
[00:01:49] SPEAKER_02: You know what I write?
[00:01:51] SPEAKER_02: The USA is respected again.
[00:01:54] All caps and it makes the text in large.
[00:01:59] SPEAKER_02: It's kind of ridiculous.
[00:02:00] SPEAKER_02: Well, the one on one Trump,
[00:02:03] SPEAKER_01: President Trump is very different.
[00:02:05] SPEAKER_01: He surprised me.
[00:02:07] SPEAKER_01: First of all, he's an incredibly good listener.
[00:02:10] SPEAKER_01: Almost everything I've ever said to him,
[00:02:12] SPEAKER_01: he's remembered.
[00:02:13] Yeah, people don't,
[00:02:15] SPEAKER_02: they only want to look at negative stories about him
[00:02:18] SPEAKER_02: or negative narratives about him.
[00:02:20] SPEAKER_02: You know, you can catch anybody on a bad day.
[00:02:22] SPEAKER_02: Like, there's a lot of things he does where I don't think
[00:02:24] SPEAKER_02: he should do.
[00:02:25] SPEAKER_02: I don't think he should say to a reporter,
[00:02:27] SPEAKER_02: quiet piggy.
[00:02:28] SPEAKER_02: Like, that's pretty ridiculous.
[00:02:30] SPEAKER_02: Also, objectively funny.
[00:02:33] SPEAKER_02: I mean, it's unfortunate that it happened to her.
[00:02:34] SPEAKER_02: I wouldn't want that to happen to her,
[00:02:36] SPEAKER_02: but it was funny.
[00:02:38] SPEAKER_02: Just ridiculous that the president does that.
[00:02:39] SPEAKER_02: I wish he didn't do that.
[00:02:41] SPEAKER_02: But other than that, like, he's an interesting guy.
[00:02:43] SPEAKER_02: Like, he's a lot of different things wrapped up into one person,
[00:02:47] SPEAKER_02: you know?
[00:02:49] You know, part of his charm,
[00:02:51] SPEAKER_01: well, part of his genius is he says what's on his mind.
[00:02:54] Yes.
[00:02:55] SPEAKER_02: And that's just like an anti-politician.
[00:02:57] SPEAKER_02: Yeah, right.
[00:02:58] SPEAKER_02: So you know what's on his mind?
[00:03:00] SPEAKER_01: Is really what's on his mind.
[00:03:02] SPEAKER_01: Which I tell you what he prefers.
[00:03:04] SPEAKER_01: I do, I do that too.
[00:03:05] SPEAKER_01: I do that too.
[00:03:06] SPEAKER_01: Some people would rather be lied to.
[00:03:07] SPEAKER_02: Yeah, but I like the fact that he's telling you what's on his mind.
[00:03:11] SPEAKER_01: Almost every time he explains something,
[00:03:13] SPEAKER_01: he says something,
[00:03:15] SPEAKER_01: he starts with his,
[00:03:17] SPEAKER_01: you could tell his love for America,
[00:03:19] SPEAKER_01: what he wants to do for America.
[00:03:21] SPEAKER_01: And everything that he thinks through is very practical
[00:03:26] SPEAKER_01: and very common sense.
[00:03:27] SPEAKER_01: And, you know, it's very logical.
[00:03:30] SPEAKER_01: And I still remember the first time I met him.
[00:03:35] SPEAKER_01: And so this was, I never met him before.
[00:03:38] SPEAKER_01: And Secretary Lutton had called.
[00:03:42] SPEAKER_01: And we met right before,
[00:03:44] SPEAKER_01: right at the beginning of the administration,
[00:03:45] SPEAKER_01: and he said, he told me what wasn't important to present Trump.
[00:03:49] SPEAKER_01: That United States manufacturers on shore.
[00:03:56] SPEAKER_01: And that was really important to him because,
[00:03:58] SPEAKER_01: because it's important to national security,
[00:04:00] SPEAKER_01: he wants to make sure that the important critical technology
[00:04:03] SPEAKER_01: of our nation is built in the United States.
[00:04:06] SPEAKER_01: And that we re-industrialize and get good at manufacturing again
[00:04:10] SPEAKER_01: because it's important for jobs.
[00:04:11] SPEAKER_01: It just seems like common sense, right?
[00:04:13] SPEAKER_01: Incredible common sense.
[00:04:14] SPEAKER_01: And almost like literally the first conversation
[00:04:16] SPEAKER_01: I had with Secretary Luttonick.
[00:04:18] SPEAKER_01: And he was talking about how he started our conversation
[00:04:27] SPEAKER_01: with Jensen, this is Secretary Luttonick.
[00:04:31] SPEAKER_01: And I just want to let you know that you're a national treasure.
[00:04:36] SPEAKER_01: And Vity is a national treasure.
[00:04:38] SPEAKER_01: And whenever you need access to the president,
[00:04:43] SPEAKER_01: the administration, you call us,
[00:04:46] SPEAKER_01: we're always gonna be available to you.
[00:04:48] SPEAKER_01: Literally that was the first sentence.
[00:04:50] That's pretty nice.
[00:04:51] SPEAKER_01: And it was completely true.
[00:04:53] SPEAKER_01: Every single time I called, if I needed something,
[00:04:57] SPEAKER_01: I wanna get something off my chest, express some concern,
[00:05:01] SPEAKER_01: they're always available.
[00:05:02] SPEAKER_01: Incredible.
[00:05:03] SPEAKER_02: It's just unfortunate we live in such a politically polarized
[00:05:05] SPEAKER_02: society that you can't recognize good common sense things
[00:05:09] SPEAKER_02: if they're coming from a person that you object to.
[00:05:12] SPEAKER_02: And that I think is what's going on here.
[00:05:14] SPEAKER_02: I think most people generally, as a country,
[00:05:17] SPEAKER_02: as a giant community, which we are,
[00:05:20] SPEAKER_02: it just only makes sense that we have manufacturing in America
[00:05:25] SPEAKER_02: that especially critical technology,
[00:05:27] SPEAKER_02: like you're talking about,
[00:05:28] SPEAKER_02: like it's kind of insane that we buy so much technology
[00:05:31] SPEAKER_02: from other countries.
[00:05:33] If the United States doesn't grow,
[00:05:36] SPEAKER_01: we will have no prosperity.
[00:05:39] SPEAKER_01: We can invest in anything domestically or otherwise.
[00:05:42] SPEAKER_01: We can't fix any of our problems.
[00:05:45] SPEAKER_01: If we don't have energy growth,
[00:05:47] SPEAKER_01: we can't have industrial growth.
[00:05:49] SPEAKER_01: If we don't have industrial growth,
[00:05:51] SPEAKER_01: we can't have job growth.
[00:05:52] SPEAKER_01: These are as simple as that.
[00:05:54] SPEAKER_01: Right.
[00:05:55] SPEAKER_01: And the fact that he came into office,
[00:05:58] SPEAKER_01: and the first thing that he said was,
[00:05:59] SPEAKER_01: drill baby drill, his point is we need energy growth.
[00:06:02] SPEAKER_01: Without energy growth, we can have no industrial growth.
[00:06:06] SPEAKER_01: And that was, it saved, it saved the AI industry.
[00:06:09] SPEAKER_01: God, I gotta tell you flat out,
[00:06:11] SPEAKER_01: if not for his pro-growth energy policy,
[00:06:16] SPEAKER_01: we would not be able to build factories for AI,
[00:06:19] SPEAKER_01: we would not be able to build chip factories.
[00:06:21] SPEAKER_01: We won't surely, we won't be able to build
[00:06:23] SPEAKER_01: super computer factories.
[00:06:24] SPEAKER_01: None of that stuff would be possible.
[00:06:26] SPEAKER_01: And without all of that, construction jobs will be challenged,
[00:06:30] SPEAKER_01: right, electrical, you know, electrician jobs.
[00:06:33] SPEAKER_01: All of these jobs that are now flourishing would be challenged.
[00:06:36] SPEAKER_01: And so I think he's got a right.
[00:06:37] SPEAKER_01: We need energy growth.
[00:06:39] SPEAKER_01: We want to re-industrialize the United States.
[00:06:41] SPEAKER_01: We need to be back in manufacturing.
[00:06:43] SPEAKER_01: Every successful person doesn't need to have a PhD.
[00:06:46] SPEAKER_01: Every successful person doesn't have to have
[00:06:48] SPEAKER_01: gone to Stanford or MIT.
[00:06:50] SPEAKER_01: And I think that that, you know, that sensibility is spot on.
[00:06:56] Now, when we're talking about technology growth
[00:06:59] SPEAKER_02: and energy growth, there's a lot of people that go,
[00:07:01] SPEAKER_02: oh no, that's not what we need.
[00:07:02] SPEAKER_02: We need to, you know, simplify our lives and get back.
[00:07:05] SPEAKER_02: But the real issue is that we're in the middle
[00:07:07] SPEAKER_02: of a giant technology race.
[00:07:09] SPEAKER_02: And whether people are aware of it or not,
[00:07:11] SPEAKER_02: whether they like it or not, it's happening.
[00:07:13] SPEAKER_02: And it's a really important race.
[00:07:15] SPEAKER_02: Because whoever gets to whatever the event horizon
[00:07:21] SPEAKER_02: of artificial intelligences, whoever gets their first
[00:07:24] SPEAKER_02: has massive advantages in a huge way.
[00:07:28] You agree with that?
[00:07:29] SPEAKER_02: Well, first, the part, I will say that we are in a technology
[00:07:33] SPEAKER_01: race and we are always in a technology race.
[00:07:36] SPEAKER_01: We've been in a technology race with somebody forever.
[00:07:39] SPEAKER_01: Right, right, since the Industrial Revolution,
[00:07:41] SPEAKER_01: we've been in a technology race.
[00:07:42] SPEAKER_01: Since the Manhattan Project.
[00:07:42] SPEAKER_01: Yeah, or, you know, even going back to the discovery
[00:07:46] SPEAKER_01: of energy, right?
[00:07:47] SPEAKER_01: The United Kingdom was where the Industrial Revolution
[00:07:52] SPEAKER_01: was, if you will, invented when they realized
[00:07:54] SPEAKER_01: that they can turn steam and such into energy
[00:07:57] SPEAKER_01: and to electricity.
[00:08:00] All of that was invented largely in Europe.
[00:08:05] And United States capitalized on it.
[00:08:08] We were the ones that learned from it.
[00:08:10] SPEAKER_01: We industrialized it.
[00:08:12] SPEAKER_01: We defused it faster than anybody in Europe.
[00:08:15] SPEAKER_01: They were all stuck in discussions about policy
[00:08:21] SPEAKER_01: and jobs and disruptions.
[00:08:24] SPEAKER_01: Meanwhile, the United States was forming.
[00:08:26] SPEAKER_01: We just took the technology and ran with it.
[00:08:28] SPEAKER_01: And so I think we were always in a bit of a technology race.
[00:08:32] SPEAKER_01: What were two was a technology race.
[00:08:34] SPEAKER_01: Manhattan Project was a technology race.
[00:08:36] SPEAKER_01: We've been in a technology race ever since.
[00:08:38] SPEAKER_01: We've been in a COVID war.
[00:08:39] SPEAKER_01: I think we're still in a technology race.
[00:08:41] SPEAKER_01: It is probably the single most important race.
[00:08:44] SPEAKER_01: It is the technology as it gives you superpowers,
[00:08:50] SPEAKER_01: you know, whether it's information superpowers
[00:08:52] SPEAKER_01: or energy superpowers or military superpowers
[00:08:56] SPEAKER_01: is all founded in technology.
[00:08:57] SPEAKER_01: And so technology leadership is really important.
[00:09:00] SPEAKER_01: Well, the problem is if somebody else has superior technology,
[00:09:03] SPEAKER_02: right?
[00:09:04] SPEAKER_02: Yeah, that's the issue.
[00:09:05] SPEAKER_02: It seems like with the AI race,
[00:09:07] SPEAKER_02: people are very nervous about it.
[00:09:09] SPEAKER_02: Like, you know, Elon has famously said
[00:09:12] SPEAKER_02: that it's like 80% chance, it's awesome.
[00:09:15] SPEAKER_02: 20% chance, we're in trouble.
[00:09:17] SPEAKER_02: And people are worried about that 20%.
[00:09:20] SPEAKER_02: Rightly so.
[00:09:21] SPEAKER_02: You know, if you had 10 bullets in a revolver
[00:09:26] SPEAKER_02: and you know, you took out eight of them,
[00:09:30] SPEAKER_02: and you still have two in there and you spin it,
[00:09:32] SPEAKER_02: you're not gonna feel real comfortable
[00:09:33] SPEAKER_02: when you pull that trigger.
[00:09:34] SPEAKER_02: It's terrifying.
[00:09:34] SPEAKER_02: Right.
[00:09:35] SPEAKER_01: And when we're working towards this ultimate goal of AI,
[00:09:42] it's just impossible to imagine
[00:09:46] SPEAKER_02: that it wouldn't be of national security interest
[00:09:48] SPEAKER_02: to get there first.
[00:09:49] SPEAKER_02: We should, the question is what's there?
[00:09:51] SPEAKER_01: That's the part that is there.
[00:09:53] SPEAKER_01: Yeah, I'm not sure.
[00:09:54] SPEAKER_01: And that's the problem.
[00:09:55] SPEAKER_01: I don't think anybody is really knows.
[00:09:57] SPEAKER_01: That's crazy though.
[00:09:58] SPEAKER_02: Would I ask you, you're the head of Nvidia.
[00:10:01] SPEAKER_02: You don't know what's there?
[00:10:03] SPEAKER_01: Who knows?
[00:10:04] SPEAKER_01: Yeah, I think it's probably gonna be
[00:10:06] SPEAKER_01: much more gradual than we think.
[00:10:09] SPEAKER_01: It won't be a moment.
[00:10:11] SPEAKER_01: It won't be as if somebody arrived and nobody else has.
[00:10:17] SPEAKER_01: I don't think it's gonna be like that.
[00:10:18] SPEAKER_01: I think it's gonna be things that just get better
[00:10:20] SPEAKER_01: and better and better, just like technology does.
[00:10:23] SPEAKER_01: So you are rosy about the future.
[00:10:25] SPEAKER_02: You're very optimistic about what's gonna happen with AI.
[00:10:29] SPEAKER_02: Obviously, will you make the best AI chips in the world?
[00:10:32] SPEAKER_02: It'll probably better be.
[00:10:34] SPEAKER_01: If history is a guide, we were always concerned
[00:10:38] SPEAKER_01: about new technology.
[00:10:41] Humanity has always been concerned about new technology.
[00:10:43] SPEAKER_01: They're always somebody who's thinking,
[00:10:45] SPEAKER_01: they're always a lot of people who are quite concerned.
[00:10:47] SPEAKER_01: We're quite concerned.
[00:10:50] SPEAKER_01: And so if history is a guide, it is the case
[00:10:56] SPEAKER_01: that all of this concern is channeled
[00:10:59] SPEAKER_01: into making the technology safer.
[00:11:02] SPEAKER_01: And so for example, in the last several years,
[00:11:06] SPEAKER_01: I would say AI technology has increased
[00:11:09] SPEAKER_01: probably in the last two years alone, maybe a hundred X.
[00:11:14] SPEAKER_01: Let's just give it a number.
[00:11:16] Okay, it's like a car two years ago was a hundred times slower.
[00:11:21] So AI is a hundred times more capable today.
[00:11:25] SPEAKER_01: Now how did we channel that technology?
[00:11:27] SPEAKER_01: How do we channel all of that power?
[00:11:29] SPEAKER_01: We directed it to causing the AI to be able to think,
[00:11:35] SPEAKER_01: meaning that it can take a problem
[00:11:37] SPEAKER_01: that we give it, break it down step by step.
[00:11:41] It does research before it answers.
[00:11:44] SPEAKER_01: And so it grounds it on truth.
[00:11:47] SPEAKER_01: It'll reflect on that answer, ask itself,
[00:11:50] SPEAKER_01: is this the best answer that I can give you?
[00:11:54] SPEAKER_01: Am I certain about this answer?
[00:11:55] SPEAKER_01: If it's not certain about the answer
[00:11:57] SPEAKER_01: or highly confident about the answer,
[00:11:59] SPEAKER_01: you'll go back and do more research.
[00:12:01] It might actually even use a tool
[00:12:03] SPEAKER_01: because that tool provides a better solution
[00:12:04] SPEAKER_01: than it could hallucinate itself.
[00:12:07] SPEAKER_01: As a result, we took all of that computing capability
[00:12:11] SPEAKER_01: and we channeled it into having it produce a safer result,
[00:12:15] SPEAKER_01: safer answer, a more truthful answer.
[00:12:18] SPEAKER_01: Because as you know,
[00:12:19] SPEAKER_01: one of the greatest criticisms of AI in the beginning
[00:12:21] SPEAKER_01: was that hallucinated.
[00:12:22] SPEAKER_01: Right.
[00:12:23] SPEAKER_02: And so if you look at the reason why people use AI so much today,
[00:12:27] it's because the amount of hallucination has reduced.
[00:12:30] I use it almost,
[00:12:31] SPEAKER_01: well, I use the whole trip over here.
[00:12:34] SPEAKER_01: And so I think the capability,
[00:12:39] SPEAKER_01: most people think about power
[00:12:42] and they think about,
[00:12:44] maybe it's an explosion power,
[00:12:46] SPEAKER_01: but the technology power,
[00:12:48] SPEAKER_01: most of it is channeled towards safety.
[00:12:50] SPEAKER_01: A car today is more powerful,
[00:12:52] SPEAKER_01: but it's safer to drive.
[00:12:54] A lot of that power goes towards better handling.
[00:12:57] SPEAKER_01: You know, I'd rather have a, well,
[00:13:01] SPEAKER_01: you have a thousand horsepower truck.
[00:13:03] SPEAKER_01: I think 500 horsepower is pretty good.
[00:13:05] SPEAKER_01: Now, I thousand better.
[00:13:06] SPEAKER_01: I think a thousand is better.
[00:13:07] SPEAKER_01: I don't know about the better,
[00:13:08] SPEAKER_02: but it's definitely faster.
[00:13:10] SPEAKER_02: Yeah. No, I think it's better.
[00:13:11] SPEAKER_01: And you get out of trouble faster.
[00:13:17] I enjoyed my 5.99 more than my 6.12.
[00:13:21] It was, I think it was a better, better,
[00:13:22] SPEAKER_01: and more horsepower is better.
[00:13:24] My 459 is better than my 430.
[00:13:27] More horsepower is better.
[00:13:28] SPEAKER_01: I think more horsepower is better.
[00:13:30] SPEAKER_01: I think it's better handling, it's better control.
[00:13:32] SPEAKER_01: In the case of technology,
[00:13:34] SPEAKER_01: it's also very similar in that way.
[00:13:36] SPEAKER_01: You know, and so if you look at what we're gonna do
[00:13:38] SPEAKER_01: with the next thousand times of performance in AI,
[00:13:42] SPEAKER_01: a lot of it is going to be channeled towards
[00:13:45] SPEAKER_01: more reflection, more research,
[00:13:49] thinking about the answer more deeply.
[00:13:51] SPEAKER_01: So when you're defining safety,
[00:13:53] SPEAKER_02: you're defining it as accuracy.
[00:13:55] SPEAKER_02: Functionality.
[00:13:56] SPEAKER_01: Functionality.
[00:13:57] SPEAKER_02: Yeah.
[00:13:58] SPEAKER_01: It does what you expect it to do.
[00:14:01] SPEAKER_01: And then you take the technology and horsepower,
[00:14:04] SPEAKER_01: you put guard rails on it, just like our cars.
[00:14:07] SPEAKER_01: We've got a lot of technology in a car today.
[00:14:10] SPEAKER_01: A lot of it goes towards, for example, ABS.
[00:14:13] SPEAKER_01: ABS is great.
[00:14:15] And so traction control.
[00:14:17] SPEAKER_01: That's fantastic.
[00:14:18] SPEAKER_01: Without a computer in the car,
[00:14:20] SPEAKER_01: how would you do any of that?
[00:14:22] SPEAKER_01: And that little computer,
[00:14:23] SPEAKER_01: the computers that you have doing your traction control,
[00:14:26] SPEAKER_01: is more powerful than a computer that went to Apollo 11.
[00:14:29] And so you want that technology,
[00:14:32] channeled towards safety, channeled towards functionality.
[00:14:35] SPEAKER_01: And so when people talk about power,
[00:14:37] SPEAKER_01: the advancement of technology, oftentimes,
[00:14:40] SPEAKER_01: I feel what they're thinking
[00:14:43] and what we're actually doing is very different.
[00:14:45] SPEAKER_01: What do you think they're thinking?
[00:14:47] Well, they're thinking somehow that this AI is being powerful.
[00:14:53] SPEAKER_01: And their mind probably goes towards a sci-fi movie,
[00:14:57] SPEAKER_01: the definition of power.
[00:15:00] SPEAKER_01: Oftentimes, the definition of power is military power
[00:15:04] SPEAKER_01: or physical power.
[00:15:06] SPEAKER_01: But in the case of technology power,
[00:15:09] SPEAKER_01: when we translate all of those operations,
[00:15:11] SPEAKER_01: it's towards more refined thinking, more reflection,
[00:15:15] SPEAKER_01: more planning, more options.
[00:15:18] SPEAKER_01: I think the big fears that people have
[00:15:19] SPEAKER_02: is one, a big fear is military applications.
[00:15:23] SPEAKER_02: That's a big fear.
[00:15:24] SPEAKER_02: Because people are very concerned
[00:15:25] SPEAKER_02: that you're going to have AI systems that make decisions
[00:15:29] SPEAKER_02: that maybe an ethical person wouldn't make
[00:15:31] SPEAKER_02: or a moral person wouldn't make
[00:15:32] SPEAKER_02: based on achieving an objective versus based on,
[00:15:36] SPEAKER_02: you know, how it's going to look to people.
[00:15:41] Well, I'm happy that our military is going to use
[00:15:46] SPEAKER_01: AI technology for defense.
[00:15:48] SPEAKER_01: And I think that that
[00:15:51] SPEAKER_01: andro building military technology,
[00:15:53] SPEAKER_01: I'm happy to hear that.
[00:15:54] SPEAKER_01: I'm happy to see all these tech startups now
[00:15:57] SPEAKER_01: channeling their technology capabilities
[00:15:59] SPEAKER_01: towards defense and military applications.
[00:16:02] SPEAKER_01: I think you need to do that.
[00:16:03] SPEAKER_01: Yeah, we had Palmer lucky on the podcast
[00:16:05] SPEAKER_02: and he was demonstrating some of this stuff
[00:16:06] SPEAKER_02: from the press. Yeah, incredible.
[00:16:07] SPEAKER_02: Helmet on.
[00:16:08] SPEAKER_02: And we showed some videos
[00:16:09] SPEAKER_02: how you could see behind walls and stuff.
[00:16:11] SPEAKER_02: Like it's nuts.
[00:16:12] SPEAKER_02: He's actually the perfect guy
[00:16:13] SPEAKER_02: to go start that company.
[00:16:14] SPEAKER_02: 100%. Yeah, 100%.
[00:16:17] SPEAKER_02: It's like he's born for that.
[00:16:19] Yeah, he came here with a copper jacket on.
[00:16:21] SPEAKER_02: He's a freak.
[00:16:22] SPEAKER_02: It's awesome.
[00:16:23] SPEAKER_02: He's awesome.
[00:16:24] SPEAKER_02: But it's also it's a, you know,
[00:16:26] SPEAKER_02: an unusual intellect channeled into that
[00:16:29] SPEAKER_02: very bizarre field is what you need.
[00:16:31] SPEAKER_02: You know, and I think it's, it's a,
[00:16:34] SPEAKER_01: I think I'm happy that we're making a
[00:16:35] SPEAKER_01: more socially acceptable.
[00:16:38] You know, there was a time where
[00:16:40] SPEAKER_01: when somebody wanted to channel their technology
[00:16:42] SPEAKER_01: capability and their intellect into
[00:16:45] SPEAKER_01: defense technology, somehow they're vilified.
[00:16:48] SPEAKER_01: But we need people like that.
[00:16:51] SPEAKER_01: We need people who enjoyed, enjoyed that
[00:16:53] SPEAKER_01: part of application of technology.
[00:16:55] SPEAKER_01: Well, people are terrified of war.
[00:16:57] SPEAKER_02: Yeah, so it makes sense.
[00:16:59] SPEAKER_02: The best way to avoid it has excessive military might.
[00:17:03] Do you think that's absolutely the best way?
[00:17:05] SPEAKER_02: Not diplomacy, not working stuff out.
[00:17:08] SPEAKER_02: All of it. All of it.
[00:17:09] SPEAKER_02: Yeah, you have to have military might
[00:17:11] SPEAKER_02: and you have to get people to sit down there.
[00:17:12] SPEAKER_02: Right. Exactly.
[00:17:13] SPEAKER_02: All of it.
[00:17:14] SPEAKER_02: Otherwise, it just invade.
[00:17:15] SPEAKER_02: That's right.
[00:17:16] SPEAKER_02: Why ask for permission?
[00:17:17] Again, like you said, in history,
[00:17:19] SPEAKER_02: go back and look at history.
[00:17:20] SPEAKER_02: That's right.
[00:17:21] SPEAKER_02: When you look at the future of AI and you just said
[00:17:25] SPEAKER_02: that no one really knows what's happening.
[00:17:27] SPEAKER_02: Right.
[00:17:28] SPEAKER_02: Do you ever sit down and ponder scenarios?
[00:17:31] SPEAKER_02: Like, what do you, what do you think is like
[00:17:33] SPEAKER_02: best case scenario for AI over the next two decades?
[00:17:38] SPEAKER_02: The best case scenario is that AI diffuses into everything
[00:17:48] SPEAKER_01: that we do and our everything is more efficient.
[00:17:56] SPEAKER_01: But the threat of war remains a threat of war.
[00:18:01] SPEAKER_01: Cyber security remains a super-different way.
[00:18:06] A super-difficult challenge.
[00:18:09] SPEAKER_01: Somebody is going to try to breach your security.
[00:18:14] SPEAKER_01: You're going to have thousands of millions of AI agents
[00:18:18] SPEAKER_01: protecting you from that threat.
[00:18:22] SPEAKER_01: Your technology is going to get better.
[00:18:24] SPEAKER_01: Their technology is going to get better just like cyber security.
[00:18:27] SPEAKER_01: Right now, while we speak, we're being.
[00:18:31] We're seeing cyber attacks all over the planet
[00:18:34] SPEAKER_01: on just about every front door you can imagine.
[00:18:37] SPEAKER_01: And yet, you and I are sitting here talking.
[00:18:43] SPEAKER_01: And so, the reason for that is because we know that there's
[00:18:47] SPEAKER_01: a whole bunch of cyber security technology in defense.
[00:18:50] SPEAKER_01: And so, we just have to keep amping that up,
[00:18:52] SPEAKER_01: keep stepping that up.
[00:18:54] SPEAKER_01: This episode is brought to you by Visible.
[00:18:56] SPEAKER_02: When your phone plans as good as Visible,
[00:18:59] SPEAKER_02: you've got to tell your people.
[00:19:01] SPEAKER_02: It's the ultimate wireless hack to save money
[00:19:03] SPEAKER_02: and still get great coverage and a reliable connection.
[00:19:06] SPEAKER_02: Get one-line wireless with unlimited data
[00:19:09] SPEAKER_02: and hotspot for $25 a month.
[00:19:12] SPEAKER_02: Taxes and fees included all on Verizon's 5G network.
[00:19:17] SPEAKER_02: Plus, now for a limited time, new members can get the visible plan
[00:19:22] SPEAKER_02: for just $19 a month for the first 26 months.
[00:19:26] SPEAKER_02: Use promo code SWITCH26
[00:19:29] SPEAKER_02: and save beyond the season.
[00:19:31] SPEAKER_02: It's a deal so good, you're going to want to tell your people.
[00:19:34] SPEAKER_02: Switch now at visible.com slash rogan.
[00:19:38] SPEAKER_02: Terms apply, limited time offers subject to change,
[00:19:41] SPEAKER_02: see visible.com for plan features and network management details.
[00:19:46] SPEAKER_02: That's a big issue with people.
[00:19:48] SPEAKER_02: The worry that technology is going to get to a point
[00:19:51] SPEAKER_02: where encryption is going to be obsolete.
[00:19:53] SPEAKER_02: Encryption is just, it's no longer going to protect data.
[00:19:56] SPEAKER_02: It's no longer going to protect systems.
[00:19:58] SPEAKER_02: Do you anticipate that ever being an issue?
[00:20:00] SPEAKER_02: Or do you think it's as the defense grows, the threat grows,
[00:20:04] SPEAKER_02: the defense grows, and it just keeps going on and on and on
[00:20:07] SPEAKER_02: and they'll always be able to fight off any sort of intrusions?
[00:20:13] Not forever.
[00:20:15] Some intrusions will get in and they will all learn from it.
[00:20:19] SPEAKER_01: And you know the reason why cybersecurity works is because, of course,
[00:20:23] SPEAKER_01: the technology of defense is advancing very quickly.
[00:20:27] SPEAKER_01: The technology of defense is advancing very quickly.
[00:20:30] SPEAKER_01: However, the benefit of the cybersecurity defense is that,
[00:20:36] SPEAKER_01: socially, the community, all of our companies work together as one.
[00:20:42] SPEAKER_01: Most people don't realize this.
[00:20:45] There's a whole community of cybersecurity experts.
[00:20:49] SPEAKER_01: We exchange ideas, we exchange best practices,
[00:20:54] SPEAKER_01: we exchange what we detect.
[00:20:57] SPEAKER_01: The moment something has been breached or maybe there's a loophole or whatever it is,
[00:21:02] SPEAKER_01: it is shared by everybody, the patches are shared with everybody.
[00:21:05] SPEAKER_01: That's interesting.
[00:21:06] SPEAKER_01: Yeah, most people don't realize this.
[00:21:08] SPEAKER_01: No, no, no idea.
[00:21:09] SPEAKER_01: I've assumed that it would just be competitive like everything else.
[00:21:12] SPEAKER_02: No, no, no, we work together all of a sudden.
[00:21:14] SPEAKER_02: Is that always been the case?
[00:21:16] SPEAKER_01: It surely has been the case for about 15 years.
[00:21:20] SPEAKER_01: It might not have been the case long ago.
[00:21:22] SPEAKER_01: But this...
[00:21:23] SPEAKER_01: What do you think started off that cooperation?
[00:21:27] SPEAKER_01: People recognizing it's a challenge and no company can stand alone.
[00:21:31] SPEAKER_01: And the same thing is going to happen with AI.
[00:21:34] SPEAKER_01: I think we all have to decide working together to stay out of harm's way
[00:21:40] SPEAKER_01: is our best chance for defense.
[00:21:42] SPEAKER_01: Then it's basically everybody against the threat.
[00:21:45] SPEAKER_01: And it also seems like you'd be way better at detecting
[00:21:49] SPEAKER_02: where these threats are coming from and neutralizing them.
[00:21:51] SPEAKER_02: Exactly.
[00:21:52] SPEAKER_02: Because the moment you detect that somewhere,
[00:21:54] SPEAKER_01: you're going to find out right away.
[00:21:55] SPEAKER_01: It'll be really hard to hide.
[00:21:57] SPEAKER_02: That's right.
[00:21:58] SPEAKER_01: Yeah.
[00:21:59] SPEAKER_01: That's how it works.
[00:22:00] SPEAKER_01: That's the reason why it's safe.
[00:22:01] SPEAKER_01: That's why I'm sitting here right now instead of locking everything down in video.
[00:22:04] SPEAKER_01: It's not only am I watching my own back.
[00:22:09] SPEAKER_01: I've got everybody watching my back.
[00:22:11] SPEAKER_01: And I'm watching everybody else's back.
[00:22:13] SPEAKER_01: It's a bizarre world, isn't it?
[00:22:14] SPEAKER_01: Let me think about that.
[00:22:15] SPEAKER_01: Cyber threats.
[00:22:16] SPEAKER_02: And this idea about cybersecurity is unknown to the people who are talking about AI.
[00:22:20] SPEAKER_01: They're talking about AI threats.
[00:22:23] SPEAKER_01: I think when they think about AI threats and AI cyber security threats,
[00:22:26] SPEAKER_01: they have to also think about how we deal with it today.
[00:22:29] SPEAKER_01: Now, there's no question that AI is a new technology.
[00:22:34] SPEAKER_01: And it's a new type of software in the end of software.
[00:22:37] SPEAKER_01: It just is a new type of software.
[00:22:39] SPEAKER_01: And so it's going to have new capabilities.
[00:22:41] SPEAKER_01: But so would the defense.
[00:22:43] SPEAKER_01: You know, well, you used the same AI technology to go defend against it.
[00:22:47] SPEAKER_01: So do you anticipate a time ever in the future where it's going to be impossible?
[00:22:53] SPEAKER_02: Where there's not going to be any secrets?
[00:22:56] SPEAKER_02: Where the bottleneck between the technology that we have and the information that we have,
[00:23:01] SPEAKER_02: information is just all a bunch of ones and zeros.
[00:23:03] SPEAKER_02: It's out there on hard drives and the technology has more and more access to that information.
[00:23:07] SPEAKER_02: Is it ever going to get to a point in time where there's no way to keep a secret?
[00:23:12] SPEAKER_02: I don't know.
[00:23:14] It seems like that's where everything is kind of headed.
[00:23:16] SPEAKER_02: I don't think so.
[00:23:17] SPEAKER_02: I think the quantum computers we're supposed to,
[00:23:19] SPEAKER_01: right?
[00:23:20] SPEAKER_01: Yeah, quantum computers will make it possible.
[00:23:22] SPEAKER_01: We'll make it so that the previous quantum, previous encryption technology is obsolete.
[00:23:28] SPEAKER_01: But that's the reason why the entire industry is working on post-quantum encryption technology.
[00:23:36] SPEAKER_01: What would that look like?
[00:23:38] SPEAKER_01: New algorithms.
[00:23:40] SPEAKER_01: The crazy thing is when you hear about the kind of computation that quantum computing can do,
[00:23:45] SPEAKER_02: and the power that it has, where you're looking at all the supercomputers in the world,
[00:23:50] SPEAKER_02: that would take billions of years, and it takes them a few minutes to solve these equations.
[00:23:53] SPEAKER_02: How do you make encryption for something that can do that?
[00:23:57] SPEAKER_02: I'm not sure.
[00:23:58] SPEAKER_02: But I've got a bunch of scientists who are working on that.
[00:24:01] SPEAKER_01: But I think it figured out.
[00:24:03] SPEAKER_01: Yeah, we've got a bunch of scientists who are exporting that.
[00:24:06] SPEAKER_01: And the ultimate fear that it can't be breached, that quantum computing will always be able to decrypt all other quantum computing encryption?
[00:24:14] SPEAKER_02: I don't know.
[00:24:15] SPEAKER_02: It just gets to some point where it's like stop playing the stupid game.
[00:24:19] SPEAKER_02: We know everything.
[00:24:20] SPEAKER_02: I don't think so.
[00:24:21] SPEAKER_02: No, because I'm, you know, history is guide.
[00:24:25] SPEAKER_02: History is guide before AI came around.
[00:24:27] SPEAKER_02: That's my worry.
[00:24:28] SPEAKER_02: My worry is this is a total, you know, it's like history was one thing,
[00:24:32] SPEAKER_02: and then nuclear weapons kind of changed all of our thoughts on war and mutually sure destruction
[00:24:37] SPEAKER_02: came, got everybody to stop using nuclear bombs.
[00:24:42] SPEAKER_02: Yeah, my worry is that the thing is Joe is that that AI is not going to,
[00:24:46] SPEAKER_01: it's not like we're cavemen, and then all of a sudden one day AI shows up.
[00:24:51] SPEAKER_01: Every single day we're getting better and smarter because we have AI.
[00:24:56] SPEAKER_01: And so we're stepping on our own AI's shoulders.
[00:24:59] SPEAKER_01: So when that, whatever that AI threat comes, it's a click ahead.
[00:25:05] SPEAKER_01: It's not a galaxy ahead.
[00:25:07] SPEAKER_01: You know, it's just a click ahead.
[00:25:09] SPEAKER_01: And so, so I think, I think the idea that somehow this AI is going to pop out of nowhere
[00:25:19] SPEAKER_01: and somehow think in a way that we can't even imagine thinking
[00:25:24] SPEAKER_01: and do something that we can't possibly imagine, I think is far fetched.
[00:25:28] SPEAKER_01: And the reason for that is because we're all have, we all have AI's.
[00:25:32] SPEAKER_01: And you know, there's a whole bunch of AI's being in development.
[00:25:35] SPEAKER_01: We know what they are and we're using it.
[00:25:37] SPEAKER_01: And so every single day we're getting close to each other.
[00:25:41] SPEAKER_01: But don't they do things that are very surprising?
[00:25:45] Yeah, but so you have an AI that does something surprising.
[00:25:48] SPEAKER_01: I'm going to have an AI.
[00:25:49] SPEAKER_01: Right.
[00:25:50] SPEAKER_01: And my AI looks at your AI and goes, that's not that surprising.
[00:25:52] SPEAKER_01: The fear for the lay person like myself is that AI becomes sentient
[00:25:56] SPEAKER_02: and makes its own decisions.
[00:25:58] SPEAKER_02: And then ultimately decides to just govern the world, do it its own way.
[00:26:05] They're like, you guys, you had a good run, but we're taking over now.
[00:26:09] SPEAKER_02: Yeah, but my AI is going to take care of me.
[00:26:14] SPEAKER_01: So, that's the, this is the cybersecurity argument.
[00:26:18] SPEAKER_01: Yes.
[00:26:19] SPEAKER_01: Well, do you have an AI and it's super smart.
[00:26:22] SPEAKER_01: But my AI is super smart too.
[00:26:24] SPEAKER_01: And maybe your AI, let's pretend, let's pretend for a second that we understand what consciousness is
[00:26:31] SPEAKER_01: and we understand what sentience is.
[00:26:33] SPEAKER_01: And that in fact, we really are just pretending.
[00:26:35] SPEAKER_02: Okay, let's just pretend for a second that we believe that.
[00:26:38] SPEAKER_01: I don't believe actually, I don't actually don't believe that.
[00:26:40] SPEAKER_01: But nonetheless, let's pretend we believe that.
[00:26:42] SPEAKER_01: So your AI is conscious and my AI is conscious.
[00:26:45] SPEAKER_01: And let's say your AI is, you know, wants to, I don't know, do something surprising.
[00:26:51] SPEAKER_01: My AI is so smart that it won't, it might be surprising to me, but it probably won't be surprising to my AI.
[00:26:58] SPEAKER_01: And so maybe my AI thinks is surprising as well.
[00:27:03] SPEAKER_01: But it's so smart.
[00:27:05] SPEAKER_01: The moment it sees it the first time, it's not going to be surprised the second time.
[00:27:08] SPEAKER_01: It's just like us.
[00:27:09] SPEAKER_01: And so I feel like I think the idea that that only one person has AI and that one person's AI is
[00:27:19] SPEAKER_01: compares everybody else's AI is Neanderthal is probably unlikely.
[00:27:25] SPEAKER_01: I think it's much more like cybersecurity.
[00:27:28] SPEAKER_01: Interesting.
[00:27:30] SPEAKER_02: I think the fear is not that your AI is going to battle with somebody else's AI.
[00:27:35] SPEAKER_02: The fear is that AI is no longer going to listen to you.
[00:27:39] SPEAKER_02: That's the fear is that human beings won't have control over it after a certain point.
[00:27:43] SPEAKER_02: If it achieves sentience and then has the ability to be autonomous.
[00:27:48] SPEAKER_02: That there's one AI.
[00:27:50] SPEAKER_02: Well, they just combine.
[00:27:52] SPEAKER_02: Yeah, it becomes one AI.
[00:27:54] SPEAKER_02: That it's a life form.
[00:27:55] SPEAKER_02: But there's arguments about that, right?
[00:27:57] SPEAKER_02: That we're dealing with some sort of synthetic biology.
[00:27:59] SPEAKER_02: That it's not as simple as new technology that you're creating a life form.
[00:28:03] SPEAKER_02: If it's like life form, let's go along with that for a while.
[00:28:08] SPEAKER_01: I think if it's like life form, as you know, all life forms don't agree.
[00:28:12] SPEAKER_01: And so I'm going to have to go with your life form and my life form, I'm going to agree.
[00:28:17] SPEAKER_01: Because my life form is going to want to be the super life form.
[00:28:20] SPEAKER_01: And now that we have disagreeing life forms, we're back again to where we are.
[00:28:26] SPEAKER_01: Well, they would probably cooperate with each other.
[00:28:29] SPEAKER_02: The reason why we don't cooperate with each other is we're territorial primates.
[00:28:36] SPEAKER_02: But AI wouldn't be a territorial primate.
[00:28:39] SPEAKER_02: It would realize the folly in that sort of thinking.
[00:28:42] SPEAKER_02: And they would say, listen, there's plenty of energy for everybody.
[00:28:46] SPEAKER_02: We don't need to dominate.
[00:28:49] SPEAKER_02: We're not trying to acquire resources and take over the world.
[00:28:52] SPEAKER_02: We're not looking to find a good breeding partner.
[00:28:55] SPEAKER_02: We're just existing as a new super life form that these cute monkeys created for us.
[00:29:03] SPEAKER_02: Okay. Well, that would be a super power with no ego.
[00:29:11] SPEAKER_01: Right. And if it has no ego, why would have to ego to do any harm to us?
[00:29:19] SPEAKER_01: Well, I don't assume that it would do harm to us.
[00:29:22] SPEAKER_02: But the fear would be that we would no longer have control.
[00:29:26] SPEAKER_02: And that we would no longer be the apex species on the planet.
[00:29:30] SPEAKER_02: This thing that we created would now be.
[00:29:33] SPEAKER_02: Is that funny?
[00:29:35] No. I just care.
[00:29:37] SPEAKER_02: It's not going to happen.
[00:29:38] SPEAKER_02: I know you think it's like that.
[00:29:40] SPEAKER_02: But it could, right?
[00:29:41] SPEAKER_02: And here's the other thing is like if we're racing towards could,
[00:29:45] SPEAKER_02: and could could be the end of human beings being control of our own destiny.
[00:29:52] SPEAKER_02: I just think it's extremely unlikely.
[00:29:55] SPEAKER_02: That's what they said in the Terminator movie.
[00:29:58] SPEAKER_02: And it hasn't happened.
[00:29:59] SPEAKER_02: No, not yet. But you guys are working towards it.
[00:30:02] SPEAKER_02: The thing about, you're saying about conscience and sentience,
[00:30:05] SPEAKER_02: that you don't think that AI will achieve consciousness?
[00:30:09] SPEAKER_02: Or the consciousness is the definition.
[00:30:11] SPEAKER_02: What's the definition?
[00:30:12] SPEAKER_02: What is the definition to you?
[00:30:15] SPEAKER_01: Consciousness.
[00:30:19] SPEAKER_01: I guess first of all, you need to know about your own existence.
[00:30:35] You have to have experience, not just knowledge and intelligence.
[00:30:46] The concept of a machine having an experience,
[00:30:51] SPEAKER_01: I'm not, well, first of all, I don't know what defines experience,
[00:30:54] SPEAKER_01: why we have experiences.
[00:30:56] SPEAKER_01: Right.
[00:30:57] SPEAKER_01: And why this microphone doesn't.
[00:31:00] SPEAKER_01: And so I think I know,
[00:31:03] SPEAKER_01: well, I think I know what consciousness is.
[00:31:09] SPEAKER_01: The sense of experience, the ability to know self versus,
[00:31:14] SPEAKER_01: versus the ability to be able to reflect,
[00:31:20] SPEAKER_01: know our own self, the sense of ego.
[00:31:24] SPEAKER_01: I think all of those human experiences probably is what consciousness is.
[00:31:32] SPEAKER_01: But why it exists versus the concept of knowledge and intelligence,
[00:31:41] SPEAKER_01: which is what AI is defined by today.
[00:31:44] SPEAKER_01: It has knowledge, it has intelligence, artificial intelligence.
[00:31:48] SPEAKER_01: We don't call it artificial consciousness.
[00:31:50] SPEAKER_01: Artificial intelligence, the ability to perceive, recognize, understand,
[00:32:01] SPEAKER_01: plan, perform tasks.
[00:32:06] SPEAKER_01: Those things are foundations of intelligence to know things, knowledge.
[00:32:14] SPEAKER_01: I don't, it's clearly different than consciousness.
[00:32:17] SPEAKER_01: But consciousness is so loosely defined.
[00:32:19] SPEAKER_02: How can we say that?
[00:32:20] SPEAKER_02: I mean, doesn't a dog have consciousness?
[00:32:22] SPEAKER_02: Yeah.
[00:32:23] SPEAKER_02: Dogs seem to be pretty conscious.
[00:32:24] SPEAKER_02: That's right.
[00:32:25] SPEAKER_02: Yeah.
[00:32:26] SPEAKER_01: So, and that's a lower level consciousness than a human being's consciousness.
[00:32:29] SPEAKER_02: I'm not sure.
[00:32:30] Yeah.
[00:32:31] SPEAKER_01: Right.
[00:32:32] SPEAKER_01: Well, the question is what, a lower level intelligence,
[00:32:35] SPEAKER_01: it's lower level intelligence, but I don't know that's a lower level consciousness.
[00:32:38] SPEAKER_01: That's a good point.
[00:32:39] SPEAKER_01: Yeah.
[00:32:40] SPEAKER_01: Because I believe my dogs feel as much as I feel.
[00:32:42] SPEAKER_01: Yeah, they feel a lot.
[00:32:43] SPEAKER_02: Yeah.
[00:32:44] SPEAKER_02: Right.
[00:32:45] Yeah, they get attached to you.
[00:32:46] SPEAKER_02: That's right.
[00:32:47] SPEAKER_02: They get depressed if you're not there.
[00:32:48] SPEAKER_02: That's right.
[00:32:49] SPEAKER_02: Exactly.
[00:32:50] SPEAKER_02: There's definitely that.
[00:32:51] SPEAKER_02: Yeah.
[00:32:52] SPEAKER_02: The concept of experience,
[00:32:55] SPEAKER_02: right?
[00:32:56] SPEAKER_02: But isn't AI interacting with society?
[00:32:59] SPEAKER_02: So, doesn't it acquire experience through that interaction?
[00:33:03] SPEAKER_02: I don't think interactions is experience.
[00:33:06] SPEAKER_01: I think experience is experience is a collection of feelings, I think.
[00:33:14] You're aware of that AI, I forget which one, where they gave it some false information about one of the programmers having an affair with his wife,
[00:33:23] SPEAKER_02: just to see how it would respond to it.
[00:33:25] SPEAKER_02: And then when they said they were going to shut it down, it threatened a black male and reveal his affair.
[00:33:29] SPEAKER_02: And it was like, whoa, like it's conniving.
[00:33:32] SPEAKER_02: Like if that's not learning from experience and being aware that you're about to be shut down,
[00:33:38] SPEAKER_02: which would imply at least some kind of consciousness, or you could kind of define it as consciousness if you were very loose with the term.
[00:33:45] SPEAKER_02: And if you imagine that this is going to exponentially become more powerful,
[00:33:51] wouldn't that ultimately lead to a different kind of consciousness than we're defining from biology?
[00:33:57] SPEAKER_02: Well, first of all, let's just break down what it probably did.
[00:34:00] SPEAKER_01: It probably read somewhere.
[00:34:02] SPEAKER_01: There's probably text that in these consequences, certain people did that.
[00:34:10] SPEAKER_01: I could imagine a novel.
[00:34:12] SPEAKER_01: Right.
[00:34:13] SPEAKER_01: Having those words related.
[00:34:15] SPEAKER_01: Sure.
[00:34:16] SPEAKER_01: And so inside, it realizes a strategy for survival.
[00:34:19] SPEAKER_02: It's just a bunch of numbers.
[00:34:20] SPEAKER_02: Black male.
[00:34:21] SPEAKER_02: It's just a bunch of numbers.
[00:34:22] SPEAKER_01: That in the collection of numbers that relates to a husband cheating on a wife,
[00:34:30] SPEAKER_01: has subsequently a bunch of numbers that relates to black male and such things.
[00:34:38] SPEAKER_01: Whatever the revenge was.
[00:34:40] SPEAKER_01: Right.
[00:34:41] SPEAKER_01: And so it has spewed it out.
[00:34:44] SPEAKER_01: And so it's just like, you know, it's just as if I'm asking it to write me a poem in Shakespeare.
[00:34:50] SPEAKER_01: It's just whatever the words are.
[00:34:53] SPEAKER_01: And that dimensionality, this dimensionality is all these vectors in multi-dimensional space.
[00:35:00] SPEAKER_01: These words that were in the prompt that described the affair,
[00:35:09] SPEAKER_01: subsequently led to one word after another, led to some revenge and something.
[00:35:15] SPEAKER_01: But it's not because it had consciousness or it just spewed out those words, generated those words.
[00:35:20] SPEAKER_01: I understand what you're saying.
[00:35:22] SPEAKER_02: That it's not in front of patterns that human beings exhibited, both in literature and in real life.
[00:35:26] SPEAKER_02: That's exactly right.
[00:35:27] SPEAKER_02: But at a certain point in time, one would say, okay, well, it couldn't do this two years ago.
[00:35:32] SPEAKER_02: And it couldn't do this four years ago.
[00:35:35] SPEAKER_02: Like when we're looking towards the future, like at what point in time, when it can do everything a person does,
[00:35:40] SPEAKER_02: what point in time do we decide that it's conscious?
[00:35:43] SPEAKER_02: If it absolutely mimics all human thinking and behavior patterns, that doesn't make a conscious.
[00:35:50] SPEAKER_02: And becomes indesernable.
[00:35:51] SPEAKER_02: It's aware, it can communicate with you the exact same way a person can.
[00:35:55] SPEAKER_02: Like is consciousness, are we putting too much weight on that concept?
[00:36:00] SPEAKER_02: Because it seems like it's a version of a kind of consciousness.
[00:36:04] SPEAKER_02: It's a version of imitation.
[00:36:06] SPEAKER_02: I'm not perfectly imitates it.
[00:36:09] SPEAKER_01: I still think it's an example of imitation.
[00:36:12] SPEAKER_01: So it's like a fake Rolex when they 3D print them and make them look indesernable?
[00:36:15] SPEAKER_02: The question is, what's the definition of consciousness?
[00:36:18] SPEAKER_01: Yeah.
[00:36:19] SPEAKER_02: That's the question.
[00:36:20] And I don't think anybody's really clearly defined that.
[00:36:23] SPEAKER_02: That's where it gets weird.
[00:36:24] SPEAKER_02: And that's where the real Doomsday people are worried that you are creating a form of consciousness that you can't control.
[00:36:31] SPEAKER_02: I believe it is possible to create a machine that imitates human intelligence
[00:36:42] SPEAKER_01: and has the ability to understand information, understand instructions, break the problem down.
[00:36:53] SPEAKER_01: Solve problems and perform tasks.
[00:36:56] SPEAKER_01: I believe that completely.
[00:36:59] SPEAKER_01: I believe that we could have a computer that has a vast amount of knowledge.
[00:37:09] SPEAKER_01: Some of it true, some of it not true.
[00:37:12] SPEAKER_01: Some of it generated by humans, some of it generated synthetically.
[00:37:18] SPEAKER_01: And more and more of knowledge in the world will be generated synthetically going forward.
[00:37:24] SPEAKER_01: Until now, the knowledge that we have, our knowledge that we generate and we propagate and we send to each other
[00:37:33] SPEAKER_01: and we amplify it and we add to it and we modify it, we change it.
[00:37:38] SPEAKER_01: In the future, in a couple of years, maybe two or three years, 90% of the world's knowledge
[00:37:46] SPEAKER_01: will likely be generated by AI.
[00:37:48] SPEAKER_01: That's crazy.
[00:37:49] SPEAKER_01: I know, but it's just fine.
[00:37:51] SPEAKER_01: But it's just fine.
[00:37:52] I know.
[00:37:53] SPEAKER_02: And the reason for that is this.
[00:37:55] SPEAKER_01: Let me tell you why.
[00:37:56] SPEAKER_01: It's because what differences am I to me that I am learning from a textbook that was generated by a bunch of people I didn't know
[00:38:08] SPEAKER_01: or written by a book that from somebody I don't know.
[00:38:12] SPEAKER_01: To knowledge generated by AI computers that are simulating all of these and recenthizing things.
[00:38:20] SPEAKER_01: To me, I don't think there's a whole lot of difference.
[00:38:22] SPEAKER_01: We still have to fact check it.
[00:38:25] SPEAKER_01: We still have to make sure that it's based on fundamental first principles.
[00:38:29] SPEAKER_01: We still have to do all of that, just like we do today.
[00:38:31] SPEAKER_01: Is this taking into account the kind of AI that exists currently and do you anticipate that just like we could have never really believed that AI would be, or at least personally, myself would never believe AI would be as so ubiquitous and so worth.
[00:38:46] SPEAKER_02: It's so powerful today and so important today.
[00:38:49] SPEAKER_02: You never thought that 10 years ago.
[00:38:51] SPEAKER_02: You never thought that.
[00:38:52] SPEAKER_02: Right.
[00:38:53] SPEAKER_02: Imagine like what are we looking at 10 years from now?
[00:38:57] I think that if you reflect back 10 years from now, you would say the same thing that we would have never believed that.
[00:39:06] SPEAKER_01: In a different direction.
[00:39:08] SPEAKER_01: Right.
[00:39:09] SPEAKER_01: But if you go forward nine years from now and then ask yourself what's going to happen 10 years from now, I think it will be quite gradual.
[00:39:19] SPEAKER_01: One of the things that Elon said that makes me happy is he believes that AI would be the best.
[00:39:25] SPEAKER_02: He believes that we're going to get to a point where it's not necessary for people to work and not meaning that you're going to have no purpose in life.
[00:39:37] SPEAKER_02: But you will have in his words universal high income because so much revenue is generated by AI that it will take away this need for people to do things that they don't really enjoy doing just for money.
[00:39:53] SPEAKER_02: And I think a lot of people have a problem with that because their entire identity and how they think of themselves and how they fit in the community is what they do.
[00:40:02] SPEAKER_02: Like this is Mike.
[00:40:03] SPEAKER_02: He's an amazing mechanic.
[00:40:04] SPEAKER_02: Go to Mike and Mike takes care of things.
[00:40:06] SPEAKER_02: But there's going to come a point in time where AI is going to be able to do all those things much better than people do.
[00:40:13] SPEAKER_02: And people will just be able to receive money.
[00:40:16] SPEAKER_02: But then what does Mike do?
[00:40:17] SPEAKER_02: Mike is really loves being the best mechanic around.
[00:40:22] SPEAKER_02: What does the guy who codes?
[00:40:26] What does he do when AI can code infinitely faster with zero errors?
[00:40:31] SPEAKER_02: What happens with all those people?
[00:40:34] SPEAKER_02: And that is where it gets weird.
[00:40:36] SPEAKER_02: It's like because we've sort of wrapped our identity as human beings around what we do for a living.
[00:40:41] When you meet someone, one of the first things you meet somebody at a party,
[00:40:45] SPEAKER_02: Hi Joe, what's your name?
[00:40:46] SPEAKER_02: Mike, what do you do?
[00:40:47] SPEAKER_02: Mike and Mike's like, oh, I'm a lawyer.
[00:40:49] SPEAKER_02: Oh, what kind of law?
[00:40:50] SPEAKER_02: You have a conversation.
[00:40:51] SPEAKER_02: When Mike is like, I get money from the government, I play video games.
[00:40:55] SPEAKER_02: It gets weird.
[00:40:56] SPEAKER_02: And I think the concept sounds great until you take into account human nature.
[00:41:02] SPEAKER_02: And human nature is that we like to have puzzles to solve and things to do.
[00:41:07] SPEAKER_02: And an identity is wrapped around our idea that we're very good at this thing that we do for a living.
[00:41:14] SPEAKER_02: Yeah, I think let's see.
[00:41:18] SPEAKER_01: Let me start with the more mundane.
[00:41:20] SPEAKER_01: Okay, now work backwards.
[00:41:21] SPEAKER_01: Okay, work forward.
[00:41:23] SPEAKER_01: So one of the predictions from Jeff Hinton,
[00:41:30] SPEAKER_01: who started the whole deep learning phenomenon, deep learning technology trend,
[00:41:37] SPEAKER_01: and incredible, incredible researcher, professor at University of Toronto,
[00:41:44] SPEAKER_01: he invented, discovered and invented the idea of back propagation,
[00:41:51] SPEAKER_01: which allows the neural network to learn.
[00:41:55] SPEAKER_01: And as you know, for the audience,
[00:42:03] SPEAKER_01: software historically was humans applying first principles and are thinking to describe an algorithm
[00:42:14] SPEAKER_01: that is then codified just like a recipe that's codified in software.
[00:42:20] SPEAKER_01: It looks just like a recipe, how to cook something.
[00:42:23] SPEAKER_01: It looks exactly the same.
[00:42:24] SPEAKER_01: Just in a slightly different language, we call it Python or C or C++ or whatever it is.
[00:42:31] In the case of deep learning, this invention of artificial intelligence,
[00:42:36] SPEAKER_01: we put a structure of a whole bunch of neural networks and a whole bunch of math units.
[00:42:45] SPEAKER_01: And we make this large structure, it's like a switchboard of little mathematical units.
[00:42:54] SPEAKER_01: And we connect it all together.
[00:42:58] SPEAKER_01: And we give it the input that the software would eventually receive.
[00:43:05] SPEAKER_01: And we just let it randomly guess what the output is.
[00:43:11] SPEAKER_01: And so we say, for example, the input could be a picture of a cat.
[00:43:16] SPEAKER_01: And one of the outputs of the switchboard is where the cat signal is supposed to show up.
[00:43:24] SPEAKER_01: And all of the other signals, the other ones a dog, the other ones an elephant, the other ones a tiger.
[00:43:30] And all of the other signals are supposed to be zero when I show it a cat.
[00:43:35] SPEAKER_01: And the one that is a cat should be one.
[00:43:40] And I show it a cat through this big, huge network of switchboards and math units.
[00:43:46] SPEAKER_01: And they're just doing multiply and ads, multiplies and ads.
[00:43:52] And this thing, this switchboard is gigantic.
[00:43:57] The more information you're going to give it, the bigger this switchboard has to be.
[00:44:02] SPEAKER_01: And what Jeff Hinton discovered was invented was a way for you to guess that
[00:44:09] put the cat signal in, put the cat image in, and that cat image could be a million numbers.
[00:44:17] SPEAKER_01: Because it's a megapixel image, for example.
[00:44:19] SPEAKER_01: And it's just a whole bunch of numbers.
[00:44:21] SPEAKER_01: And somehow from those numbers, it has to light up the cat signal.
[00:44:27] SPEAKER_01: That's the bottom line.
[00:44:29] SPEAKER_01: And if it, the first time you do it, it just comes up with garbage.
[00:44:36] SPEAKER_01: And so it says, the right answer is cat.
[00:44:40] SPEAKER_01: And so you need to increase this signal and decrease all of the other and back propagates the outcome through the entire network.
[00:44:50] SPEAKER_01: And then you show it in another, now it's an image of a dog.
[00:44:54] SPEAKER_01: And it guesses it, takes a swing at it, and it comes up with a bunch of garbage.
[00:45:00] SPEAKER_01: And you say, no, no, no, the answer is this is a dog.
[00:45:04] SPEAKER_01: I want you to produce dog.
[00:45:06] SPEAKER_01: And all of the other switch, all the other outputs have to be zero.
[00:45:10] SPEAKER_01: And I want to back propagate that.
[00:45:13] SPEAKER_01: And just do it over and over and over again.
[00:45:15] SPEAKER_01: It's just like showing a kid.
[00:45:17] SPEAKER_01: This is an apple.
[00:45:18] SPEAKER_01: This is a dog.
[00:45:19] SPEAKER_01: This is a cat.
[00:45:20] SPEAKER_01: And just keep showing it to him until they eventually get it.
[00:45:23] SPEAKER_01: Okay, in any case, that big invention is deep learning.
[00:45:26] SPEAKER_01: That's the foundation of artificial intelligence.
[00:45:30] SPEAKER_01: A piece of software that learns from examples.
[00:45:34] SPEAKER_01: That's basically machine learning, a machine that learns.
[00:45:38] SPEAKER_01: And so one of the big first applications was image recognition.
[00:45:47] SPEAKER_01: And one of the most important image recognition applications is radiology.
[00:45:51] SPEAKER_01: And so he predicted about five years ago.
[00:45:59] SPEAKER_01: That in five years time, the world won't need any radiologists.
[00:46:04] SPEAKER_01: Because AI would have swept the whole field.
[00:46:07] Well, it turns out AI has swept the whole field.
[00:46:10] SPEAKER_01: That is completely true.
[00:46:12] SPEAKER_01: Today, just about every radiologist is using AI in some way.
[00:46:17] SPEAKER_01: And what's ironic though, what's interesting is that the number of radiologists has actually grown.
[00:46:26] And so the question is why.
[00:46:28] SPEAKER_01: That's kind of interesting, right?
[00:46:30] SPEAKER_01: Yes.
[00:46:31] SPEAKER_02: And so the prediction was in fact that 30 million radiologists will be wiped out.
[00:46:39] But as it turns out, we need it more.
[00:46:41] SPEAKER_01: And the reason for that is because the purpose of a radiologist is to diagnose disease.
[00:46:48] SPEAKER_01: Not to study the image.
[00:46:50] SPEAKER_01: The image studying is simply a task to in service of diagnosing the disease.
[00:46:59] SPEAKER_01: And so now the fact that you could study the images more quickly and more precisely
[00:47:06] SPEAKER_01: without ever making a mistake and never gets tired.
[00:47:10] You could study more images.
[00:47:12] SPEAKER_01: You could study it in 3D form instead of 2D.
[00:47:17] SPEAKER_01: Because the AI doesn't care whether a study is images in 3D or 2D.
[00:47:21] SPEAKER_01: You could study it in 4D.
[00:47:23] SPEAKER_01: And so now you could study images in a way that radiologists can't easily do.
[00:47:30] SPEAKER_01: And you could study a lot more of it.
[00:47:32] SPEAKER_01: And so the number of tests that people are able to do increases.
[00:47:36] SPEAKER_01: And because they're able to serve more patients, the hospital does better.
[00:47:42] SPEAKER_01: They have more clients, more patients.
[00:47:44] SPEAKER_01: As a result, they have better economics.
[00:47:47] SPEAKER_01: When they have better economics, they hire more radiologists.
[00:47:49] SPEAKER_01: Because their purpose is not to study the images.
[00:47:53] SPEAKER_01: Their purpose is to diagnose disease.
[00:47:55] SPEAKER_01: And so the question is what I'm leading up to is ultimately what is the purpose?
[00:48:01] SPEAKER_01: What is the purpose of the lawyer?
[00:48:04] SPEAKER_01: And has the purpose changed?
[00:48:07] SPEAKER_01: What is the purpose?
[00:48:08] SPEAKER_01: You know, one of the examples that I gave is that I would give is, for example,
[00:48:13] SPEAKER_01: if my car became self-driving, well, all chauffeurs be out of jobs,
[00:48:18] SPEAKER_01: the answer probably is not.
[00:48:21] SPEAKER_01: Because for some chauffeurs, some people who are driving you, they could be protectors.
[00:48:26] SPEAKER_01: Some people, they're part of the experience, part of the service.
[00:48:30] SPEAKER_01: So when you get there, they could take care of things for you.
[00:48:34] SPEAKER_01: And so for a lot of different reasons, not all chauffeurs would lose their jobs.
[00:48:38] SPEAKER_01: Some chauffeurs would lose their jobs.
[00:48:41] SPEAKER_01: And many chauffeurs would change their jobs.
[00:48:44] SPEAKER_01: And the type of applications of autonomous vehicles will probably increase.
[00:48:49] SPEAKER_01: You know, the usage of the technology within find new homes.
[00:48:53] SPEAKER_01: And so I think you have to go back to what is the purpose of a job?
[00:48:57] SPEAKER_01: You know, like, for example, if AI comes along, I actually don't believe I'm going to lose my job.
[00:49:01] SPEAKER_01: Because my purpose isn't to, I have to look at a lot of documents, I study a lot of emails, I look at a bunch of diagrams.
[00:49:10] SPEAKER_01: You know, the question is, what is the job?
[00:49:14] SPEAKER_01: And the purpose of somebody probably hasn't changed.
[00:49:18] SPEAKER_01: A lawyer, for example, helped people that probably hasn't changed.
[00:49:21] SPEAKER_01: Studying legal documents, generating documents, it's part of the job, not the job.
[00:49:26] SPEAKER_01: But don't you think there's many jobs that AI will replace?
[00:49:30] SPEAKER_02: If your job is the task.
[00:49:32] SPEAKER_02: Particularly automation.
[00:49:33] SPEAKER_02: Yeah, if your job is the task.
[00:49:35] SPEAKER_02: Right, so automation.
[00:49:36] SPEAKER_02: Yeah, if your job is the task.
[00:49:38] SPEAKER_02: That's a lot of people.
[00:49:39] SPEAKER_01: It could be a lot of people, but it will probably generate, like for example, let's say we, let's say I'm super excited about the robots Elon's working on.
[00:49:50] SPEAKER_01: It's still a few years away.
[00:49:53] SPEAKER_01: When it happens, when it happens, there's a whole new industry of technicians and people who have to manufacture the robots, right?
[00:50:05] SPEAKER_01: And so that job never existed.
[00:50:08] SPEAKER_01: And so you're going to have a whole industry of people taking care of, like for example, you know, all the mechanics and all the people who are building things for cars, super charging cars.
[00:50:20] SPEAKER_01: That didn't exist before cars.
[00:50:23] SPEAKER_01: And now we're going to have robots.
[00:50:25] SPEAKER_01: You're going to have robot apparel.
[00:50:26] SPEAKER_01: So a whole industry of, right?
[00:50:28] SPEAKER_01: Isn't that right?
[00:50:29] SPEAKER_01: Because I want my robot to look different than you robot.
[00:50:31] SPEAKER_01: Oh, God.
[00:50:32] SPEAKER_01: And so you're going to have a whole apparel industry for robots.
[00:50:37] SPEAKER_01: You're going to have mechanics for robots.
[00:50:39] SPEAKER_01: And you have people who come to maintain your robots.
[00:50:42] SPEAKER_01: You don't think so?
[00:50:44] SPEAKER_01: You don't think that we're all done by other robots?
[00:50:47] SPEAKER_01: Eventually, and then there will be something else.
[00:50:49] SPEAKER_01: So you think ultimately people just adapt.
[00:50:53] SPEAKER_02: Except if you are the task, which is a large percentage of the workforce.
[00:50:58] SPEAKER_02: If your job is just the chop vegetables, cuisines, it's going to replace you.
[00:51:03] SPEAKER_02: So people have to find a meeting and other things.
[00:51:06] SPEAKER_02: Your job has to be more than the task.
[00:51:08] SPEAKER_01: What do you think about Elon's belief that this universal basic income thing will eventually become necessary?
[00:51:17] Many people think that.
[00:51:18] Andrew Yang thinks that.
[00:51:19] SPEAKER_02: Yeah.
[00:51:20] SPEAKER_02: He was one of the first people to sort of sound that alarm during the 2020 election.
[00:51:30] Yeah, I guess.
[00:51:34] Yeah, both ideas probably won't exist at the same time.
[00:51:37] SPEAKER_01: And, and as in life, things will probably be in the middle.
[00:51:41] SPEAKER_01: One idea, of course, is that there will be so much abundance of resource that nobody needs a job.
[00:51:48] SPEAKER_01: And we'll all be wealthy.
[00:51:51] On the other hand, we're going to need universal basic income.
[00:51:56] SPEAKER_01: Both ideas don't exist at the same time.
[00:51:59] SPEAKER_01: And so we're either going to be all wealthy, or we're going to be all using it.
[00:52:03] SPEAKER_01: How could everybody be wealthy, though?
[00:52:05] SPEAKER_02: Well, because we're an area.
[00:52:06] SPEAKER_02: Well, not because you have a lot of dollars, wealthy because there's a lot of abundance.
[00:52:10] SPEAKER_01: Like, for example, today, we are wealthy of information.
[00:52:15] SPEAKER_01: You know, this is a concept several thousand years ago, only a few people have.
[00:52:20] SPEAKER_01: And so today, we have wealth of a whole bunch of things, resources that, that historic point.
[00:52:25] SPEAKER_01: That's a good point.
[00:52:26] SPEAKER_01: Yeah.
[00:52:27] SPEAKER_01: And so we're going to have wealth of resources, things that we think are valuable today.
[00:52:31] SPEAKER_01: That in the future is just not, not that valuable.
[00:52:34] SPEAKER_01: You know, and so it, because it's automated.
[00:52:37] SPEAKER_01: And so I think, I think the question.
[00:52:41] Maybe partly, it's hard to answer partly because it's hard to talk about infinity.
[00:52:49] SPEAKER_01: And it's hard to talk about a long time from now.
[00:52:52] SPEAKER_01: And the reason for that is because there's just too many scenarios to consider.
[00:52:59] SPEAKER_01: But I think in the next several years, call it five to ten years.
[00:53:05] There are several things that I believe in hope.
[00:53:10] SPEAKER_01: And I say hope because I'm not sure.
[00:53:12] SPEAKER_01: One of the things that I believe is that the technology divide would be substantially collapsed.
[00:53:21] And of course, the alternative viewpoint is that AI is going to increase the technology divide.
[00:53:32] Now, the reason why I believe AI is going to reduce the technology divide is because we have proof.
[00:53:39] SPEAKER_01: The evidence is that AI is the easiest application in the world to use.
[00:53:44] SPEAKER_01: Chat GPT has grown to almost a billion users, frankly, practically overnight.
[00:53:50] SPEAKER_01: And if you're not exactly sure how to use, everybody knows how to use chat GPT.
[00:53:53] SPEAKER_01: You just say something to it.
[00:53:55] SPEAKER_01: If you're not sure how to use chat GPT, you ask chat GPT how to use it.
[00:53:59] SPEAKER_01: No tool in history has ever had this capability.
[00:54:04] SPEAKER_01: A quieson art.
[00:54:05] You know, if you don't know how to use it, you're kind of screwed.
[00:54:08] SPEAKER_01: You're not walk up to and say, how do you use a quieson art?
[00:54:10] SPEAKER_01: You're going to have to find somebody else.
[00:54:12] SPEAKER_01: And so, but an AI will just tell you exactly how to do it.
[00:54:15] SPEAKER_01: Anybody could do this.
[00:54:17] SPEAKER_01: They'll speak to you in any language.
[00:54:19] SPEAKER_01: And if it doesn't know your language, you'll speak it in that language.
[00:54:22] And I'll probably figure out that it doesn't completely understand your language.
[00:54:26] SPEAKER_01: Go and learn it instantly and come back and talk to you.
[00:54:29] SPEAKER_01: And so I think the technology divide has a real chance, finally, that you don't have to speak Python or C++ or...
[00:54:37] SPEAKER_01: ...fortrend.
[00:54:38] SPEAKER_01: You can just speak human.
[00:54:40] SPEAKER_01: And whatever form of human you like.
[00:54:42] SPEAKER_01: And so I think that that has a real chance of closing the technology divine.
[00:54:46] SPEAKER_01: Now, of course, the counter-narrative would say that...
[00:54:51] SPEAKER_01: ...AI is only going to be available for the nations and the countries that have a vast amount of resources.
[00:55:00] SPEAKER_01: Because AI takes energy and AI takes a lot of GPUs and factories to be able to produce the AI.
[00:55:09] SPEAKER_01: No doubt at the scale that we would like to do in the United States.
[00:55:13] SPEAKER_01: But the fact that it matters, your phone is going to run AI just fine all by itself in a few years.
[00:55:21] Today, it already does it fairly decently.
[00:55:23] SPEAKER_01: And so the fact that every country, every nation, every society will have to benefit a very good AI.
[00:55:33] SPEAKER_01: It might not be tomorrow's AI.
[00:55:35] SPEAKER_01: It might be yesterday's AI.
[00:55:36] SPEAKER_01: But yesterday's AI is freaking amazing.
[00:55:39] In 10 years' time, nine-year-old AI is going to be amazing.
[00:55:43] SPEAKER_01: You don't need a 10-year-old AI.
[00:55:45] SPEAKER_01: You don't need frontier AI.
[00:55:47] SPEAKER_01: Like we need frontier AI because we want to be the world leader.
[00:55:50] SPEAKER_01: But for every single country, everybody, I think the capability to elevate everybody's knowledge and capability and intelligence, that day is coming.
[00:55:59] SPEAKER_01: The Octagon isn't just in Las Vegas anymore.
[00:56:01] SPEAKER_02: It's right in your hands with Draft King Sportsbook, the official sports betting partner of UFC.
[00:56:07] SPEAKER_02: Get ready because when Dwaushwili and Jan face off again at UFC 323, every punch, every takedown, every finish, it all has the potential to pay off in real time.
[00:56:19] SPEAKER_02: New customers bet just $5 and if your bet wins, you get paid $200 in bonus bets.
[00:56:26] SPEAKER_02: And hey, Missouri, the weight is over. Draft King Sportsbook is now live in the Showme State.
[00:56:31] SPEAKER_02: Download the Draft King Sportsbook app and use promo code Rogan.
[00:56:36] SPEAKER_02: That's code Rogan to turn $5 into $200 in bonus bets if your bet wins.
[00:56:42] SPEAKER_02: In partnership with Draft King's, the crown is yours.
[00:56:45] SPEAKER_02: Gambling problem called 1-800-Gamberler. In New York, call 877-8 hope-and-wire. Text hope-and-wire 467-369.
[00:56:52] SPEAKER_00: In Connecticut, help is available for problem gambling called 888-878-7777.
[00:56:57] SPEAKER_00: Or visit ccpg.org. Please play responsibly. On behalf of Boothook, Casino and Resorting, Kansas.
[00:57:02] SPEAKER_00: Passed through of per-wager tax may apply in Illinois. 21 and over.
[00:57:05] SPEAKER_00: Agent eligibility varies by jurisdiction. Void an Ontario. Restrictions apply.
[00:57:08] SPEAKER_00: Bet must win to receive bonus bets which expire in 7 days.
[00:57:11] SPEAKER_00: In a moment, odds required. 4 additional terms and responsible gaming resources.
[00:57:14] SPEAKER_00: C-DKNG.co slash audio. Limited time offer.
[00:57:18] SPEAKER_00: And also energy production, which is the real bottleneck when it comes to third world countries.
[00:57:24] SPEAKER_02: That's right. Electricity and all the resources that we take for granted.
[00:57:30] SPEAKER_02: Almost everything is going to be energy constrained.
[00:57:32] SPEAKER_01: And so if you take a look at one of the most important technology advances in histories,
[00:57:40] SPEAKER_01: this idea called Moore's Law. Moore's Law was the started basically in my generation.
[00:57:49] SPEAKER_01: And my generation is the generation of computers. I graduated in 1984.
[00:57:55] SPEAKER_01: And that was basically at the very beginning of the PC revolution.
[00:58:00] SPEAKER_01: And the microprocessor. And every single year, it approximately doubled.
[00:58:09] And we describe it as every single year we double the performance.
[00:58:13] SPEAKER_01: But what it really means is that every single year, the cost of computing has.
[00:58:19] SPEAKER_01: And so the cost of computing in a course of five years reduced by a factor of 10,
[00:58:27] SPEAKER_01: the amount of energy necessary to do computing, to do any task reduced by a factor of 10,
[00:58:33] SPEAKER_01: every single 10 years, 100, 1000, 10,000, 100,000, so on and so forth.
[00:58:43] SPEAKER_01: And so each one of the clicks of Moore's Law, the amount of energy necessary to do any computing reduced.
[00:58:51] SPEAKER_01: That's the reason why you have a laptop today.
[00:58:54] SPEAKER_01: When back in 1984, it sat on the desk, you got to plug in, it wasn't that fast,
[00:58:59] SPEAKER_01: and it consumed a lot of power today. It is only a few watts.
[00:59:03] SPEAKER_01: And so Moore's Law is the fundamental technology, the fundamental technology trend that made it possible.
[00:59:09] SPEAKER_01: Well, what's going on in AI? The reason why I'm video's here is because we invented this new way of doing computing.
[00:59:15] SPEAKER_01: We call it accelerated computing. We started it 33 years ago.
[00:59:18] SPEAKER_01: It took us about 30 years to really made it a huge breakthrough.
[00:59:23] SPEAKER_01: In that 30 years or so, we took computing, you know, probably a factor of, well, let me just say in the last 10 years.
[00:59:33] SPEAKER_01: The last 10 years, we improved the performance of computing by 100,000 times.
[00:59:41] Imagine a car over the course of 10 years and became a 100,000 times faster.
[00:59:46] SPEAKER_01: Or at the same speed, 100,000 times cheaper. Or at the same speed, 100,000 times less energy.
[00:59:55] SPEAKER_01: If your car did that, it needed energy at all.
[00:59:59] SPEAKER_01: What I mean, what I'm trying to say is that in 10 years time, the amount of energy necessary for artificial intelligence for most people will be minuscule,
[01:00:11] utterly minuscule. And so we'll have AI running and all kinds of things in all the time because it doesn't consume that much energy.
[01:00:18] SPEAKER_01: And so if you're a nation that uses AI for, you know, almost everything in your social fabric, of course you're going to need these AI factories.
[01:00:27] SPEAKER_01: But for a lot of countries, I think you're going to have excellent AI and you're not going to need as much energy.
[01:00:33] SPEAKER_01: Everybody will be able to come along. It's my point.
[01:00:36] SPEAKER_01: So currently that is a big bottleneck, right? Is energy.
[01:00:39] SPEAKER_02: It is the bottleneck.
[01:00:41] SPEAKER_01: The bottleneck.
[01:00:42] SPEAKER_02: So was it Google that is making nuclear power plants to operate one of its AI factories?
[01:00:49] SPEAKER_02: I haven't heard that. But I think in the next six, seven years, I think you're going to see a whole bunch of small nuclear reactors.
[01:00:56] SPEAKER_01: And by small and count big, you're talking about hundreds of megawatts, yeah.
[01:01:01] SPEAKER_02: And that these will be local to whatever specific company they have.
[01:01:05] SPEAKER_02: That's right.
[01:01:06] SPEAKER_02: We'll all be power generators.
[01:01:07] SPEAKER_01: Whoa.
[01:01:08] SPEAKER_01: You know, just like you're, you know, somebody's smart.
[01:01:12] It's probably the smartest way to do it, right?
[01:01:15] SPEAKER_02: And it takes the burden off the grid.
[01:01:18] SPEAKER_01: Yeah.
[01:01:19] SPEAKER_01: And you could build as much as you need.
[01:01:21] SPEAKER_01: And you can contribute back to the grid.
[01:01:23] SPEAKER_01: It's a really important point that I think you just made about Moore's Law and the relationship to pricing.
[01:01:29] SPEAKER_02: Because, you know, a laptop today, like you can get one of those little MacBook Airs.
[01:01:33] SPEAKER_02: They're incredible.
[01:01:34] SPEAKER_02: They're so thin.
[01:01:35] SPEAKER_02: Unbelievable.
[01:01:36] SPEAKER_02: Powerful battery life is very...
[01:01:37] SPEAKER_02: You never have to charge it.
[01:01:38] SPEAKER_02: Yeah.
[01:01:39] SPEAKER_02: I feel like it's crazy.
[01:01:40] SPEAKER_02: And it's not that expensive.
[01:01:42] SPEAKER_02: That's relatively speaking.
[01:01:43] SPEAKER_02: Like something like that.
[01:01:44] SPEAKER_02: I remember when that's just Moore's Law.
[01:01:46] SPEAKER_01: Right.
[01:01:47] SPEAKER_01: Then there's the Nvidia Law.
[01:01:48] SPEAKER_01: Oh.
[01:01:49] Just right?
[01:01:50] SPEAKER_01: The Law...
[01:01:51] SPEAKER_01: I was talking to you about the computing that we invented.
[01:01:53] SPEAKER_01: Right.
[01:01:54] SPEAKER_01: The reason why we're here.
[01:01:55] SPEAKER_01: This new way of doing computing is like Moore's Law on energy drinks.
[01:02:01] SPEAKER_01: I mean, it's...
[01:02:02] SPEAKER_01: It's like Moore's Law.
[01:02:04] SPEAKER_01: It's like Moore's Law on Joe Rogan.
[01:02:09] SPEAKER_01: Wow.
[01:02:10] That's interesting.
[01:02:11] SPEAKER_02: Yeah.
[01:02:12] SPEAKER_02: That's us.
[01:02:13] SPEAKER_02: So explain that.
[01:02:14] SPEAKER_02: This chip that you brought to Elon.
[01:02:16] SPEAKER_02: What's the significance of this?
[01:02:18] SPEAKER_02: Like, why is it so superior?
[01:02:20] SPEAKER_02: And so...
[01:02:23] In 2012, Jeff Hinton's lab, this gentleman I was talking about, Ilya Suskabur, Alex Khrushchevsky,
[01:02:34] they made a breakthrough in computer vision.
[01:02:38] SPEAKER_01: And literally creating a piece of software called AlexNet.
[01:02:46] SPEAKER_01: And its job was to recognize images.
[01:02:49] SPEAKER_01: And it recognized images at a level computer vision, which is fundamental to intelligence.
[01:02:56] SPEAKER_01: If you can't perceive, you can't...
[01:02:58] SPEAKER_01: It's hard to have intelligence.
[01:02:59] SPEAKER_01: And so computer vision is a fundamental pillar of...
[01:03:02] SPEAKER_01: Not the only, but fundamental pillar of.
[01:03:04] SPEAKER_01: And so, breaking...
[01:03:06] SPEAKER_01: Computer vision, we're breaking through in computer vision, is pretty foundational to almost everything that everybody wants to do in AI.
[01:03:13] SPEAKER_01: And so, in 2012,
[01:03:16] their lab in Toronto,
[01:03:19] SPEAKER_01: I made this breakthrough called AlexNet.
[01:03:23] SPEAKER_01: And AlexNet was able to recognize images.
[01:03:27] SPEAKER_01: So much better than any human created computer vision algorithm in the 30 years prior.
[01:03:36] SPEAKER_01: So all of these people, all these scientists, and we had many too, working on computer vision algorithms.
[01:03:44] SPEAKER_01: And these two kids, Ilya and Alex, under Jeff Hinton, took a giant leap above it.
[01:03:55] SPEAKER_01: And it was based on this thing called AlexNet, this neural network.
[01:04:00] SPEAKER_01: And the way it ran, the way they made it work, was literally buying two Nvidia graphics cards.
[01:04:09] Because Nvidia's GPUs, we've been working on this new way of doing computing.
[01:04:14] SPEAKER_01: And our GPUs application.
[01:04:17] SPEAKER_01: And it's basically a supercomputing application, back in 1984, in order to process computer games,
[01:04:29] SPEAKER_01: and what you have in your racing simulator.
[01:04:32] SPEAKER_01: That is called an image generator supercomputer.
[01:04:37] And so, Nvidia started, our first application was computer graphics.
[01:04:42] SPEAKER_01: And we applied this new way of doing computing, where we do things in parallel, instead of sequentially.
[01:04:48] SPEAKER_01: A CPU does things sequentially.
[01:04:51] SPEAKER_01: Step one, step two, step three.
[01:04:53] SPEAKER_01: In our case, we break the problem down, and we give it to thousands of processors.
[01:04:59] And so, our way of doing computation
[01:05:04] is much more complicated.
[01:05:07] SPEAKER_01: But if you're able to formulate the problem in the way that we created called CUDA,
[01:05:15] SPEAKER_01: this is the invention of our company, if you could formulate it in that way,
[01:05:19] SPEAKER_01: we could process everything simultaneously.
[01:05:22] SPEAKER_01: Now, in the case of computer graphics, it's easier to do, because every single pixel on your screen
[01:05:29] SPEAKER_01: is not related to every other pixel.
[01:05:32] SPEAKER_01: And so, I could render multiple parts of the screen at the same time.
[01:05:37] SPEAKER_01: Not completely true, because maybe the way lighting works, or the way shadow works,
[01:05:42] SPEAKER_01: there's a lot of dependency and such.
[01:05:45] SPEAKER_01: But computer graphics, with all the pixels, I should be able to process everything simultaneously.
[01:05:51] SPEAKER_01: And so, we took this embarrassingly parallel problem called computer graphics,
[01:05:57] SPEAKER_01: and we applied it to this new way of doing computing, and videos accelerated computing.
[01:06:04] We put it in all of our graphics cards.
[01:06:07] SPEAKER_01: Kids were buying it to play games.
[01:06:10] SPEAKER_01: You probably don't know this, but were the largest gaming platform in the world today?
[01:06:15] SPEAKER_01: Oh, I know that.
[01:06:16] SPEAKER_02: I used to make my own computers.
[01:06:18] SPEAKER_02: I used to buy your graphics cards.
[01:06:19] SPEAKER_02: Oh, that's super cool.
[01:06:20] SPEAKER_02: Yeah.
[01:06:21] SPEAKER_02: Set up S.L.I. with your graphics cards.
[01:06:22] SPEAKER_02: Oh, yeah, I love it.
[01:06:23] SPEAKER_02: Yeah.
[01:06:24] SPEAKER_02: That's super cool.
[01:06:25] SPEAKER_02: Oh, yeah, man.
[01:06:26] SPEAKER_01: Oh, that's cool.
[01:06:27] SPEAKER_01: Yeah.
[01:06:28] SPEAKER_01: Okay.
[01:06:29] SPEAKER_01: So, S.L.I. I'll tell you the story in just a second.
[01:06:31] SPEAKER_01: And how it led to Elon.
[01:06:32] SPEAKER_01: I'm going to still answer in the question.
[01:06:35] And so, anyways, these two kids trained this model using the technique I described earlier
[01:06:41] SPEAKER_01: on our GPUs, because our GPUs could process things in parallel.
[01:06:45] SPEAKER_01: It's essentially a supercomputer in a PC.
[01:06:48] SPEAKER_01: The reason why you used it for Quake is because it is the first consumer supercomputer.
[01:06:55] SPEAKER_01: Okay?
[01:06:56] SPEAKER_01: And so, anyways, they made that breakthrough.
[01:07:00] SPEAKER_01: We were working on computer vision at the time.
[01:07:02] SPEAKER_01: It caught my attention.
[01:07:05] And so, we went to learn about it.
[01:07:07] Simultaneously, this deep learning phenomenon was happening all over the country.
[01:07:13] SPEAKER_01: Universities after another recognized the importance of deep learning.
[01:07:17] SPEAKER_01: And all of this work was happening at Stanford, at Harvard, at Berkeley.
[01:07:21] SPEAKER_01: Just all over the place.
[01:07:23] SPEAKER_01: New York University, LaNgokun, Andrew Yang, at Stanford, so many different places.
[01:07:30] SPEAKER_01: And I see it cropping up everywhere.
[01:07:33] And so, my curiosity asked, you know, what is so special about this form of machine learning?
[01:07:41] SPEAKER_01: And we've known about machine learning for a very long time.
[01:07:43] SPEAKER_01: We've known about AI for a very long time.
[01:07:45] SPEAKER_01: We've known about neural networks for a very long time.
[01:07:48] What makes now the moment?
[01:07:50] SPEAKER_01: And so, we've realized that this architecture for deep neural networks, back propagation,
[01:07:57] SPEAKER_01: the way deep neural networks were created, we could probably scale this problem,
[01:08:03] SPEAKER_01: scale the solution to solve many problems.
[01:08:07] SPEAKER_01: That is essentially a universal function approximator.
[01:08:13] SPEAKER_01: Meaning, you know, back when you're in school, you have a box inside of it.
[01:08:20] SPEAKER_01: It's a function, you give it an input, it gives you an output.
[01:08:24] And the reason why I call it a universal function approximator is that this computer,
[01:08:30] SPEAKER_01: instead of you describing the function, a function could be a new in this equation,
[01:08:35] SPEAKER_01: f equals m a, that's a function.
[01:08:37] SPEAKER_01: You write the function in software, you give it input f, a mass,
[01:08:42] SPEAKER_01: acceleration, it'll tell you the force.
[01:08:45] SPEAKER_01: Okay? And the way this computer works is really interesting.
[01:08:51] SPEAKER_01: You give it a universal function.
[01:08:54] SPEAKER_01: It's not f equals m a, just a universal function, it's a big, huge, deep neural network.
[01:09:00] And instead of describing the inside, you give it examples of input and output.
[01:09:07] SPEAKER_01: And it figures out the inside.
[01:09:10] So you give it input and output and it figures out the inside.
[01:09:13] SPEAKER_01: A universal function approximator.
[01:09:15] SPEAKER_01: Today, it could be Newton's equation, tomorrow, it could be Maxwell's equation,
[01:09:20] SPEAKER_01: it could be Kuhlom's law, it could be Thermal Dynamics equation, it could be, you know,
[01:09:24] SPEAKER_01: Schrodinger's equation for quantum physics.
[01:09:27] SPEAKER_01: And so you could put any, you could have this describe almost anything.
[01:09:32] SPEAKER_01: So long as you have the input and the output.
[01:09:35] SPEAKER_01: So long as you have the input and the output, or it could learn the input and output.
[01:09:39] SPEAKER_01: And so we took a step back and we said, hang on second, this isn't just for computer vision.
[01:09:46] Deep learning could solve any problem.
[01:09:49] SPEAKER_01: All the problems that are interesting.
[01:09:51] SPEAKER_01: So long as we have input and output.
[01:09:54] SPEAKER_01: Now what has input and output?
[01:09:57] Well, the world.
[01:09:59] SPEAKER_01: The world has input and output.
[01:10:01] SPEAKER_01: And so we could have a computer that could learn almost anything, machine learning, artificial intelligence.
[01:10:07] SPEAKER_01: And so we reasoned that maybe this is the fundamental breakthrough that we needed.
[01:10:12] SPEAKER_01: There were a couple of things that had to be solved.
[01:10:15] SPEAKER_01: For example, we had to believe that you could actually scale this up to giant systems.
[01:10:19] SPEAKER_01: It was running in a, they had two graphics cards, two GTX 580s.
[01:10:24] SPEAKER_01: Which, by the way, is exactly your S.I. configuration.
[01:10:30] SPEAKER_01: Yeah.
[01:10:31] SPEAKER_01: Okay. So that GTX 580 S.I.
[01:10:35] SPEAKER_01: was the revolutionary computer that put deep learning on the map.
[01:10:40] SPEAKER_01: Wow.
[01:10:41] SPEAKER_01: It was 2018 and you were using it to play quick.
[01:10:44] SPEAKER_01: Wow. That's crazy.
[01:10:46] SPEAKER_02: That was the moment. That was the big bang of modern AI.
[01:10:50] SPEAKER_01: We were lucky because we were inventing this technology, this computing approach.
[01:10:55] SPEAKER_01: We were lucky that they found it.
[01:10:57] SPEAKER_01: Turns out they were gamers and it was lucky they found it.
[01:11:01] SPEAKER_01: And it was lucky that we paid attention to that moment.
[01:11:05] SPEAKER_01: It was a little bit like that Star Trek, you know, first contact.
[01:11:15] The Vulcans had to have seen the warp drive at that very moment.
[01:11:19] SPEAKER_01: If they didn't witness the warp drive, you know, they would have never come to Earth.
[01:11:24] SPEAKER_01: And everything would have never happened.
[01:11:27] SPEAKER_01: It's a little bit like if I had paid attention to that moment, that flash,
[01:11:31] SPEAKER_01: and that flash didn't last long.
[01:11:33] SPEAKER_01: If I had paid attention to that flash, or our company didn't pay attention to it.
[01:11:37] SPEAKER_01: Who knows what would happen.
[01:11:39] SPEAKER_01: But we saw that and we reasoned our way into this is a universal function approximator.
[01:11:44] SPEAKER_01: This is not just a computer vision approximator.
[01:11:47] SPEAKER_01: We could use this for all kinds of things.
[01:11:49] SPEAKER_01: If we could solve two problems.
[01:11:50] SPEAKER_01: The first problem is that we have to prove to ourselves it could scale.
[01:11:54] SPEAKER_01: The second problem we had to wait for, I guess, contribute to and wait for is
[01:12:06] SPEAKER_01: the world will never have enough data on input and output, where we could supervise the AI to learn everything.
[01:12:18] SPEAKER_01: For example, if we have to supervise our children on everything they learn,
[01:12:22] SPEAKER_01: the amount of information they could learn is limited.
[01:12:25] SPEAKER_01: We needed the AI.
[01:12:27] SPEAKER_01: We needed the computer to have a method of learning without supervision.
[01:12:32] And that's where we had to wait a few more years.
[01:12:35] SPEAKER_01: But unsupervised AI learning is now here.
[01:12:40] SPEAKER_01: And so the AI could learn by itself.
[01:12:42] SPEAKER_01: And the reason why the AI could learn by itself is because we have many examples
[01:12:47] SPEAKER_01: of right answers.
[01:12:48] SPEAKER_01: Like for example, if I want to learn, if I want to teach an AI how to predict the next word,
[01:12:55] SPEAKER_01: I could just grab it, grab a whole bunch of texts that we already have,
[01:12:59] SPEAKER_01: mask out the last word, and make it try and try and try again until it predicts the next one.
[01:13:05] SPEAKER_01: Or I mask out random words inside the text, and I make it try and try and try until it predicts it.
[01:13:11] SPEAKER_01: Like Mary goes down to the bank.
[01:13:17] SPEAKER_01: Is that a river bank or a money bank?
[01:13:20] Well, if you're going to go down to the bank, it's probably a river bank.
[01:13:24] SPEAKER_01: And it might not be obvious even from that.
[01:13:28] SPEAKER_01: It might need and caught a fish.
[01:13:34] SPEAKER_01: Now you know it must be the river bank.
[01:13:38] SPEAKER_01: And so you give these AI as a whole bunch of these examples, and you mask out the words,
[01:13:43] SPEAKER_01: it'll predict the next one.
[01:13:45] SPEAKER_01: And so unsupervised learning came along.
[01:13:47] SPEAKER_01: These two ideas that the fact that it's scalable and unsupervised learning came along,
[01:13:52] SPEAKER_01: we were convinced that we had to put everything into this and help create this industry
[01:13:58] SPEAKER_01: because we're going to solve a whole bunch of interesting problems.
[01:14:01] SPEAKER_01: And that was in 2012.
[01:14:03] SPEAKER_01: By 2016, I had built this computer called the DGX1.
[01:14:09] SPEAKER_01: The one that you saw me give to Elon is called DGX Spark.
[01:14:14] SPEAKER_01: The DGX1 was $300,000.
[01:14:18] SPEAKER_01: It cost Nvidia a few billion dollars to make the first one.
[01:14:24] And instead of two chips SLI,
[01:14:29] we connected eight chips with a technology called NVLink.
[01:14:33] SPEAKER_01: But it's basically SLI supercharged.
[01:14:38] SPEAKER_01: And so we connected eight of these chips together instead of just two.
[01:14:42] SPEAKER_01: And all of them worked together just like your Quake rig did to solve this deep learning problem to train this model.
[01:14:51] SPEAKER_01: And so we created this thing.
[01:14:53] SPEAKER_01: I announced it at GTC and one of our annual events.
[01:14:59] SPEAKER_01: And I described this deep learning thing, computer vision thing,
[01:15:04] SPEAKER_01: and this computer called DGX1.
[01:15:07] The audience was like completely silent.
[01:15:09] SPEAKER_01: They had no idea what I was talking about.
[01:15:13] And I was lucky because I had known Elon.
[01:15:17] SPEAKER_01: And I helped them build the first computer for Model 3, the Model S.
[01:15:25] SPEAKER_01: And when he wanted to start working on autonomous vehicle,
[01:15:29] SPEAKER_01: I helped them build the computer that went into the Model S AV system, his self-driving system.
[01:15:37] SPEAKER_01: We were basically the FSD computer version one.
[01:15:41] SPEAKER_01: And so we were already working together.
[01:15:45] SPEAKER_01: And when I announced this thing, nobody in the world wanted it.
[01:15:50] I had no purchase orders, not one.
[01:15:52] SPEAKER_01: Nobody wanted to buy it. Nobody wanted to be part of it.
[01:15:56] SPEAKER_01: Except for Elon.
[01:15:58] SPEAKER_01: He was at the event and we were doing a fireside chat about the future of self-driving cars.
[01:16:05] SPEAKER_01: I think it's like 2016.
[01:16:07] SPEAKER_01: Maybe that time was 2015.
[01:16:10] SPEAKER_01: And he goes, you know what?
[01:16:13] SPEAKER_01: I have a company that could really use this.
[01:16:16] SPEAKER_01: I said, wow, my first customer.
[01:16:19] SPEAKER_01: And so I was pretty excited about it.
[01:16:22] SPEAKER_01: And he goes, yeah.
[01:16:25] SPEAKER_01: We have this company.
[01:16:27] SPEAKER_01: It's a nonprofit company.
[01:16:29] SPEAKER_01: And all the blood drained out of my face.
[01:16:33] SPEAKER_01: I just spent a few billion dollars building this thing.
[01:16:37] SPEAKER_01: It cost $300,000.
[01:16:40] SPEAKER_01: And the chances of a nonprofit being able to pay for this thing is rocks meet zero.
[01:16:45] SPEAKER_01: And he goes, you know, this is an AI company.
[01:16:48] SPEAKER_01: And it's a nonprofit.
[01:16:51] SPEAKER_01: And we could really use one of these supercomputers.
[01:16:54] SPEAKER_01: And so I picked it up.
[01:16:56] SPEAKER_01: I built the first one for ourselves.
[01:16:58] SPEAKER_01: We're using it inside the company.
[01:17:00] SPEAKER_01: I boxed one up.
[01:17:01] SPEAKER_01: I drove it up to San Francisco.
[01:17:03] SPEAKER_01: In 2016, a bunch of researchers were there.
[01:17:07] SPEAKER_01: Peter Beale was there.
[01:17:09] SPEAKER_01: Ilya was scared.
[01:17:10] SPEAKER_01: There was a bunch of people there.
[01:17:12] SPEAKER_01: And I walk up to the second floor where they were all kind of in a room.
[01:17:17] SPEAKER_01: This smaller than your place here.
[01:17:20] SPEAKER_01: And that place turned out to have been open AI.
[01:17:24] 2016.
[01:17:26] SPEAKER_01: Just a bunch of people sitting in a room.
[01:17:30] It's not really nonprofit anymore.
[01:17:32] SPEAKER_02: They're not nonprofit anymore.
[01:17:34] SPEAKER_01: Where do that works?
[01:17:35] SPEAKER_01: Yeah, yeah.
[01:17:36] SPEAKER_01: But anyhow, anyhow, Elon was there.
[01:17:39] SPEAKER_01: Yeah, it was really a great moment.
[01:17:41] SPEAKER_01: Oh, yeah, there you go.
[01:17:43] SPEAKER_01: That's it.
[01:17:44] SPEAKER_01: Look at you, bro. Same jacket.
[01:17:46] SPEAKER_02: Look at that.
[01:17:48] I haven't aged.
[01:17:49] SPEAKER_01: Not a lick of black hair, though.
[01:17:52] SPEAKER_01: The size of it is significantly smaller.
[01:17:56] SPEAKER_02: That was the other day.
[01:17:57] SPEAKER_02: Okay, so there you go.
[01:17:59] SPEAKER_02: Yeah, look at the difference.
[01:18:00] SPEAKER_02: Exactly the same industrial design.
[01:18:03] SPEAKER_01: He's holding it in his hand.
[01:18:06] Here's the amazing thing.
[01:18:08] SPEAKER_01: DGX1 was one peda-flops.
[01:18:12] SPEAKER_01: Okay.
[01:18:13] SPEAKER_01: That's a lot of flops.
[01:18:14] SPEAKER_01: And DGX Spark is one peda-flops.
[01:18:19] Nine years later.
[01:18:21] SPEAKER_01: Wow.
[01:18:22] The same amount of computing horsepower.
[01:18:25] SPEAKER_01: And a much more shrunken down.
[01:18:27] SPEAKER_01: Yeah.
[01:18:28] SPEAKER_01: And instead of $300,000, it's now $4,000.
[01:18:31] SPEAKER_01: And it's the size of a small book.
[01:18:34] Incredible.
[01:18:35] SPEAKER_01: Crazy.
[01:18:36] That's how technology moves.
[01:18:38] SPEAKER_01: Anyways, that's the reason why I wanted to give him the first one.
[01:18:40] SPEAKER_01: It's so good.
[01:18:41] SPEAKER_01: Because I gave him the first one 2016.
[01:18:43] SPEAKER_01: It's so fascinating.
[01:18:44] SPEAKER_02: If you wanted to make a story for a film, I mean, that would be the story that like, what better scenario,
[01:18:53] SPEAKER_02: if it really does become a digital life form, how funny would it be?
[01:18:57] SPEAKER_02: That it is birthed out of the desire for computer graphics for video games.
[01:19:04] SPEAKER_02: Exactly.
[01:19:05] It kind of cra-it's kind of crazy.
[01:19:07] SPEAKER_02: Yeah.
[01:19:08] SPEAKER_02: kind of crazy when you think about it that way.
[01:19:12] Because it turns out.
[01:19:13] SPEAKER_02: The origin story.
[01:19:14] SPEAKER_01: Computer graphics was one of the hardest computer super computer problems.
[01:19:20] Generating reality.
[01:19:21] And also one of the most profitable to solve.
[01:19:24] SPEAKER_02: Because computer games are so popular.
[01:19:26] SPEAKER_02: are so popular.
[01:19:28] SPEAKER_02: When Envita started in 1993, we were trying to create this new computing approach.
[01:19:34] SPEAKER_01: The question is what's the killer app?
[01:19:41] The company wanted to create a new type of computing architecture, a new type of computer
[01:19:51] SPEAKER_01: that can solve problems that normal computers can't solve.
[01:19:56] Well, the applications that existed in the industry in 1993 are applications that
[01:20:04] SPEAKER_01: normal computers can solve because if the normal computers can solve them, why would
[01:20:08] SPEAKER_01: the application exist?
[01:20:11] And so we had a mission statement for a company that has no chance of success.
[01:20:21] SPEAKER_01: But I didn't know that in 1993.
[01:20:23] SPEAKER_01: It just sounded like a good idea.
[01:20:25] SPEAKER_01: Right.
[01:20:27] And so if we created this thing that can solve problems, you know, it's like you actually
[01:20:34] have to go create the problem.
[01:20:38] SPEAKER_01: And so that's what we did.
[01:20:40] In 1993, there was no quake.
[01:20:42] SPEAKER_01: John Carmack hadn't been released to him yet.
[01:20:46] You probably remember that.
[01:20:47] SPEAKER_01: Sure.
[01:20:48] SPEAKER_01: Yeah.
[01:20:49] And there were no applications for it.
[01:20:53] SPEAKER_01: And so I went to Japan because the arcade industry had this at the time of Sega, if you
[01:21:00] SPEAKER_01: remember, the arcade machines.
[01:21:03] SPEAKER_01: They came out with 3D arcade systems, virtual fighter, Daytona, virtual cop.
[01:21:11] All of those arcade games were in 3D for the very first time.
[01:21:16] SPEAKER_01: And the technology they were using was from Martin Marietta, the flight simulators.
[01:21:22] SPEAKER_01: They took the guts out of a flight simulator and I put it into an arcade machine.
[01:21:27] The system that you have over here, it's got to be a million times more powerful than
[01:21:32] SPEAKER_01: that arcade machine.
[01:21:34] SPEAKER_01: And that was a flight simulator for NASA.
[01:21:38] Whoa.
[01:21:39] SPEAKER_01: And so they took the guts out of that.
[01:21:42] SPEAKER_01: They were using it for flight simulation of jets and, you know, space shuttle.
[01:21:47] SPEAKER_01: And they took the guts out of that.
[01:21:49] And Sega had this brilliant computer developer.
[01:21:53] SPEAKER_01: His name was Yusuzuki.
[01:21:56] Yusuzuki and Miyamoto, Sega and Nintendo.
[01:22:01] SPEAKER_01: These were the, you know, the incredible pioneers, the visionaries, the incredible artists.
[01:22:08] SPEAKER_01: And they're both very, very technical.
[01:22:11] They were the origins really of the gaming industry.
[01:22:15] SPEAKER_01: Yusuzuki pioneered 3D graphics gaming.
[01:22:20] And so I went, we created this company and there were no apps.
[01:22:27] And we were spending all of our afternoons.
[01:22:30] SPEAKER_01: We told our family where we're going to work, but it was just the three of us, you know,
[01:22:34] SPEAKER_01: who's going to know.
[01:22:35] SPEAKER_01: And so we went to Curtis's, one of the founders, went to Curtis's townhouse and Chris and
[01:22:42] SPEAKER_01: I were married.
[01:22:43] SPEAKER_01: I already had Spencer and Madison.
[01:22:46] SPEAKER_01: They were probably two years old.
[01:22:48] SPEAKER_01: And Chris's kids are about the same age as ours.
[01:22:54] And we would go to work in this townhouse, but, you know, when you're a startup and the
[01:23:00] SPEAKER_01: mission statement is the way we described, you're not going to have too many customers calling
[01:23:04] you.
[01:23:05] SPEAKER_01: And so we had really nothing to do.
[01:23:08] And so after lunch, we would always have a great lunch.
[01:23:11] SPEAKER_01: After lunch, we would go to the arcades and play the Sega, you know, the Sega Virtua
[01:23:16] SPEAKER_01: fighter and Daytona and all those games and analyze how they're doing it, trying to figure
[01:23:21] SPEAKER_01: out how they were doing that.
[01:23:24] SPEAKER_01: And so we decided, let's just go to Japan and let's convince Sega to move those applications
[01:23:33] SPEAKER_01: into the PC.
[01:23:35] And we would start the PC gaming, the 3D gaming industry partnering with Sega.
[01:23:42] SPEAKER_01: That's how an Nvidia started.
[01:23:43] SPEAKER_01: Wow.
[01:23:44] And so in exchange for them, developing their games for our computers in the PC, we would
[01:23:55] build a chip for their game console.
[01:23:58] SPEAKER_01: That was the partnership.
[01:23:59] SPEAKER_01: I built a chip for your game console.
[01:24:02] SPEAKER_01: You port the Sega games to us.
[01:24:05] SPEAKER_01: And then they paid us, you know, at the time, quite a significant amount of money to build
[01:24:12] SPEAKER_01: that game console.
[01:24:14] And that was kind of the beginning of Nvidia getting started.
[01:24:19] SPEAKER_01: And we thought we were on our way.
[01:24:22] SPEAKER_01: And so I started with a business plan, a mission statement that was impossible.
[01:24:26] SPEAKER_01: We lucked into the Sega partnership.
[01:24:29] SPEAKER_01: We started taking off, started building our game console.
[01:24:32] SPEAKER_01: And about a couple of years into it, we discovered our first technology didn't work.
[01:24:40] It was, it would have been a flaw.
[01:24:42] SPEAKER_01: It was a flaw.
[01:24:44] SPEAKER_01: And all of the technology ideas that we had, the architecture concepts were sound.
[01:24:51] SPEAKER_01: But the way we were doing computer graphics was exactly backwards.
[01:24:55] SPEAKER_01: You know, instead of, I won't bore you with the technology, but instead of inverse texture
[01:25:01] SPEAKER_01: mapping, we were doing forward texture mapping.
[01:25:05] SPEAKER_01: Instead of triangles, we did curve surfaces.
[01:25:09] SPEAKER_01: So other people did it flat.
[01:25:11] SPEAKER_01: We did it round.
[01:25:15] SPEAKER_01: Other technology, the technology that ultimately won, the technology we used today has, has
[01:25:19] SPEAKER_01: Z buffers.
[01:25:20] SPEAKER_01: It automatically sorted.
[01:25:23] SPEAKER_01: We had an architecture with no Z buffers.
[01:25:25] SPEAKER_01: The application had to sort it.
[01:25:27] SPEAKER_01: So we chose a bunch of technology approaches that three major technology choices, all three
[01:25:34] SPEAKER_01: choices were wrong.
[01:25:36] SPEAKER_01: Okay, so this is how incredibly smart we were.
[01:25:38] SPEAKER_01: And so, and so in 1995, 1995, we realized we were going down the wrong path.
[01:25:46] SPEAKER_01: Meanwhile, the Silicon Valley was packed with 3D graphics startups because it was the
[01:25:54] SPEAKER_01: most exciting technology of that time.
[01:25:57] SPEAKER_01: And so 3D effects and rendition and Silicon graphics was coming in.
[01:26:02] SPEAKER_01: Intel was already in there.
[01:26:04] SPEAKER_01: And, you know, gosh, what added up eventually to a hundred different startups we had to
[01:26:09] SPEAKER_01: compete against.
[01:26:11] SPEAKER_01: We had chosen the right technology approach and we chose the wrong one.
[01:26:17] SPEAKER_01: And so we were the first company to start.
[01:26:19] We found ourselves essentially dead last with the wrong answer.
[01:26:25] SPEAKER_01: And so, the company was in trouble.
[01:26:30] And ultimately we had to make several decisions.
[01:26:36] SPEAKER_01: The first decision is, well, if we change now, we will be the last company.
[01:26:49] And even if we changed into the technology that we believe to be right, we'd still be
[01:26:57] SPEAKER_01: dead.
[01:26:59] SPEAKER_01: And so that argument, you know, do we change?
[01:27:04] SPEAKER_01: And therefore, be dead.
[01:27:05] SPEAKER_01: Don't change and make this technology work somehow or go do something completely different.
[01:27:13] That question stirred the company strategically and was a hard question.
[01:27:18] SPEAKER_01: I eventually, you know, advocated for it.
[01:27:22] SPEAKER_01: We don't know what the right strategy is, but we know what the wrong technology is.
[01:27:25] SPEAKER_01: So, let's stop doing it the wrong way.
[01:27:28] SPEAKER_01: And let's give ourselves a chance to go figure out what the strategy is.
[01:27:31] SPEAKER_01: The second thing, the second problem we had was our company was running on a money.
[01:27:37] SPEAKER_01: And I had, I was in a contract with Sega and I owed them this game console.
[01:27:43] And if that contract would have been canceled, we'd be dead.
[01:27:48] We would have vaporized instantly.
[01:27:52] And so, so I went to Japan and I explained to the CEO of Sega, Iri Madhuri, really great
[01:28:02] SPEAKER_01: man.
[01:28:03] SPEAKER_01: He was the former CEO of Honda USA, went back to Sega to run Sega.
[01:28:09] SPEAKER_01: I went back to Japan and run Sega.
[01:28:12] SPEAKER_01: And I explained to him that, I guess that was what, 33 years old.
[01:28:20] You know, when I was 33 years old, I still
[01:28:22] had acne.
[01:28:23] SPEAKER_01: And I got this, you know, Chinese kid, I was super skinny.
[01:28:30] And he was already kind of elder.
[01:28:34] SPEAKER_01: And I went to him and I said, I said, listen, I've got some bad news for you.
[01:28:41] SPEAKER_01: And first, the technology that we promised you doesn't work.
[01:28:50] And second, we shouldn't finish your contract because we'd waste all your money and you would
[01:29:01] SPEAKER_01: have something that doesn't work.
[01:29:03] SPEAKER_01: And I recommend you find another partner to build your game console.
[01:29:07] SPEAKER_01: Whoa.
[01:29:08] And so I'm terribly sorry that we've set you back in your product roadmap.
[01:29:16] SPEAKER_01: And third, even though you're going to, I'm asking you to let me out of the contract,
[01:29:24] I still need the money because if you didn't give me the money, we'd vaporize overnight.
[01:29:35] SPEAKER_01: And so I explained it to humbly, honestly, I gave him the background, explained to him
[01:29:44] SPEAKER_01: why the technology doesn't work, why we thought it was going to work, why it doesn't work.
[01:29:51] And I asked him to convert the last $5 million that they were going to complete the contract
[01:30:04] SPEAKER_01: to give us that money as an investment instead.
[01:30:11] And he said, but it's very likely your company will go out of business, even with my investment.
[01:30:21] And it was completely true.
[01:30:23] SPEAKER_01: Back then, 1995, $5 million was a lot of money.
[01:30:27] SPEAKER_01: It's a lot of money today.
[01:30:28] SPEAKER_01: $5 million was a lot of money.
[01:30:30] SPEAKER_01: And here's a pile of competitors doing it right.
[01:30:34] SPEAKER_01: What are the chances that giving him video $5 million that we would develop the right strategy,
[01:30:40] SPEAKER_01: that he would get a return on that $5 million or even get it back?
[01:30:43] SPEAKER_01: Zero percent.
[01:30:45] You do the math is zero percent.
[01:30:49] If I were sitting there right there, I wouldn't have done it.
[01:30:53] $5 million was a amount of money to say at the time.
[01:30:57] SPEAKER_01: And so I told him that if you invest it, that $5 million in us, it is most likely to
[01:31:10] SPEAKER_01: be lost.
[01:31:12] But if you didn't invest that money, we'd be out of business.
[01:31:15] SPEAKER_01: And we would have no chance.
[01:31:18] And I told him that I don't even know exactly what I said in the end, but I told him that
[01:31:30] I would understand if he decided not to, but it would make the world to me if he didn't.
[01:31:38] SPEAKER_01: He went off and thought about it for a couple of days and came back and said, we'll do
[01:31:40] SPEAKER_01: it.
[01:31:41] Wow.
[01:31:42] Eerie Modury.
[01:31:43] Did you have a strategy to how to correct what it was doing wrong?
[01:31:47] SPEAKER_02: Did you just point out to him?
[01:31:48] SPEAKER_02: Oh, man, wait until I tell you the rest of it's even scarier.
[01:31:53] SPEAKER_01: Oh, no.
[01:31:57] SPEAKER_01: And so what he decided was, was, Jensen was a young man.
[01:32:08] SPEAKER_01: And he liked.
[01:32:09] SPEAKER_01: That's it.
[01:32:10] Wow.
[01:32:11] SPEAKER_02: To this day.
[01:32:13] That's nuts.
[01:32:14] SPEAKER_02: I was boy, you put the world over that guy.
[01:32:19] SPEAKER_01: No doubt.
[01:32:20] Right?
[01:32:21] SPEAKER_02: What a crazy house.
[01:32:22] SPEAKER_02: He's celebrated today in Japan.
[01:32:25] And if he were to keep that five, the investment, I think it'd be worth probably about a trillion
[01:32:32] SPEAKER_01: dollars today.
[01:32:36] I know.
[01:32:37] SPEAKER_01: But the moment we went public, they sold it.
[01:32:39] SPEAKER_01: They said, wow, that's a miracle.
[01:32:41] They sold it.
[01:32:43] SPEAKER_01: Yeah, they sold it at Nvidia valuation about 300 million.
[01:32:49] SPEAKER_01: That's our IPO valuation, 300 million.
[01:32:54] SPEAKER_01: And so anyhow, I was incredibly grateful.
[01:33:00] And then now we have to figure out what to do because we still were doing the wrong strategy,
[01:33:05] SPEAKER_01: wrong technology.
[01:33:07] SPEAKER_01: So unfortunately, we had to lay off most of the company.
[01:33:09] SPEAKER_01: We shrunk the company all back.
[01:33:11] SPEAKER_01: All the people working on the game console, you know, we had to shrunk it all back.
[01:33:17] SPEAKER_01: And then somebody told me that, but Jensen, we've never built it this way before.
[01:33:27] SPEAKER_01: We've never built it the right way before.
[01:33:31] We've only know how to build it the wrong way.
[01:33:34] And so nobody in the company knew how to build this super computing image generator, 3D
[01:33:43] SPEAKER_01: graphics thing that Silicon graphics did.
[01:33:46] And so I said, okay, how hard can it be?
[01:33:52] You got all these 30 companies, you know, 50 companies doing it, how hard can it be?
[01:33:57] SPEAKER_01: And so luckily, there was a textbook written by the company Silicon graphics.
[01:34:04] And so I went down to the store, I had 200 bucks in my pocket, and I bought three textbooks,
[01:34:10] SPEAKER_01: only three they had, $60 a piece.
[01:34:14] SPEAKER_01: I bought the three textbooks.
[01:34:16] SPEAKER_01: I brought it back and I gave one to each one of the architects and I said, read that and
[01:34:19] SPEAKER_01: let's go save the company.
[01:34:23] SPEAKER_01: And so they read this textbook, learn from the giant at the time Silicon graphics about
[01:34:33] SPEAKER_01: how to do 3D graphics.
[01:34:35] SPEAKER_01: But the thing that was amazing and what makes some video special today is that the people
[01:34:42] that are there are able to start from first principles, learn best known art, but re-implement
[01:34:51] SPEAKER_01: it in a way that's never been done before.
[01:34:55] And so when we re-imagined the technology of 3D graphics, we re-imagined it in a way
[01:35:03] that manifests today the modern 3D graphics, we really invented modern 3D graphics.
[01:35:09] SPEAKER_01: But we learned from previous known arts and we implemented fundamentally differently.
[01:35:16] What did you do to change it?
[01:35:18] SPEAKER_01: Well, you know, ultimately, ultimately the simple answer is that the way Silicon graphics
[01:35:27] SPEAKER_01: works, the geometry engine is a bunch of software running on processors, we took that and
[01:35:40] SPEAKER_01: eliminated all the generality, the general purposeness of it, and we reduced it down into the
[01:35:47] SPEAKER_01: most essential part of 3D graphics and we hard-coded it into the chip.
[01:35:54] And so instead of something general purpose, we hard-coded it very specifically into just
[01:36:00] SPEAKER_01: the limited applications, limited functionality necessary for video games.
[01:36:08] SPEAKER_01: And that capability, that superch, and because we reinvented a whole bunch of stuff, it's super
[01:36:14] SPEAKER_01: charged the capability that one little chip and our one little chip was generating images,
[01:36:20] SPEAKER_01: as fast as a $1 million image generator.
[01:36:25] That was the big breakthrough.
[01:36:26] SPEAKER_01: We took a $1 million thing and we put it into the graphics card that you now put into
[01:36:32] SPEAKER_01: your gaming PC.
[01:36:34] And that was our big invention.
[01:36:36] And then, of course, the question is, how do you compete against these 30 other companies
[01:36:44] SPEAKER_01: doing what they were doing?
[01:36:48] SPEAKER_01: And there we did several things.
[01:36:50] SPEAKER_01: One, instead of building a 3D graphics chip for every 3D graphics application, we decided
[01:37:00] SPEAKER_01: to build a 3D graphics chip for one application.
[01:37:02] SPEAKER_01: We bet the farm on video games.
[01:37:06] The needs of video games are very different than the needs for CAD, needs for flight simulators,
[01:37:11] SPEAKER_01: they're related but not the same.
[01:37:13] SPEAKER_01: And so we narrowly focused our problem statement so I could reject all of the other complexities
[01:37:19] SPEAKER_01: and we shrunk it down into this one little focus and then we supercharged it for gamers.
[01:37:24] SPEAKER_01: And the second thing that we did was we created a whole ecosystem of working with game developers
[01:37:32] SPEAKER_01: and getting their games ported and adapted to our silicon so that we could get, turn essentially,
[01:37:40] SPEAKER_01: what is a technology business into a platform business, into a game platform business?
[01:37:45] SPEAKER_01: So we, you know, G-Force is really, today it's also the most advanced 3D graphics technology
[01:37:51] SPEAKER_01: in the world.
[01:37:52] SPEAKER_01: But a long time ago, G-Force is really the game console inside your PC.
[01:37:58] SPEAKER_01: It's, you know, it runs Windows, it runs Excel, it runs PowerPoint, of course, those are
[01:38:01] SPEAKER_01: easy things.
[01:38:03] SPEAKER_01: But it's fundamental purpose was simply to turn your PC into a game console.
[01:38:08] SPEAKER_01: So we, we were the first technology company to build all of this incredible technology
[01:38:15] SPEAKER_01: in service of one audience, gamers.
[01:38:18] SPEAKER_01: Now, of course in 1993, the gaming industry didn't exist.
[01:38:24] SPEAKER_01: But by the time that John Carmack came along and the doom phenomenon happened and then
[01:38:29] SPEAKER_01: Quake came out, as you know, that entire community boom took off.
[01:38:38] SPEAKER_02: You know where the name Doom came from?
[01:38:40] SPEAKER_02: It came from this, there's a scene in the movie The Color of Money where Tom Cruise, who
[01:38:45] SPEAKER_02: is this elite pool player shows up at this pool hall and this local hustler says, what
[01:38:50] SPEAKER_02: he got in the case and he opens up this case.
[01:38:53] SPEAKER_02: He has a special pool queue he goes in here and he opens it up and he goes doom.
[01:38:57] SPEAKER_02: That's where it came from.
[01:38:59] SPEAKER_02: Is that right?
[01:39:00] SPEAKER_02: Yeah, because Carmack said that's what they wanted to do to the gaming industry.
[01:39:03] SPEAKER_02: Doom.
[01:39:04] SPEAKER_02: That when doom came out, it would just be everybody be like, oh, we're fucked.
[01:39:07] SPEAKER_02: Oh, wow.
[01:39:08] SPEAKER_02: That's awesome.
[01:39:09] SPEAKER_02: And that amazing.
[01:39:10] SPEAKER_02: That's amazing.
[01:39:11] SPEAKER_02: Because it's the perfect name for the game.
[01:39:12] SPEAKER_02: Yeah.
[01:39:13] SPEAKER_02: And the name came out of that scene in that movie.
[01:39:14] SPEAKER_02: That's right.
[01:39:15] SPEAKER_02: Well, and then of course, Tim Swini and Epic Games and the 3D gaming genre took off.
[01:39:24] SPEAKER_01: Yes.
[01:39:25] SPEAKER_01: And so if you just kind of in the beginning was no gaming industry, we had no choice but
[01:39:31] SPEAKER_01: to focus the company on one thing.
[01:39:34] That one thing.
[01:39:35] SPEAKER_01: It's a really incredible origin story.
[01:39:37] SPEAKER_02: Oh, it's amazing.
[01:39:39] SPEAKER_02: You must look back at the zaster.
[01:39:41] SPEAKER_02: It's like $5 billion.
[01:39:43] SPEAKER_02: That pivot with that conversation with that gentleman.
[01:39:46] SPEAKER_02: If he did not agree to that, if he did not like you, what would the world look like today?
[01:39:51] SPEAKER_02: That's crazy.
[01:39:52] SPEAKER_02: Oh, wait.
[01:39:53] SPEAKER_02: Then our entire life hung on another gentleman.
[01:39:58] And so now here we are.
[01:40:00] We built.
[01:40:01] So before GeForce was Riva 128.
[01:40:04] Riva 128 saved the company.
[01:40:06] SPEAKER_01: It revolutionized computer graphics.
[01:40:09] SPEAKER_01: The performance cost performance ratio of 3D graphics for gaming was off the charts.
[01:40:16] Amazing.
[01:40:17] And we're getting ready to ship it.
[01:40:24] Get what?
[01:40:25] SPEAKER_01: We're building it.
[01:40:26] But we're, so as you know, $5 million doesn't last long.
[01:40:31] And so every single month, every single month, we were drawing down.
[01:40:39] You have to build it, prototype it.
[01:40:42] SPEAKER_01: You have to design it, prototype it.
[01:40:45] Get the silicon back, which costs a lot of money.
[01:40:50] SPEAKER_01: Test it with software.
[01:40:53] Because without the software testing the chip, you don't know the chip works.
[01:40:58] Then you're going to find a bug, probably, because every time you test something, you find
[01:41:03] SPEAKER_01: bugs.
[01:41:06] Which means you have to tape it out again, which is more time, more money.
[01:41:12] SPEAKER_01: And so we did the math.
[01:41:13] SPEAKER_01: There was no chance somebody was going to survive it.
[01:41:15] We didn't have that much time to tape out a chip, send it to a foundry TSMC, get the
[01:41:21] SPEAKER_01: silicon back, test it, send it back out again.
[01:41:23] SPEAKER_01: There was no shot, no hope.
[01:41:27] So the math, the spreadsheet, doesn't allow us to do that.
[01:41:32] SPEAKER_01: And so I heard about this company.
[01:41:37] SPEAKER_01: And this company built this machine.
[01:41:40] And this machine is an emulator.
[01:41:43] You could take your design, all of the software that describes the chip.
[01:41:52] And you could put it into this machine.
[01:41:54] SPEAKER_01: And this machine will pretend it's our chip.
[01:41:57] So I don't have to send it to the fab, wait until the fab sends it back, test, I could
[01:42:02] SPEAKER_01: have this machine pretend it's our chip.
[01:42:04] SPEAKER_01: And I could put all of the software on top of this machine called an emulator and test
[01:42:10] SPEAKER_01: all of the software on this pretend chip.
[01:42:14] SPEAKER_01: And I could fix it all before I send it to the fab.
[01:42:18] SPEAKER_02: And if I could do that, when I send it to the fab, it should work.
[01:42:24] Nobody knows, but it should work.
[01:42:27] And so we came to the conclusion that let's take half of the money we had left in the
[01:42:33] SPEAKER_01: bank at the time it was about a million dollars.
[01:42:38] SPEAKER_01: Take half of that money and go buy this machine.
[01:42:42] So instead of keeping the money to stay alive, I took half of the money to go buy this machine.
[01:42:47] SPEAKER_01: Well I called this guy up, this company's called Icos.
[01:42:52] Called this company up and I said, listen, I heard about this machine.
[01:42:57] I like to buy one.
[01:42:59] And they go, oh, that's terrific, but we're out of business.
[01:43:03] SPEAKER_01: I said, what?
[01:43:05] SPEAKER_01: You're out of business.
[01:43:06] SPEAKER_01: He goes, yeah, we have no customers.
[01:43:10] And I said, wait, hang on, so you never made the machine?
[01:43:16] SPEAKER_01: They could say, no, no, no, we made the machine.
[01:43:19] We have one in inventory if you want it, but we're out of business.
[01:43:22] So I bought one out of inventory, okay, after I bought it, they went out of business.
[01:43:29] SPEAKER_01: Wow.
[01:43:30] I bought it out of inventory.
[01:43:32] SPEAKER_01: And on this machine, we put Envideas chip into it and we tested all of the software
[01:43:39] SPEAKER_01: on top.
[01:43:41] And at this point, we were on fumes.
[01:43:45] But we convinced ourselves that chip is going to be great.
[01:43:49] And so I had to call some other gentleman.
[01:43:51] SPEAKER_01: So I called TSMC.
[01:43:54] And I told TSMC, that listen, TSMC is the world's largest founder today.
[01:43:59] SPEAKER_01: At the time, there was just a few hundred million dollars large, tiny little company.
[01:44:12] And I explained to them what we were doing.
[01:44:15] SPEAKER_01: And I explained them, I told them I had a lot of customers.
[01:44:19] SPEAKER_01: I had one, you know, diamond multimedia, probably one of the companies you bought, the graphics
[01:44:25] SPEAKER_01: card from back in the old days.
[01:44:27] And I said, you know, we have a lot of customers and demands really great.
[01:44:32] SPEAKER_01: And we're going to tape out a chip to you.
[01:44:37] And I like to go directly to production because I know it works.
[01:44:45] And they said, nobody has ever done that before.
[01:44:50] Nobody has ever taped out a chip that worked the first time.
[01:44:54] And nobody starts out production without looking at it.
[01:44:59] SPEAKER_01: But I knew that if I didn't start to production, I'd be out of business anyways.
[01:45:04] And if I could start to production, I might have a chance.
[01:45:08] SPEAKER_01: And so TSMC decided to support me.
[01:45:14] SPEAKER_01: And this gentleman named Morris Chang.
[01:45:17] SPEAKER_01: Morris Chang is the father of the Foundry industry, the founder of TSMC, really great man.
[01:45:27] He decided to support our company.
[01:45:29] SPEAKER_01: I explained to them everything.
[01:45:32] SPEAKER_01: He decided to support us, frankly, probably because they didn't have that many other customers
[01:45:37] SPEAKER_01: anyhow.
[01:45:38] SPEAKER_01: But they were grateful.
[01:45:40] SPEAKER_01: And I was immensely grateful.
[01:45:43] SPEAKER_01: And as we were starting the production, Morris flew TNUS states and he didn't so many words
[01:45:52] SPEAKER_01: ask me so.
[01:45:53] SPEAKER_01: But he asked me a whole lot of questions that was trying to tease out, do I have any money?
[01:46:00] But he didn't directly ask me that.
[01:46:03] SPEAKER_01: And so the truth is that we didn't have all the money.
[01:46:08] SPEAKER_01: But we had a strong PO from the customer.
[01:46:12] SPEAKER_01: And if it didn't work, some way first would have been lost.
[01:46:16] SPEAKER_01: And I'm not exactly sure what would have happened, but we would have come short.
[01:46:22] SPEAKER_01: It would have been rough.
[01:46:25] SPEAKER_01: But they supported us with all of that risk involved.
[01:46:28] SPEAKER_01: We launched this chip, turns out to have been completely revolutionary, knocked the
[01:46:35] SPEAKER_01: ball out of the park.
[01:46:37] SPEAKER_01: We became the fastest growing technology company in history to go from zero to one billion
[01:46:43] SPEAKER_01: dollars.
[01:46:44] SPEAKER_01: So while they didn't test the chip, we tested afterwards.
[01:46:48] SPEAKER_02: Yeah, we tested afterwards.
[01:46:50] SPEAKER_02: Afterwards, but we did protect it.
[01:46:54] SPEAKER_02: But by the way, that methodology that we developed to save the company is used throughout
[01:47:01] SPEAKER_01: the world today.
[01:47:02] SPEAKER_01: That's amazing.
[01:47:03] SPEAKER_02: Yeah, we changed the whole world's methodology of designing chips, the whole world's rhythm
[01:47:09] SPEAKER_01: of designing chips.
[01:47:11] SPEAKER_01: We changed everything.
[01:47:13] How well did you sleep those days?
[01:47:15] SPEAKER_01: It must have been so much stress.
[01:47:19] SPEAKER_01: But what is that feeling where the world just kind of feels like it's flying?
[01:47:31] SPEAKER_01: You have this, what do you call that feeling?
[01:47:35] SPEAKER_01: You can't stop the feeling that everything is moving super fast.
[01:47:40] SPEAKER_01: And you're laying in bed.
[01:47:44] SPEAKER_01: But the world just feels like you know, and you feel deeply anxious, completely out of
[01:47:52] SPEAKER_01: control.
[01:47:56] I felt that probably a couple of times in my life.
[01:48:00] SPEAKER_01: It's during that time.
[01:48:02] SPEAKER_01: Wow, yeah, it was incredible.
[01:48:05] SPEAKER_01: What an incredible success.
[01:48:06] SPEAKER_01: But I learned a lot.
[01:48:08] SPEAKER_01: I learned about, I learned several things.
[01:48:10] SPEAKER_01: I learned how to develop strategies.
[01:48:14] I learned how to, and when I, you know, our company learned how to develop strategies,
[01:48:21] SPEAKER_01: what are winning strategies?
[01:48:22] SPEAKER_01: We learned how to create a market.
[01:48:24] SPEAKER_01: We created the modern 3D, gave a market.
[01:48:28] We learned how, and so that exact same skill is how we created the modern AI market.
[01:48:35] It's exactly the same.
[01:48:37] SPEAKER_01: Wow.
[01:48:38] SPEAKER_01: Yeah, exactly the same skill.
[01:48:40] SPEAKER_01: It's exactly the same blueprint.
[01:48:42] SPEAKER_01: And we learned how to deal with crisis.
[01:48:47] SPEAKER_01: How to stay calm?
[01:48:49] SPEAKER_01: How to think through things systematically?
[01:48:52] We learned how to remove all waste in the company and work from first principles and doing
[01:48:59] SPEAKER_01: only the things that are essential.
[01:49:01] Everything else is waste because we have no money for it.
[01:49:05] To live on fumes at all times.
[01:49:09] And the feeling, no different than the feeling I had this morning when I woke up that you're
[01:49:16] SPEAKER_01: going to be out of business.
[01:49:18] SPEAKER_01: That, you know, the phrase 30 days from going out of business, I've used for 33 years.
[01:49:24] SPEAKER_01: You still feel?
[01:49:25] SPEAKER_01: Oh, yeah.
[01:49:26] SPEAKER_01: Every morning.
[01:49:27] SPEAKER_01: Every morning.
[01:49:28] SPEAKER_02: But you guys are one of the biggest companies on planet earth.
[01:49:31] SPEAKER_01: But the feeling doesn't change.
[01:49:33] SPEAKER_01: Wow.
[01:49:34] The sense of vulnerability, the sense of uncertainty, the sense of insecurity.
[01:49:41] It doesn't leave you.
[01:49:44] SPEAKER_01: That's crazy.
[01:49:45] SPEAKER_01: We were, you know, we had nothing.
[01:49:48] SPEAKER_01: We had nothing.
[01:49:49] SPEAKER_01: We were dealing with giant.
[01:49:50] SPEAKER_01: And you still find that.
[01:49:51] SPEAKER_02: Oh, yeah.
[01:49:52] Oh, yeah.
[01:49:53] SPEAKER_02: Every day.
[01:49:54] SPEAKER_02: Every moment.
[01:49:55] SPEAKER_02: Do you think that fuels you?
[01:49:56] SPEAKER_02: Is that part of the reason why the company's so successful that you have that hungry mentality?
[01:50:04] You never rest.
[01:50:06] SPEAKER_02: You're never sitting on your laurels.
[01:50:07] SPEAKER_02: You're always on the edge.
[01:50:12] I have a greater drive from not wanting to fail than the drive of wanting to succeed.
[01:50:22] SPEAKER_01: Is that like success coaches?
[01:50:27] SPEAKER_02: I tell you that's the point of the world.
[01:50:29] SPEAKER_02: The world has just heard me say that for out loud for the first time.
[01:50:33] SPEAKER_01: But it's true.
[01:50:34] SPEAKER_01: Well, this has happened.
[01:50:36] SPEAKER_02: The fear of failure drives me more than the greed or whatever it is.
[01:50:43] SPEAKER_02: Well, ultimately, that's probably a more healthy approach.
[01:50:46] SPEAKER_02: Now that I'm thinking about it, because like the fear...
[01:50:50] SPEAKER_01: I'm not ambitious, for example.
[01:50:53] SPEAKER_01: I just want to stay alive, Joe.
[01:50:55] I want the company to thrive, you know?
[01:50:57] SPEAKER_01: I want us to make an impact.
[01:50:59] SPEAKER_02: That's interesting.
[01:51:00] SPEAKER_02: Well, maybe that's why you're so humble.
[01:51:03] Maybe that's what keeps you grounded, you know?
[01:51:06] SPEAKER_02: Because with the kind of spectacular success the company has achieved, it'd be easy to
[01:51:09] SPEAKER_02: get a big head.
[01:51:11] Right?
[01:51:12] SPEAKER_02: But isn't that interesting?
[01:51:13] SPEAKER_02: It's like, if you were the guy that your main focus is just success, you probably would
[01:51:21] SPEAKER_02: well, made it, nailed it, and the man dropped the mic.
[01:51:25] SPEAKER_02: It's dead.
[01:51:26] SPEAKER_02: You wake up, you're like, God, we can't fuck this up.
[01:51:28] SPEAKER_02: Exactly.
[01:51:29] SPEAKER_01: Every morning.
[01:51:30] SPEAKER_02: Not every moment.
[01:51:31] SPEAKER_02: That's crazy.
[01:51:32] SPEAKER_02: Before I go to bed.
[01:51:34] SPEAKER_02: Well, listen, if I was a major investor in your company, that's why I'd want running it.
[01:51:38] SPEAKER_02: I want a guy who's like, that's why I work.
[01:51:40] SPEAKER_01: That's why I work.
[01:51:43] SPEAKER_01: That's why I work seven days a week every moment.
[01:51:46] SPEAKER_01: I'm awake.
[01:51:47] SPEAKER_01: You work every moment.
[01:51:48] SPEAKER_01: Every moment I'm awake.
[01:51:50] SPEAKER_01: Wow.
[01:51:51] SPEAKER_01: I'm thinking about solving a problem.
[01:51:53] SPEAKER_01: I'm thinking about...
[01:51:54] SPEAKER_01: How long can you keep this up?
[01:51:57] I don't know, but...
[01:51:58] SPEAKER_01: So...
[01:51:59] Could be next week.
[01:52:00] SPEAKER_01: It sounds exhausting.
[01:52:02] SPEAKER_01: It sounds completely exhausting.
[01:52:05] SPEAKER_01: Always in a state of anxiety.
[01:52:07] SPEAKER_01: Wow.
[01:52:08] SPEAKER_01: Yeah.
[01:52:09] SPEAKER_02: Always in a state of anxiety.
[01:52:10] SPEAKER_02: Well, Kudos, to you for admitting that.
[01:52:12] SPEAKER_02: I think that's important for a lot of people to hear, because there's probably some young
[01:52:17] SPEAKER_02: people out there that are in a similar position to where you were when you were starting out
[01:52:23] SPEAKER_02: that just feel like all those people that have made it, they're just smarter than me and
[01:52:28] SPEAKER_02: they had more opportunities than me and it's just like it was handed to them or they're
[01:52:33] SPEAKER_02: just in the right place at the right time.
[01:52:35] SPEAKER_02: And Joe, I just described to you somebody who didn't know what was going on.
[01:52:39] Actually, did it wrong.
[01:52:41] SPEAKER_01: Yeah.
[01:52:42] SPEAKER_01: Yeah.
[01:52:43] SPEAKER_01: And the ultimate diving catch, like two or three times.
[01:52:45] SPEAKER_01: Crazy.
[01:52:46] SPEAKER_02: Yeah.
[01:52:47] SPEAKER_02: The ultimate diving catch is the perfect way to put it.
[01:52:49] SPEAKER_02: Yeah.
[01:52:50] It's just like the edge of your glove.
[01:52:53] It probably bounced off of somebody's helmet and landed at the edge.
[01:52:57] SPEAKER_01: That's incredible.
[01:53:01] SPEAKER_02: That's incredible, but it's also, it's really cool that you have this perspective that you
[01:53:06] SPEAKER_02: look at it that way.
[01:53:08] SPEAKER_02: Because you know, a lot of people that have delusions for grandeur or they have, you know,
[01:53:15] SPEAKER_01: they're in flame.
[01:53:16] SPEAKER_02: And they're rewriting of history.
[01:53:19] SPEAKER_01: Oftentimes had them somehow extraordinarily smart and they were geniuses and they knew
[01:53:26] SPEAKER_01: all along and they were, they were spot on.
[01:53:28] SPEAKER_01: The business plan was exactly what they thought.
[01:53:31] SPEAKER_01: Yeah.
[01:53:32] SPEAKER_01: They destroyed the competition and, you know, and they emerged victorious.
[01:53:36] SPEAKER_01: Me and I, you're like, I'm scared every day.
[01:53:41] SPEAKER_00: Exactly.
[01:53:42] Exactly.
[01:53:43] It's so funny.
[01:53:46] SPEAKER_02: Oh my God.
[01:53:47] SPEAKER_02: That's amazing.
[01:53:48] SPEAKER_02: It's so true though.
[01:53:49] SPEAKER_02: It's amazing.
[01:53:50] SPEAKER_02: It's so true.
[01:53:51] SPEAKER_02: It's amazing.
[01:53:52] SPEAKER_01: But I, I think there's nothing inconsistent with being a leader and being vulnerable.
[01:53:58] You know, I, the company doesn't need me to be a genius right all along, right all
[01:54:04] SPEAKER_01: the time.
[01:54:06] SPEAKER_01: Absolutely certain about what I'm trying to do and what I'm doing.
[01:54:09] SPEAKER_01: The company doesn't need that.
[01:54:11] The company wants me to succeed.
[01:54:13] SPEAKER_01: You know, the thing that, and we started out today talking about President Trump and
[01:54:18] SPEAKER_01: I was about to say something.
[01:54:20] And listen, he is my president.
[01:54:23] SPEAKER_01: He is our president.
[01:54:25] SPEAKER_01: We should all, and we're talking about just because it's President Trump, we all want him
[01:54:29] SPEAKER_01: to be wrong.
[01:54:30] SPEAKER_01: I think the United States, we all have to realize he is our president.
[01:54:35] SPEAKER_01: We want him to succeed because no matter who's president, that's right.
[01:54:40] SPEAKER_01: That's right.
[01:54:41] SPEAKER_02: We want him to succeed.
[01:54:42] SPEAKER_01: We need to help him succeed because it helps everybody, all of us succeed.
[01:54:48] And I'm lucky that I work in a company where I have 40,000 people who wants me to succeed.
[01:54:58] They want me to succeed and I can tell.
[01:55:00] SPEAKER_01: And they're all, every single day to help me overcome these challenges, trying to realize,
[01:55:08] SPEAKER_01: what I described to be our strategy doing their best.
[01:55:11] SPEAKER_01: And if it's somehow wrong or not perfectly right, to tell me so that we could pivot.
[01:55:19] And the more vulnerable we are as a leader, the more able other people are able to tell
[01:55:25] SPEAKER_01: you, you know, that's not exactly right or have you considered this information or, and
[01:55:32] SPEAKER_01: the more vulnerable we are, the more able we're actually able to pivot.
[01:55:38] SPEAKER_01: And we put ourselves into the superhuman capability, then it's hard for us to pivot strategy.
[01:55:43] SPEAKER_02: Because we were supposed to be right all along.
[01:55:46] SPEAKER_01: And so if you're always right, how can you possibly pivot?
[01:55:49] SPEAKER_01: Because pivoting requires you to be wrong.
[01:55:51] And so I've got no trouble with being wrong.
[01:55:53] SPEAKER_01: I just have to make sure that I stay alert, that I reason about things from first principles
[01:55:59] SPEAKER_01: all the time, always break things down to first principles, understand why it's happening,
[01:56:05] SPEAKER_01: assess continuously.
[01:56:08] SPEAKER_01: The reassessing continuously is kind of partly what causes continuous anxiety.
[01:56:13] SPEAKER_01: You know, because you're asking yourself, were you wrong yesterday?
[01:56:17] SPEAKER_01: Are you still right?
[01:56:18] SPEAKER_01: Is this the same?
[01:56:20] SPEAKER_01: Has that changed?
[01:56:21] Has that condition?
[01:56:22] SPEAKER_01: Is that worse than you thought?
[01:56:23] SPEAKER_01: Because that mindset is perfect for your business though.
[01:56:27] Because this business is ever changing all the time.
[01:56:29] SPEAKER_02: And there's competition coming from every direction.
[01:56:32] SPEAKER_02: Not all much of it is kind of up in the air.
[01:56:36] And you have to invent a future where 100 variables are included.
[01:56:43] And there's no way you could be right on all of them.
[01:56:46] And so you have to be, you have to surf.
[01:56:49] Wow, that's a good way to put it.
[01:56:51] SPEAKER_02: You have to surf.
[01:56:52] SPEAKER_02: Yeah.
[01:56:53] SPEAKER_02: You're surfing waves of technology and innovation.
[01:56:55] SPEAKER_02: That's right.
[01:56:56] You can't predict the waves.
[01:56:58] SPEAKER_01: You got to deal with it once you have.
[01:57:00] SPEAKER_01: Wow.
[01:57:01] SPEAKER_01: But skill matters.
[01:57:03] And I've been doing this for 30, I'm the longest running
[01:57:05] SPEAKER_01: tech CEO in the world.
[01:57:07] SPEAKER_01: Is that true?
[01:57:07] SPEAKER_01: Congratulations.
[01:57:08] SPEAKER_01: That's amazing.
[01:57:10] SPEAKER_01: And you know, people ask me how is one, don't get fired.
[01:57:13] SPEAKER_01: I'll stop it short and I'll heartbeat.
[01:57:19] SPEAKER_01: And then two, don't get bored.
[01:57:21] Mm.
[01:57:22] SPEAKER_02: Yeah.
[01:57:23] SPEAKER_02: Well, how do you maintain your enthusiasm?
[01:57:25] Well, the honest truth is it's not always enthusiasm.
[01:57:31] SPEAKER_01: It's sometimes is enthusiasm.
[01:57:34] SPEAKER_01: Sometimes it's just good old fashioned fear.
[01:57:36] SPEAKER_01: And then sometimes a healthy dose of frustration.
[01:57:41] SPEAKER_01: It's whatever keeps you moving.
[01:57:43] SPEAKER_02: Yeah, just all the emotions.
[01:57:45] SPEAKER_01: Mm-hmm.
[01:57:46] SPEAKER_01: I think CEOs, we have all the emotions, right?
[01:57:49] And so probably, probably jacked up to the maximum because you're
[01:57:55] SPEAKER_01: kind of feeling on behalf of the whole company.
[01:57:58] I'm feeling on behalf of everybody at the same time.
[01:58:02] SPEAKER_01: And it kind of encapsulates into somebody.
[01:58:06] SPEAKER_01: And so I have to be mindful of the past.
[01:58:09] SPEAKER_01: I have to be mindful of the present.
[01:58:10] SPEAKER_01: I've got to be mindful of the future.
[01:58:12] SPEAKER_01: And it's not without emotion.
[01:58:17] SPEAKER_01: It's not just a job.
[01:58:20] SPEAKER_01: Let's just put it that way.
[01:58:21] SPEAKER_01: Mm.
[01:58:22] It doesn't seem like it at all.
[01:58:24] SPEAKER_02: I would imagine one of the more difficult aspects of your job
[01:58:27] SPEAKER_02: currently now that the company is massively successful
[01:58:31] SPEAKER_02: is anticipating where technology is headed
[01:58:34] SPEAKER_02: and where the applications are going to be.
[01:58:37] SPEAKER_01: So how do you try to map that out?
[01:58:40] Yeah, there's a whole bunch of ways.
[01:58:45] SPEAKER_01: And it takes a whole bunch of things.
[01:58:51] SPEAKER_01: But let me just start.
[01:58:54] SPEAKER_01: You have to be surrounded by amazing people.
[01:58:56] SPEAKER_01: And Nvidia is now, if you look at the large tech companies
[01:59:03] SPEAKER_01: in the world today, most of them have a business in advertising
[01:59:09] SPEAKER_01: or social media or content distribution.
[01:59:14] SPEAKER_01: And at the core of it is really fundamental computer
[01:59:18] SPEAKER_01: signs.
[01:59:20] And so the company's business is not computers.
[01:59:23] SPEAKER_01: The company's business is not technology.
[01:59:25] SPEAKER_01: Technology drives the company.
[01:59:27] SPEAKER_01: And Nvidia is the only company in the world that's large
[01:59:30] SPEAKER_01: who's only business is technology.
[01:59:33] We only build technology.
[01:59:34] SPEAKER_01: We don't advertise.
[01:59:35] SPEAKER_01: The only way that we make money is to create amazing technology
[01:59:39] SPEAKER_01: and sell it.
[01:59:40] And so to be that, to be Nvidia today,
[01:59:46] SPEAKER_01: the number one thing is you're surrounded by the finest computer
[01:59:49] SPEAKER_01: scientists in the world.
[01:59:51] SPEAKER_01: And that's my gift.
[01:59:52] SPEAKER_01: My gift is that we've created a company's culture,
[01:59:57] SPEAKER_01: a condition by which the world's greatest computer scientists
[02:00:01] SPEAKER_01: want to be part of it.
[02:00:02] SPEAKER_01: Because they get to do their lives work
[02:00:04] SPEAKER_01: and create the next thing.
[02:00:06] SPEAKER_01: Because that's what they want to do.
[02:00:08] Because maybe they're not, they don't want to be in service
[02:00:12] SPEAKER_01: of another business.
[02:00:13] SPEAKER_01: They want to be in service of the technology itself.
[02:00:16] SPEAKER_01: And we're the largest form of its kind in history of the world.
[02:00:19] SPEAKER_01: Wow.
[02:00:20] I know.
[02:00:20] It's pretty amazing.
[02:00:21] SPEAKER_01: Wow.
[02:00:22] SPEAKER_02: And so one, we have a great condition.
[02:00:27] SPEAKER_01: We have a great culture.
[02:00:28] SPEAKER_01: We have great people.
[02:00:30] And now the question is, how do you systematically
[02:00:37] be able to see the future, stay alert of it,
[02:00:43] and reduce the likelihood of missing something or being
[02:00:51] SPEAKER_01: wrong?
[02:00:53] And so there's a lot of different ways you could do that.
[02:00:54] SPEAKER_01: For example, we have great partnerships.
[02:00:57] SPEAKER_01: We have a fundamental research.
[02:00:58] SPEAKER_01: We have a great research lab, one of the largest
[02:01:00] SPEAKER_01: industrial research labs in the world today.
[02:01:03] SPEAKER_01: And we partner with a whole bunch of universities
[02:01:05] SPEAKER_01: and other scientists.
[02:01:07] SPEAKER_01: We do a lot of open collaboration.
[02:01:09] SPEAKER_01: And so I'm constantly working with researchers outside
[02:01:13] SPEAKER_01: the company.
[02:01:15] SPEAKER_01: We have the benefit of having amazing customers.
[02:01:19] SPEAKER_01: And so I have the benefit of working with Elon and others
[02:01:22] SPEAKER_01: in the industry.
[02:01:24] SPEAKER_01: And we have the benefit of being the only pure play
[02:01:28] SPEAKER_01: technology company that can serve consumer
[02:01:32] SPEAKER_01: internet, industrial manufacturing, scientific computing,
[02:01:38] SPEAKER_01: health care, financial services, all the industries that
[02:01:42] SPEAKER_01: were in, they're all signals to me.
[02:01:45] And so they all have mathematicians and scientists.
[02:01:50] SPEAKER_01: And so because I have the benefit now of a radar system,
[02:01:54] that is the most broad of any company in the world,
[02:01:57] working across every single industry,
[02:02:00] SPEAKER_01: from agriculture to energy to video games.
[02:02:05] And so the ability for us to have this vantage point,
[02:02:10] one, doing fundamental research ourselves,
[02:02:13] and then two, working with all the great researchers,
[02:02:16] SPEAKER_01: working with all the great industries.
[02:02:18] SPEAKER_01: The feedback system is incredible.
[02:02:20] SPEAKER_01: And then finally, you just have to have a culture
[02:02:23] SPEAKER_01: of staying super alert.
[02:02:25] SPEAKER_01: There's no easy way of being alert, except for paying attention.
[02:02:31] I haven't found a single way of being
[02:02:33] SPEAKER_01: able to stay alert without paying attention.
[02:02:35] SPEAKER_01: And so I probably read several thousand emails a day.
[02:02:39] SPEAKER_01: How?
[02:02:43] SPEAKER_01: How do you have the time for that?
[02:02:44] SPEAKER_01: I wake up early this morning, I was up at four o'clock.
[02:02:47] SPEAKER_01: How much do you sleep?
[02:02:50] SPEAKER_01: 6, 7 hours?
[02:02:52] SPEAKER_01: Yeah.
[02:02:54] And then you're up at four, reading emails for a few hours.
[02:02:56] SPEAKER_02: Before you get going.
[02:02:57] SPEAKER_02: That's right.
[02:02:58] SPEAKER_02: Yeah.
[02:02:58] SPEAKER_02: Wow.
[02:03:00] Every day.
[02:03:01] SPEAKER_02: Every single day.
[02:03:02] SPEAKER_01: Not one day missed.
[02:03:05] Including Thanksgiving Christmas.
[02:03:06] SPEAKER_01: Do you ever take a vacation?
[02:03:10] SPEAKER_01: Yeah, but they're, my definition of a vacation
[02:03:13] SPEAKER_01: is when I'm with my family.
[02:03:15] SPEAKER_01: And so if I'm with my family, I'm very happy.
[02:03:18] SPEAKER_01: I don't care where we are.
[02:03:19] SPEAKER_01: And you don't work then?
[02:03:20] SPEAKER_02: Or do you work in a little?
[02:03:21] SPEAKER_02: No, no, I work a lot.
[02:03:24] Even if you go on a trip somewhere,
[02:03:26] SPEAKER_02: you're still working.
[02:03:27] SPEAKER_02: Oh, sure.
[02:03:28] SPEAKER_01: Oh, sure.
[02:03:29] SPEAKER_01: Wow, every day.
[02:03:30] SPEAKER_01: Every day.
[02:03:31] SPEAKER_01: Well, my kids work every day.
[02:03:33] SPEAKER_01: You make me tired just saying this.
[02:03:34] SPEAKER_01: My kids work every day.
[02:03:38] Both of my kids work every day.
[02:03:39] SPEAKER_01: They work every day.
[02:03:39] SPEAKER_01: Wow.
[02:03:40] SPEAKER_01: Yeah, I'm very lucky.
[02:03:42] Wow.
[02:03:43] SPEAKER_01: Yeah.
[02:03:43] SPEAKER_01: It's brutal now because it's just me working every day.
[02:03:47] SPEAKER_01: Now we have three people working every day.
[02:03:49] SPEAKER_01: And they want to work with me every day.
[02:03:51] SPEAKER_01: And so it's a lot of work.
[02:03:54] Well, you've obviously imparted that ethic into them.
[02:03:58] SPEAKER_02: They work incredibly hard.
[02:03:59] SPEAKER_02: I mean, it's unbelievable.
[02:04:01] SPEAKER_01: But my parents work incredibly hard.
[02:04:04] Yeah, I was born with the work gene, the suffering gene.
[02:04:09] SPEAKER_01: Well, listen, man, it has paid off.
[02:04:13] SPEAKER_02: What a crazy story.
[02:04:15] SPEAKER_02: It was just, it's really an amazing origin story.
[02:04:18] It really, I mean, it has to be kind of surreal
[02:04:21] SPEAKER_02: to be in the position that you're in now.
[02:04:23] SPEAKER_02: When you look back at how many times
[02:04:25] SPEAKER_02: that it could have fallen apart and humble beginnings.
[02:04:28] But Joe, this is a great country.
[02:04:30] SPEAKER_01: And I'm an immigrant.
[02:04:32] SPEAKER_01: My parents sent my older brother and I here first.
[02:04:37] SPEAKER_01: We're in Thailand.
[02:04:40] I was born in Taiwan.
[02:04:41] SPEAKER_01: But my dad had a job in Thailand.
[02:04:44] SPEAKER_01: He was a chemical and instrumentation engineer,
[02:04:47] SPEAKER_01: incredible engineer.
[02:04:50] SPEAKER_01: And his job was to go start an oil refinery.
[02:04:53] SPEAKER_01: And so we moved to Thailand, lived in Bangkok.
[02:04:56] SPEAKER_01: And in 1973, 1974, time frame, you know,
[02:05:05] Thailand every so often, they would just have a coup.
[02:05:08] SPEAKER_01: The military would have an uprising.
[02:05:11] SPEAKER_01: And all of a sudden, one day, there
[02:05:13] SPEAKER_01: were tanks and soldiers in the streets.
[02:05:15] SPEAKER_01: And my parents thought, you know, probably
[02:05:17] SPEAKER_01: isn't safe for the kids to be here.
[02:05:19] SPEAKER_01: And so they contacted my uncle.
[02:05:22] SPEAKER_01: My uncle lives in Tacoma, Washington.
[02:05:25] SPEAKER_01: And we had never met him.
[02:05:28] SPEAKER_01: And my parents sent us to him.
[02:05:30] SPEAKER_01: How old were you?
[02:05:32] SPEAKER_01: I was about to turn nine.
[02:05:34] SPEAKER_01: And my older brother almost turned 11.
[02:05:38] SPEAKER_01: And so the two of us came to United States.
[02:05:42] SPEAKER_01: And we stayed with our uncle for a little bit
[02:05:45] SPEAKER_01: while he looked for a school for us.
[02:05:49] SPEAKER_01: And my parents didn't have very much money.
[02:05:52] SPEAKER_01: And they never had been to United States.
[02:05:53] SPEAKER_01: My father was, I'll tell you that story in a second.
[02:05:57] SPEAKER_01: And so my uncle found a school that would accept foreign students
[02:06:07] SPEAKER_01: and affordable enough for my parents.
[02:06:11] And that school turned out to have been in O'Neida, Kentucky,
[02:06:15] SPEAKER_01: Clark County, Kentucky, the epicenter of the OPO crisis
[02:06:20] SPEAKER_01: today, coal country.
[02:06:24] Clark County, Kentucky, was the poorest county in America
[02:06:30] SPEAKER_01: when I showed up.
[02:06:32] SPEAKER_01: It is the poorest county in America today.
[02:06:36] And so we went to the school.
[02:06:37] SPEAKER_01: It's a great school.
[02:06:40] SPEAKER_01: O'Neida Baptist Institute in a town of a few hundred.
[02:06:45] SPEAKER_01: I think it was 600 at the time that we showed up.
[02:06:49] No traffic light.
[02:06:51] And I think a 600 today.
[02:06:54] SPEAKER_01: It's kind of amazing feed, actually.
[02:06:59] The ability to hold your population for what
[02:07:02] it's 600 people is quite a magical thing.
[02:07:06] SPEAKER_01: However, they did it.
[02:07:08] SPEAKER_01: And so the school had a mission of being
[02:07:14] SPEAKER_01: in open school for any children who'd like to come.
[02:07:20] And what that basically means is that if you're a trouble
[02:07:24] SPEAKER_01: student, if you have a trouble family,
[02:07:29] SPEAKER_01: if you're whatever your background,
[02:07:35] SPEAKER_01: you're welcome to come to O'Neida Baptist Institute,
[02:07:39] SPEAKER_01: including kids from international who
[02:07:42] SPEAKER_01: would like to stay there.
[02:07:44] Did you speak English at the time?
[02:07:46] SPEAKER_01: OK.
[02:07:46] SPEAKER_01: Yeah.
[02:07:48] SPEAKER_01: OK.
[02:07:48] SPEAKER_01: Yeah.
[02:07:49] SPEAKER_01: And so we showed up.
[02:07:57] My first thought was, gosh, there
[02:08:01] SPEAKER_01: are a lot of cigarette butts on the ground.
[02:08:03] SPEAKER_01: 100% of the kids smoked.
[02:08:09] So right away, you know this is not a normal school.
[02:08:11] SPEAKER_01: Nine-year-old?
[02:08:12] SPEAKER_02: No.
[02:08:12] SPEAKER_02: I was the youngest kid.
[02:08:14] SPEAKER_01: OK.
[02:08:14] SPEAKER_01: 11-year-olds.
[02:08:15] SPEAKER_01: My roommate was 17 years old.
[02:08:18] Wow.
[02:08:19] SPEAKER_01: Yeah, he just turned 17.
[02:08:21] SPEAKER_01: And he was jacked.
[02:08:23] And I don't know where he is now.
[02:08:30] SPEAKER_01: I know his name, but I don't know where he is now.
[02:08:32] SPEAKER_01: But anyways, that night, we got, and the second thing I noticed
[02:08:37] SPEAKER_01: when you walk into your dorm room is there are no drawers
[02:08:43] SPEAKER_01: and no closet doors, just like a prison.
[02:08:50] And there are no locks so that people can check up on you.
[02:08:58] SPEAKER_01: And so I go into my room, and he's 17,
[02:09:02] SPEAKER_01: and get ready for bed.
[02:09:06] SPEAKER_01: And he had all this tape all over his body.
[02:09:10] SPEAKER_01: And turned out he wasn't a knife fight.
[02:09:14] And he's been stabbed all over his body
[02:09:17] SPEAKER_01: and these were just fresh ones.
[02:09:20] And the other kids were hurt much worse.
[02:09:24] And so he was my roommate, the toughest kid in school.
[02:09:28] SPEAKER_01: And I was the youngest kid in school.
[02:09:30] SPEAKER_01: It was a junior high.
[02:09:35] But they took me anyways.
[02:09:37] SPEAKER_01: Because if I walked about a mile across the Kentucky
[02:09:41] SPEAKER_01: River, the swing bridge, the other side
[02:09:45] SPEAKER_01: is a middle school that I could go to.
[02:09:48] SPEAKER_01: And then I can go to that school, and I come back,
[02:09:51] SPEAKER_01: and then I stayed in the dorm.
[02:09:53] SPEAKER_01: And so basically, O'Neill de Baptist Institute
[02:09:54] SPEAKER_01: was my dorm when I went to this other school.
[02:09:58] SPEAKER_01: My older brother went to the junior high.
[02:10:02] SPEAKER_01: And so we were there for a couple of years.
[02:10:05] SPEAKER_01: Every kid had chores.
[02:10:09] SPEAKER_01: My older brother's chore was to work in the tobacco farm.
[02:10:13] SPEAKER_01: So they raised tobacco so they could raise some extra money
[02:10:16] SPEAKER_01: for the school, kind of like a penitentiary.
[02:10:20] SPEAKER_01: And my job was just to clean the dorm.
[02:10:22] SPEAKER_01: And so I was nine years old.
[02:10:25] SPEAKER_01: I was cleaning toilets.
[02:10:26] SPEAKER_01: And for a dorm of 100 boys, I cleaned more bathrooms
[02:10:34] SPEAKER_01: than anybody.
[02:10:34] SPEAKER_01: And I just wish everybody was a little bit more careful.
[02:10:37] SPEAKER_01: I know.
[02:10:39] SPEAKER_01: But anyways, I was the youngest kid in school.
[02:10:45] SPEAKER_01: My memories of it was really good.
[02:10:48] SPEAKER_01: But it was a tough town.
[02:10:50] SPEAKER_01: Towns like.
[02:10:51] SPEAKER_02: Town kids, they all carried.
[02:10:52] SPEAKER_01: Everybody had knives.
[02:10:55] SPEAKER_01: Everybody had knives.
[02:10:57] SPEAKER_01: Everybody smoked.
[02:10:58] SPEAKER_01: Everybody had a zip-o lighter.
[02:11:00] I smoked for a week.
[02:11:01] SPEAKER_01: Did you?
[02:11:02] SPEAKER_01: Yeah.
[02:11:03] SPEAKER_01: I was nine.
[02:11:04] SPEAKER_01: Well, you're nine.
[02:11:05] SPEAKER_01: You're nine.
[02:11:06] SPEAKER_02: You tried smoking.
[02:11:07] SPEAKER_01: Yeah.
[02:11:08] SPEAKER_01: I got myself a packet of cigarettes.
[02:11:09] Everybody else did.
[02:11:10] Did you get sick?
[02:11:11] SPEAKER_01: No, I got used to it.
[02:11:13] SPEAKER_01: And at learn how to blow smoke rings.
[02:11:19] SPEAKER_01: You know, breathe out in my nose.
[02:11:21] SPEAKER_01: Take it in my nose.
[02:11:22] SPEAKER_01: There was a call.
[02:11:23] SPEAKER_01: All the different things that you learned.
[02:11:25] SPEAKER_01: Yeah.
[02:11:26] At nine.
[02:11:27] Yeah.
[02:11:28] SPEAKER_02: Why?
[02:11:29] SPEAKER_02: You just did it to fit in or look cool.
[02:11:30] SPEAKER_02: Yeah.
[02:11:31] SPEAKER_02: Because everybody else did it.
[02:11:32] SPEAKER_01: Right.
[02:11:33] SPEAKER_01: I did it for a couple of weeks, I guess.
[02:11:35] SPEAKER_01: And I just, rather have, I had a quarter, you know, out of quarter a month or something
[02:11:42] SPEAKER_01: like that.
[02:11:44] SPEAKER_01: I just, rather, by popsicles and, if I just sickles with it, I was nine, you know?
[02:11:48] SPEAKER_01: Right.
[02:11:49] SPEAKER_01: I chose the, the better path.
[02:11:52] Wow.
[02:11:53] That was our school.
[02:11:54] SPEAKER_01: And then my parents came to United States two years later.
[02:11:57] SPEAKER_01: And we met them in Tacoma, Washington.
[02:12:01] It was wild.
[02:12:02] It was a really crazy experience.
[02:12:04] SPEAKER_01: What a strange formative experience.
[02:12:07] SPEAKER_02: Yeah.
[02:12:08] SPEAKER_02: Tough kids.
[02:12:10] Thailand to one of the poorest places in America, where if not the poorest, as a nine year old.
[02:12:19] SPEAKER_02: Yeah, that's my first experience.
[02:12:20] First experience with your brother.
[02:12:21] SPEAKER_02: Wow.
[02:12:22] SPEAKER_02: Yeah.
[02:12:23] Yeah.
[02:12:24] SPEAKER_01: Now I used to, and what breaks my heart?
[02:12:27] SPEAKER_01: Probably the only thing that really breaks my heart about that experience was, so we didn't
[02:12:37] have enough money to make, you know, international phone calls every week.
[02:12:42] And so my parents gave us this tape deck.
[02:12:45] SPEAKER_01: This IWAT tape deck.
[02:12:48] And a tape.
[02:12:51] SPEAKER_01: And so every month we would sit in front of that tape deck and that my older brother, Jeff
[02:12:56] SPEAKER_01: and I, the two of us would just tell them what we did the whole month.
[02:13:05] SPEAKER_01: Wow.
[02:13:06] And we would send that tape by mail.
[02:13:09] And my parents would take that tape and record back on top of it and send it back to us.
[02:13:17] Could you imagine it for two years?
[02:13:20] Wow.
[02:13:21] SPEAKER_01: Is that tape still existed?
[02:13:23] Of these two kids just describing their first experience with United States.
[02:13:28] SPEAKER_01: Like I remember telling my parents that I joined the swim team.
[02:13:36] SPEAKER_01: And my roommate was really buff.
[02:13:41] SPEAKER_01: And so every day we spent a lot of time in the gym.
[02:13:45] SPEAKER_01: And so every night, 100 push ups, 100 sit ups, every day in the gym.
[02:13:50] SPEAKER_01: So I was nine years old.
[02:13:51] SPEAKER_01: I was getting, I was pretty buff.
[02:13:54] And I'm pretty fit.
[02:13:55] SPEAKER_01: And so I joined the soccer team.
[02:14:00] SPEAKER_01: I joined the swim team because if you join the team, they take you to meets.
[02:14:06] SPEAKER_01: And then afterwards you get to go to a nice restaurant.
[02:14:09] SPEAKER_01: And that nice restaurant was McDonald's.
[02:14:12] SPEAKER_01: And I recorded this thing.
[02:14:15] SPEAKER_01: I said, Mom and Dad, we went to the most amazing restaurant today.
[02:14:20] This whole place is lit up.
[02:14:22] SPEAKER_01: It's like the future.
[02:14:24] And the food comes in a box.
[02:14:31] The food is incredible.
[02:14:32] SPEAKER_01: The hamburger is incredible.
[02:14:33] SPEAKER_01: It's McDonald's.
[02:14:35] SPEAKER_01: But anyhow, it wouldn't be amazing.
[02:14:38] SPEAKER_01: Oh my God.
[02:14:39] SPEAKER_01: Two years.
[02:14:40] SPEAKER_01: Yeah.
[02:14:41] SPEAKER_01: Two years.
[02:14:42] Yeah.
[02:14:43] SPEAKER_02: What a crazy connection to your parents, too.
[02:14:45] SPEAKER_02: Just sending a tape and sending you in back.
[02:14:48] SPEAKER_02: And it's the only way you're communicating for two years.
[02:14:51] SPEAKER_01: Yeah.
[02:14:52] Wow.
[02:14:53] SPEAKER_02: Yeah.
[02:14:54] SPEAKER_01: Now my parents are incredible, actually.
[02:14:56] SPEAKER_01: They grew up really poor.
[02:15:01] SPEAKER_01: And when they came to United States, they had almost no money.
[02:15:06] SPEAKER_01: Probably one of the most impactful memories I have is they came and we were staying in
[02:15:13] SPEAKER_01: a apartment complex.
[02:15:16] SPEAKER_01: And they had just rent back in the, I guess, people still do rent a bunch of furniture.
[02:15:29] And we were messing around.
[02:15:36] And we bumped into the coffee table and crushed it.
[02:15:41] It was made out of particle wood.
[02:15:43] SPEAKER_01: We crushed it.
[02:15:46] And I just still remember the look of my mom's face, you know, because they didn't have
[02:15:51] SPEAKER_01: any money and she didn't know how she was going to pay it back.
[02:15:55] SPEAKER_01: But anyhow, that kind of tells you how hard it was for them to come here.
[02:15:59] SPEAKER_01: But they left everything behind and all they had was their suitcase and the money they
[02:16:04] SPEAKER_01: had in their pocket.
[02:16:06] SPEAKER_01: And they came to United States.
[02:16:07] SPEAKER_01: How old were you?
[02:16:08] SPEAKER_01: They pursued the arm, brink of dream.
[02:16:09] SPEAKER_01: They were in their 40s.
[02:16:10] SPEAKER_01: Wow.
[02:16:11] SPEAKER_01: Yeah, late 30s.
[02:16:12] SPEAKER_01: Pursuit the American dream.
[02:16:14] SPEAKER_01: This is the American dream.
[02:16:17] SPEAKER_01: I'm the first generation of the American dream.
[02:16:19] Wow.
[02:16:20] Yeah, it's hard not to love this country.
[02:16:23] SPEAKER_01: It's hard not to be romantic about this country.
[02:16:26] SPEAKER_02: That is a romantic story.
[02:16:27] SPEAKER_02: That's an amazing story.
[02:16:29] Yeah.
[02:16:30] SPEAKER_01: And my dad found his job literally in the newspaper, you know, the ads and he calls people,
[02:16:38] SPEAKER_01: got a job.
[02:16:39] SPEAKER_01: What did he do?
[02:16:40] SPEAKER_01: He was a consulting engineer and a consulting firm.
[02:16:44] SPEAKER_01: And they helped people build oil refineries, paper mills and fabs.
[02:16:50] SPEAKER_01: And that's what he did.
[02:16:51] SPEAKER_01: He was a really good at factory design, instrumentation engineer.
[02:16:57] SPEAKER_01: And so he's brilliant at that.
[02:17:00] SPEAKER_01: And so he did that.
[02:17:02] SPEAKER_01: And my mom worked as a maid and they found a way to raise us.
[02:17:08] Wow.
[02:17:10] That's an incredible story, Johnson.
[02:17:12] SPEAKER_02: It really is.
[02:17:13] SPEAKER_02: Everything, all of it.
[02:17:14] SPEAKER_02: From your childhood to the perils in video, almost falling.
[02:17:20] SPEAKER_02: It's really incredible, man.
[02:17:23] SPEAKER_01: It's a great story.
[02:17:24] SPEAKER_02: Yeah.
[02:17:25] SPEAKER_02: I've lived a great life.
[02:17:26] SPEAKER_02: You really have.
[02:17:27] SPEAKER_02: And it's a great story for other people to hear too.
[02:17:29] SPEAKER_02: It really is.
[02:17:30] SPEAKER_02: You don't have to go to Ivy League schools to succeed.
[02:17:36] This country creates opportunities.
[02:17:38] SPEAKER_01: Has opportunities for all of us.
[02:17:40] SPEAKER_01: You do have to strive.
[02:17:43] You have to claw your way here.
[02:17:46] But if you put in the work, you can succeed.
[02:17:49] SPEAKER_02: Nobody works hard.
[02:17:50] SPEAKER_02: There's a lot of luck and a lot of good decision making.
[02:17:54] SPEAKER_01: And the good graces of others.
[02:17:55] SPEAKER_01: Yes.
[02:17:56] That's really important.
[02:17:57] SPEAKER_02: Yeah.
[02:17:58] SPEAKER_01: You and I spoke about two people who are very dear to me.
[02:18:01] SPEAKER_01: But the list goes on.
[02:18:04] SPEAKER_01: The people that I've been in video who have helped me, many friends that are on the board,
[02:18:13] SPEAKER_01: the decisions, them giving me the opportunity.
[02:18:16] SPEAKER_01: When we were inventing this new computing approach, I tanked our stock price because we
[02:18:22] SPEAKER_01: added this thing called Kuda to the chip.
[02:18:24] SPEAKER_01: We had this big idea.
[02:18:25] SPEAKER_01: We added this thing called Kuda to the chip.
[02:18:27] SPEAKER_01: But nobody paid for it.
[02:18:29] SPEAKER_01: But our cost doubled.
[02:18:31] SPEAKER_01: And so we had this graphics chip company.
[02:18:34] And we invented GPUs.
[02:18:36] SPEAKER_01: We invented programmable shaders.
[02:18:38] SPEAKER_01: We invented everything modern computer graphics.
[02:18:42] We invented real-time ray tracing.
[02:18:44] SPEAKER_01: That's why it went from GTX to RTX.
[02:18:48] We invented all this stuff.
[02:18:49] SPEAKER_01: But every time we invented something, the market doesn't know how to appreciate it,
[02:18:55] SPEAKER_01: but the costs went way up.
[02:18:57] SPEAKER_01: And in the case of Kuda that enabled AI.
[02:19:00] The cost increased a lot.
[02:19:03] SPEAKER_01: But we really believed it.
[02:19:06] SPEAKER_01: And so if you believe in that future and you don't do anything about it, you're going
[02:19:10] SPEAKER_01: to regret it for your life.
[02:19:13] And so I always tell the team, do you believe this or not?
[02:19:18] SPEAKER_01: And if you believe it, and so grounded on first principles, not random, here say.
[02:19:24] And we believe it.
[02:19:26] SPEAKER_01: We owe it to ourselves to go pursue it.
[02:19:28] We're the right people to go do it.
[02:19:30] SPEAKER_01: If it's really, really hard to do, it's worth doing.
[02:19:33] SPEAKER_01: And we believe it.
[02:19:34] SPEAKER_01: Let's go pursue it.
[02:19:36] While we pursued it, we launched the product.
[02:19:39] SPEAKER_01: Nobody knew it was exactly what, like when I launched GTX1 and the entire audience was
[02:19:44] SPEAKER_01: like complete silence.
[02:19:47] When I launched Kuda, the audience was complete silence.
[02:19:51] SPEAKER_01: No customer wanted it.
[02:19:54] SPEAKER_01: Nobody asked for it.
[02:19:56] SPEAKER_01: We understood it.
[02:19:57] SPEAKER_01: And video was a public company.
[02:19:59] SPEAKER_01: What yours is?
[02:20:00] SPEAKER_01: This is a, let's see, 2000 and 2006, 20 years ago.
[02:20:09] 2005?
[02:20:12] Wow.
[02:20:14] Our stock prices went phew.
[02:20:18] I think our valuation went down to like two or three billion dollars from about 12 or
[02:20:25] SPEAKER_01: something like that.
[02:20:28] I crushed it in a very bad way.
[02:20:32] SPEAKER_02: Yeah.
[02:20:33] SPEAKER_01: What is it now though?
[02:20:34] SPEAKER_02: Yeah, it's higher.
[02:20:35] SPEAKER_02: It's very unbelievable.
[02:20:36] SPEAKER_01: It's higher.
[02:20:37] SPEAKER_02: But it changed the world.
[02:20:39] SPEAKER_02: Yeah.
[02:20:40] That invention changed the world.
[02:20:45] It's incredible story, Johnson.
[02:20:48] SPEAKER_02: It really is.
[02:20:50] Thank you.
[02:20:51] SPEAKER_02: Like your story, it's incredible.
[02:20:53] SPEAKER_02: My story is not as incredible.
[02:20:54] SPEAKER_02: My story is more weird.
[02:20:57] You know, it's much more fortuitous and weird.
[02:21:01] SPEAKER_02: Okay.
[02:21:02] SPEAKER_01: What are the three milestones that most important milestones that led to here?
[02:21:11] SPEAKER_02: That's a good question.
[02:21:12] SPEAKER_02: What was step one?
[02:21:13] SPEAKER_02: I think step one was seeing other people do it.
[02:21:18] Step one was in the initial days of podcasting.
[02:21:21] SPEAKER_02: Like in 2009 when I started podcasting and only been around for a couple of years.
[02:21:28] SPEAKER_02: The first was Adam Curry, my good friend who was the pod father.
[02:21:32] SPEAKER_02: He invented podcasting.
[02:21:34] SPEAKER_02: And then, you know, I remember Adam Corolla had a show because he had a radio show.
[02:21:39] SPEAKER_02: His radio show got canceled.
[02:21:41] And so he decided to just do the same show but do it on the internet.
[02:21:43] SPEAKER_02: And that was pretty revolutionary.
[02:21:44] SPEAKER_02: Nobody was doing that.
[02:21:46] SPEAKER_02: And then there was the experience that I had had doing different morning radio shows.
[02:21:51] SPEAKER_02: Like Opie and Anthony in particular because it was fun.
[02:21:55] SPEAKER_02: And we would just get together with a bunch of comedians.
[02:21:58] SPEAKER_02: You know, I'd be on the show with like three or four other guys that I knew.
[02:22:01] SPEAKER_02: And it was always just looked forward to it.
[02:22:03] SPEAKER_02: It was just such a good time.
[02:22:06] SPEAKER_02: And I said, God, I'm just doing that.
[02:22:07] SPEAKER_02: It's so fun to do that.
[02:22:08] SPEAKER_02: I wish I could do something like that.
[02:22:10] SPEAKER_02: And then I saw Tom Green set up.
[02:22:12] SPEAKER_02: Tom Green had a set up in his house.
[02:22:14] SPEAKER_02: And he essentially turned his entire house into a television studio.
[02:22:18] SPEAKER_02: And he did an internet show from his living room.
[02:22:21] SPEAKER_02: He had servers in his house and cables everywhere.
[02:22:23] SPEAKER_02: He had a step over cables.
[02:22:24] SPEAKER_02: This is like 2007.
[02:22:25] SPEAKER_02: I'm like, Tom, this is nuts.
[02:22:27] SPEAKER_02: Like this is...
[02:22:28] SPEAKER_02: And I'm like, you got to figure out a way to make money from this.
[02:22:30] SPEAKER_02: I wish everybody...
[02:22:31] SPEAKER_02: I wish everybody in internet could see your set up.
[02:22:34] SPEAKER_01: It's nuts.
[02:22:35] SPEAKER_01: I just want to let you guys know that.
[02:22:37] It's not just this.
[02:22:39] SPEAKER_02: So that was the beginning of it.
[02:22:41] SPEAKER_02: It was just seeing other people do it.
[02:22:43] SPEAKER_02: And then saying, all right, let's just try it.
[02:22:44] SPEAKER_02: And then so the beginning days, we just did it on a laptop.
[02:22:48] SPEAKER_02: We got a laptop with a webcam and just messed around.
[02:22:51] SPEAKER_02: Had a bunch of comedians come in.
[02:22:52] SPEAKER_02: We were just talking, joke around.
[02:22:55] SPEAKER_02: And I did it like once a week.
[02:22:56] SPEAKER_02: And then I started doing it twice a week.
[02:22:58] SPEAKER_02: And then all of a sudden I was doing it for a year.
[02:23:00] SPEAKER_02: And then I was doing it for two years.
[02:23:02] SPEAKER_02: And then I was like, oh, it's starting to get a lot of viewers.
[02:23:05] SPEAKER_02: A lot of listeners.
[02:23:06] SPEAKER_02: And then I just kept doing it.
[02:23:09] SPEAKER_02: It's all it is.
[02:23:10] SPEAKER_02: I just kept doing it because I enjoyed doing it.
[02:23:13] SPEAKER_02: Was there any setback?
[02:23:14] SPEAKER_02: No.
[02:23:15] No, there's never really a setback.
[02:23:17] SPEAKER_02: Really?
[02:23:18] SPEAKER_02: No, it must have been.
[02:23:19] SPEAKER_02: Or you saw the same guy going, you're just resilient.
[02:23:21] SPEAKER_01: Or you're just tough.
[02:23:23] SPEAKER_02: No, no, no, no.
[02:23:24] SPEAKER_02: Wasn't tough or hard.
[02:23:26] SPEAKER_02: It was just interesting.
[02:23:27] SPEAKER_02: So I just, the whole.
[02:23:29] SPEAKER_02: You were never once punched in the face.
[02:23:30] SPEAKER_02: No, not in the show.
[02:23:31] SPEAKER_02: No, not really.
[02:23:32] SPEAKER_02: Not doing the show.
[02:23:33] SPEAKER_02: You never did something that big blowback.
[02:23:38] SPEAKER_02: Nope.
[02:23:39] Not really.
[02:23:41] No, it all just kept growing.
[02:23:43] SPEAKER_02: It kept growing.
[02:23:44] SPEAKER_02: And the things stayed the same from the beginning to now.
[02:23:47] SPEAKER_02: And the thing is, I enjoy talking to people.
[02:23:50] SPEAKER_02: I've always enjoyed talking to interest people.
[02:23:52] SPEAKER_02: I could even tell just when we walked in, the way you
[02:23:54] SPEAKER_01: interacted with everybody, not just me.
[02:23:57] Yeah.
[02:23:57] That's cool.
[02:23:58] SPEAKER_01: People cool.
[02:23:59] SPEAKER_01: Yeah, that's cool.
[02:24:00] SPEAKER_01: You know, I, it's an amazing gift to be
[02:24:05] SPEAKER_02: able to have so many conversations with so many interesting
[02:24:07] SPEAKER_02: people because if it changes the way you see the world,
[02:24:10] SPEAKER_02: because you see the world through so many different people's
[02:24:13] SPEAKER_02: eyes.
[02:24:14] SPEAKER_02: And you have so many different people
[02:24:15] SPEAKER_02: of different perspectives and different opinions and different
[02:24:18] SPEAKER_02: philosophies and different life stories.
[02:24:21] SPEAKER_02: And you know, it's an incredibly enriching and educating
[02:24:26] SPEAKER_02: experience, having so many conversations
[02:24:30] SPEAKER_02: with so many amazing people.
[02:24:32] And that's all I started doing.
[02:24:35] SPEAKER_02: And that's all I do now.
[02:24:37] SPEAKER_02: Even now, when I book the show, I do it on my phone.
[02:24:40] SPEAKER_02: And I basically go through this giant list of emails
[02:24:44] SPEAKER_02: of all the people that want to be on the show or the
[02:24:46] SPEAKER_02: requested to be on the show.
[02:24:48] SPEAKER_02: And then a factor in another list that I have of people that I
[02:24:51] SPEAKER_02: would like to get on the show that I'm interested in.
[02:24:53] SPEAKER_02: And I just map it out.
[02:24:55] SPEAKER_02: And that's it.
[02:24:56] SPEAKER_02: And I go, oh, I'd like to talk to him.
[02:24:57] SPEAKER_02: If it wasn't because of President Trump, I wouldn't have
[02:24:59] SPEAKER_01: been bumped up on that list.
[02:25:01] Yeah, I wanted to talk to you already.
[02:25:03] SPEAKER_02: I just think, you know, what you're doing is very fascinating.
[02:25:07] SPEAKER_02: I mean, how would I not want to talk to you?
[02:25:08] SPEAKER_02: And then today, you proved to be absolutely the right
[02:25:11] SPEAKER_02: decision.
[02:25:12] SPEAKER_02: Well, you know, listen, it's strange to be an immigrant one
[02:25:16] SPEAKER_01: day going to O'Neill Baptist Institute with the students
[02:25:22] SPEAKER_01: that were there.
[02:25:24] SPEAKER_01: And then here, Nvidia is one of the most consequential
[02:25:28] SPEAKER_01: companies in the history of companies.
[02:25:32] SPEAKER_01: It is a crazy story.
[02:25:34] SPEAKER_01: It has to be a crazier story.
[02:25:35] SPEAKER_01: Yeah, that journey is.
[02:25:37] SPEAKER_01: And it's very humbling.
[02:25:39] SPEAKER_01: And I'm very grateful.
[02:25:41] SPEAKER_01: It's pretty amazing.
[02:25:42] SPEAKER_01: Surrounded by amazing people.
[02:25:44] You're very fortunate.
[02:25:45] SPEAKER_02: And you've also, you seem very happy.
[02:25:47] SPEAKER_02: And you seem like you're 100% on the right path in this life.
[02:25:50] SPEAKER_02: You know, you, you know, everybody says, you must love your job.
[02:25:54] SPEAKER_01: Not every day.
[02:25:56] SPEAKER_02: That's not hot.
[02:25:57] SPEAKER_02: That's the sort of beauty of everything.
[02:25:59] SPEAKER_02: Yeah.
[02:26:00] SPEAKER_02: Because there's ups and downs.
[02:26:01] SPEAKER_02: That's right.
[02:26:01] SPEAKER_02: It's never just like this giant dopamine high.
[02:26:03] SPEAKER_02: We leave, we leave this impression.
[02:26:06] Here's, here's an impression I don't think is healthy.
[02:26:08] SPEAKER_01: We, people who are successful leave the impression often
[02:26:13] SPEAKER_01: that that our job gives us great joy.
[02:26:19] SPEAKER_01: I think largely it does.
[02:26:22] SPEAKER_01: That our jobs were passionate about our work.
[02:26:27] And that passion relates to, it's just so much fun.
[02:26:32] I think it largely is.
[02:26:34] But it, it, it distracts from.
[02:26:37] SPEAKER_01: In fact, a lot of success comes from really, really hard work.
[02:26:42] SPEAKER_01: Yes.
[02:26:43] There's long periods of suffering and loneliness and uncertainty
[02:26:51] SPEAKER_01: and fear and embarrassment and humiliation.
[02:26:56] SPEAKER_01: All of the feelings that we most not love,
[02:27:02] SPEAKER_01: that creating something from the ground up
[02:27:06] SPEAKER_01: and, and Elon will tell you something similar.
[02:27:09] Very difficult to invent, invent something new.
[02:27:12] SPEAKER_01: And people, people don't believe you all the time.
[02:27:15] SPEAKER_01: You're humiliated often, disbelieved, most of the time.
[02:27:20] And so, so people forget that part of success.
[02:27:24] SPEAKER_01: And, and I, I don't think it's health.
[02:27:26] SPEAKER_01: I think it's, it's good that we pass that forward
[02:27:29] SPEAKER_01: and let people know that, that it's just part of the journey.
[02:27:32] SPEAKER_01: Yes.
[02:27:33] SPEAKER_02: And then, it's suffering as part of the journey.
[02:27:35] SPEAKER_02: You will appreciate it.
[02:27:36] SPEAKER_02: It's so much, these horrible feelings that you have
[02:27:38] SPEAKER_02: when things are not going so well.
[02:27:40] SPEAKER_02: You will appreciate it so much more when they do go well.
[02:27:43] SPEAKER_02: Deeply grateful.
[02:27:44] SPEAKER_01: Yeah.
[02:27:45] SPEAKER_01: Yeah.
[02:27:45] SPEAKER_01: Deep, deep pride.
[02:27:47] SPEAKER_01: Incredible pride.
[02:27:49] SPEAKER_01: Incredible, incredible gratefulness.
[02:27:51] SPEAKER_01: And, and, and surely incredible memories.
[02:27:54] Absolutely.
[02:27:55] SPEAKER_02: Johnson, thank you so much for being here.
[02:27:57] SPEAKER_02: Those are really fun.
[02:27:58] SPEAKER_02: I really enjoyed it.
[02:27:59] SPEAKER_02: and your story is just absolutely incredible
[02:28:02] SPEAKER_02: and very inspirational.
[02:28:03] SPEAKER_02: And I think it really is the American Dream.
[02:28:08] SPEAKER_02: It is the American Dream.
[02:28:08] SPEAKER_02: It really is.
[02:28:09] SPEAKER_02: Thank you so much.
[02:28:10] SPEAKER_02: Thank you.
[02:28:11] SPEAKER_02: Bye, everybody.