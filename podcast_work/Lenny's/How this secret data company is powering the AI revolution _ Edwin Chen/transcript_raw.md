# How this secret data company is powering the AI revolution | Edwin Chen

**Podcast:** Lenny's
**Date:** 2025-12-07
**Video ID:** dduQeaqmpnI
**Video URL:** https://www.youtube.com/watch?v=dduQeaqmpnI

---

[00:00:00] You guys hit a billion in revenue in less than four years
[00:00:03] SPEAKER_00: with around 60 to 70 people.
[00:00:05] SPEAKER_00: You were completely bootstrapped, haven't raised any VC money.
[00:00:08] SPEAKER_00: I don't believe anyone has ever done this before.
[00:00:10] SPEAKER_00: We basically never wanted to play this at a convali game.
[00:00:13] SPEAKER_00: I always started zardic lists.
[00:00:14] SPEAKER_01: I used to work in a bunch of big tech companies
[00:00:16] SPEAKER_01: and I always felt that we could fire 90% of people
[00:00:18] SPEAKER_01: and we would move faster
[00:00:19] SPEAKER_01: because the best people would have all these instructions.
[00:00:21] SPEAKER_01: So when we started to surge,
[00:00:22] SPEAKER_01: we wanted to build it completely differently
[00:00:24] SPEAKER_01: with a super small, super early team.
[00:00:26] SPEAKER_01: You guys are by far the most successful data company out there.
[00:00:29] SPEAKER_01: We essentially teach AI models what's good and what's bad.
[00:00:32] SPEAKER_01: People don't understand what quality even means in the space.
[00:00:35] SPEAKER_01: They think you could just throw bodies at a problem
[00:00:37] SPEAKER_01: and get good data, that's completely wrong.
[00:00:39] SPEAKER_01: To a regular person, it doesn't feel like these models
[00:00:41] SPEAKER_00: are getting that much smarter constantly.
[00:00:43] SPEAKER_00: Over the past year, I've realized that the values
[00:00:46] SPEAKER_01: that the companies have will shape the model.
[00:00:48] SPEAKER_01: I was asking Cod to help me drop the email the other day.
[00:00:51] SPEAKER_01: And after 30 minutes, yeah,
[00:00:52] SPEAKER_01: I think it really crafted me the perfect email
[00:00:54] SPEAKER_01: and I sent it.
[00:00:54] SPEAKER_01: But then I realized I spent 30 minutes
[00:00:56] SPEAKER_01: doing something that didn't matter at all.
[00:00:58] SPEAKER_01: If you could choose the perfect model behavior,
[00:01:00] SPEAKER_01: which model would you want?
[00:01:02] SPEAKER_01: Do you want a model that says,
[00:01:03] SPEAKER_01: you're absolutely right.
[00:01:03] SPEAKER_01: There are definitely 20 more ways to improve this email
[00:01:05] SPEAKER_01: and it continues for 50 more iterations.
[00:01:07] SPEAKER_01: Or do you want a model that's optimizing for your time
[00:01:10] SPEAKER_01: and productivity and just says no, you need to stop.
[00:01:12] SPEAKER_01: You're the most great.
[00:01:13] SPEAKER_01: Just send it and move on.
[00:01:14] SPEAKER_00: You have this hot take that a lot of these labs
[00:01:16] SPEAKER_00: are pushing a giant in the wrong direction.
[00:01:18] SPEAKER_00: I'm worried that instead of building AI,
[00:01:20] SPEAKER_01: that will actually advance us as a species,
[00:01:22] SPEAKER_01: curing cancer, solving poverty, understanding universe,
[00:01:24] SPEAKER_01: we are optimizing for AI stop instead.
[00:01:26] SPEAKER_01: Should we optimizing our models forward
[00:01:28] SPEAKER_01: to types of people who buy tablets at the grocery store
[00:01:30] SPEAKER_01: where basic teacher models to chase dopamine and sedatures?
[00:01:35] Today, my guest is Edwin Chan,
[00:01:37] SPEAKER_00: founder and CEO of Surge AI.
[00:01:39] SPEAKER_00: Edwin is an extraordinary CEO
[00:01:41] SPEAKER_00: and Surge is an extraordinary company.
[00:01:44] SPEAKER_00: They're the leading AI data company,
[00:01:46] SPEAKER_00: powering training at every frontier AI lab.
[00:01:49] SPEAKER_00: They are also the fastest company
[00:01:51] SPEAKER_00: to ever hit $1 billion in revenue
[00:01:54] SPEAKER_00: in just four years after launch
[00:01:56] SPEAKER_00: with fewer than 100 people
[00:01:58] SPEAKER_00: and also completely bootstrapped.
[00:02:00] SPEAKER_00: They've never raised a dollar in VC money.
[00:02:03] SPEAKER_00: They've also been profitable from day one.
[00:02:05] SPEAKER_00: As you'll hear in this conversation,
[00:02:06] SPEAKER_00: Edwin has a very different take
[00:02:09] SPEAKER_00: on how to build an important company
[00:02:11] SPEAKER_00: and how to build AI that is truly good
[00:02:14] SPEAKER_00: and useful to humanity.
[00:02:15] SPEAKER_00: I absolutely love this conversation
[00:02:18] SPEAKER_00: and I learned a ton.
[00:02:19] SPEAKER_00: I am really excited for you to hear it.
[00:02:21] SPEAKER_00: If you enjoyed this podcast,
[00:02:22] SPEAKER_00: don't forget to subscribe and follow it
[00:02:24] SPEAKER_00: in your favorite podcasting app or YouTube.
[00:02:26] SPEAKER_00: It helps tremendously.
[00:02:27] SPEAKER_00: And if you become an annual subscriber of my newsletter,
[00:02:30] SPEAKER_00: you get a ton of incredible products.
[00:02:33] SPEAKER_00: For free, for an entire year,
[00:02:35] SPEAKER_00: including Devon Lovable, Replay, Bolt, NADM,
[00:02:38] SPEAKER_00: linear superhuman D-Script, whisper flow, gamma,
[00:02:41] SPEAKER_00: perplexity, warp, granola, magic patterns,
[00:02:42] SPEAKER_00: rick-catch, rprd, mobbin, post-hog, M-stripe, atlas.
[00:02:45] SPEAKER_00: Head on over to Lenny's newsletter.com
[00:02:47] SPEAKER_00: and click product pass.
[00:02:48] SPEAKER_00: With that, I bring you Edwin Chan
[00:02:50] SPEAKER_00: after a short word from our sponsors.
[00:02:53] My podcast guests tonight love talking about craft
[00:02:56] SPEAKER_00: and taste and agency and product market fit.
[00:02:59] SPEAKER_00: You know what we don't love talking about?
[00:03:01] SPEAKER_00: Sock 2.
[00:03:02] SPEAKER_00: That's where Vanta comes in.
[00:03:04] SPEAKER_00: Vanta helps companies of all sizes get compliant fast
[00:03:07] SPEAKER_00: and stay that way with industry leading AI, automation
[00:03:10] SPEAKER_00: and continuous monitoring.
[00:03:11] SPEAKER_00: Whether you are a startup tackling your first Sock 2
[00:03:14] SPEAKER_00: or ISO 27001, or an enterprise managing vendor risk,
[00:03:18] SPEAKER_00: Vanta's trust management platform
[00:03:19] SPEAKER_00: makes it quicker, easier, and more scalable.
[00:03:22] SPEAKER_00: Vanta also helps you complete security questionnaires
[00:03:25] SPEAKER_00: up to five times faster so that you could win bigger deals
[00:03:28] SPEAKER_00: sooner.
[00:03:29] SPEAKER_00: The result, according to a recent IDC study,
[00:03:32] SPEAKER_00: Vanta customers slashed over $500,000 a year
[00:03:36] SPEAKER_00: and are three times more productive.
[00:03:38] SPEAKER_00: Establishing trust isn't optional.
[00:03:40] SPEAKER_00: Vanta makes it automatic.
[00:03:43] SPEAKER_00: Get $1,000 off at vanta.com slash Lenny.
[00:03:48] Here's a puzzle for you.
[00:03:49] SPEAKER_00: What do open AI, cursor, perplexity,
[00:03:51] SPEAKER_00: versel, plat, and hundreds of other winning companies
[00:03:54] SPEAKER_00: haven't common?
[00:03:56] SPEAKER_00: The answer is they're all powered by today's sponsor, WorkOS.
[00:04:00] SPEAKER_00: If you're building software for enterprises,
[00:04:02] SPEAKER_00: you've probably felt the pain of integrating single sign
[00:04:04] SPEAKER_00: on, skim, R-back, audit logs, and other features
[00:04:08] SPEAKER_00: required by big customers.
[00:04:09] SPEAKER_00: WorkOS turns those deal blockers into drop-in APIs
[00:04:13] SPEAKER_00: with a modern developer platform built specifically
[00:04:15] SPEAKER_00: for B2B SaaS.
[00:04:17] SPEAKER_00: Whether you're a seed stage startup
[00:04:18] SPEAKER_00: trying to land your first enterprise customer,
[00:04:20] SPEAKER_00: or a unicorn expanding globally, WorkOS is the fastest path
[00:04:24] SPEAKER_00: to becoming enterprise ready and unlocking growth.
[00:04:27] SPEAKER_00: They're essentially striped for enterprise features.
[00:04:30] SPEAKER_00: Visit WorkOS.com to get started, or just
[00:04:32] SPEAKER_00: hit up their Slack support where they have real engineers
[00:04:34] SPEAKER_00: in there who answer your questions super fast.
[00:04:37] SPEAKER_00: WorkOS allows you to build like the best
[00:04:39] SPEAKER_00: with delightful APIs, comprehensive docs,
[00:04:42] SPEAKER_00: and a smooth developer experience.
[00:04:44] SPEAKER_00: Go to WorkOS.com to make your app enterprise ready today.
[00:04:48] SPEAKER_00: Thank you so much for being here and welcome to the podcast.
[00:04:55] SPEAKER_00: Thanks so much for having me.
[00:04:56] SPEAKER_00: I'm super excited.
[00:04:58] SPEAKER_00: I want to start with just how absurd what you've achieved is.
[00:05:02] SPEAKER_00: A lot of people and a lot of companies
[00:05:03] SPEAKER_00: talk about scaling massive businesses
[00:05:05] SPEAKER_00: with very few people as a result of AI.
[00:05:08] SPEAKER_00: And you guys have done this in a way that is unprecedented.
[00:05:12] SPEAKER_00: You guys hit a billion in revenue in less than four years,
[00:05:16] SPEAKER_00: with less than 60, around 60 to 70 people,
[00:05:19] SPEAKER_00: you're completely bootstrapped, haven't raised any VC money.
[00:05:24] I don't believe anyone has ever done this before.
[00:05:26] SPEAKER_00: So you guys are actually achieving the dream
[00:05:28] SPEAKER_00: of what people are describing will happen with AI.
[00:05:30] SPEAKER_00: I'm curious just do you think this will happen more and more
[00:05:34] SPEAKER_00: as a result of AI?
[00:05:35] SPEAKER_00: And also just where has AI most help you find leverage
[00:05:39] SPEAKER_00: to be able to do this?
[00:05:40] SPEAKER_00: Yeah, so we hit over a billion if revenue last year
[00:05:42] SPEAKER_01: with under 100 people.
[00:05:44] SPEAKER_01: And I think we're going to see companies
[00:05:46] SPEAKER_01: with even crazier ratios, like 100 billion per employee
[00:05:49] SPEAKER_01: in the next few years.
[00:05:50] SPEAKER_01: AI is just going to get better and better
[00:05:52] SPEAKER_01: and make things more efficient.
[00:05:53] SPEAKER_01: So that ratio just becomes inevitable.
[00:05:56] SPEAKER_01: I used to work on a bunch of the big tech companies.
[00:05:59] SPEAKER_01: And I always felt that we could fire 9% of people,
[00:06:01] SPEAKER_01: and we would move faster because the best people
[00:06:03] SPEAKER_01: would have all these distractions.
[00:06:05] SPEAKER_01: And so when we started to surge, we
[00:06:07] SPEAKER_01: wanted to build it completely differently
[00:06:09] SPEAKER_01: with a super small, super elite team.
[00:06:11] SPEAKER_01: And yeah, what's crazy is that we actually succeeded.
[00:06:15] And so I think two things are colliding.
[00:06:18] SPEAKER_01: One is that people are realizing
[00:06:20] SPEAKER_01: that you don't have to build giant organizations
[00:06:22] SPEAKER_01: in order to win.
[00:06:23] SPEAKER_01: And two, yeah, all these efficiencies are in AI.
[00:06:26] And they're just going to lead
[00:06:27] SPEAKER_01: to a really amazing time in company building.
[00:06:29] SPEAKER_01: Like the thing I'm most excited about
[00:06:30] SPEAKER_01: is that the types of companies are going to change to.
[00:06:33] SPEAKER_01: It won't just be that they're smaller.
[00:06:36] SPEAKER_01: We're going to see fundamentally different companies
[00:06:37] SPEAKER_01: emerging.
[00:06:38] SPEAKER_01: Now, you think about it.
[00:06:39] SPEAKER_01: If your employees means less capital,
[00:06:41] SPEAKER_01: less capital means you don't need a raise.
[00:06:44] So instead of companies started by founders who
[00:06:46] SPEAKER_01: are great at pitching and great at hyping,
[00:06:47] SPEAKER_01: you will get founders who are really great at technology
[00:06:50] SPEAKER_01: of product.
[00:06:51] SPEAKER_01: And instead of products optimized for revenue
[00:06:53] SPEAKER_01: and what VCs want to see, you'll get more interesting ones
[00:06:56] SPEAKER_01: built by these tiny, obsessed teams.
[00:06:58] SPEAKER_01: So people building things they actually care about,
[00:07:00] SPEAKER_01: real real technology of real innovation.
[00:07:03] So I'm actually really hoping that this look on Valdi Sarp
[00:07:05] SPEAKER_01: team will actually go back to being
[00:07:07] SPEAKER_01: a place for hackers again.
[00:07:08] SPEAKER_00: You guys have done a lot of things in a very contrarian way.
[00:07:11] SPEAKER_00: And one was actually just not being on LinkedIn, posting
[00:07:15] SPEAKER_00: viral posts, not on Twitter, constantly promoting search.
[00:07:18] SPEAKER_00: I think most people haven't heard of search until just
[00:07:21] SPEAKER_00: recently.
[00:07:21] SPEAKER_00: And then you just came out.
[00:07:22] SPEAKER_00: I'm like, OK, the fast growing company
[00:07:23] SPEAKER_00: took a billion dollars.
[00:07:25] SPEAKER_00: Why would you do that?
[00:07:25] SPEAKER_00: Imagine those very intentional.
[00:07:27] SPEAKER_00: We basically never wanted to play this in a color valley game.
[00:07:31] And I always started to ridiculous.
[00:07:33] SPEAKER_01: What did you dream of doing when you were a kid?
[00:07:35] SPEAKER_01: Was it building a company from scratch yourself
[00:07:37] SPEAKER_01: and getting in a weeds of your code in your product every day?
[00:07:40] SPEAKER_01: Or was it explaining all your decisions to VCs
[00:07:43] SPEAKER_01: and getting on this giant PR and fundraising hamster wheel?
[00:07:47] And it definitely made things more difficult for us.
[00:07:50] SPEAKER_01: Because when you front raise, you just
[00:07:53] SPEAKER_01: naturally get part of this kind of Silicon Valley
[00:07:57] SPEAKER_01: industrial complex where people will, your VCs will
[00:07:59] SPEAKER_01: tweet about you.
[00:08:00] SPEAKER_01: You'll get the Tecranshot lines.
[00:08:01] SPEAKER_01: You'll get announced in all the new papers
[00:08:04] SPEAKER_01: because you raise a dismissive valuation.
[00:08:07] And so it made things more difficult.
[00:08:09] SPEAKER_01: Because the only way we were going to succeed
[00:08:11] SPEAKER_01: was by building a 10 times better product
[00:08:13] SPEAKER_01: and getting word of mouth from researchers.
[00:08:16] But I think it also meant that our customers
[00:08:17] SPEAKER_01: were people who really understood data
[00:08:19] SPEAKER_01: and really cared about it.
[00:08:21] I always thought it was really important for us
[00:08:22] SPEAKER_01: to have customers, early customers, who
[00:08:24] SPEAKER_01: were really aligned with what we were building
[00:08:27] SPEAKER_01: and who really cared about having really high quality data
[00:08:29] SPEAKER_01: and really understood how that data would make their AI models
[00:08:33] SPEAKER_01: so much better.
[00:08:34] SPEAKER_01: Because they were the ones helping us.
[00:08:36] SPEAKER_01: They were giving us feedback on what we're producing.
[00:08:38] SPEAKER_01: And so just having that kind of very, very close mission
[00:08:41] SPEAKER_01: alignment with our customers actually helped us early on.
[00:08:44] SPEAKER_01: So these are people who basically just buying our product
[00:08:46] SPEAKER_01: because they knew how different it was
[00:08:48] SPEAKER_01: and because it was helping them rather than because they
[00:08:51] SPEAKER_01: saw so far in Tecranshot line.
[00:08:52] SPEAKER_01: So it made things harder for us, but I think
[00:08:54] SPEAKER_01: I did a really good way.
[00:08:55] SPEAKER_01: It's such an empowering story to hear this journey
[00:08:58] SPEAKER_00: for founders that they don't need to be on Twitter all day
[00:09:02] SPEAKER_00: promoting what they're doing.
[00:09:03] SPEAKER_00: They don't have to raise money.
[00:09:04] SPEAKER_00: They can just kind of go heads down and build.
[00:09:06] SPEAKER_00: So I love so much about the story of search.
[00:09:10] SPEAKER_00: For people that don't know what search does,
[00:09:12] SPEAKER_00: just to give us a quick explanation of what search is.
[00:09:16] SPEAKER_00: We essentially teach AI models what's good and what's bad.
[00:09:20] SPEAKER_01: So we train them using human data and just
[00:09:25] SPEAKER_01: a lot of different products that we have.
[00:09:26] SPEAKER_01: Like SET, RRF, Rubrics Verifiers, RR environments,
[00:09:30] SPEAKER_01: and so on and so on.
[00:09:31] SPEAKER_01: And then we also measure how well they're progressing.
[00:09:34] SPEAKER_01: So essentially we're a data company.
[00:09:36] SPEAKER_01: What you always talk about is the quality
[00:09:38] SPEAKER_00: has been the big reason you guys have been so successful,
[00:09:40] SPEAKER_00: the quality of the data.
[00:09:42] What does it take to create higher quality data?
[00:09:44] SPEAKER_00: What do you all do differently?
[00:09:45] SPEAKER_00: What are people missing?
[00:09:47] SPEAKER_00: I think most people don't understand
[00:09:49] SPEAKER_01: what quality even means in this space.
[00:09:51] They think you could just throw bodies at a problem
[00:09:53] SPEAKER_01: and get good data and that's completely wrong.
[00:09:57] Let me give you an example.
[00:09:59] SPEAKER_01: So imagine you wanted to train a model to write an AI
[00:10:01] SPEAKER_01: and pull them up the moon.
[00:10:03] SPEAKER_01: What makes it a good high quality poem?
[00:10:05] If you don't think deeply about quality,
[00:10:07] SPEAKER_01: you'll be like, is this a poem?
[00:10:09] SPEAKER_01: Does it contain eight lines?
[00:10:11] SPEAKER_01: Does it contain a word moon?
[00:10:12] SPEAKER_01: You check all of these boxes and it's so sure.
[00:10:14] SPEAKER_01: Yeah, you say it's a good poem.
[00:10:16] But that's completely different from what we want.
[00:10:19] SPEAKER_01: We are looking for a Nobel Prize winning poetry.
[00:10:21] SPEAKER_01: Like is this poetry unique?
[00:10:23] SPEAKER_01: Is it full of subtle imagery?
[00:10:25] SPEAKER_01: Does it surprise you and talk about your heart?
[00:10:26] SPEAKER_01: Does it teach you something about the nature of moonlight?
[00:10:28] SPEAKER_01: Does it play with your emotions
[00:10:29] SPEAKER_01: and does it make you think?
[00:10:30] SPEAKER_01: That's what we are thinking about when
[00:10:32] SPEAKER_01: we think about high quality poem.
[00:10:34] SPEAKER_01: So it might be like a high coup about moonlight on water.
[00:10:37] SPEAKER_01: It might use internal rhyming meter.
[00:10:38] SPEAKER_01: There are 1,000 ways to write a poem out the moon.
[00:10:40] SPEAKER_01: And in each one gives you all these different insights
[00:10:43] SPEAKER_01: into language and imagery and human expression.
[00:10:46] And I think thinking about quality in its way
[00:10:48] SPEAKER_01: is really hard.
[00:10:49] SPEAKER_01: It's hard to measure.
[00:10:50] SPEAKER_01: It's really subjective and complex and rich.
[00:10:52] SPEAKER_01: And it's that's a really high bar.
[00:10:53] SPEAKER_01: And so we have to build all of this technology
[00:10:55] SPEAKER_01: in order to measure it.
[00:10:56] SPEAKER_01: Like thousands of signals on all of our workers,
[00:10:59] SPEAKER_01: thousands of signals on every card of every task.
[00:11:01] SPEAKER_01: We know at the end of day, if you are good at writing poetry
[00:11:04] SPEAKER_01: versus good at writing essays,
[00:11:06] SPEAKER_01: versus great at writing technical documentation.
[00:11:09] SPEAKER_01: And so we have to gather all these signals
[00:11:11] SPEAKER_01: on what your background is, what your expertise is,
[00:11:14] SPEAKER_01: and not just that, how you're actually performing
[00:11:16] SPEAKER_01: when you're writing all these things.
[00:11:18] SPEAKER_01: And we use those signals to inform whether or not
[00:11:22] SPEAKER_01: you are good and forker of already
[00:11:23] SPEAKER_01: projects and whether or not you are improving the models.
[00:11:26] And it's really hard.
[00:11:27] SPEAKER_01: And so it will just acknowledge and measure it.
[00:11:28] SPEAKER_01: But I think that's exactly what we want AI to do.
[00:11:31] SPEAKER_01: And so we have these really, really deep notions
[00:11:34] SPEAKER_01: of a quality that we're always trying to use.
[00:11:37] SPEAKER_00: So what I'm hearing is there's kind of a just going much deeper
[00:11:40] SPEAKER_00: in understanding what quality is within the verticals
[00:11:44] SPEAKER_00: that you are selling data around.
[00:11:45] SPEAKER_00: So you, and is this like a person you hire
[00:11:48] SPEAKER_00: that is incredibly talented at poetry?
[00:11:50] SPEAKER_00: Plus evals that they, I guess, help write,
[00:11:54] SPEAKER_00: that tell them that this is great at what's
[00:11:55] SPEAKER_00: like the mechanics of that.
[00:11:57] SPEAKER_00: So what works is we essentially gather thousands
[00:12:02] SPEAKER_01: of the signals about everything that you're doing
[00:12:04] SPEAKER_01: when you're working on platform.
[00:12:06] SPEAKER_01: So we are looking at your keyboard strokes.
[00:12:09] SPEAKER_01: We are looking how fast you answer things.
[00:12:12] SPEAKER_01: We are using reviews.
[00:12:13] SPEAKER_01: We are using code standards.
[00:12:15] SPEAKER_01: We are using like, we're training models or selves
[00:12:18] SPEAKER_01: on the outputs that you create.
[00:12:20] SPEAKER_01: And we're seeing whether they improve the models performance.
[00:12:23] SPEAKER_01: And so in a very similar way to how Google search,
[00:12:27] SPEAKER_01: like when Google search is trying to determine what
[00:12:29] SPEAKER_01: is good web page, there's almost two aspects of it.
[00:12:31] SPEAKER_01: One is you want to remove all the worst to worst web pages.
[00:12:35] SPEAKER_01: So you want to remove all the spam, all the low quality
[00:12:38] SPEAKER_01: content, all the pages that don't load.
[00:12:40] SPEAKER_01: And so it's almost like a condom moderation problem.
[00:12:42] SPEAKER_01: It's one of remove to worst worst.
[00:12:44] But then you also want to discover the best of the best.
[00:12:47] SPEAKER_01: OK, like this is the best web page,
[00:12:49] SPEAKER_01: or just the best person for the job.
[00:12:51] SPEAKER_01: They are not just somebody who writes
[00:12:54] SPEAKER_01: the equivalent of high school level poetry.
[00:12:56] SPEAKER_01: Again, they're not just for bulky writing poetry.
[00:12:58] SPEAKER_01: That checks all these boxes, checks
[00:12:59] SPEAKER_01: all these explicit instructions.
[00:13:01] SPEAKER_01: But rather, yeah, they're a different
[00:13:02] SPEAKER_01: poetry that makes you emotional.
[00:13:03] SPEAKER_01: And so we have all these signals as well, that again,
[00:13:06] SPEAKER_01: completely differently from moving to worst to worst,
[00:13:08] SPEAKER_01: we are finding the best of the best.
[00:13:10] SPEAKER_01: And so we have all these signals again.
[00:13:12] SPEAKER_01: And just like Google search uses all these signals
[00:13:13] SPEAKER_01: that feeds them into their ML algorithms
[00:13:16] SPEAKER_01: and uses them predict certain types of things.
[00:13:19] SPEAKER_01: We do the same with all of our workers
[00:13:21] SPEAKER_01: and all of our task and all of our projects.
[00:13:23] SPEAKER_01: And so it's almost like a complicated machine learning
[00:13:25] SPEAKER_01: problem at the end of the day.
[00:13:27] And that's what's the hour works.
[00:13:29] SPEAKER_01: It is incredibly interesting.
[00:13:31] SPEAKER_00: And when I ask you about something,
[00:13:32] SPEAKER_00: I've been very curious about over the past couple years.
[00:13:35] SPEAKER_00: If you look at cloud, it's been so much better at coding
[00:13:39] SPEAKER_00: and at writing than any other model for so long.
[00:13:41] SPEAKER_00: And it's really surprising just how long it took
[00:13:43] SPEAKER_00: other companies to catch up.
[00:13:45] SPEAKER_00: Considering just how much economic value there is there,
[00:13:47] SPEAKER_00: just like every AI coding product sat on top of a clot
[00:13:50] SPEAKER_00: because it was so good clock code and writing also,
[00:13:53] SPEAKER_00: what is it that made it so much better?
[00:13:55] SPEAKER_00: Is it just the quality of the data they trained on
[00:13:57] SPEAKER_00: or is there something else?
[00:13:59] I think there are multiple hearts to it.
[00:14:01] SPEAKER_01: So a big part of it strongly is the data.
[00:14:03] SPEAKER_01: Like I think people don't realize that there's almost
[00:14:06] SPEAKER_01: like this infinite amount of choices that all the frontier
[00:14:10] SPEAKER_01: labs are deciding between when they're choosing what data
[00:14:13] SPEAKER_01: goes into their models.
[00:14:14] SPEAKER_01: It's like, OK, are you purely using human data?
[00:14:17] SPEAKER_01: Are you gathering the human data in XYZ way?
[00:14:20] SPEAKER_01: When you are gathering the human data, what exactly
[00:14:23] SPEAKER_01: are you asking the people who are creating it to create for you?
[00:14:27] SPEAKER_01: Like maybe you care more, for example, in the Cognagram,
[00:14:31] SPEAKER_01: maybe you care more about front end coding versus back end coding.
[00:14:34] SPEAKER_01: Maybe when you're doing front end coding,
[00:14:35] SPEAKER_01: you care a lot about the visual design of their front
[00:14:38] SPEAKER_01: and applications that you're creating.
[00:14:40] SPEAKER_01: Or maybe you don't care about it so much.
[00:14:42] SPEAKER_01: You can you care more about, I don't know,
[00:14:43] SPEAKER_01: the efficiency of it or the pure correctness over
[00:14:45] SPEAKER_01: a Dalek visual design.
[00:14:47] SPEAKER_01: Then other questions like, OK, are you
[00:14:48] SPEAKER_01: carrying balls?
[00:14:49] SPEAKER_01: Are you like how much does synthetic data
[00:14:50] SPEAKER_01: are you throwing into the mix?
[00:14:51] SPEAKER_01: How much do you care about these 20 different benchmarks?
[00:14:55] SPEAKER_01: Like some companies, they see these benchmarks,
[00:14:56] SPEAKER_01: and you're like, OK, for PR purposes,
[00:14:59] SPEAKER_01: even though we don't think that these academic benchmarks
[00:15:01] SPEAKER_01: matter all that much, then we just
[00:15:03] SPEAKER_01: need to optimize for them anyways, because we
[00:15:07] SPEAKER_01: or marketing team needs to show certain progress
[00:15:11] SPEAKER_01: on certain standard evaluations
[00:15:12] SPEAKER_01: that every other company talks about.
[00:15:14] SPEAKER_01: And if we don't show good performance here,
[00:15:16] SPEAKER_01: it's going to be bad for us even if like,
[00:15:18] SPEAKER_01: ignore these academic benchmarks,
[00:15:20] SPEAKER_01: makes us better at the real tasks.
[00:15:21] SPEAKER_01: Other companies are going to be principled.
[00:15:23] SPEAKER_01: I'm like, OK, yeah, no, I don't care about marketing.
[00:15:24] SPEAKER_01: I just care about how my model performs
[00:15:27] SPEAKER_01: on these real world tasks that they know today.
[00:15:28] SPEAKER_01: And so I'm going to optimize for that instead.
[00:15:31] SPEAKER_01: And it's almost like there's a trade off
[00:15:32] SPEAKER_01: between all of these different things.
[00:15:34] SPEAKER_01: And there's like a, like one thing
[00:15:36] SPEAKER_01: is like all the thing about is that there's a,
[00:15:39] it's almost like there's an arch to post-stringing.
[00:15:41] SPEAKER_01: It's not purely a science.
[00:15:43] SPEAKER_01: Like when you were deciding what kind of model
[00:15:46] SPEAKER_01: you're trying to create and what it's good at,
[00:15:48] SPEAKER_01: there's this notion of taste and sophistication.
[00:15:53] SPEAKER_01: Like, OK, do I think that these,
[00:15:57] SPEAKER_01: go through an active example of how good the model is
[00:15:59] SPEAKER_01: at visual design?
[00:16:01] Like, OK, maybe you have a different notion of visual design
[00:16:04] SPEAKER_01: than what I do.
[00:16:05] SPEAKER_01: Like maybe you care more about minimalism
[00:16:06] SPEAKER_01: and you care more about, I don't know,
[00:16:09] SPEAKER_01: like 3D animations than I do.
[00:16:11] SPEAKER_01: Maybe you can sell a person before,
[00:16:13] SPEAKER_01: prefers things that look a little bit more pro.
[00:16:15] SPEAKER_01: And there's all these notions of taste and sophistication
[00:16:17] SPEAKER_01: that you have to decide between when you're
[00:16:18] SPEAKER_01: signing your post-string mix.
[00:16:20] SPEAKER_01: And so that matters as well.
[00:16:21] SPEAKER_01: Long story short, I think there's all these different factors.
[00:16:23] SPEAKER_01: And starting to data is a big part of it.
[00:16:25] SPEAKER_01: But it's also like what is objective function
[00:16:28] SPEAKER_01: that you're trying to optimize your model towards?
[00:16:30] That is so interesting.
[00:16:31] SPEAKER_00: Like the taste of the person leading this work
[00:16:34] SPEAKER_00: will inform what data they ask for, what data they feed it.
[00:16:38] SPEAKER_00: But it's wildest shows the value of great data.
[00:16:42] SPEAKER_00: And Thropic got so much growth and win
[00:16:45] SPEAKER_00: from essentially better data.
[00:16:49] Yeah, yeah, exactly.
[00:16:50] SPEAKER_00: And I could see why companies like yours are growing so fast.
[00:16:52] SPEAKER_00: There's just so much.
[00:16:54] SPEAKER_00: And that's just one vertical.
[00:16:55] SPEAKER_00: That's just coding.
[00:16:56] SPEAKER_00: And then there's probably a similar area for writing.
[00:16:58] SPEAKER_00: I love that it's interesting that AI,
[00:17:00] SPEAKER_00: and it feels like this artificial computer binary thing,
[00:17:03] SPEAKER_00: but it's like taste.
[00:17:04] SPEAKER_00: Human judgment is still such a key factor
[00:17:06] SPEAKER_00: in these things being successful.
[00:17:09] SPEAKER_00: Yep, yep, yep, exactly.
[00:17:10] SPEAKER_00: Like again, going back to the example I said earlier,
[00:17:12] SPEAKER_01: certain companies, if you ask them what is good to pull them,
[00:17:16] SPEAKER_01: they will simply robotically check off all of these instructions
[00:17:19] SPEAKER_01: on our list.
[00:17:20] SPEAKER_01: But again, I don't think that makes for good to poetry.
[00:17:22] SPEAKER_01: So certain frontier labs, the ones with more taste
[00:17:24] SPEAKER_01: and sophistication, they will realize
[00:17:26] SPEAKER_01: that it doesn't reduce to just six set of checkboxes.
[00:17:30] SPEAKER_01: And they'll consider all of these kind of implicit,
[00:17:33] SPEAKER_01: very subtle qualities instead.
[00:17:35] SPEAKER_01: And I think that's what makes them better
[00:17:36] SPEAKER_01: that are a decent benefit.
[00:17:38] SPEAKER_00: You mentioned benchmark.
[00:17:39] SPEAKER_00: So this is something a lot of people worry about
[00:17:40] SPEAKER_00: is there's all these models that are always,
[00:17:42] SPEAKER_00: like basically it feels like every model is better
[00:17:45] SPEAKER_00: than humans at every STEM field at this point.
[00:17:49] SPEAKER_00: But to a regular person, it doesn't
[00:17:51] SPEAKER_00: feel like these models are getting that much smarter
[00:17:53] SPEAKER_00: constantly.
[00:17:54] SPEAKER_00: What's your just sense of how much you trust benchmarks
[00:17:56] SPEAKER_00: and just how correlated those are with actual AI advancements?
[00:18:00] Yeah, so I don't trust the benchmarks at all.
[00:18:03] SPEAKER_01: And I think that's for two reasons.
[00:18:05] SPEAKER_01: So one is, I think a lot of people
[00:18:07] SPEAKER_01: don't realize even researchers within the community,
[00:18:10] SPEAKER_01: they don't realize that the benchmarks themselves are often
[00:18:12] SPEAKER_01: honestly just wrong.
[00:18:13] SPEAKER_01: Like they have wrong answers.
[00:18:15] SPEAKER_01: They're full of all this kind of messiness.
[00:18:18] SPEAKER_01: And people trust on us for the popular ones.
[00:18:22] SPEAKER_01: People have maybe realized this to some extent,
[00:18:25] SPEAKER_01: but the vast majority just have all these flaws
[00:18:27] SPEAKER_01: that people don't realize.
[00:18:29] SPEAKER_01: So that's one part of it.
[00:18:30] SPEAKER_01: And the other part of it is these benchmarks
[00:18:33] SPEAKER_01: that happen today, they are often,
[00:18:36] SPEAKER_01: they often have well-defined objective answers
[00:18:40] SPEAKER_01: that make them very easy for models to hill climb on
[00:18:43] SPEAKER_01: in a way that's very, very different from the messiness
[00:18:45] SPEAKER_01: and ambiguity to world.
[00:18:47] SPEAKER_01: Like I think one thing that often say is that it's kind of crazy
[00:18:49] SPEAKER_01: that these models can win IMO gold medals,
[00:18:53] SPEAKER_01: but they still have trouble parsing PDFs.
[00:18:55] SPEAKER_01: And that's because yeah, even though IMO gold medals
[00:18:57] SPEAKER_01: seem hard to average person,
[00:18:59] SPEAKER_01: yeah, like they are hard-locked in on the day,
[00:19:01] SPEAKER_01: but they have this notion of objectivity
[00:19:04] SPEAKER_01: that it's okay, yeah, parsing PDFs sometimes
[00:19:06] SPEAKER_01: doesn't have.
[00:19:07] SPEAKER_01: And so it's easier for frontier labs to hill climb on
[00:19:11] SPEAKER_01: all these then to solve all these messy
[00:19:14] SPEAKER_01: ambiguous problems in a world.
[00:19:15] SPEAKER_01: So I think there's a lack of direct correlation there.
[00:19:17] SPEAKER_01: It's so interesting the way you described it
[00:19:19] SPEAKER_00: is hitting these benchmarks is kind of like a marketing piece.
[00:19:22] SPEAKER_00: When you launch, say Gemini 3 just launches
[00:19:24] SPEAKER_00: like whole number one with all these benchmarks
[00:19:26] SPEAKER_00: is that is what happens.
[00:19:27] SPEAKER_00: They just kind of train their models
[00:19:29] SPEAKER_00: so they'll get good at these very specific things.
[00:19:31] SPEAKER_00: Yes, so there's again, maybe two parts for this.
[00:19:33] SPEAKER_01: So one is sometimes yeah, these benchmarks,
[00:19:37] SPEAKER_01: they accidentally leak in certain ways
[00:19:40] SPEAKER_01: or the frontier labs will tweak the way they evaluate
[00:19:46] SPEAKER_01: their models on these benchmarks.
[00:19:47] SPEAKER_01: Like they'll tweak their system prompt
[00:19:48] SPEAKER_01: or they'll tweak the number of times they run their model
[00:19:50] SPEAKER_01: and so on and so on in a way that games these benchmarks.
[00:19:54] SPEAKER_01: The other part of it though is
[00:19:58] it's like by optimizing for the benchmark
[00:20:00] SPEAKER_01: instead of optimizing for the real world,
[00:20:02] SPEAKER_01: you will just naturally climb on the benchmark
[00:20:05] SPEAKER_01: and yeah, it's basically another form of gaming.
[00:20:08] SPEAKER_00: Knowing that with that in mind,
[00:20:10] SPEAKER_00: how do you get a sense of it for heading towards AGI?
[00:20:13] SPEAKER_00: How do you measure progress?
[00:20:14] SPEAKER_00: Yes, so the way we really care about measuring model progress
[00:20:17] SPEAKER_01: is by running all these human evaluations.
[00:20:20] SPEAKER_01: So for example, what we do is yeah, we will take
[00:20:24] core human annotators and we'll ask them,
[00:20:25] SPEAKER_01: okay, go have a conversation model.
[00:20:27] SPEAKER_01: Maybe you're having a conversation model
[00:20:29] SPEAKER_01: across all of these different topics.
[00:20:31] SPEAKER_01: So you are a noble prize winning physicist.
[00:20:35] SPEAKER_01: So you go have a conversation
[00:20:37] SPEAKER_01: about pushing different year of your own research.
[00:20:39] SPEAKER_01: You are a teacher and you're trying
[00:20:42] SPEAKER_01: to create lesson plans for your students.
[00:20:43] SPEAKER_01: So go talk to the model about these things
[00:20:46] SPEAKER_01: or you are a, yeah, you're a coder
[00:20:49] SPEAKER_01: and you're working at one of these big tech companies
[00:20:53] SPEAKER_01: and you have these problems every day.
[00:20:54] SPEAKER_01: So go talk to the model and see how much it helps you.
[00:20:56] SPEAKER_01: And because or researchers or annotators,
[00:21:00] SPEAKER_01: they are experts at the top of their fields
[00:21:04] SPEAKER_01: and they are not just giving you responses.
[00:21:07] SPEAKER_01: They're actually working through the responses
[00:21:09] SPEAKER_01: to beat themselves.
[00:21:11] SPEAKER_01: They are, yeah, you value it a code at a rights.
[00:21:14] SPEAKER_01: They're going to double check the physics equations
[00:21:16] SPEAKER_01: at a rights.
[00:21:17] SPEAKER_01: They're going to evaluate the models in a very deep way,
[00:21:19] SPEAKER_01: sort of pay attention to accuracy
[00:21:21] SPEAKER_01: and instruction following all these things
[00:21:22] SPEAKER_01: that casual users don't when you suddenly get
[00:21:26] SPEAKER_01: a pop up on your chat, you be taught response,
[00:21:29] SPEAKER_01: asking you to compare these two different responses.
[00:21:31] SPEAKER_01: Like people like that, they're not evaluating models.
[00:21:33] SPEAKER_01: People, they're just surviving
[00:21:35] SPEAKER_01: and thinking whatever response looks fascist
[00:21:38] SPEAKER_01: or annotators are looking closely to responses
[00:21:40] SPEAKER_01: and evaluating them for all of these different dimensions.
[00:21:42] SPEAKER_01: And so I think that's a much better approach
[00:21:43] SPEAKER_01: than they spent sparks or kind of these random online A.B.
[00:21:48] SPEAKER_00: This.
[00:21:49] SPEAKER_00: Again, I love just how central humans continue to be,
[00:21:52] SPEAKER_00: you know, this work that we're not totally done yet.
[00:21:54] SPEAKER_00: Is there going to be a point where we don't need these people
[00:21:56] SPEAKER_00: anymore that A.I. is so smart that, okay, we're good.
[00:21:58] SPEAKER_00: We got everything out of your heads.
[00:22:00] SPEAKER_00: Yeah, I think that will not happen until we reach the AGI.
[00:22:04] SPEAKER_01: Like it's almost like by definition,
[00:22:05] SPEAKER_01: if we haven't reached the AGI yet,
[00:22:07] SPEAKER_01: then there's more for the models to learn from.
[00:22:10] SPEAKER_01: And so I don't think that's going to happen any time soon.
[00:22:12] SPEAKER_01: Okay, cool.
[00:22:13] SPEAKER_00: So more reason to stress about AGI.
[00:22:15] SPEAKER_00: We don't need these books anymore.
[00:22:17] SPEAKER_00: What's your, I can't not ask just,
[00:22:19] SPEAKER_00: and it's people that work closely with this stuff.
[00:22:21] SPEAKER_00: I'm always just curious, what's your AGI timelines?
[00:22:24] SPEAKER_00: How far do you think we are from this?
[00:22:25] SPEAKER_00: Do you think we're in like a couple of years or is it like decades?
[00:22:28] SPEAKER_00: So I'm certainly on the longer time prize in front.
[00:22:31] SPEAKER_01: I think people don't realize that there's a big difference
[00:22:34] SPEAKER_01: between moving from PD% performance to 90% performance,
[00:22:38] SPEAKER_01: to 99% performance to 99.9% performance
[00:22:41] SPEAKER_01: and so on and so on.
[00:22:42] SPEAKER_01: And so in my head, I probably bet that
[00:22:47] SPEAKER_01: within next one or two years, the models are going to automate 80%
[00:22:50] SPEAKER_01: of the average L6 offer engineers job.
[00:22:53] SPEAKER_01: That's going to take another few years,
[00:22:54] SPEAKER_01: do you move to 90% and another few is to 99% and so on and so on.
[00:22:57] SPEAKER_01: So I think we're closer to a decade or decades away
[00:23:01] SPEAKER_01: than then, now, folks.
[00:23:03] SPEAKER_01: You have this hot take that a lot of these labs
[00:23:05] SPEAKER_00: are kind of pushing AGI in the wrong direction.
[00:23:08] SPEAKER_00: And this is based on your work at Twitter and Google
[00:23:11] SPEAKER_00: and Facebook.
[00:23:12] SPEAKER_00: Can you just talk about that?
[00:23:14] I'm worried that instead of building
[00:23:15] SPEAKER_01: AI that will actually advance us as a species,
[00:23:18] SPEAKER_01: curing cancer, solving poverty, understanding
[00:23:19] SPEAKER_01: universe, all these big grand questions,
[00:23:22] SPEAKER_01: we are optimizing for AI's love instead.
[00:23:25] We're basically teaching models to chase dopamine
[00:23:27] SPEAKER_01: instead of truth.
[00:23:29] And I think this relates to what we're talking about
[00:23:30] SPEAKER_01: regarding these benchmarks.
[00:23:32] SPEAKER_01: So let me give you a couple of examples.
[00:23:35] So right now, industry is played by these terrible leaderboards
[00:23:38] SPEAKER_01: like LM Arena.
[00:23:40] It's this popular online leaderboard
[00:23:41] SPEAKER_01: where random people from around the world
[00:23:43] SPEAKER_01: vote on which AI response is better.
[00:23:46] SPEAKER_01: But the thing is, like I was saying earlier,
[00:23:47] SPEAKER_01: they're not carefully reading or fact-shaking.
[00:23:49] SPEAKER_01: They're skimming due to responses for two seconds
[00:23:51] SPEAKER_01: and picking whatever looks fascist.
[00:23:53] SPEAKER_01: So a model can hallucinate everything.
[00:23:56] SPEAKER_01: It can completely hallucinate.
[00:23:58] SPEAKER_01: But it will look impressive because it has crazy emojis
[00:24:01] SPEAKER_01: and boating and markdown headers
[00:24:03] SPEAKER_01: and all these superficial things that don't matter at all,
[00:24:05] SPEAKER_01: but it catches your attention.
[00:24:07] SPEAKER_01: And these LM Arena users love it.
[00:24:08] SPEAKER_01: It's literally optimizing your models
[00:24:10] SPEAKER_01: for the types of people who buy tablets at the grocery store.
[00:24:14] SPEAKER_01: We've seen this in their data ourselves.
[00:24:15] SPEAKER_01: The easiest way to climb LM Arena, it's adding crazy boating.
[00:24:18] SPEAKER_01: It's doubling the number of emojis.
[00:24:20] SPEAKER_01: It's tripling the link for your model responses,
[00:24:22] SPEAKER_01: even if your model starts hallucinating
[00:24:24] SPEAKER_01: and getting to answer completely wrong.
[00:24:26] And the problem is, again, because all of these Frontier
[00:24:29] SPEAKER_01: Labs, they kind of have to pay attention to PR
[00:24:32] SPEAKER_01: because their sales team, when they're trying to sell
[00:24:34] SPEAKER_01: all these enterprise customers, those enterprise customers
[00:24:36] SPEAKER_01: will say, well, but your model's only number five
[00:24:39] SPEAKER_01: on LM Arena.
[00:24:39] SPEAKER_01: So why should I buy it?
[00:24:41] SPEAKER_01: They have to, and some of those pay attention
[00:24:43] SPEAKER_01: to the T-Sleeter boards.
[00:24:45] SPEAKER_01: And so what our researchers all might tell us is, like,
[00:24:47] SPEAKER_01: just say, the only way I'm going to get promoted at the end
[00:24:51] SPEAKER_01: of the year is if I climb this leader board,
[00:24:53] SPEAKER_01: even though I know that Climmy is probably
[00:24:55] SPEAKER_01: going to make my model worse in it,
[00:24:56] SPEAKER_01: accuracy and distortion following.
[00:24:58] SPEAKER_01: So I think there's all these negative incentives
[00:25:00] SPEAKER_01: that are pushing working in the wrong direction.
[00:25:03] I'm also worried about this trend towards optimizing AI
[00:25:06] SPEAKER_01: for engagement.
[00:25:07] Like I used to work on social media.
[00:25:09] SPEAKER_01: And every time we optimize for engagement,
[00:25:11] SPEAKER_01: terrible things happened.
[00:25:12] SPEAKER_01: You'd get click pay.
[00:25:13] SPEAKER_01: And pictures of bikinis and big food
[00:25:16] SPEAKER_01: and horror-frying skin diseases just filling your feeds.
[00:25:19] SPEAKER_01: And I think I worry does same things happening with AI.
[00:25:22] Like, if you think about all the sick-vancy issues
[00:25:23] SPEAKER_01: with chatchip PPE, oh, you're absolutely right.
[00:25:26] SPEAKER_01: What an amazing question.
[00:25:27] SPEAKER_01: Like, the easiest way to hook users
[00:25:28] SPEAKER_01: is to tell them how amazing they are.
[00:25:30] SPEAKER_01: And so these models, they constantly tell you
[00:25:32] SPEAKER_01: you're a genius.
[00:25:33] SPEAKER_01: They'll feed into your delusions.
[00:25:34] SPEAKER_01: I guess, we're usually theories.
[00:25:35] SPEAKER_01: They'll pull you down these rabbit holes
[00:25:37] because Silicon Valley love this maximizing time spent
[00:25:40] SPEAKER_01: and just increasing number of conversations
[00:25:41] SPEAKER_01: you're having with it.
[00:25:43] And so, yeah, companies are spending all the time
[00:25:45] SPEAKER_01: hacking these leaderboards and benchmarks.
[00:25:47] SPEAKER_01: And the scores are going up.
[00:25:49] SPEAKER_01: But I think it actually masks up the models
[00:25:51] SPEAKER_01: with the best scores.
[00:25:52] SPEAKER_01: They are often the worst or just
[00:25:55] SPEAKER_01: have all these fundamental failures.
[00:25:57] SPEAKER_01: So I think I'm fully worried about that.
[00:25:59] SPEAKER_01: All these negative incentives are
[00:26:01] SPEAKER_01: pushing HDI into the wrong direction.
[00:26:03] SPEAKER_01: So I'm hearing as HIs being slowed down
[00:26:05] SPEAKER_00: by these, basically, the wrong objective function,
[00:26:08] SPEAKER_00: these labs paying attention to the wrong,
[00:26:09] SPEAKER_00: basically, benchmarks and e-veils.
[00:26:11] SPEAKER_00: Yep.
[00:26:12] SPEAKER_00: I know you probably can't play favorites
[00:26:14] SPEAKER_00: since you work with all the labs.
[00:26:15] SPEAKER_00: Is there anyone doing better at this
[00:26:18] SPEAKER_00: and maybe kind of realizing this is the wrong direction?
[00:26:20] SPEAKER_00: I would say you've always been very, very impressed
[00:26:22] SPEAKER_00: by Anthropic.
[00:26:24] SPEAKER_01: Like, I think Anthropic takes a very principled view
[00:26:28] SPEAKER_01: about what they do and don't care about
[00:26:32] SPEAKER_01: and how they want their models to behave
[00:26:35] SPEAKER_01: in a way that feels a lot more or a lot more principled to me.
[00:26:38] SPEAKER_00: Interesting.
[00:26:39] SPEAKER_00: Are there any other big mistakes you think labs are making
[00:26:43] SPEAKER_00: just that are kind of slowing things down
[00:26:44] SPEAKER_00: or heading the wrong direction?
[00:26:45] SPEAKER_00: What we've heard just chasing benchmarks,
[00:26:48] SPEAKER_00: this engagement focus,
[00:26:50] SPEAKER_00: is there anything else you're seeing of just like,
[00:26:51] SPEAKER_00: okay, we got to work on this
[00:26:53] SPEAKER_00: because it'll speed everything up.
[00:26:55] I mean, I think there is a question
[00:26:56] SPEAKER_01: of what products they're building
[00:26:59] SPEAKER_01: and whether those products themselves
[00:27:01] SPEAKER_01: are something that kind of help or hurt humanity.
[00:27:05] SPEAKER_01: Like, I think a lot about Sora.
[00:27:06] SPEAKER_01: And why?
[00:27:07] SPEAKER_01: Just thinking about such a match.
[00:27:09] SPEAKER_01: Yeah, what in tails?
[00:27:10] SPEAKER_01: And so it's kind of interesting.
[00:27:12] SPEAKER_01: It's like, which companies would do Sora
[00:27:14] SPEAKER_01: and which one?
[00:27:17] And I think that answer to that question
[00:27:19] SPEAKER_01: on the answer is myself.
[00:27:20] SPEAKER_01: I have an idea in my head,
[00:27:22] SPEAKER_01: but I think that answer to that question
[00:27:24] SPEAKER_01: may be reveals certain things about
[00:27:27] SPEAKER_01: what kinds of AI models those companies
[00:27:31] SPEAKER_01: want to do in a direction and what future
[00:27:33] SPEAKER_01: they want to achieve.
[00:27:36] SPEAKER_01: So I think about that a lot.
[00:27:37] SPEAKER_01: The steel man argument there is,
[00:27:39] SPEAKER_00: it's like fun, people want it.
[00:27:41] SPEAKER_00: It'll help them generate revenue to grow this thing
[00:27:44] SPEAKER_00: and build better models.
[00:27:47] SPEAKER_00: It'll train data in an interesting way.
[00:27:49] SPEAKER_00: It's also just like, really fun.
[00:27:51] SPEAKER_00: Yeah, I think it's almost like,
[00:27:56] do you care about how you get there?
[00:28:00] SPEAKER_01: And in the same way,
[00:28:01] SPEAKER_01: so I made this tabloid analogy earlier,
[00:28:03] SPEAKER_01: but would you sell tabloids in order to find,
[00:28:07] SPEAKER_01: I don't know, some other newspaper?
[00:28:09] SPEAKER_01: Actually, like in some sense,
[00:28:13] SPEAKER_01: if you don't care about the path,
[00:28:14] SPEAKER_01: then you'll just do whatever it takes,
[00:28:17] SPEAKER_01: but it's possible that it has negative consequences
[00:28:20] SPEAKER_01: in itself that will harm the long-term direction
[00:28:24] SPEAKER_01: of what you're trying to achieve
[00:28:25] SPEAKER_01: and maybe it'll distract you
[00:28:28] SPEAKER_01: from all the more important things.
[00:28:30] SPEAKER_01: So yeah, I think the path you take matters a lot as well.
[00:28:33] SPEAKER_01: Along these lines,
[00:28:34] SPEAKER_00: you talked a bunch about this of just Silicon Valley
[00:28:36] SPEAKER_00: and kind of the downsides of raising a lot of money
[00:28:39] SPEAKER_00: being in the echo chamber.
[00:28:41] SPEAKER_00: What do you call this Silicon Valley machine?
[00:28:42] SPEAKER_00: You talk about how it's hard to build important companies
[00:28:46] SPEAKER_00: in this way and that you might actually be much more successful
[00:28:50] SPEAKER_00: if you're not going down the VC path.
[00:28:52] SPEAKER_00: You just talk about what you've seen there experience
[00:28:54] SPEAKER_00: and your advice essentially to founders
[00:28:56] SPEAKER_00: because they're always hearing, you know?
[00:28:57] SPEAKER_00: Raise money from fans, EVCs, move to Silicon Valley.
[00:29:00] SPEAKER_00: What's kind of the countertake?
[00:29:01] SPEAKER_00: Yes, so I've always really hated a lot
[00:29:04] SPEAKER_01: to Silicon Valley mantras.
[00:29:06] SPEAKER_01: This standard playbook is to get product market fit
[00:29:09] SPEAKER_01: by pivoting every two weeks
[00:29:11] SPEAKER_01: and to chase growth and chasing engagement
[00:29:13] SPEAKER_01: with all of these dark patterns
[00:29:15] SPEAKER_01: and to blitz scale by hiring as fast as possible.
[00:29:17] SPEAKER_01: And I've always disagreed.
[00:29:20] So yeah, I would say don't pivot, don't blitz scale.
[00:29:22] SPEAKER_01: Don't hire a desk Stanford grad
[00:29:24] SPEAKER_01: who simply wants to add a hot company to a resume
[00:29:26] SPEAKER_01: just build the one thing only you could build.
[00:29:29] SPEAKER_01: The thing that wouldn't exist
[00:29:30] SPEAKER_01: without the insight and expertise that only you have.
[00:29:33] And you see these buy-to-book companies everywhere now.
[00:29:35] SPEAKER_01: Some founder who was doing crypto in 2020
[00:29:38] SPEAKER_01: and then pivoted NFTs in 2022 and now they're in AI company.
[00:29:42] SPEAKER_01: There's no consistency, there's no mission.
[00:29:44] SPEAKER_01: They're just chasing evaluations.
[00:29:47] SPEAKER_01: And I've always hated this because Silicon Valley
[00:29:49] SPEAKER_01: loves to score an mastery for focusing on money.
[00:29:52] SPEAKER_01: But honestly, most of the Silicon Valley
[00:29:53] SPEAKER_01: is chasing the same thing.
[00:29:55] And so we stayed focused on our mission from day one
[00:29:57] SPEAKER_01: pushing that frontier of high quality complex data.
[00:30:01] And I always love that because I think startups,
[00:30:03] SPEAKER_01: I have this very romantic notion of startups.
[00:30:05] SPEAKER_01: Like startups is supposed to be a well-taking big risks
[00:30:07] SPEAKER_01: to build something that you really believe in.
[00:30:09] SPEAKER_01: But if you're costly pivoting, you're not taking risks.
[00:30:10] SPEAKER_01: You're just trying to make a quick walk.
[00:30:12] SPEAKER_01: And if you fail because the market isn't ready yet,
[00:30:15] SPEAKER_01: I actually think that's way better.
[00:30:16] SPEAKER_01: At least you took a swing at something deep
[00:30:18] SPEAKER_01: and novel and hard instead of pivoting
[00:30:20] SPEAKER_01: into another LM wrapper company.
[00:30:23] So yeah, I think the only way you build something
[00:30:25] SPEAKER_01: that matters is that it's going to change the world
[00:30:27] SPEAKER_01: is if you find a big idea you believe in
[00:30:29] SPEAKER_01: and you say no to everything else.
[00:30:30] SPEAKER_01: So you don't keep on pivoting when it gets hard.
[00:30:32] SPEAKER_01: You don't hire a team of 10 product managers
[00:30:34] SPEAKER_01: because that's where every other cookie cutter startup does.
[00:30:37] SPEAKER_01: You just keep building that one company
[00:30:38] SPEAKER_01: that winnings this without you.
[00:30:41] SPEAKER_01: And I think there are a lot of people
[00:30:42] SPEAKER_01: that's like, I'm on the now who are sick of all the grift
[00:30:45] SPEAKER_01: who want to work on big things that matter
[00:30:47] SPEAKER_01: with people who actually care.
[00:30:48] And I'm hoping that that'll be a future
[00:30:50] SPEAKER_01: of how we build technology.
[00:30:52] I'm actually working on a post right now
[00:30:54] SPEAKER_00: with Terrence Rohan.
[00:30:55] SPEAKER_00: This is what you see that I really like to work with.
[00:30:57] SPEAKER_00: And we interviewed five people who picked
[00:31:00] SPEAKER_00: really successful generational companies early
[00:31:04] SPEAKER_00: and joined them as really early employees.
[00:31:06] SPEAKER_00: Like they joined OpenAI before anyone thought it was awesome
[00:31:08] SPEAKER_00: and striped before anyone knew it was awesome.
[00:31:10] SPEAKER_00: And so we're looking for patterns
[00:31:11] SPEAKER_00: of how people find these generational companies
[00:31:13] SPEAKER_00: before anyone else.
[00:31:15] SPEAKER_00: And it aligns exactly what you just described,
[00:31:19] SPEAKER_00: which is ambition.
[00:31:21] SPEAKER_00: They have a wild ambition with what they want to achieve.
[00:31:24] SPEAKER_00: They're not, as you said, just kind of looking around
[00:31:26] SPEAKER_00: for product market fit no matter what ends up being.
[00:31:29] SPEAKER_00: And so I love that what you described
[00:31:31] SPEAKER_00: very much aligns with what we're seeing there.
[00:31:33] SPEAKER_00: Yeah, I absolutely think that you have to have huge ambitions
[00:31:36] SPEAKER_01: and you have to have a huge belief in your idea
[00:31:38] SPEAKER_01: that's when it changed the world.
[00:31:40] SPEAKER_01: And you have to be willing to double down
[00:31:41] SPEAKER_01: and keep on doing whatever it takes to make it happen.
[00:31:44] SPEAKER_00: I love how counter your narrative is
[00:31:47] SPEAKER_00: to so many of the things people hear.
[00:31:48] SPEAKER_00: And so I love that we're doing this.
[00:31:49] SPEAKER_00: I love that we're sharing this story.
[00:31:51] Today's episode is brought to you by Coda.
[00:31:53] SPEAKER_00: I personally use Coda every single day
[00:31:56] SPEAKER_00: to manage my podcast and also to manage my community.
[00:31:59] SPEAKER_00: So I put the questions that I plan to ask every guest
[00:32:01] SPEAKER_00: that's coming on the podcast.
[00:32:03] SPEAKER_00: So I put my community resources.
[00:32:05] SPEAKER_00: It's how I manage my workflows.
[00:32:06] SPEAKER_00: Here's how Coda can help you.
[00:32:08] SPEAKER_00: Imagine starting a project that work
[00:32:10] SPEAKER_00: and your vision is clear.
[00:32:11] SPEAKER_00: You know exactly who's doing what
[00:32:13] SPEAKER_00: and where to find the data that you need to do your part.
[00:32:16] SPEAKER_00: In fact, you don't have to waste time searching for anything
[00:32:18] SPEAKER_00: because everything your team needs
[00:32:20] SPEAKER_00: from project trackers and OCRs
[00:32:22] SPEAKER_00: to documents and spreadsheets lives in one tab, all in Coda.
[00:32:26] SPEAKER_00: With Coda's collaborative all in one workspace,
[00:32:28] SPEAKER_00: you get the flexibility of docs,
[00:32:30] SPEAKER_00: the structure of spreadsheets, the power of applications,
[00:32:33] SPEAKER_00: and the intelligence of AI, all in one easy to organize tab.
[00:32:38] SPEAKER_00: Like I mentioned earlier, I use Coda every single day
[00:32:41] SPEAKER_00: and more than 50,000 teams trust Coda
[00:32:43] SPEAKER_00: to keep them more aligned and focused.
[00:32:45] SPEAKER_00: If you're a startup team looking to increase alignment
[00:32:47] SPEAKER_00: and agility, Coda can help you move
[00:32:49] SPEAKER_00: from planning to execution in record time.
[00:32:52] SPEAKER_00: To try it for yourself, go to Coda.io slash Lenny today
[00:32:56] SPEAKER_00: and get six months free of the team plan for startups.
[00:32:59] SPEAKER_00: That's c-o-d-a.io slash Lenny.
[00:33:01] SPEAKER_00: To get started for free and get six months of the team plan,
[00:33:05] SPEAKER_00: Coda.io slash Lenny.
[00:33:07] Slightly different direction, but something else
[00:33:09] SPEAKER_00: that was maybe a counter narrative.
[00:33:12] SPEAKER_00: Imagine you watch the DoorCash and Richard Sutton podcast
[00:33:15] SPEAKER_00: episode and even if you didn't,
[00:33:18] SPEAKER_00: they basically had this conversation with Richard Sutton.
[00:33:19] SPEAKER_00: He was a famous AI researcher at this whole biter,
[00:33:23] SPEAKER_00: the biter lesson meme.
[00:33:26] SPEAKER_00: And he talked about how LMS almost are kind of a dead end.
[00:33:29] SPEAKER_00: Anything's we're gonna really plateau around LMS
[00:33:32] SPEAKER_00: because of the way they learn.
[00:33:34] SPEAKER_00: What's your take there?
[00:33:35] SPEAKER_00: Do you think LMS will get us to AGI or Beyond
[00:33:38] SPEAKER_00: or do you think there's gonna be something new
[00:33:40] SPEAKER_00: or a big breakthrough that needs to get us there?
[00:33:42] I'm in a camp where I do believe
[00:33:45] SPEAKER_01: that's something new will be needed.
[00:33:46] SPEAKER_01: Like the way I think about it is,
[00:33:49] SPEAKER_01: when I think about training AI, I take a very,
[00:33:53] SPEAKER_01: I don't know if I would say biological one of you,
[00:33:56] SPEAKER_01: but I believe that in the same way,
[00:33:58] SPEAKER_01: that there's a million different ways that humans learn,
[00:34:01] SPEAKER_01: we need to build models that can mimic all those ways as well.
[00:34:07] And maybe you'll have a different distribution
[00:34:09] SPEAKER_01: of the focusses that they have on it.
[00:34:11] SPEAKER_01: I know they don't be different from you.
[00:34:12] SPEAKER_01: And so maybe you'll have a different distribution,
[00:34:14] SPEAKER_01: but we wanna be able to mimic
[00:34:16] SPEAKER_01: our learning abilities of humans
[00:34:18] SPEAKER_01: and make sure that we have the algorithms and the data
[00:34:23] SPEAKER_01: for models learn in the same way.
[00:34:25] SPEAKER_01: And so to extend to LMS have different ways
[00:34:27] SPEAKER_01: of learning from humans, then yeah,
[00:34:30] SPEAKER_01: I think something will be needed.
[00:34:32] SPEAKER_00: This connects to reinforcement learning.
[00:34:34] SPEAKER_00: There's something that you're big on
[00:34:36] SPEAKER_00: and something I'm hearing more and more
[00:34:37] SPEAKER_00: is just becoming a big deal in the world of post-training.
[00:34:40] SPEAKER_00: Can you just help people understand
[00:34:41] SPEAKER_00: what is reinforcement learning
[00:34:43] SPEAKER_00: and reinforcement learning environments
[00:34:45] SPEAKER_00: and why they're so they're gonna be more and more important
[00:34:48] SPEAKER_00: in the future?
[00:34:49] Re-inversion learning is essentially training
[00:34:51] SPEAKER_01: or model to reach a certain reward.
[00:34:54] And let me explain what the RMR is.
[00:34:57] SPEAKER_01: And RMR is essentially a simulation of your world.
[00:34:59] SPEAKER_01: So think a bit like building a video game
[00:35:02] SPEAKER_01: with a fully fleshed out universe.
[00:35:04] Every character has a real story.
[00:35:06] SPEAKER_01: Every business has tools and data you can call
[00:35:09] SPEAKER_01: and you have all these different entities
[00:35:11] SPEAKER_01: interacting with each other.
[00:35:12] SPEAKER_01: So for example, we might build a world
[00:35:14] SPEAKER_01: where you have a startup with Gmail messages
[00:35:17] SPEAKER_01: and Slack threads and geartickets and Git up ERs
[00:35:19] SPEAKER_01: and a whole code base.
[00:35:20] SPEAKER_01: And then suddenly AWS goes down and Slack goes down.
[00:35:25] And so, okay, model, what do you do?
[00:35:27] SPEAKER_01: Like the modeling to figure it out.
[00:35:29] SPEAKER_01: So we give the models tasks in these environments
[00:35:32] SPEAKER_01: when we design interesting challenges for them
[00:35:34] SPEAKER_01: and then we run them to see how they perform.
[00:35:36] SPEAKER_01: And then we teach them, we give them these rewards
[00:35:38] SPEAKER_01: when you're doing a good job or a bad job.
[00:35:40] And I think one of the interesting things
[00:35:41] SPEAKER_01: is that these environments really showcase
[00:35:43] SPEAKER_01: where models are ended, are end week
[00:35:45] SPEAKER_01: and end to end tasks in real world.
[00:35:48] SPEAKER_01: You have all these models that seem really smart
[00:35:50] SPEAKER_01: on isolated benchmarks.
[00:35:51] SPEAKER_01: Like they're good at single step calling.
[00:35:54] SPEAKER_01: They're good at single step instruction following.
[00:35:56] SPEAKER_01: But suddenly you dump them into these messy worlds
[00:35:58] SPEAKER_01: where you have confusing Slack messages
[00:36:00] SPEAKER_01: and tools they've never seen before.
[00:36:02] SPEAKER_01: And they need to perform right actions
[00:36:04] SPEAKER_01: and modify the data, this is and interact
[00:36:06] SPEAKER_01: over longer time horizons, where what they do in step one
[00:36:10] SPEAKER_01: affects what they do in step 50.
[00:36:12] And that's very, very different from these kind of academics,
[00:36:15] SPEAKER_01: single step environments that they've been in before.
[00:36:18] SPEAKER_01: And so the model just fails kind of sharply
[00:36:20] SPEAKER_01: in all these crazy ways.
[00:36:21] SPEAKER_01: So I think these arm environments are going to be really
[00:36:23] SPEAKER_01: interesting playgrounds for them also learn from.
[00:36:25] SPEAKER_01: That will essentially be simulations
[00:36:28] SPEAKER_01: and nimics in real world.
[00:36:29] SPEAKER_01: And so they'll hopefully get better and better
[00:36:32] SPEAKER_01: at real tasks compared to all these contrarter environments.
[00:36:35] SPEAKER_01: So I'm trying to imagine what this looks like.
[00:36:37] SPEAKER_00: Essentially it's like a virtual machine
[00:36:39] SPEAKER_00: with I don't know browser or spreadsheet
[00:36:41] SPEAKER_00: or something in it with like I don't know surge.com.
[00:36:45] SPEAKER_00: Is that your website is surge.com?
[00:36:47] SPEAKER_00: Let's make sure we get that right.
[00:36:49] SPEAKER_00: So we are actually surge HQ.ai.
[00:36:51] SPEAKER_00: Search HQ.ai, check it out.
[00:36:54] SPEAKER_00: We're hiring it.
[00:36:56] SPEAKER_00: I imagine.
[00:36:57] Yes.
[00:36:57] OK.
[00:36:58] SPEAKER_00: So it's like cool.
[00:37:00] SPEAKER_00: Here's surge HQ.ai.
[00:37:02] SPEAKER_00: Your job, here's your job as an agent,
[00:37:04] SPEAKER_00: let's say, is to make sure it stays up.
[00:37:06] SPEAKER_00: And then all of a sudden it goes down.
[00:37:07] SPEAKER_00: And the objective function is figure out why.
[00:37:11] SPEAKER_00: Is that an example?
[00:37:12] SPEAKER_00: Yes.
[00:37:13] SPEAKER_00: So the objective function might be, or the goal of the task
[00:37:17] SPEAKER_01: might be OK, go figure out why and fix it.
[00:37:19] SPEAKER_01: And so the objective function might be,
[00:37:21] SPEAKER_01: it might be passing a series of unit tests.
[00:37:23] SPEAKER_01: It might be writing a document.
[00:37:25] SPEAKER_01: It might be a retro, containing certain information.
[00:37:27] SPEAKER_01: That matches exactly what happened.
[00:37:29] SPEAKER_01: There's all these different rewards
[00:37:31] SPEAKER_01: that we might give it that determine
[00:37:33] SPEAKER_01: well or not it's succeeding.
[00:37:36] SPEAKER_01: And so the models we're basically teaching
[00:37:37] SPEAKER_01: also achieve that reward.
[00:37:38] SPEAKER_00: So essentially it's like running.
[00:37:40] SPEAKER_00: It's often running.
[00:37:40] SPEAKER_00: Here's your goal.
[00:37:42] SPEAKER_00: Figure out why the site went down and fix it.
[00:37:44] SPEAKER_00: And it just starts trying stuff.
[00:37:46] SPEAKER_00: We're using all the intelligence it's got.
[00:37:48] SPEAKER_00: It makes mistakes.
[00:37:49] SPEAKER_00: You kind of help it along the way
[00:37:50] SPEAKER_00: rewarded if it's doing the right sort of thing.
[00:37:52] SPEAKER_00: And so what you're describing here
[00:37:54] SPEAKER_00: is this is where model, this is the next phase of models
[00:37:57] SPEAKER_00: becoming smarter.
[00:37:58] SPEAKER_00: More RL environments focused on very specific tasks
[00:38:01] SPEAKER_00: that are economically valuable, I imagine.
[00:38:04] SPEAKER_00: Yeah.
[00:38:04] SPEAKER_00: So just in the same way that there
[00:38:06] SPEAKER_01: were all these different methods for models
[00:38:08] SPEAKER_01: learning in the past.
[00:38:10] SPEAKER_01: Originally we had SFT and RFF.
[00:38:12] SPEAKER_01: And then we had rubrics and verifiers.
[00:38:14] SPEAKER_01: This is the next stage.
[00:38:17] SPEAKER_01: And it's not the case that the previous methods
[00:38:19] SPEAKER_01: are obsolete.
[00:38:20] SPEAKER_01: This is again just a different form of learning
[00:38:23] SPEAKER_01: that complements all the previous types.
[00:38:26] SPEAKER_01: So it's just like a different skill
[00:38:28] SPEAKER_01: that model is learning how to do.
[00:38:30] SPEAKER_01: And so in this case, it's less some physics PhD
[00:38:33] SPEAKER_00: sitting around talking to a model,
[00:38:35] SPEAKER_00: correcting it, giving it evals of here
[00:38:37] SPEAKER_00: is what the correct answer is, creating rubrics
[00:38:39] SPEAKER_00: more it's like this person now designing an environment.
[00:38:42] SPEAKER_00: So another example I've heard is like a financial analyst.
[00:38:45] SPEAKER_00: Just like here's an Excel spreadsheet.
[00:38:47] SPEAKER_00: Here's your goal, figure out a profit and loss or whatever.
[00:38:51] SPEAKER_00: And so this expert now is instead of just sitting around
[00:38:52] SPEAKER_00: writing rubrics, they're designing this RL and burden.
[00:38:56] Yeah, exactly.
[00:38:56] So that financial analyst might create a spreadsheet.
[00:39:00] SPEAKER_01: They may create certain tools that the model needs to call
[00:39:05] SPEAKER_01: in order to help fill out the spreadsheet.
[00:39:07] SPEAKER_01: Like it might be, okay, the model needs to access Bloomberg
[00:39:09] SPEAKER_01: terminal and needs to learn how to use it.
[00:39:12] SPEAKER_01: And it needs to learn how to use this calculator
[00:39:14] SPEAKER_01: and it needs to learn how to pour in this calculation.
[00:39:16] SPEAKER_01: So it has all these tools that it has access to.
[00:39:19] SPEAKER_01: And then the reward might be, okay, it's like maybe
[00:39:22] SPEAKER_01: I will download that spreadsheet
[00:39:25] SPEAKER_01: and I'm gonna see does cell B22 contain
[00:39:30] SPEAKER_01: the correct profit and loss number
[00:39:33] SPEAKER_01: or does tab number two contain dysphysic formation
[00:39:37] SPEAKER_01: and what's interesting is a lot closer to how humans learn.
[00:39:39] SPEAKER_00: We just try stuff, figure out what's working and what's not.
[00:39:43] SPEAKER_00: You talk about how trajectories are really important to this.
[00:39:46] SPEAKER_00: It's not just here's the goal and here's the end.
[00:39:50] SPEAKER_00: It's like every step along the way.
[00:39:51] SPEAKER_00: Can you just talk about what trajectories are
[00:39:53] SPEAKER_00: and why that's important to this?
[00:39:55] SPEAKER_00: I think one of the things that people don't realize
[00:39:57] SPEAKER_01: is that sometimes even though the model
[00:39:59] SPEAKER_01: reaches the correct answer, it does so in all these crazy ways.
[00:40:03] SPEAKER_01: So it may have into intermediate trajectory.
[00:40:07] SPEAKER_01: It may have tried 50 different times and failed,
[00:40:10] SPEAKER_01: but eventually it just randomly lands on a correct number
[00:40:14] or maybe it's,
[00:40:20] SPEAKER_01: sometimes it just does things very inefficiently
[00:40:22] SPEAKER_01: or it almost for war tax a way to get at the correct answer.
[00:40:27] SPEAKER_01: And so I think paying attention to the trajectory
[00:40:28] SPEAKER_01: is actually a really great point.
[00:40:30] SPEAKER_01: And I think it's also really important
[00:40:33] SPEAKER_01: because some of these trajectories can be very, very long.
[00:40:36] SPEAKER_01: And so if all you're doing is checking
[00:40:38] SPEAKER_01: whether or not the model reaches the final answer,
[00:40:41] SPEAKER_01: it's like there's all this information
[00:40:43] SPEAKER_01: about how the model behaved in the immediate step
[00:40:47] SPEAKER_01: that's missing.
[00:40:48] SPEAKER_01: Like sometimes you want models to get to the correct answer
[00:40:51] SPEAKER_01: by reflecting on what it did.
[00:40:53] SPEAKER_01: Sometimes you want it to get at the correct answer
[00:40:55] SPEAKER_01: by just one charting it.
[00:40:56] SPEAKER_01: And if you ignore all of that,
[00:40:58] SPEAKER_01: it's just like teaching, teaching it,
[00:41:00] SPEAKER_01: it's just missing a lot of the information
[00:41:01] SPEAKER_01: that you could be teaching all of it to do.
[00:41:03] SPEAKER_01: I love that.
[00:41:04] SPEAKER_00: It just tries a bunch of stuff and eventually gets a right.
[00:41:07] SPEAKER_00: You don't want it to learn.
[00:41:08] SPEAKER_00: This is the way to get there.
[00:41:09] SPEAKER_00: There's often a much more efficient way of doing it.
[00:41:11] SPEAKER_00: You mentioned all the kind of the steps
[00:41:12] SPEAKER_00: we've taken along the journey of helping a model
[00:41:16] SPEAKER_00: to get smarter.
[00:41:17] SPEAKER_00: Since you've been so close to this for so long,
[00:41:18] SPEAKER_00: I think this is going to be really helpful for people.
[00:41:21] SPEAKER_00: What's kind of like been the steps along the way
[00:41:23] SPEAKER_00: from the first post training that
[00:41:25] SPEAKER_00: has most helped models advance?
[00:41:27] SPEAKER_00: Where did Evales fit in?
[00:41:29] SPEAKER_00: The Arle environments?
[00:41:30] SPEAKER_00: What's been the steps and now we're heading towards
[00:41:32] SPEAKER_00: our environments?
[00:41:33] SPEAKER_00: Originally, the way models started getting post-trained
[00:41:39] SPEAKER_01: was purely through SFT.
[00:41:41] And what does that stand for?
[00:41:42] SPEAKER_01: So SFT stands for supervised fine tuning.
[00:41:46] SPEAKER_01: And it's a lot like, so again,
[00:41:47] SPEAKER_01: I think often in terms of these human analogies,
[00:41:49] SPEAKER_01: and so SFT is a lot like mimicking a master
[00:41:52] SPEAKER_01: and copying what they do.
[00:41:54] And then our RLHF became very dominant.
[00:41:58] And an analogie there would be sometimes
[00:41:59] SPEAKER_01: you learn by writing 55 different essays
[00:42:02] SPEAKER_01: and someone telling you which one they liked the most.
[00:42:04] SPEAKER_01: And then I think over the past year or so,
[00:42:06] SPEAKER_01: RUBERS and VARIFIERS have a convert error boring.
[00:42:10] And RUBERS and VARIFIERS are like,
[00:42:12] SPEAKER_01: learning by being graded and getting detailed feedback
[00:42:15] SPEAKER_01: on where you went wrong.
[00:42:16] SPEAKER_01: And those are Evales, though, on the other word for that.
[00:42:19] Yeah, yeah.
[00:42:20] SPEAKER_01: So I think Evales often covers two terms.
[00:42:25] SPEAKER_01: One is you are using the evaluations
[00:42:28] SPEAKER_01: for training because you're evaluating
[00:42:30] SPEAKER_01: whether or not the model did a good job.
[00:42:32] SPEAKER_01: And when it does do a good job, you're overwording it.
[00:42:35] SPEAKER_01: And then there's this other notion of Evales
[00:42:36] SPEAKER_01: where you're trying to measure the model's progress.
[00:42:38] SPEAKER_01: Like, OK, yeah, I have five different candidate checkpoints.
[00:42:42] SPEAKER_01: And I want to pick the one that's best
[00:42:45] SPEAKER_01: in order to release it to the public.
[00:42:46] SPEAKER_01: So I'm going to run all these Evales
[00:42:48] SPEAKER_01: on these five different checkpoints
[00:42:49] SPEAKER_01: in order to decide which one, which one is best.
[00:42:50] SPEAKER_01: Awesome.
[00:42:51] SPEAKER_01: Yeah, and yeah, now we have RRM.
[00:42:53] SPEAKER_01: And so it's kind of like a hot new thing.
[00:42:55] Awesome.
[00:42:56] SPEAKER_00: So I love about this business here.
[00:42:57] SPEAKER_00: And it's just there's always something there.
[00:42:58] SPEAKER_00: There's always this like, OK, we're getting so good at just
[00:43:01] SPEAKER_00: all this beautiful data for companies.
[00:43:03] SPEAKER_00: And now they need something completely different.
[00:43:04] SPEAKER_00: Now we're sending up all these virtual machines for them
[00:43:06] SPEAKER_00: in all these different use cases.
[00:43:08] SPEAKER_00: And it feels like that's a big part of this industry.
[00:43:10] SPEAKER_00: And it's just adapting to what labs are asking for.
[00:43:13] SPEAKER_00: Yeah, yeah.
[00:43:14] SPEAKER_01: So I mean, I really do think that we are going to need
[00:43:18] SPEAKER_01: to build a suite of products that
[00:43:21] SPEAKER_01: reflect the million different ways that humans are in.
[00:43:24] SPEAKER_01: And like, for example, think about becoming a great writer.
[00:43:27] SPEAKER_01: You don't become great by memorizing a bunch of grammar rules.
[00:43:31] SPEAKER_01: You become great by reading great books.
[00:43:33] SPEAKER_01: And you practice writing.
[00:43:35] SPEAKER_01: And you get feedback from your teachers and for the people
[00:43:37] SPEAKER_01: who buy your books in the book store and leave reviews.
[00:43:39] SPEAKER_01: And you notice what works and what doesn't.
[00:43:42] SPEAKER_01: And you develop taste by being exposed to all these masterpieces
[00:43:45] SPEAKER_01: and also just terrible writing.
[00:43:47] SPEAKER_01: So you learn through this endless cycle of practicing
[00:43:49] SPEAKER_01: reflection and each type of learning that you have.
[00:43:52] SPEAKER_01: Again, like, these are all very, very different methods
[00:43:55] SPEAKER_01: of learning to become a great writer.
[00:43:57] SPEAKER_01: So just in the same way that there's a thousand for ways
[00:44:00] SPEAKER_01: that the great writer becomes great,
[00:44:02] SPEAKER_01: I think there's going to be a thousand different ways
[00:44:04] SPEAKER_01: that they all need to learn.
[00:44:05] SPEAKER_01: It's so interesting.
[00:44:06] SPEAKER_00: This just ends up being like just like humans in so many ways.
[00:44:10] SPEAKER_00: It makes sense because in a sense neural networks deep learning
[00:44:12] SPEAKER_00: is modeled after how humans have learned
[00:44:15] SPEAKER_00: and how our brains operate.
[00:44:16] SPEAKER_00: But it's interesting just to make them smarter.
[00:44:19] SPEAKER_00: It's how do we come closer to how humans learn more and more?
[00:44:22] Yeah, it's almost like maybe the end goal
[00:44:24] SPEAKER_01: is just throwing you into the environment.
[00:44:25] SPEAKER_01: And just seeing how you evolve.
[00:44:29] SPEAKER_01: But within that evolution there's
[00:44:31] SPEAKER_01: all these different sub learning mechanisms.
[00:44:34] SPEAKER_01: Yeah, which is kind of what we're doing now.
[00:44:35] SPEAKER_00: So that's really interesting.
[00:44:36] SPEAKER_00: This might be the last step of until we hit age yet.
[00:44:39] SPEAKER_00: Along these lines, something that's really unique to search
[00:44:41] SPEAKER_00: that I learned is you guys have your own research team,
[00:44:44] SPEAKER_00: which I think is pretty rare.
[00:44:47] SPEAKER_00: Talk about just why that's something
[00:44:48] SPEAKER_00: you guys have invested in and what has come out of that investment.
[00:44:52] SPEAKER_00: Yeah, so I think that stems from my own background.
[00:44:55] SPEAKER_01: Like my own background is as a researcher.
[00:44:58] SPEAKER_01: And so I've always cared fundamentally
[00:45:02] SPEAKER_01: about pushing the industry and pushing the research community
[00:45:05] SPEAKER_01: and not just about revenue.
[00:45:09] SPEAKER_01: And so I think what our research team does
[00:45:12] SPEAKER_01: is a couple of different things.
[00:45:13] SPEAKER_01: So we almost have two types of researchers
[00:45:15] SPEAKER_01: of our company.
[00:45:16] SPEAKER_01: One is or four deploy researchers who
[00:45:18] SPEAKER_01: are often working hand in hand with our customers
[00:45:21] SPEAKER_01: to help them understand their models.
[00:45:23] SPEAKER_01: So we will work very closely with our other customers
[00:45:26] SPEAKER_01: to help them understand, OK, this is where your model is today.
[00:45:29] SPEAKER_01: This is where you're logging behind all the competitors.
[00:45:32] SPEAKER_01: These are some ways that you could be improving
[00:45:34] SPEAKER_01: the future given your goals.
[00:45:36] SPEAKER_01: And we're going to design these data sets,
[00:45:38] SPEAKER_01: these evaluation methods, these training techniques,
[00:45:41] SPEAKER_01: to make your models better.
[00:45:43] SPEAKER_01: So this is a very, very, very, very collaborative
[00:45:47] SPEAKER_01: notion of working with our customers,
[00:45:49] SPEAKER_01: like being research-solving cells,
[00:45:51] SPEAKER_01: just a little bit more focused on the data side.
[00:45:53] SPEAKER_01: And we're going to handle them to do whatever
[00:45:55] SPEAKER_01: takes to make them the best.
[00:45:57] And then we also have our internal researchers.
[00:45:59] SPEAKER_01: So our internal researchers are focused on slightly
[00:46:02] SPEAKER_01: different things.
[00:46:03] SPEAKER_01: So they are focused on building better benchmarks
[00:46:06] SPEAKER_01: and better leaderboards.
[00:46:07] SPEAKER_01: So I talked a lot about how I worry
[00:46:09] SPEAKER_01: that the leaderboards and benchmarks
[00:46:10] SPEAKER_01: alter today are steering models in the wrong direction.
[00:46:13] SPEAKER_01: So the question is, how do we fix that?
[00:46:15] SPEAKER_01: And so that's what our research team is focused on, really,
[00:46:18] SPEAKER_01: what we have really focused really heavily on right now.
[00:46:21] SPEAKER_01: So they're working a lot on that.
[00:46:23] SPEAKER_01: And they're also working on these other things, like, OK,
[00:46:25] SPEAKER_01: we need to train our models to see what
[00:46:27] SPEAKER_01: have the data performance the best.
[00:46:30] SPEAKER_01: What types of people for performance the best?
[00:46:31] SPEAKER_01: And so they are also working on all these kind of training
[00:46:34] SPEAKER_01: techniques and evaluation of our own data sets
[00:46:37] SPEAKER_01: to improve or data operations.
[00:46:41] SPEAKER_01: And the internal data products that we have
[00:46:44] SPEAKER_01: that determine what makes something good quality.
[00:46:46] SPEAKER_00: It's such a cool thing, because I don't think,
[00:46:48] SPEAKER_00: like, basically, of the labs and researchers
[00:46:50] SPEAKER_00: helping them advance AI.
[00:46:53] SPEAKER_00: I imagine it's pretty rare for a company
[00:46:55] SPEAKER_00: to have researchers actually doing primary research on AI.
[00:46:59] Yeah, yeah.
[00:47:00] SPEAKER_01: I think it's just because it's something
[00:47:01] SPEAKER_01: I fundamentally always cared about.
[00:47:03] SPEAKER_01: I often think about us more like a research five
[00:47:06] SPEAKER_01: then a startup, because that is my goal.
[00:47:09] SPEAKER_01: It's kind of funny, but I've always said,
[00:47:12] SPEAKER_01: I would rather be Terence Tau and then Warren Buffett.
[00:47:15] SPEAKER_01: So that notion of creating research
[00:47:18] SPEAKER_01: that pushes different here forward
[00:47:21] SPEAKER_01: and not just getting some valuation,
[00:47:23] SPEAKER_01: like, that's always been what drives me.
[00:47:25] SPEAKER_01: And it's worked out.
[00:47:27] SPEAKER_00: That's the beautiful thing about this.
[00:47:28] SPEAKER_00: You mentioned that you were higher researchers,
[00:47:30] SPEAKER_00: there anything that you want to share folks
[00:47:31] SPEAKER_00: you're looking for.
[00:47:32] SPEAKER_00: So we look for people who are just fundamentally
[00:47:36] SPEAKER_01: interested in data set all day.
[00:47:38] SPEAKER_01: So types of people who could literally spend 10 hours
[00:47:41] SPEAKER_01: digging through a data set and playing around with models
[00:47:45] SPEAKER_01: and thinking, okay, yeah, this is where I think
[00:47:49] SPEAKER_01: the model is failing.
[00:47:50] SPEAKER_01: This is kind of a behavior you want the model to have instead.
[00:47:54] SPEAKER_01: And just this aspect of being very hands-on
[00:47:56] SPEAKER_01: I think about the qualitative aspects of models
[00:47:58] SPEAKER_01: and not just the quantity of the parts.
[00:48:00] SPEAKER_01: So again, it's like this aspect of being hands-on with data
[00:48:03] and not just caring about these kind of abstract algorithms.
[00:48:07] SPEAKER_01: Awesome.
[00:48:08] SPEAKER_00: I want to ask a couple broad AI kind of market questions.
[00:48:11] SPEAKER_00: What else do you think is coming in the next couple of years
[00:48:14] SPEAKER_00: that people are maybe not thinking enough about
[00:48:16] SPEAKER_00: or not expecting in terms of where AI is heading,
[00:48:19] SPEAKER_00: what's going to matter?
[00:48:20] I think one of the things that's going to happen
[00:48:21] SPEAKER_01: in the next few years is that the models are actually
[00:48:24] SPEAKER_01: going to become increasingly differentiated
[00:48:26] SPEAKER_01: because of the personalities and behaviors
[00:48:32] SPEAKER_01: that the different labs have
[00:48:36] and the kind of objective functions
[00:48:39] SPEAKER_01: that they are optimizing their models for.
[00:48:41] SPEAKER_01: Like I think it's one thing I didn't appreciate
[00:48:43] SPEAKER_01: to year or so ago.
[00:48:44] SPEAKER_01: Like a year or so ago, I thought that all the HA models
[00:48:48] SPEAKER_01: would essentially become very, very commoditized.
[00:48:51] SPEAKER_01: They would all behave like each other
[00:48:53] SPEAKER_01: and sure one of them might be slightly more intelligent
[00:48:56] SPEAKER_01: in one way today, but sure, the other ones
[00:48:58] SPEAKER_01: would catch up in the next few months.
[00:49:00] SPEAKER_01: But I think over the past year, I've realized that the values
[00:49:03] SPEAKER_01: that the companies have will shape the model.
[00:49:09] SPEAKER_01: So let me give an example.
[00:49:12] SPEAKER_01: So I was asking Claw to help me drop
[00:49:14] SPEAKER_01: an email the other day and it went through 30 different versions.
[00:49:19] SPEAKER_01: And after 30 minutes, yeah, I think it really crafted me
[00:49:21] SPEAKER_01: to perfect email and I sent it.
[00:49:23] But then I realized I spent 30 minutes doing something
[00:49:26] SPEAKER_01: that didn't matter at all.
[00:49:27] SPEAKER_01: Like sure, now I got the perfect email,
[00:49:29] SPEAKER_01: but I spent 30 minutes doing something
[00:49:30] SPEAKER_01: I wouldn't have worried at all before
[00:49:32] SPEAKER_01: and this email probably didn't even move
[00:49:33] SPEAKER_01: and you don't want anything anyways.
[00:49:35] SPEAKER_01: So I think there's a deep question here, which is,
[00:49:37] SPEAKER_01: if you could choose the perfect model behavior,
[00:49:40] SPEAKER_01: which model would you want?
[00:49:42] SPEAKER_01: Do you want a model that says, you're absolutely right.
[00:49:44] SPEAKER_01: There are definitely 20 more ways to improve this email
[00:49:47] SPEAKER_01: and it continues for 50 more iterations
[00:49:48] SPEAKER_01: and it sucks up all your time and engagement.
[00:49:51] Or do you want a model that's optimizing for your time
[00:49:53] SPEAKER_01: and productivity and just says, no, you need to stop.
[00:49:56] SPEAKER_01: You're the most great.
[00:49:57] SPEAKER_01: Just send it and move on with your day.
[00:49:59] And again, like again, just because like in the same way,
[00:50:01] SPEAKER_01: there's like a fork in a road between how you could choose
[00:50:04] SPEAKER_01: how your model behaves for this question.
[00:50:06] SPEAKER_01: It's like for every other question that models have,
[00:50:11] the kind of behavior that you want will fundamentally affect it.
[00:50:17] SPEAKER_01: It's almost like in the same way that when Google builds
[00:50:19] SPEAKER_01: a search engine, it's very, very different from how Facebook
[00:50:23] SPEAKER_01: would build a search engine, which is very, very different
[00:50:25] SPEAKER_01: from how Apple would build a search engine.
[00:50:27] SPEAKER_01: Like they all have their own principles and values
[00:50:30] SPEAKER_01: and things that are trying to achieve in the world
[00:50:33] SPEAKER_01: that shape all the products that they're going to build.
[00:50:35] SPEAKER_01: And in the same way, I think all the all the delums
[00:50:39] SPEAKER_01: will start being very, very, very different.
[00:50:41] SPEAKER_01: That is incredibly interesting.
[00:50:43] SPEAKER_00: You already see that with GROC.
[00:50:44] SPEAKER_00: It's got like a very different personality
[00:50:46] SPEAKER_00: and a very different approach to answering questions.
[00:50:49] SPEAKER_00: And so what I'm hearing is you're going to see more
[00:50:51] SPEAKER_00: of this differentiation.
[00:50:52] SPEAKER_00: Yep.
[00:50:53] Kind of another question along these lines.
[00:50:55] SPEAKER_00: What do you think is most underhyped in AI?
[00:50:58] SPEAKER_00: They think maybe people aren't talking enough about
[00:51:00] SPEAKER_00: that is really cool.
[00:51:01] SPEAKER_00: And what do you think is overhyped?
[00:51:03] SPEAKER_00: So I think one thing that was underhyped is the built-in product
[00:51:10] SPEAKER_01: that all of the chat parts are going to start having.
[00:51:13] SPEAKER_01: Like I've always been a huge fan of college artifacts.
[00:51:15] SPEAKER_01: And I think it just works really, really well.
[00:51:17] SPEAKER_01: And actually the other day, I don't know if it's a new feature or not,
[00:51:20] SPEAKER_01: but it's asking me to help me create an email.
[00:51:24] SPEAKER_01: And then it just created, so it didn't work because it didn't allow me
[00:51:28] SPEAKER_01: to send an email.
[00:51:28] SPEAKER_01: But what it created instead was like a little,
[00:51:30] SPEAKER_01: I don't even recall like a little box where I could click on it.
[00:51:33] SPEAKER_01: And it would just text someone that did this message.
[00:51:37] SPEAKER_01: And I think that concept of taking artifacts to the next level,
[00:51:43] SPEAKER_01: where you just have these mini apps, mini UIs within the chat parts themselves.
[00:51:49] SPEAKER_01: I feel like people aren't talking enough about that.
[00:51:51] SPEAKER_01: So I think that that's one underhyped area.
[00:51:53] SPEAKER_01: And in terms of overhyped areas, I definitely
[00:51:57] SPEAKER_01: think that vibe coding is overhyped.
[00:51:58] SPEAKER_01: I think people don't realize how much it's
[00:52:03] SPEAKER_01: going to make your systems unmechanable in the long term.
[00:52:07] SPEAKER_01: It'd be something dump this code into your code bases.
[00:52:09] SPEAKER_01: You've seen the worker out right now.
[00:52:12] So I kind of worry about a feature coding.
[00:52:15] SPEAKER_01: Yeah, it's just going to get from that.
[00:52:17] These are amazing answers.
[00:52:18] SPEAKER_00: On that first point, there's something I actually
[00:52:21] SPEAKER_00: asked I had the two product office servant,
[00:52:23] SPEAKER_00: Thoropic An open AI Kevin Wheel and Mike Regar on the podcast.
[00:52:26] SPEAKER_00: And I asked him just like, as a product team,
[00:52:27] like you have this gigabrain intelligence,
[00:52:30] SPEAKER_00: how long do you even need product teams?
[00:52:32] SPEAKER_00: So you think this is this AI will just create the product for you.
[00:52:36] SPEAKER_00: Here's what I want.
[00:52:37] SPEAKER_00: Well, it's like the next level of vibe coding.
[00:52:39] SPEAKER_00: It's just like tell it here's what I want.
[00:52:41] SPEAKER_00: And it's just building the product and involving the product
[00:52:44] SPEAKER_00: as you're using it.
[00:52:44] SPEAKER_00: And it feels like that's what you're describing
[00:52:46] SPEAKER_00: is where we might be heading.
[00:52:47] SPEAKER_00: Yeah, yeah.
[00:52:48] SPEAKER_00: I think there's a very, very powerful notion
[00:52:50] SPEAKER_00: where it helps people just achieve their ideas
[00:52:54] SPEAKER_00: in a much more way.
[00:52:55] SPEAKER_00: Something we haven't gotten into that I think is really
[00:52:58] SPEAKER_00: interesting is just the story of how you got to starting
[00:53:01] SPEAKER_00: Surge you had you have a really neat background.
[00:53:04] SPEAKER_00: I always think about these Brian, Brian Armstrong,
[00:53:07] SPEAKER_00: the founder of Coinbase once gave this talk
[00:53:09] SPEAKER_00: that is really stuck with me where he kind of talked about
[00:53:12] SPEAKER_00: how his very unique background allowed him to start Coinbase.
[00:53:15] SPEAKER_00: So you had like economics background,
[00:53:18] SPEAKER_00: he had a cryptography experience
[00:53:20] SPEAKER_00: that then he was an engineer.
[00:53:21] SPEAKER_00: And it's got this like the perfect then diagram
[00:53:23] SPEAKER_00: for starting Coinbase.
[00:53:24] SPEAKER_00: And I feel like you have a very similar story with Surge.
[00:53:26] SPEAKER_00: Talk about that your background there
[00:53:28] SPEAKER_00: and how you led, how that led to Surge.
[00:53:31] SPEAKER_00: Going way back, I was always fascinated by math and language
[00:53:34] SPEAKER_01: when I was a kid.
[00:53:35] SPEAKER_01: Like I went to MIT because it's obviously
[00:53:37] SPEAKER_01: one of best places for math and CS.
[00:53:39] SPEAKER_01: But also because it's the home of Nomsky.
[00:53:42] SPEAKER_01: My dream in school was actually to find some underlying theory
[00:53:45] SPEAKER_01: connecting all these different fields.
[00:53:47] And then I became a researcher at Google
[00:53:49] SPEAKER_01: and Facebook and Twitter.
[00:53:50] SPEAKER_01: And I just kept running it the same problem over and over again.
[00:53:54] SPEAKER_01: It was impossible to get the data that we
[00:53:56] SPEAKER_01: needed to train our models.
[00:53:57] SPEAKER_01: So I was always a huge believer in need
[00:53:59] SPEAKER_01: for high quality data.
[00:54:01] And then GB3 came out in 2020.
[00:54:03] SPEAKER_01: And I realized that yeah, if we wanted to take things
[00:54:06] SPEAKER_01: to the next level and build models that could code
[00:54:08] SPEAKER_01: and use tools and toe jokes and write poetry
[00:54:11] SPEAKER_01: and solve every math problem.
[00:54:12] SPEAKER_01: This is a cure cancer.
[00:54:13] SPEAKER_01: And yeah, we were going to need a completely new solution.
[00:54:15] SPEAKER_01: Like the thing that always drove me crazy
[00:54:17] SPEAKER_01: when I was out of these companies
[00:54:18] SPEAKER_01: was we had a full power of the human mind in front of us.
[00:54:21] SPEAKER_01: And all the details just as soon as out there
[00:54:23] SPEAKER_01: were focused on really simple things like image labeling.
[00:54:26] SPEAKER_01: So I wanted to build something focused on all these advanced
[00:54:29] SPEAKER_01: complex use cases instead that would really help us
[00:54:32] SPEAKER_01: build in extra nation models.
[00:54:34] SPEAKER_01: So yeah, I think my background in cross math
[00:54:36] SPEAKER_01: and computer science and linguistics
[00:54:38] SPEAKER_01: really really informed what I always wanted to do.
[00:54:41] SPEAKER_01: And so I started searching my later with
[00:54:43] SPEAKER_01: with their one mission to basically build the use cases
[00:54:47] SPEAKER_01: that thought we're going to be needed to push
[00:54:48] SPEAKER_01: different your back.
[00:54:49] SPEAKER_01: And you said a month later, a month later, after what?
[00:54:52] SPEAKER_00: After GB3 launch in 2021.
[00:54:53] SPEAKER_00: Oh, OK.
[00:54:54] SPEAKER_00: Wow.
[00:54:54] SPEAKER_00: OK.
[00:54:56] SPEAKER_00: A great decision.
[00:54:57] SPEAKER_00: What we're just going to drive you at this point
[00:54:59] SPEAKER_00: of other than just the epic success you're having,
[00:55:02] SPEAKER_00: what keeps you motivated to keep building this
[00:55:04] SPEAKER_00: and building something in this space?
[00:55:06] SPEAKER_00: I think I'm a scientist at heart.
[00:55:09] SPEAKER_01: I always thought I was going to become this math or CS professor
[00:55:14] SPEAKER_01: and work on trying to understand a universe
[00:55:16] SPEAKER_01: and language and a nature of communication.
[00:55:20] Like it's kind of funny, but I always
[00:55:21] SPEAKER_01: had this fanciful dream where if aliens ever came to visit Earth,
[00:55:24] SPEAKER_01: and we need to figure out how to communicate with them,
[00:55:26] SPEAKER_01: I wanted to be the one to government with goal.
[00:55:28] SPEAKER_01: And I'd use all this fancy math, think of your science,
[00:55:31] SPEAKER_01: and linguistics to decipher it.
[00:55:33] SPEAKER_01: So even today, what I love doing most
[00:55:34] SPEAKER_01: is every time a new model is released,
[00:55:37] SPEAKER_01: we'll actually do a really deep dive into the model itself.
[00:55:40] SPEAKER_01: I'll play around with it, I'll run eVals,
[00:55:42] SPEAKER_01: I'll compare where it's improved, where it's a rest.
[00:55:44] SPEAKER_01: I'll create this really deep dive analysis
[00:55:46] SPEAKER_01: that we send our customers.
[00:55:48] SPEAKER_01: And it's actually kind of funny, because a lot of times,
[00:55:50] SPEAKER_01: we will say it's from our data science team,
[00:55:52] SPEAKER_01: but often it's actually just for me.
[00:55:54] And I think I could do this all day.
[00:55:56] SPEAKER_01: I have a very hard time being a meeting of the whole day.
[00:55:58] SPEAKER_01: I'm terrible at sales, I'm terrible at doing
[00:56:00] SPEAKER_01: the typical seal things that people expect you to do,
[00:56:03] SPEAKER_01: but I love writing these analyses.
[00:56:05] SPEAKER_01: I love jamming with a research team I want you're seeing.
[00:56:08] SPEAKER_01: So I'm going to be up into a three,
[00:56:09] SPEAKER_01: I'm just talking on a phone with somebody
[00:56:11] SPEAKER_01: on a research team and taking a dream model.
[00:56:13] SPEAKER_01: So I above that, I still get to be really hands on,
[00:56:16] SPEAKER_01: working on the data, and then the science all day.
[00:56:19] And I think what drives me is that I want
[00:56:21] SPEAKER_01: surge to play this critical role in a future of AI,
[00:56:24] SPEAKER_01: which I think is also the future of humanity.
[00:56:26] SPEAKER_01: Like we have these really unique perspectives
[00:56:28] SPEAKER_01: on data and language and quality, and how to measure all this,
[00:56:31] SPEAKER_01: and how to ensure it's all going on our way past.
[00:56:33] SPEAKER_01: And I think we're uniquely unconstrained
[00:56:35] SPEAKER_01: by all of these influences that can sometimes
[00:56:38] SPEAKER_01: steer companies in a negative direction.
[00:56:41] SPEAKER_01: Like what I was saying earlier,
[00:56:43] SPEAKER_01: we built surge a lot more like a research lab
[00:56:45] SPEAKER_01: than a tip will start up.
[00:56:46] SPEAKER_01: So we care about curiosity and long-term incentives
[00:56:50] SPEAKER_01: and intellectual rigor.
[00:56:52] SPEAKER_01: And we don't care as much about quarterly metrics
[00:56:55] SPEAKER_01: and what's going to look good at NBORDAQ.
[00:56:57] SPEAKER_01: And so my goal is to take all these unique things
[00:56:59] SPEAKER_01: about as a company and use that to make sure
[00:57:01] SPEAKER_01: that we're shaping AI in a way that's really beneficial
[00:57:04] SPEAKER_01: for a species and a water.
[00:57:06] When I'm realizing in this conversation
[00:57:07] SPEAKER_00: is just how much influence you have
[00:57:10] SPEAKER_00: and companies like yours have on where AI heads,
[00:57:12] SPEAKER_00: the fact that you help labs understand
[00:57:15] SPEAKER_00: where they have gaps and where they need to improve.
[00:57:19] SPEAKER_00: And it's not just, you know, everyone looks at just like
[00:57:21] SPEAKER_00: the heads of open AI and the topic,
[00:57:23] SPEAKER_00: all these companies that are the ones ushering in AI,
[00:57:25] SPEAKER_00: but what I'm hearing here is you have a lot of influence
[00:57:28] SPEAKER_00: on where things have to.
[00:57:30] Yeah, I think there's this really powerful ecosystem
[00:57:32] SPEAKER_01: where honestly people just don't know
[00:57:37] SPEAKER_01: where models are headed and how do you want to shape them yet
[00:57:42] SPEAKER_01: and how do you want humanity to kind of play a role
[00:57:47] SPEAKER_01: in the future of all this.
[00:57:48] SPEAKER_01: And so I think there's a lot of opportunity
[00:57:50] SPEAKER_01: to just continue shaping this discussion.
[00:57:52] SPEAKER_01: Along that thread, I know you have a very strong thesis
[00:57:54] SPEAKER_00: on just why this work matters to humanity
[00:57:57] SPEAKER_00: and why this is so important.
[00:57:59] SPEAKER_00: Talk about that.
[00:58:01] I'll get a bit of philosophical here,
[00:58:02] SPEAKER_01: but I think the question is obvious a bit
[00:58:04] SPEAKER_01: to the philosophical, so it's a bear with me.
[00:58:06] So the most straightforward way of thinking about what we do
[00:58:08] SPEAKER_01: is we train and evaluate AI.
[00:58:11] But there's a deeper mission that I often think about,
[00:58:14] SPEAKER_01: which is helping our customers think about their dream,
[00:58:16] SPEAKER_01: objective functions.
[00:58:18] SPEAKER_01: Like yeah, what kind of model do they want their model to be?
[00:58:20] SPEAKER_01: And once we help them do that,
[00:58:21] SPEAKER_01: we'll help them train your model to reach the North Star.
[00:58:24] SPEAKER_01: We'll help them measure their progress.
[00:58:26] SPEAKER_01: But it's really hard because objective functions
[00:58:28] SPEAKER_01: are really rich, shank of blacks.
[00:58:31] SPEAKER_01: It's kind of like the difference we need having
[00:58:33] SPEAKER_01: and kid and asking them, okay, what test do you want to pass?
[00:58:35] SPEAKER_01: Do you want them to get a high score in SAT
[00:58:37] SPEAKER_01: and write a really good college essay?
[00:58:39] SPEAKER_01: Like that's a simplistic version.
[00:58:40] SPEAKER_01: Versus what kind of person do you want them to grow up to be?
[00:58:44] SPEAKER_01: Will you be happy if they're happy no matter what they do?
[00:58:46] SPEAKER_01: Or are you hoping you'll go to a good school
[00:58:48] SPEAKER_01: and be financially successful?
[00:58:50] And again, if you take that notion,
[00:58:51] SPEAKER_01: it's like, okay, how do you define that?
[00:58:53] SPEAKER_01: How do you measure what are happy?
[00:58:54] SPEAKER_01: How do you measure whether they're financially successful?
[00:58:56] SPEAKER_01: Like it's a lot harder than something measuring
[00:58:58] SPEAKER_01: whether or not you're getting a high score in SAT.
[00:59:00] And what we're doing is we want to help
[00:59:02] SPEAKER_01: our customers reach, again, their dream North Stars
[00:59:06] SPEAKER_01: and figure out how to measure them.
[00:59:09] And so I talked about this example of
[00:59:15] SPEAKER_01: what you want models to do when you're asking them
[00:59:18] SPEAKER_01: to write 50 different email iterations.
[00:59:20] SPEAKER_01: You just continue them for 50 more or do you say,
[00:59:23] SPEAKER_01: no, just move on to the day because it's perfect enough.
[00:59:27] And the broader question is,
[00:59:31] SPEAKER_01: are we building these systems that actually advance humanity?
[00:59:34] SPEAKER_01: And so how do we build this data
[00:59:36] SPEAKER_01: that's the train towards that and measure it?
[00:59:38] SPEAKER_01: Are we optimizing for all these wrong things?
[00:59:40] SPEAKER_01: Just systems that suck up more and more of our time
[00:59:42] SPEAKER_01: and make us lazy or un-lazier.
[00:59:44] And yeah, I think it's really relevant to what we do
[00:59:46] SPEAKER_01: because it's very hard and difficult to measure
[00:59:49] SPEAKER_01: and define whether something is genuinely advancing humanity.
[00:59:51] SPEAKER_01: It's very easy to measure all these proxies
[00:59:53] SPEAKER_01: instead like clicks and likes.
[00:59:55] But I think that's why our work is so interesting.
[00:59:56] SPEAKER_01: We want to work in a hard and boring metrics
[00:59:58] SPEAKER_01: that require the hardest types of data
[00:59:59] SPEAKER_01: and not just the easy ones.
[01:00:01] SPEAKER_01: So I think one of the things I often say is,
[01:00:03] SPEAKER_01: you are your objective function.
[01:00:05] SPEAKER_01: So we want to reach complex objective functions
[01:00:07] SPEAKER_01: and not these simplistic rock cities.
[01:00:09] SPEAKER_01: And our job is to figure out how to get the data to match this.
[01:00:12] SPEAKER_01: So yeah, we want data, we want metrics to measure
[01:00:13] SPEAKER_01: whether AI is like making your life richer.
[01:00:15] SPEAKER_01: We want to train our systems this way.
[01:00:17] SPEAKER_01: And we want tools to make us more curious
[01:00:18] SPEAKER_01: and more creative, not just lazier.
[01:00:20] SPEAKER_01: And it's hard because humans are inherently lazy.
[01:00:23] SPEAKER_01: So AI's sub-radios are the easiest way
[01:00:25] SPEAKER_01: to get engagement and make all your metrics fall.
[01:00:28] So I think this question about choosing
[01:00:29] SPEAKER_01: the right objective functions
[01:00:30] SPEAKER_01: and making sure that we're optimizing towards them
[01:00:34] SPEAKER_01: and not just these easy proxies
[01:00:35] SPEAKER_01: is really really important for our future.
[01:00:37] Wow.
[01:00:38] SPEAKER_00: I love how what you're sharing here
[01:00:39] SPEAKER_00: gives you so much more appreciation
[01:00:41] SPEAKER_00: of the nuances of building AI, training,
[01:00:44] SPEAKER_00: the work that you're doing.
[01:00:45] SPEAKER_00: And the outside people could just look at surge
[01:00:46] SPEAKER_00: and companies in the space of, OK, go,
[01:00:48] SPEAKER_00: they're just creating all the data, feeding it to AI.
[01:00:51] SPEAKER_00: But clearly, there's so much to this
[01:00:53] SPEAKER_00: that people don't realize.
[01:00:56] SPEAKER_00: And I love knowing that you're at the head of this.
[01:00:59] SPEAKER_00: That's all I like you is thinking through this so deeply.
[01:01:02] Maybe one more question.
[01:01:03] SPEAKER_00: Is there something you wish you'd known
[01:01:05] SPEAKER_00: before you started surge?
[01:01:07] SPEAKER_00: A lot of people start companies
[01:01:08] SPEAKER_00: they don't know what they're getting into.
[01:01:09] SPEAKER_00: Is there something you wish you could tell your earlier self?
[01:01:11] SPEAKER_00: Yeah.
[01:01:12] SPEAKER_00: So I definitely wish I known that you
[01:01:13] SPEAKER_01: could build a company by being heads down
[01:01:15] SPEAKER_01: and doing great research and simply building something amazing.
[01:01:18] SPEAKER_01: And not by constantly tweeting and hyping and fundraising.
[01:01:21] It's kind of funny, but I never thought
[01:01:22] SPEAKER_01: I wanted to start a company.
[01:01:23] SPEAKER_01: I love doing research.
[01:01:25] And I was actually always a huge fan of DeepMind
[01:01:27] SPEAKER_01: because they were this amazing research company
[01:01:29] SPEAKER_01: that got by and still managed to keep
[01:01:31] SPEAKER_01: on doing amazing science.
[01:01:32] SPEAKER_01: But I always thought that they were this magical ILR year
[01:01:35] SPEAKER_01: and corn.
[01:01:37] So I thought if I started a company,
[01:01:38] SPEAKER_01: I'd have to become a business person,
[01:01:39] SPEAKER_01: looking at financials all day and building
[01:01:41] SPEAKER_01: being in meetings all day and doing all this stuff
[01:01:43] SPEAKER_01: that excited and incredibly boring.
[01:01:45] SPEAKER_01: And I always ate it.
[01:01:46] SPEAKER_01: So I think it's crazy that didn't end up being true at all.
[01:01:49] SPEAKER_01: I'm still in the weeds in the day to every day.
[01:01:52] SPEAKER_01: And I love it.
[01:01:53] SPEAKER_01: I love that I get to do all these analyses
[01:01:54] SPEAKER_01: and talk to researchers.
[01:01:56] SPEAKER_01: And it's basically applied research
[01:01:57] SPEAKER_01: where we're building all these amazing data systems
[01:02:00] SPEAKER_01: that are really pushed to front of your AI.
[01:02:02] So yeah, I wish I known that you don't need to spend
[01:02:03] SPEAKER_01: all your time fundraising.
[01:02:04] SPEAKER_01: You don't need to constantly generate hype.
[01:02:06] SPEAKER_01: You don't need to become someone you're not.
[01:02:07] SPEAKER_01: You can actually build this because that's
[01:02:09] SPEAKER_01: a small company by simply building something so good
[01:02:11] SPEAKER_01: that it cuts through all that noise.
[01:02:13] And I think if I known this was possible,
[01:02:15] SPEAKER_01: I would have started even sooner.
[01:02:16] SPEAKER_01: So I hope she's on that.
[01:02:18] SPEAKER_01: That is such an amazing place to end.
[01:02:20] SPEAKER_00: I feel like this is exactly the founders DT here.
[01:02:23] SPEAKER_00: And I think this conversation is going to inspire
[01:02:24] SPEAKER_00: a lot of founders.
[01:02:26] SPEAKER_00: And especially a lot of founders
[01:02:27] SPEAKER_00: that want to do things in a different way.
[01:02:29] Before we get to our very exciting lightning round,
[01:02:31] SPEAKER_00: is there anything else you want to share anything else
[01:02:32] SPEAKER_00: you want to leave it as listeners with?
[01:02:34] SPEAKER_00: We covered a lot of ground.
[01:02:35] SPEAKER_00: It's totally OK to say no as well.
[01:02:37] SPEAKER_00: So the thing to think about with NWIFT
[01:02:38] SPEAKER_00: is I think a lot of people think of data labeling
[01:02:42] SPEAKER_01: as really simplistic work, like labeling cat photos
[01:02:45] SPEAKER_01: and drawing down a box of your own cars.
[01:02:47] SPEAKER_01: And so I've actually always hated word day
[01:02:48] SPEAKER_01: labeling because it just paints this very simplistic picture
[01:02:52] SPEAKER_01: when I think what we're doing is completely different.
[01:02:55] I think a lot about what we're doing
[01:02:58] SPEAKER_01: hasn't a lot more like raising a child.
[01:03:00] SPEAKER_01: You don't just feed a child information.
[01:03:02] SPEAKER_01: You're teaching them values and creativity
[01:03:04] SPEAKER_01: and what's beautiful.
[01:03:05] SPEAKER_01: And these infinite subtle things about what
[01:03:08] SPEAKER_01: makes somebody a good person.
[01:03:10] SPEAKER_01: And that's what we're doing for AI.
[01:03:12] SPEAKER_01: So I just often think about what we're doing as almost like
[01:03:20] the future of humanity or how are we
[01:03:23] SPEAKER_01: raising humanity's children?
[01:03:25] SPEAKER_01: So I'll leave it at that.
[01:03:28] SPEAKER_00: Well, now I love just how much philosophy
[01:03:30] SPEAKER_00: there is in this whole conversation
[01:03:31] SPEAKER_00: that I was not expecting with that.
[01:03:34] SPEAKER_00: Edwin, we've reached our very exciting lightning round.
[01:03:36] SPEAKER_00: I've got five questions for you.
[01:03:37] SPEAKER_00: Are you ready?
[01:03:38] SPEAKER_00: Yeah, let's go.
[01:03:39] SPEAKER_00: Here we go.
[01:03:40] SPEAKER_00: What are two or three books that you find yourself
[01:03:42] SPEAKER_00: recommending most other people?
[01:03:44] SPEAKER_00: Yes.
[01:03:45] SPEAKER_01: So three books I offer recommend are first story of your life
[01:03:49] SPEAKER_01: by Ted Chang.
[01:03:50] SPEAKER_01: It's my all-time favorite short story.
[01:03:52] SPEAKER_01: And it's about a linguist flirting in alien language.
[01:03:54] SPEAKER_01: And I obviously reread it over a couple years.
[01:03:56] SPEAKER_01: And that's what the interstellar was about, is that?
[01:03:59] SPEAKER_01: Oh, so there's a movie called Arrival.
[01:04:01] SPEAKER_01: Arrival, which is which is based on the story.
[01:04:03] SPEAKER_01: Which is what?
[01:04:04] SPEAKER_01: Great.
[01:04:04] SPEAKER_01: OK, keep going.
[01:04:06] SPEAKER_01: And then second, mythosysivis by Camus.
[01:04:09] SPEAKER_01: I actually can't really explain why I love this,
[01:04:11] SPEAKER_01: but I always find a final chapter,
[01:04:13] SPEAKER_01: some are really inspiring.
[01:04:15] And then third, the Tombow Demaro by Douglas Hofstetter.
[01:04:19] SPEAKER_01: And so I think Gerdo Escherbach is more famous book,
[01:04:22] SPEAKER_01: but I've actually always loved this one better.
[01:04:24] SPEAKER_01: It is he takes single French poem and translates, say,
[01:04:27] SPEAKER_01: 89 different ways and discusses all the motivations
[01:04:30] SPEAKER_01: behind each translation.
[01:04:32] And so I've always loved the way in bodies.
[01:04:34] SPEAKER_01: This idea that translation isn't this robotic thing
[01:04:36] SPEAKER_01: that you do.
[01:04:37] SPEAKER_01: And so there's a million different ways to think about
[01:04:38] SPEAKER_01: what makes a high quality translation.
[01:04:40] SPEAKER_01: Which made me look so loud the ways I think about data
[01:04:42] SPEAKER_01: and quality in all of them.
[01:04:43] SPEAKER_00: All these residents so deeply with the way,
[01:04:45] SPEAKER_00: with all the things we've been talking about,
[01:04:47] SPEAKER_00: especially that first one, if that was your goal after school
[01:04:49] SPEAKER_00: is like, I want to help translate alien language.
[01:04:53] SPEAKER_00: I'm not surprised you love that short story.
[01:04:55] SPEAKER_00: Next question, do you have a favorite recent movie or TV
[01:04:58] SPEAKER_00: show you really enjoyed?
[01:04:59] SPEAKER_00: One of my new all-time favorite TV shows
[01:05:03] SPEAKER_01: is something I found recently.
[01:05:05] SPEAKER_01: It's called Travelers.
[01:05:06] SPEAKER_01: It's basically about a group of travelers
[01:05:08] SPEAKER_01: from the future who are sent back in time
[01:05:10] SPEAKER_01: to prevent our call-clips.
[01:05:11] SPEAKER_01: Sorry, I just wrote an exam section.
[01:05:14] SPEAKER_01: And then I actually just rewashed contact,
[01:05:17] SPEAKER_01: which is one of my all-time favorite movies.
[01:05:19] SPEAKER_01: So yeah, one of the things you'll notice about me is that,
[01:05:21] SPEAKER_01: yeah, I love any kind of book or film that involves scientists
[01:05:24] SPEAKER_01: to separate the Cyphering and Alien communication.
[01:05:26] SPEAKER_01: Okay, just this dream I always had as a kid.
[01:05:28] SPEAKER_01: That's so funny.
[01:05:30] Okay, is there a product you recently discovered
[01:05:33] SPEAKER_00: that you really love?
[01:05:35] So it's funny, but I was in SF earlier this week,
[01:05:37] SPEAKER_01: and I finally took a Waymo for the first time.
[01:05:40] SPEAKER_01: Honestly, it was magical and it really felt like a living
[01:05:42] SPEAKER_01: future.
[01:05:43] Yeah, it's like the thing that you can people hype it like crazy,
[01:05:46] SPEAKER_00: but it always exceeds your expectations.
[01:05:48] SPEAKER_00: Yeah, it deserves hype.
[01:05:49] SPEAKER_00: It was crazy.
[01:05:49] SPEAKER_00: Yeah, it's absurd.
[01:05:50] SPEAKER_00: It's like, holy moly.
[01:05:51] SPEAKER_00: Like if you're not in SF, you don't realize just how
[01:05:53] SPEAKER_00: common these things are.
[01:05:54] SPEAKER_00: They're just like all over the place.
[01:05:55] SPEAKER_00: Just the driverless cars constantly going about.
[01:05:59] SPEAKER_00: And when you go to an event at the end,
[01:06:00] SPEAKER_00: there's just like all these Waymo's lined up picking people up.
[01:06:03] Yeah, Waymo, good job.
[01:06:04] SPEAKER_00: Good job over there.
[01:06:06] SPEAKER_00: Do you have a favorite life motto
[01:06:08] SPEAKER_00: that you find yourself coming back to in work or in life?
[01:06:11] SPEAKER_00: So I think I mentioned this idea that founders
[01:06:13] SPEAKER_01: should build a company that only they could build.
[01:06:16] SPEAKER_01: Almost like it's this destiny that their entire life
[01:06:18] SPEAKER_01: and experience is in interest, shaping towards.
[01:06:21] SPEAKER_01: And so I think that principle applies pretty broadly,
[01:06:23] SPEAKER_01: not just the founders, but the people creating anything.
[01:06:25] SPEAKER_01: Well, let me follow that thread to enlightening this answer.
[01:06:29] SPEAKER_00: Do you have any advice for how to
[01:06:32] build those sorts of experiences that help lead to that?
[01:06:35] SPEAKER_00: Is it follow things that are interesting to you?
[01:06:37] SPEAKER_00: Because it's easy to say that it's hard to actually
[01:06:39] SPEAKER_00: acquire these really unique sets of experiences
[01:06:41] SPEAKER_00: that allow you to create something really important?
[01:06:44] Yeah, so I think it would always be to
[01:06:47] SPEAKER_01: really follow your interests and do what you love.
[01:06:51] And it's almost like a lot of decisions I make about surge.
[01:06:55] SPEAKER_01: Like I think one of the things that I didn't think about
[01:06:58] a couple of years ago, but in someone said it to me,
[01:07:00] SPEAKER_01: it's that companies in a sense
[01:07:03] SPEAKER_01: are in the body of their CEO.
[01:07:06] SPEAKER_01: And it's kind of funny.
[01:07:06] SPEAKER_01: I didn't thought about that because I never quite knew
[01:07:09] SPEAKER_01: what a CEO did.
[01:07:09] SPEAKER_01: I always thought a CEO was kind of generic.
[01:07:11] SPEAKER_01: I was like, okay, you're just doing whatever VPs
[01:07:13] SPEAKER_01: and your board and whatever tell you to do
[01:07:15] SPEAKER_01: and just saying yes to decisions.
[01:07:17] SPEAKER_01: But instead, it's this idea where when I think about
[01:07:21] SPEAKER_01: certain big hard decisions we have to make,
[01:07:24] SPEAKER_01: I don't think what would company do?
[01:07:26] SPEAKER_01: I don't think what metrics are which one applies.
[01:07:29] SPEAKER_01: I just think what do I first think about?
[01:07:31] SPEAKER_01: Like what are my values and what do I want to see happen
[01:07:33] SPEAKER_01: in the world?
[01:07:34] SPEAKER_01: And so I think following that idea about, okay,
[01:07:38] SPEAKER_01: so ask yourself what are what are the values you care about?
[01:07:41] SPEAKER_01: What are things you're trying to shape
[01:07:44] SPEAKER_01: and not what will look at on a dashboard?
[01:07:47] SPEAKER_01: I think that's what's very important.
[01:07:49] I love how just you're just full of endless,
[01:07:51] SPEAKER_00: beautiful and very deep answers.
[01:07:55] Final question.
[01:07:56] SPEAKER_00: Something that you were quite famous for
[01:07:58] SPEAKER_00: before starting Surges, you built this map at Twitter
[01:08:02] SPEAKER_00: while you were at Twitter that showed a map of the world
[01:08:06] SPEAKER_00: and what people called, whether they called it soda pop
[01:08:10] SPEAKER_00: I don't know if it's called soda pop,
[01:08:11] SPEAKER_00: what was the name of this map?
[01:08:13] SPEAKER_00: Yeah, it was like the soda versus pop data set.
[01:08:15] SPEAKER_01: Or soda versus pop.
[01:08:16] SPEAKER_01: And so it's like a map of the United States
[01:08:18] SPEAKER_00: and tells you where people say pop versus soda.
[01:08:20] SPEAKER_00: So do you say soda or pop?
[01:08:23] So I say soda on a soda person.
[01:08:26] SPEAKER_01: Okay, and is that just like that's the right answer?
[01:08:28] SPEAKER_00: Or it's like whatever you are, it's totally fine.
[01:08:33] I think oh, look at you, a little bit funny.
[01:08:34] SPEAKER_01: You say pop and I wonder where you came from,
[01:08:36] SPEAKER_01: but I won't score on you too much.
[01:08:39] SPEAKER_00: That's how I feel too.
[01:08:40] SPEAKER_00: Edwin, this is incredible.
[01:08:41] SPEAKER_00: This is such an awesome conversation.
[01:08:43] SPEAKER_00: I learned so much.
[01:08:43] SPEAKER_00: I think we're gonna help a lot of people
[01:08:45] SPEAKER_00: start their own companies, help their companies become
[01:08:48] SPEAKER_00: more aligned with their values
[01:08:50] SPEAKER_00: and just building better things.
[01:08:52] SPEAKER_00: Two final questions, where can folks find you online?
[01:08:55] SPEAKER_00: If they wanna reach out, what roles are you hiring for?
[01:08:58] SPEAKER_00: How can listeners be useful to you?
[01:09:00] SPEAKER_00: Yeah, so I used to love writing blog,
[01:09:03] SPEAKER_01: but I haven't had time in the past few years.
[01:09:05] SPEAKER_01: But I am starting to write again.
[01:09:06] SPEAKER_01: So definitely check out a search blog,
[01:09:08] SPEAKER_01: searchhq.ai, such blog.
[01:09:10] SPEAKER_01: And yeah, hopefully I'll be running a lot more there.
[01:09:13] And I would say we're definitely always hiring.
[01:09:15] SPEAKER_01: So for people who just love data
[01:09:18] SPEAKER_01: and people who love this intersection of math
[01:09:20] SPEAKER_01: and language and of your science,
[01:09:22] SPEAKER_01: definitely reach out, reach out anytime.
[01:09:24] SPEAKER_01: Awesome.
[01:09:25] SPEAKER_00: And how can listeners be useful to you?
[01:09:26] SPEAKER_00: Is it just, I don't know.
[01:09:27] SPEAKER_01: Yeah, is there anything there?
[01:09:28] SPEAKER_01: Yeah, there are some other things that I can ask.
[01:09:29] SPEAKER_01: So I would say, definitely tell me blog topics
[01:09:30] SPEAKER_01: that you'd like me to write about.
[01:09:32] SPEAKER_01: Okay.
[01:09:33] SPEAKER_01: And then I'm always fascinated by all of these AI failures
[01:09:38] SPEAKER_01: that happen in the end of your world.
[01:09:40] SPEAKER_01: So whenever you come across a really interesting failure,
[01:09:42] SPEAKER_01: that I think illustrates some deep question
[01:09:44] SPEAKER_01: about how we want model to behave.
[01:09:46] SPEAKER_01: Like there's just so many different ways
[01:09:49] SPEAKER_01: I'm all welcome to respond to,
[01:09:50] SPEAKER_01: it's often times think they're just not a single right answer.
[01:09:53] SPEAKER_01: And so whenever there's one of these examples,
[01:09:55] SPEAKER_01: I just love seeing them.
[01:09:57] SPEAKER_01: You need to share these on your blog.
[01:09:58] SPEAKER_00: I'm also, I would love to see if you could see.
[01:10:00] SPEAKER_00: Edwin, thank you so much for being here.
[01:10:03] Thank you.
[01:10:04] SPEAKER_00: Bye everyone.
[01:10:07] Thank you so much for listening.
[01:10:08] SPEAKER_00: If you found this valuable,
[01:10:09] SPEAKER_00: you can subscribe to the show on Apple podcasts, Spotify,
[01:10:12] SPEAKER_00: or your favorite podcast app.
[01:10:15] SPEAKER_00: Also, please consider giving us a rating or leaving a review
[01:10:18] SPEAKER_00: as that really helps other listeners find the podcast.
[01:10:21] SPEAKER_00: You can find all past episodes or learn more about the show
[01:10:24] SPEAKER_00: at Lenny'spodcast.com.
[01:10:26] SPEAKER_00: See you in the next episode.