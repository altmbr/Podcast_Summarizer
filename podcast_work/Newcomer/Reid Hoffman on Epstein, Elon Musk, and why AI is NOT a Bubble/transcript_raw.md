# Reid Hoffman on Epstein, Elon Musk, and why AI is NOT a Bubble

**Podcast:** Newcomer
**Date:** 2025-12-02
**Video ID:** b5JkloLay6M
**Video URL:** https://www.youtube.com/watch?v=b5JkloLay6M

---

[00:00:00] You're ruling out a bubble verse.
[00:00:02] SPEAKER_03: Yes.
[00:00:03] SPEAKER_01: Did you know it was going to be trillions when you first get into this space?
[00:00:05] SPEAKER_02: No, that was probably a bit of a surprise.
[00:00:08] SPEAKER_01: You know, when Sam and I have talked about the various,
[00:00:10] SPEAKER_01: called the Pantheon of AI people,
[00:00:12] SPEAKER_01: Mustafa's not on his list of the people who he most worries about.
[00:00:16] SPEAKER_01: Mm-hmm.
[00:00:17] SPEAKER_01: Oh!
[00:00:18] The thing that I think pissed off Elon with me and then started,
[00:00:22] SPEAKER_01: like, Elon's own relationship with Epstein,
[00:00:24] SPEAKER_01: the reason why he made a whole bunch of lies about me on this was...
[00:00:28] Today's guest is Reed Hoffman, entrepreneur, investor,
[00:00:31] SPEAKER_03: best-selling author and co-founder of LinkedIn.
[00:00:33] SPEAKER_03: Reed is known for shaping how the world connects
[00:00:35] SPEAKER_03: and for backing some of the most transformative companies of our time.
[00:00:38] SPEAKER_03: He is highly influential in political circles,
[00:00:41] SPEAKER_03: and most recently was named an Epstein emails.
[00:00:43] SPEAKER_03: What they should do is literally take every single piece of intel
[00:00:47] SPEAKER_01: that they have about Epstein and release it.
[00:00:50] In this episode, we're getting into all of it.
[00:00:52] SPEAKER_03: This is the newcomer podcast.
[00:00:57] Reed Hoffman, create Tavya on the newcomer podcast.
[00:01:05] SPEAKER_03: Thanks for coming back.
[00:01:06] SPEAKER_03: Always a pleasure. I look forward to it.
[00:01:08] SPEAKER_01: And, you know, done this before,
[00:01:10] SPEAKER_01: and I'm certain we're going to do it again,
[00:01:11] SPEAKER_01: but it's a great time to be doing it.
[00:01:13] SPEAKER_01: Busier audience this time.
[00:01:14] SPEAKER_02: I think it was just you and Eric last time.
[00:01:16] SPEAKER_02: Now you've got a facet of inquirers in front of you.
[00:01:18] SPEAKER_02: I've got the whole crew.
[00:01:19] SPEAKER_01: We have the full inquisition panel here.
[00:01:22] Yeah.
[00:01:23] Now, bring out the comfy chairs.
[00:01:24] SPEAKER_01: I think it'd be hard to resume to bring out the comfy chairs,
[00:01:27] SPEAKER_01: but you know.
[00:01:28] Yeah, not too comfy.
[00:01:30] SPEAKER_02: Yeah, I mean, look, Reed, you've been early on the whole AI boom,
[00:01:34] SPEAKER_02: maybe earlier than most investors.
[00:01:36] SPEAKER_02: So early, so early, you didn't get equity, right?
[00:01:38] SPEAKER_03: I mean, almost two or two early.
[00:01:41] SPEAKER_03: You're doing okay.
[00:01:43] SPEAKER_02: But look, it's been an interesting, I would say,
[00:01:47] SPEAKER_02: couple of weeks, at least on the public markets front.
[00:01:49] SPEAKER_02: Like you saw a run up in all of these AI stocks
[00:01:51] SPEAKER_02: over the course of 2025.
[00:01:53] SPEAKER_02: Like we're entering the end of the year now,
[00:01:55] SPEAKER_02: when all of a sudden it seems like the market is like blipping
[00:01:58] SPEAKER_02: and has lost faith in at least a lot of the kind of tech,
[00:02:01] SPEAKER_02: tech-fueled public stocks.
[00:02:02] SPEAKER_02: I mean, if you had to just give a like our macro look
[00:02:05] SPEAKER_02: at like why there's a little bit of cynicism or skepticism
[00:02:09] SPEAKER_02: about the technology, I mean, what's your read on the space?
[00:02:12] SPEAKER_02: So I don't really, I'm much more of a private market investor,
[00:02:17] SPEAKER_01: you know, long-term, multi-year than public market investors.
[00:02:20] SPEAKER_01: I tend to think public markets has a whole bunch
[00:02:23] SPEAKER_01: of different randomness in it.
[00:02:25] SPEAKER_01: Because it has anything from, you know,
[00:02:27] oh, the war in Ukraine is going badly, you know,
[00:02:30] SPEAKER_01: stock trades down, et cetera, you know,
[00:02:32] SPEAKER_01: all this kind of stuff.
[00:02:33] SPEAKER_01: And you know, there's reasons for it.
[00:02:36] Now that being said, I'd say the tech company is over the last,
[00:02:41] SPEAKER_01: two years, over the last five years, over the last 10 years
[00:02:43] SPEAKER_01: have been, you know, when you get down to these indexes
[00:02:45] SPEAKER_01: are part of what the, you know, kind of massive growth
[00:02:48] SPEAKER_01: overall.
[00:02:49] SPEAKER_01: And so even if there's a modest near-term correction,
[00:02:52] SPEAKER_01: if you're looking at it over a year, over two years,
[00:02:54] SPEAKER_01: it's still pretty far up there.
[00:02:57] SPEAKER_01: Now that being said, you know,
[00:02:59] SPEAKER_01: part of what people are trying to track is,
[00:03:01] SPEAKER_01: when at what level do you get to the kind of different forms
[00:03:04] SPEAKER_01: of business and industry and economic transformations
[00:03:08] SPEAKER_01: that are anticipated by AI?
[00:03:12] SPEAKER_01: You know, people like to have a little bit of a,
[00:03:16] SPEAKER_01: you know, this is a classic, you guys are all expert in this.
[00:03:18] SPEAKER_01: The, you know, week one is, oh my God,
[00:03:20] SPEAKER_01: AI is going to be everything.
[00:03:21] SPEAKER_01: Week two, AI is an overblown disaster,
[00:03:23] SPEAKER_01: and as a bubble waiting to happen, week three,
[00:03:25] SPEAKER_01: AI is going to be everything.
[00:03:26] SPEAKER_01: And so you, you know, you have a cycle that kind of goes through that.
[00:03:29] We may be in the, oh, look, it's overblown, you know, cycle.
[00:03:32] SPEAKER_01: But I, you know, yeah.
[00:03:34] Ilya, Ilya Sutskyver, I think, as I say, Ilya,
[00:03:38] SPEAKER_03: just said, these companies might make a lot of revenue,
[00:03:42] SPEAKER_03: but he wasn't sure that they make a lot of profit, you know,
[00:03:46] SPEAKER_03: which as an investor has got to be, you know, sort of worrying
[00:03:49] SPEAKER_03: or which you're making.
[00:03:50] SPEAKER_03: I think his point was, you know, there's a lot of opportunity,
[00:03:52] SPEAKER_03: but then the model companies are, you know,
[00:03:55] SPEAKER_03: clustering around, around each other.
[00:03:59] SPEAKER_03: Obviously we just saw sort of a great work from Gemini,
[00:04:04] SPEAKER_03: it seems like what you read on, I guess the state of the models
[00:04:08] SPEAKER_03: and how much they're sort of clustering together
[00:04:12] SPEAKER_03: and if that's changing your view on how they are,
[00:04:15] it's just so.
[00:04:16] SPEAKER_03: So there's definitely, like one of the things
[00:04:19] SPEAKER_01: that was really a happy month is for kind of AI being American
[00:04:25] SPEAKER_01: intelligence and having Silicon Valley leadership
[00:04:27] SPEAKER_01: as we got GBD 5.1, we got Gemini 3
[00:04:31] SPEAKER_01: and we got, you know, Opus 4.5, all of which are massively
[00:04:35] SPEAKER_01: great world leading models in the stuff.
[00:04:38] SPEAKER_01: And he said, well, you know, does that mean that, you know,
[00:04:41] SPEAKER_01: you won't be able to command margins on any of these things
[00:04:44] SPEAKER_01: because of competitive pressures?
[00:04:46] SPEAKER_01: And I think there's a couple of different things.
[00:04:48] SPEAKER_01: One, it may very well be that margins don't come
[00:04:51] SPEAKER_01: from a uniqueness of a model, but from rather a kind of where you live
[00:04:55] SPEAKER_01: within a product service, you know, kind of a ecosystem,
[00:05:00] SPEAKER_01: whether it's the services of how you're delivering
[00:05:02] SPEAKER_01: particular applications or whether or not,
[00:05:04] SPEAKER_01: you're better at some things versus others
[00:05:06] SPEAKER_01: because, you know, one of the things that's certainly true
[00:05:08] SPEAKER_01: is actually in fact, each of the models are better at some things
[00:05:11] SPEAKER_01: than others.
[00:05:12] SPEAKER_01: And it's one of the things that I kind of track
[00:05:13] SPEAKER_01: because whenever I have a serious thing that I'm doing,
[00:05:16] SPEAKER_01: I tend to put it in the five plus models
[00:05:19] SPEAKER_01: and then compare the answers.
[00:05:21] SPEAKER_01: And it kind of leads to interesting things.
[00:05:23] SPEAKER_01: So like I'm going to have to try this with Opus 4.5,
[00:05:26] SPEAKER_01: but like, Claude tends to be better at fiction writing
[00:05:29] SPEAKER_01: than any other ones.
[00:05:30] SPEAKER_01: You know, like, not just like the code stuff,
[00:05:32] SPEAKER_01: but these kind of things.
[00:05:33] SPEAKER_01: So I think that's great.
[00:05:34] SPEAKER_01: Now, that being said, the margin comment,
[00:05:37] SPEAKER_01: there's two things I think about margins.
[00:05:39] SPEAKER_01: And even if you said, hey, look, these trained models
[00:05:43] SPEAKER_01: will be run in a hyper competitive way.
[00:05:46] SPEAKER_01: So there'll be rails of the little room for margin.
[00:05:49] One, yeah, obviously if you tie that into an interesting business
[00:05:52] SPEAKER_01: where you have an application, you know,
[00:05:54] SPEAKER_01: I'll give you the two Microsoft cases.
[00:05:56] SPEAKER_01: One is enterprise software and you're in enterprise.
[00:05:59] SPEAKER_01: That will obviously have margin, you know,
[00:06:01] SPEAKER_01: positive margin capabilities.
[00:06:04] SPEAKER_01: And then the other one is for all hyperscalers,
[00:06:07] SPEAKER_01: part of the reason why the hyperscalers can invest in this
[00:06:09] SPEAKER_01: is if you said, well, look, what comes out of this
[00:06:12] SPEAKER_01: is the software is just running lots of cloud compute.
[00:06:16] SPEAKER_01: Well, you can still sell cloud compute
[00:06:19] at your margins of your cloud compute.
[00:06:21] And so that's another place where there will be margin.
[00:06:23] SPEAKER_01: And that's one of the reasons I think you have to be careful
[00:06:26] when you're a startup trying to do frontier models
[00:06:29] SPEAKER_01: as you better address this question
[00:06:30] SPEAKER_01: because you don't have the fallback of the cloud compute.
[00:06:32] SPEAKER_01: So the first one had probably your answer.
[00:06:35] SPEAKER_01: Right.
[00:06:36] SPEAKER_03: Yeah, if you're Microsoft or Google and you're like,
[00:06:38] SPEAKER_03: well, I'm safe.
[00:06:39] SPEAKER_03: I have my cloud services to provide anyway.
[00:06:43] SPEAKER_03: Yeah.
[00:06:44] Yeah.
[00:06:45] SPEAKER_00: I was curious though, to this question about margins,
[00:06:48] SPEAKER_00: where are you seeing applications?
[00:06:51] SPEAKER_00: Where are you seeing applications that have maybe better
[00:06:54] SPEAKER_00: margins because we've talked a lot about coding assistance
[00:06:57] SPEAKER_00: obviously in ThropX Cloud code is really high performing.
[00:06:59] SPEAKER_00: And cursor obviously is on everyone's mind,
[00:07:02] SPEAKER_00: but it's margins aren't exactly the best.
[00:07:05] SPEAKER_00: So as you play mostly in the private investing space,
[00:07:08] SPEAKER_00: what are you looking for when backing a startup
[00:07:12] SPEAKER_00: like in this space to make sure it's defensible?
[00:07:14] SPEAKER_00: So what you're looking for, collecting margins early
[00:07:20] SPEAKER_01: is usually a sign of a very dominant business
[00:07:27] SPEAKER_01: or a sign that you're not focused on growing enough.
[00:07:31] SPEAKER_01: So actually initial margins don't tend to matter that much.
[00:07:34] SPEAKER_01: What tends to matter is what your theory of the game is
[00:07:36] SPEAKER_01: for margins when you're kind of in the mid-scale to scaling.
[00:07:39] SPEAKER_01: Like for example, in the very early days,
[00:07:41] SPEAKER_01: let the series A and Airbnb,
[00:07:44] SPEAKER_01: some of the investors were like,
[00:07:45] SPEAKER_01: what's the margin structure?
[00:07:46] SPEAKER_01: You know, it's a software business.
[00:07:48] SPEAKER_01: We don't care about the margin structure right now.
[00:07:50] SPEAKER_01: We hear about growing this business
[00:07:51] SPEAKER_01: because it's irrelevant unless it really, really grows.
[00:07:54] SPEAKER_01: And then after that, we can see
[00:07:56] SPEAKER_01: what are the different things we tune to margins.
[00:07:57] SPEAKER_01: So you do want margins.
[00:08:00] SPEAKER_01: And the kind of things that give you margins
[00:08:02] SPEAKER_01: are obviously one of the things that I have
[00:08:05] SPEAKER_01: a multi-decade career in as things with network effects.
[00:08:08] SPEAKER_01: But it can also be enterprise integrations,
[00:08:11] SPEAKER_01: it can also be scale effects,
[00:08:12] SPEAKER_01: it can also be by being blitz scaling the first to scale
[00:08:18] SPEAKER_01: that you then just have a kind of a general market dominance
[00:08:21] SPEAKER_01: that people tend to use you and prefer you.
[00:08:25] SPEAKER_01: All of these things can lead to that.
[00:08:26] SPEAKER_01: And that's what I think you're playing for,
[00:08:28] SPEAKER_01: but you're playing for future margins
[00:08:31] SPEAKER_01: more than current margins.
[00:08:33] SPEAKER_01: And if you're sometimes current margins are best predictor,
[00:08:36] SPEAKER_01: but most often it's the theory of how you grow
[00:08:39] SPEAKER_01: to the future margins.
[00:08:40] SPEAKER_01: Can you make the case to me that,
[00:08:42] SPEAKER_02: especially at the foundational frontier lab companies
[00:08:45] SPEAKER_02: that these margins will get better,
[00:08:47] SPEAKER_02: that they are gonna follow the path
[00:08:48] SPEAKER_02: of a traditional software company?
[00:08:50] SPEAKER_02: And the reason I'm skeptical,
[00:08:52] at least I'm presenting the devil's advocate side
[00:08:54] SPEAKER_02: is that inferencing and specifically,
[00:08:58] SPEAKER_02: these reasoning models are highly resource intensive
[00:09:00] SPEAKER_02: and like per query are extremely expensive to run.
[00:09:04] SPEAKER_02: And I mean, I guess you can hope
[00:09:05] SPEAKER_02: that there'll be some breakthrough in inferencing technology,
[00:09:08] SPEAKER_02: but do you foresee this going down the same exact path
[00:09:11] SPEAKER_02: that every software that's existed,
[00:09:14] SPEAKER_02: companies existed before this will go down,
[00:09:15] SPEAKER_02: that it'll get much more margin appreciative,
[00:09:18] SPEAKER_02: or this is just always gonna be a lower margin business
[00:09:21] SPEAKER_02: to potentially negative margin.
[00:09:23] So I can make both cases.
[00:09:26] SPEAKER_01: I do think it'll ultimately be like all good smart people.
[00:09:29] SPEAKER_01: It's like, yeah, it's a little bit of a universe
[00:09:31] SPEAKER_01: in my head, yeah.
[00:09:33] Well, but I like, because you're charting.
[00:09:35] SPEAKER_01: Now, if you actually ask me to guess,
[00:09:37] SPEAKER_01: I do think it'll be margin positive.
[00:09:39] SPEAKER_01: And the real question is, is it lightly margin positive?
[00:09:42] SPEAKER_01: All of the fallback of, you know, kind of hyper-scaler compute
[00:09:46] SPEAKER_01: selling cloud, that kind of stuff,
[00:09:48] SPEAKER_01: or majorly margin competitive, you know,
[00:09:50] SPEAKER_01: collect classic software businesses at scale.
[00:09:53] SPEAKER_01: The thesis, because you've asked me to make the software case,
[00:09:57] is to say, actually, in fact, what will matter,
[00:10:01] SPEAKER_01: the reason why it's important to have a multi-gigawatt
[00:10:03] SPEAKER_01: centers that are training the largest possible models,
[00:10:06] SPEAKER_01: is that when you train those large possible models,
[00:10:08] SPEAKER_01: they will have a bunch of software derivative products.
[00:10:11] SPEAKER_01: So even though running them will be expensive,
[00:10:14] SPEAKER_01: you can distill much smaller models,
[00:10:17] SPEAKER_01: and agents will be compositions of smaller models.
[00:10:20] SPEAKER_01: Those compositions of smaller models
[00:10:21] SPEAKER_01: will be a substantially better product
[00:10:24] SPEAKER_01: than anything that isn't distilled from the largest possible model,
[00:10:27] SPEAKER_01: that they can be trained to specific kinds
[00:10:30] SPEAKER_01: of high margin businesses, which can include coding,
[00:10:33] SPEAKER_01: but it also includes legal, and medical,
[00:10:34] SPEAKER_01: and a bunch of other things,
[00:10:36] SPEAKER_01: and that the fact of having to get the really large scale
[00:10:41] SPEAKER_01: allows you to create the computationally efficient,
[00:10:44] SPEAKER_01: but very high quality, small scale, in terms of how you're operating,
[00:10:47] SPEAKER_01: and those can operate in software margins.
[00:10:49] SPEAKER_01: That would be one of several different possible these,
[00:10:53] which give you software margins in the future.
[00:10:58] And that's essentially the case that OpenAI SAM is making
[00:11:00] SPEAKER_02: for why they need to be raising hundreds of billions,
[00:11:04] SPEAKER_02: trillions of dollars to build these multi-gigawatt data centers,
[00:11:07] SPEAKER_02: is that you've got to make the investment.
[00:11:08] SPEAKER_02: It's essentially an R&D expense that will eventually allow them
[00:11:11] SPEAKER_02: to create these smaller topic-specific models
[00:11:14] SPEAKER_02: that won't be money pits.
[00:11:16] SPEAKER_02: It's essentially the case you buy that.
[00:11:18] Yeah, exactly.
[00:11:19] SPEAKER_01: And it's also, I do.
[00:11:22] SPEAKER_01: Did you know it was going to be trillions
[00:11:23] SPEAKER_01: when you first got into the space?
[00:11:25] SPEAKER_02: This is going to be the scale that you were operating at here,
[00:11:27] SPEAKER_02: that this was going to have to tie up?
[00:11:28] SPEAKER_02: No, that was probably a bit of a surprise.
[00:11:33] SPEAKER_01: But part of what I love about SAM and the OpenAI team
[00:11:37] SPEAKER_01: is it's literally the shoot for Mars.
[00:11:42] SPEAKER_01: No, we're going for Alpha Centauri.
[00:11:44] SPEAKER_01: And you're like, you know,
[00:11:47] SPEAKER_01: like we're throwing all of our chips in repetitively
[00:11:52] SPEAKER_01: to a complete bat.
[00:11:56] Yeah, not a moonshot, a galaxy shot, basically.
[00:11:59] SPEAKER_00: Yes, good line.
[00:12:00] SPEAKER_03: The, are you,
[00:12:03] Greylocks and Messian,
[00:12:04] SPEAKER_03: aetheropic?
[00:12:05] SPEAKER_03: Are you exposed to OpenAI?
[00:12:06] SPEAKER_03: What's your, like, are you?
[00:12:08] SPEAKER_03: Oh, I am.
[00:12:10] SPEAKER_03: I am.
[00:12:11] SPEAKER_01: I, both earlier when it was the 501C3,
[00:12:14] SPEAKER_01: I donated money.
[00:12:16] SPEAKER_01: But when it's first commercial and when it's set up its LP fund,
[00:12:20] we just converted equity.
[00:12:22] I led that round.
[00:12:24] SPEAKER_01: Oh, you did.
[00:12:25] SPEAKER_01: Oh, okay, sorry.
[00:12:26] SPEAKER_01: Oh, man.
[00:12:27] SPEAKER_03: Oh, okay.
[00:12:28] SPEAKER_03: Was that one Coastal Investitor before?
[00:12:30] SPEAKER_03: Yeah, Coastal, look, love the node.
[00:12:33] SPEAKER_01: And he's very carefully says, first venture fund,
[00:12:35] SPEAKER_01: because it was me from my foundation,
[00:12:38] SPEAKER_01: but I actually led that round.
[00:12:40] SPEAKER_01: And did you have more equity than him?
[00:12:43] I think I shouldn't say.
[00:12:47] Do you feel like,
[00:12:49] are you happy with the for profit conversion?
[00:12:51] SPEAKER_03: Like, I mean, that round sort of got crammed down a little.
[00:12:54] SPEAKER_03: Obviously it's a great return,
[00:12:56] SPEAKER_03: but it could have been, you know,
[00:12:57] SPEAKER_03: the best venture return ever in the history of the world, right?
[00:13:01] SPEAKER_03: If the cap table played out differently.
[00:13:03] SPEAKER_03: Look, it's obviously a strange path.
[00:13:07] SPEAKER_01: You know, you know, when I led the round,
[00:13:10] SPEAKER_01: it was a 501 Z3, you know, with a commercial,
[00:13:14] SPEAKER_01: with some commercial technology that come out of it.
[00:13:17] SPEAKER_01: It's literally, you know,
[00:13:18] SPEAKER_01: one of the strangest,
[00:13:20] SPEAKER_01: probably the strangest investment I've ever made.
[00:13:22] SPEAKER_01: Right.
[00:13:23] SPEAKER_01: Because you're like commercial technology coming out
[00:13:26] SPEAKER_01: of a 501 Z3, what's that gonna be worth?
[00:13:29] SPEAKER_01: You know, kind of sort of thing, but it was like,
[00:13:32] SPEAKER_01: look, it's a potentially, you know,
[00:13:36] you know, world changing technology,
[00:13:38] SPEAKER_01: and they're playing a really smart coherent bet.
[00:13:40] SPEAKER_01: And what's more, it's both for the way it affects, you know,
[00:13:45] SPEAKER_01: societies and industries, but also the fact that it's a,
[00:13:53] SPEAKER_01: you know, so that hence the economic investment,
[00:13:56] SPEAKER_01: but it's also a four humanity.
[00:13:58] SPEAKER_01: And part of the four profit conversion
[00:14:01] SPEAKER_01: that I think was really good is, and you know,
[00:14:02] SPEAKER_01: there's a whole bunch of noise about this and press both,
[00:14:06] SPEAKER_01: you know, criticism and positive and all this,
[00:14:08] SPEAKER_01: but it really comes down to is,
[00:14:10] SPEAKER_01: the organization has kept a true north of,
[00:14:12] SPEAKER_01: how do we create AI for the benefit of humanity?
[00:14:16] And that requires us getting partnerships,
[00:14:18] SPEAKER_01: mass amounts of capital and all the rest.
[00:14:21] SPEAKER_01: And so the mechanism of doing that as a 501 Z3,
[00:14:24] SPEAKER_01: we now have to go to a public benefit court
[00:14:26] SPEAKER_01: with governance from the 501 Z3,
[00:14:27] SPEAKER_01: but it's still the same true north
[00:14:30] SPEAKER_01: in terms of what they're trying to accomplish.
[00:14:31] SPEAKER_01: And I think that I've never seen them vary from them.
[00:14:35] And so you're happy with it?
[00:14:37] SPEAKER_03: Yeah, yeah, yeah.
[00:14:38] SPEAKER_01: Delighted on both as an investor and as a human.
[00:14:41] SPEAKER_01: You're happy as an investor or how much did you try
[00:14:43] SPEAKER_03: to protect that tranche, I guess.
[00:14:47] Like one of the things that's very good
[00:14:48] SPEAKER_01: about the Silicon Valley process of this kind of,
[00:14:50] SPEAKER_01: you know, series A, series B, series C,
[00:14:52] SPEAKER_01: is you understand the whole format?
[00:14:54] SPEAKER_01: When you're kind of going from,
[00:14:55] SPEAKER_01: well, we had this LP commercial agreement
[00:14:57] SPEAKER_01: that's now converting in equity.
[00:14:59] SPEAKER_01: And by the way, it did require, you know,
[00:15:02] SPEAKER_01: this kind of billions of dollars from Microsoft
[00:15:04] SPEAKER_01: in order to get there, you know, et cetera,
[00:15:07] SPEAKER_01: it's like, well, what's the right fair thing
[00:15:09] SPEAKER_01: in retrospect is always kind of hard
[00:15:10] SPEAKER_01: because when you're doing it from the forward back,
[00:15:13] SPEAKER_01: it's like, well, you should get a really good return,
[00:15:15] SPEAKER_01: but how much of a great return, you know, becomes a,
[00:15:18] SPEAKER_01: and I think that I think all the parties
[00:15:20] SPEAKER_01: balanced out the negotiation well.
[00:15:23] Your relationship with OpenAI over the years
[00:15:25] SPEAKER_02: has always really fascinated me
[00:15:26] SPEAKER_02: because you're obviously this early investor,
[00:15:28] SPEAKER_02: you're on the board of the company,
[00:15:30] SPEAKER_02: you step down from the board around the time
[00:15:32] SPEAKER_02: that you make the investment in, in flex, no, in flexion.
[00:15:37] SPEAKER_02: In flexion.
[00:15:38] SPEAKER_02: That was about a, that was seven or eight months later,
[00:15:40] SPEAKER_01: but yeah.
[00:15:41] SPEAKER_01: But you in your statement around that sort of set,
[00:15:43] SPEAKER_02: I'm kind of increasingly conflicted
[00:15:44] SPEAKER_02: because of my investments, so it wouldn't make sense
[00:15:46] SPEAKER_02: for me to stay on the board.
[00:15:47] SPEAKER_02: That was essentially your rationale
[00:15:49] SPEAKER_02: for stepping down from open.
[00:15:50] SPEAKER_02: Yeah, it was more that I was getting this flood
[00:15:52] SPEAKER_01: of startups from, from Greylock saying,
[00:15:55] SPEAKER_01: please give me a privileged access and so forth.
[00:15:57] SPEAKER_01: And I was like, being a board member of 501C3,
[00:16:00] SPEAKER_01: I can't even ask for that.
[00:16:02] SPEAKER_01: Right, right.
[00:16:03] SPEAKER_03: So because it was a 501C3 that I was on the board of,
[00:16:06] SPEAKER_01: right, right.
[00:16:07] But if you look at kind of like the,
[00:16:09] SPEAKER_02: the quirk of history, that decision,
[00:16:11] SPEAKER_02: some people argue to step down,
[00:16:12] SPEAKER_02: ends up opening up, you know, the vulnerability to Sam
[00:16:16] SPEAKER_02: when the other board members vote to AUSTEM.
[00:16:18] SPEAKER_02: And you know, my understanding,
[00:16:20] SPEAKER_02: and I wrote in a couple of stories in the journal,
[00:16:22] SPEAKER_02: was that Sam, prior to even his being AUST,
[00:16:24] SPEAKER_02: it was pretty pissed off at you for investing in,
[00:16:27] SPEAKER_02: in flexion, which was, you know,
[00:16:29] SPEAKER_02: headed up by Mustafa Suleiman,
[00:16:30] SPEAKER_02: who Sam doesn't really like.
[00:16:32] SPEAKER_02: And then, you know, down the road,
[00:16:33] SPEAKER_02: you end up kind of leaving Sam vulnerable to this coup,
[00:16:36] SPEAKER_02: with this short term coup.
[00:16:38] SPEAKER_02: I guess like, what's your take on that trajectory of history?
[00:16:41] SPEAKER_02: And then also, you know, what's the relationship
[00:16:43] SPEAKER_02: between you and Sam these days like?
[00:16:46] Good reporter question.
[00:16:48] Great reporter question.
[00:16:49] SPEAKER_01: Look, as far as I know,
[00:16:50] SPEAKER_01: there are a few of this.
[00:16:51] SPEAKER_01: So I've written in so many stories.
[00:16:53] SPEAKER_01: Yeah, well, as far as I know,
[00:16:55] SPEAKER_01: but, you know, Sam and I continue to have dinner together,
[00:17:02] SPEAKER_01: have very productive conversations, et cetera.
[00:17:03] SPEAKER_01: As far as I know, my relationship with Sam is strong and good.
[00:17:08] SPEAKER_01: Was there tension for a time?
[00:17:10] SPEAKER_01: Well, I think, I think what I did learn was that,
[00:17:14] SPEAKER_01: Sam is one of those ultra competitive people.
[00:17:18] SPEAKER_01: And a little bit of how I got,
[00:17:22] SPEAKER_01: so we say a little surprised by it as I,
[00:17:24] SPEAKER_01: like, I'm doing OpenAI at that time,
[00:17:27] SPEAKER_01: which was a 501C3 on the board.
[00:17:30] SPEAKER_01: And I'm then doing the separate commercial thing.
[00:17:32] SPEAKER_01: And I kind of don't think commercial things
[00:17:33] SPEAKER_01: in 501C3s are in conflict.
[00:17:35] SPEAKER_01: It literally didn't occur to me as an issue.
[00:17:39] SPEAKER_01: Because, you know, commercial conflict,
[00:17:41] SPEAKER_01: something I track a lot,
[00:17:42] SPEAKER_01: but 501C3s are structurally and positionally
[00:17:45] SPEAKER_01: not supposed to be competing with 501C3s.
[00:17:48] SPEAKER_01: And I was like, well,
[00:17:49] what we're gonna be competing for talent
[00:17:52] SPEAKER_01: and competing for deals.
[00:17:54] SPEAKER_01: And I was like, oh, okay, I see some of this issue,
[00:17:57] SPEAKER_01: although I don't think I think there's room
[00:17:58] SPEAKER_01: for multiple players here.
[00:18:01] SPEAKER_01: So it did create some, you know,
[00:18:04] SPEAKER_01: like, I like friendship conversations
[00:18:06] SPEAKER_01: that get into conflict because in conflict,
[00:18:09] SPEAKER_01: you either get further away a closer part.
[00:18:10] SPEAKER_01: I think Sam and I got better understanding of each other
[00:18:13] SPEAKER_01: and kind of as we're higher trust in how to collaborate.
[00:18:17] SPEAKER_01: But through a, you know, a little bit of a,
[00:18:20] SPEAKER_01: what are you doing, like investing in inflection,
[00:18:23] SPEAKER_01: you know, kind of starting as a basis?
[00:18:25] SPEAKER_03: So yeah, he was angry,
[00:18:26] SPEAKER_03: but you're still friendly.
[00:18:27] SPEAKER_03: He sounds like he can get a good friendship.
[00:18:30] SPEAKER_02: Like he can get over these things.
[00:18:31] SPEAKER_02: But look, this is historical stuff.
[00:18:33] SPEAKER_02: So I don't wanna spend too much time on it,
[00:18:34] SPEAKER_02: but I, you know, just because I have the opportunity here,
[00:18:37] SPEAKER_02: was it because of them as a competitor
[00:18:39] SPEAKER_02: or you think you specific animus towards Mustafa
[00:18:42] SPEAKER_02: that he was mad about your investment?
[00:18:45] Well, as far as I know, it's competitor.
[00:18:48] SPEAKER_01: Again, you know, when Sam and I have talked about the various,
[00:18:53] SPEAKER_01: you know, kind of called the Pantheon of AI people.
[00:18:57] Mm-hmm.
[00:18:59] SPEAKER_01: Mustafa's not on his list of the people who he most worries about.
[00:19:04] SPEAKER_01: Oh.
[00:19:05] SPEAKER_00: Yeah.
[00:19:06] Careful.
[00:19:07] SPEAKER_00: I guess the next, and the next question is who?
[00:19:09] SPEAKER_01: And I was like, ah, I don't think I could say that.
[00:19:12] SPEAKER_01: That question is what that said.
[00:19:13] SPEAKER_01: Yeah, but there is a little bit of that.
[00:19:14] SPEAKER_01: See, that's the real reporter thing is like,
[00:19:16] SPEAKER_02: I gotta get that list now.
[00:19:17] SPEAKER_02: That's a interesting question.
[00:19:18] SPEAKER_02: Very succinct question.
[00:19:20] SPEAKER_03: I don't wanna go down a whole rabbit hole.
[00:19:21] SPEAKER_03: Do you have an AGI year that you think right now?
[00:19:25] SPEAKER_03: Like if you were to say my best guess for AGI,
[00:19:28] SPEAKER_03: it's this year, what's your answer?
[00:19:30] So my typical quip about AGI is it's the AI
[00:19:34] SPEAKER_01: we haven't invented yet.
[00:19:36] SPEAKER_01: So therefore by definition,
[00:19:38] SPEAKER_01: it'll always be.
[00:19:39] SPEAKER_01: It'll always be.
[00:19:40] SPEAKER_01: It'll always be.
[00:19:42] SPEAKER_01: If what you mean by, and obviously people mean different things
[00:19:45] SPEAKER_01: by things, what you say, AI is capable of doing
[00:19:51] more than half of the tasks that human beings currently do
[00:19:57] SPEAKER_01: and could do, at better than, call it 50% competency
[00:20:02] SPEAKER_01: or 70% competency, my guess is,
[00:20:09] you're at least 10 years up.
[00:20:11] SPEAKER_01: Okay.
[00:20:12] SPEAKER_03: Okay.
[00:20:13] SPEAKER_03: Yup.
[00:20:14] SPEAKER_01: Right, and it might be substantially further.
[00:20:16] SPEAKER_01: I think that people who do this,
[00:20:18] SPEAKER_01: tend to go well, but aren't all the tasks like coding?
[00:20:21] SPEAKER_01: And it's like, no, there's a lot of great things in coding
[00:20:24] SPEAKER_01: and we're all gonna have coding,
[00:20:25] SPEAKER_01: co-pilots and assistants and what we're doing.
[00:20:28] SPEAKER_01: And that'll be a huge amplifier of ours,
[00:20:30] SPEAKER_01: but there's a huge amount of tasks.
[00:20:32] SPEAKER_01: For example, most people don't track the level of metacognition
[00:20:35] SPEAKER_01: that we do in the current uses of our AI agents
[00:20:37] SPEAKER_01: and don't really focus on what that kind of context
[00:20:40] SPEAKER_01: awareness, goal setting, pruning and editing
[00:20:43] SPEAKER_01: and a bunch of other stuff that goes into this.
[00:20:46] And when you look at it, you go, okay,
[00:20:50] and of course we're making progress,
[00:20:52] SPEAKER_01: GBD3, GBD4, GBD5, but there's still mass of room
[00:20:57] SPEAKER_01: for metacognition.
[00:20:58] SPEAKER_01: Here's a simple way of doing it.
[00:21:00] SPEAKER_01: Like one of the things that Microsoft has been doing
[00:21:02] SPEAKER_01: for years now is having long, longitudinal agents
[00:21:05] SPEAKER_01: interacting with each other to kind of see what it is.
[00:21:07] SPEAKER_01: And they end up in the lacunas all the time.
[00:21:10] SPEAKER_01: And you know, as a simplistic example,
[00:21:12] SPEAKER_01: it's like, you know, Eric, thank you.
[00:21:13] SPEAKER_01: No, Reed, thank you.
[00:21:14] SPEAKER_01: No, Eric, thank you.
[00:21:15] SPEAKER_01: Eric, Eric, Eric.
[00:21:16] SPEAKER_01: You know, and it's kind of ways of doing it.
[00:21:18] SPEAKER_01: And they do that in all kinds of weird work processes
[00:21:22] SPEAKER_01: and so forth.
[00:21:23] SPEAKER_01: You don't have to set them in a strange zone
[00:21:24] SPEAKER_01: to kind of get into that area.
[00:21:26] SPEAKER_01: And metacognition is kind of the awareness of it.
[00:21:29] SPEAKER_01: Now here's a more kind of like cult last year one,
[00:21:32] SPEAKER_01: which is you'd ask one of these things
[00:21:35] SPEAKER_01: a prime number question and we get it wrong.
[00:21:37] SPEAKER_01: And you go, you got it wrong?
[00:21:38] SPEAKER_01: It's like, oh yeah, sorry, I got it wrong
[00:21:39] SPEAKER_01: and give you another answer.
[00:21:40] SPEAKER_01: You go, well, you got that one wrong.
[00:21:41] SPEAKER_01: It's all wrong, I got it wrong
[00:21:42] SPEAKER_01: and give you another answer.
[00:21:43] SPEAKER_01: You look, you keep doing that for the 20th answer.
[00:21:45] SPEAKER_01: If you ask a human being, the third time
[00:21:46] SPEAKER_01: I got a prime number wrong in answering you,
[00:21:49] SPEAKER_01: Adelene or Utah, I go, oh, sorry,
[00:21:52] SPEAKER_01: I clearly don't have this problem.
[00:21:53] SPEAKER_01: I'm like, I'm getting this wrong.
[00:21:55] SPEAKER_01: I have the meta-awareness of what's going on.
[00:21:57] SPEAKER_01: Right.
[00:21:58] SPEAKER_01: These artifacts still lack that meta-awareness,
[00:22:01] SPEAKER_01: that metacognition.
[00:22:02] SPEAKER_01: Right, one of the most infuriating things
[00:22:04] SPEAKER_03: about the models is their unwillingness to say,
[00:22:06] SPEAKER_03: I don't know.
[00:22:07] SPEAKER_03: They just always, they're programmed
[00:22:09] SPEAKER_03: to like give an answer every time.
[00:22:10] SPEAKER_03: And so it's like, yeah, there's so many things
[00:22:12] SPEAKER_03: you ask me a question, I'd be like,
[00:22:14] SPEAKER_03: I don't know, like, yeah.
[00:22:16] SPEAKER_03: By that philosophy, would you land more
[00:22:18] SPEAKER_00: in the sort of Andre Carpathi timeline of AGI?
[00:22:21] SPEAKER_00: Yeah, probably.
[00:22:22] SPEAKER_00: We have several years to go,
[00:22:23] SPEAKER_00: but that doesn't mean that I use full now.
[00:22:26] SPEAKER_00: A thousand percent.
[00:22:27] SPEAKER_01: Like, you said, like, is it AGI that makes them useful?
[00:22:31] SPEAKER_01: No, they're incredibly useful today.
[00:22:33] SPEAKER_01: Like, one of the things that I tell everybody is
[00:22:34] SPEAKER_01: if you're not finding uses of it,
[00:22:37] SPEAKER_01: new uses every month that really help you,
[00:22:41] SPEAKER_01: you're not engaging enough.
[00:22:43] Right, full stop.
[00:22:45] SPEAKER_01: And I don't mean sonnets for your kid's birthday
[00:22:47] SPEAKER_01: or because that's all fun.
[00:22:49] SPEAKER_01: But like, things that matter to you in your life
[00:22:52] SPEAKER_01: or in your work, if you're not discovering that,
[00:22:54] SPEAKER_01: that's because you're not actually trying hard enough.
[00:22:57] SPEAKER_02: It's an interesting topic for us.
[00:22:58] SPEAKER_02: You know, a couple of months ago,
[00:23:00] SPEAKER_02: we had an episode of the show where we had a real AGI
[00:23:02] SPEAKER_02: skeptic on and we've sort of been tagged
[00:23:05] SPEAKER_02: as being kind of AGI.
[00:23:06] SPEAKER_02: I don't know, optimists or shills at times
[00:23:10] SPEAKER_02: for the industry, but I'm curious about like,
[00:23:13] SPEAKER_02: the capabilities of the models now
[00:23:15] SPEAKER_02: and the valuations of these companies.
[00:23:17] SPEAKER_02: Like, for OpenAI to truly be worth $500 billion
[00:23:21] SPEAKER_02: or anthropic, you know, reportedly
[00:23:23] SPEAKER_02: in like the $350 billion range,
[00:23:25] SPEAKER_02: do the models need to be much better
[00:23:27] SPEAKER_02: than they currently are or the capabilities
[00:23:28] SPEAKER_02: where they're currently at enough to justify
[00:23:30] SPEAKER_02: this level of, you know, market valuation?
[00:23:34] SPEAKER_02: Well, I tend to not overly try to say,
[00:23:38] SPEAKER_01: like, I'm an expert at, it should be valuation x versus y.
[00:23:42] SPEAKER_01: It's one of the reasons why I like the long compounding
[00:23:45] SPEAKER_01: of private investing because it's kind of like, okay,
[00:23:47] SPEAKER_01: whatever this is, if I'm right,
[00:23:49] SPEAKER_01: it'll go into something really great, right?
[00:23:52] SPEAKER_01: Even if I might have like got lucky and underpaid by half
[00:23:57] SPEAKER_01: or been foolish and overpaid by 2x, you know, like whatever.
[00:24:01] SPEAKER_01: Now, that being said, I think we are massively
[00:24:04] SPEAKER_01: underusing the capabilities of these systems today.
[00:24:07] SPEAKER_01: Like, one of the reasons why I push people on this
[00:24:10] SPEAKER_01: is that most people, even AI, you know, evangelists,
[00:24:16] SPEAKER_01: don't actually use AI nearly as well as they could.
[00:24:22] SPEAKER_01: Because I know I have like vibe coding projects
[00:24:25] SPEAKER_03: I need to spend more time on.
[00:24:26] SPEAKER_03: Yeah, I mean, we use it, we use it a lot like copy editing,
[00:24:31] SPEAKER_03: we have a human proof reader,
[00:24:32] SPEAKER_03: but we also, you know, run everything through that.
[00:24:35] SPEAKER_03: That's, and like reporting ideas,
[00:24:38] SPEAKER_03: but you're making me feel guilty,
[00:24:39] SPEAKER_03: and like, oh, I should use it.
[00:24:40] SPEAKER_03: Yeah, but let me give you an example.
[00:24:41] SPEAKER_03: So you guys are running a multi-party podcast.
[00:24:46] One of the things I realize is AI gives you
[00:24:48] SPEAKER_01: is ability to translate the language.
[00:24:50] SPEAKER_01: So one of the things that we did with, you know,
[00:24:52] SPEAKER_01: R&D finger and I do this possible podcast,
[00:24:54] SPEAKER_01: and we have this read-riff section of it,
[00:24:56] SPEAKER_01: which is just R&D my voice.
[00:24:58] We may, we already had a read AI voice,
[00:25:01] SPEAKER_01: we made an R&D AI voice,
[00:25:02] SPEAKER_01: and now we're releasing it in different languages.
[00:25:05] We started with French, we've already released it.
[00:25:08] SPEAKER_01: That's there now with AI, right?
[00:25:12] SPEAKER_01: And so like, it's just simple of you,
[00:25:15] SPEAKER_01: you develop the process by which you run it through the compute,
[00:25:19] SPEAKER_01: and now you have your podcast now available in other languages.
[00:25:23] SPEAKER_01: And while of course we have a fortune of the fact
[00:25:25] SPEAKER_01: that a lot of the world speaks English
[00:25:27] SPEAKER_01: to the combination of the British and American,
[00:25:29] SPEAKER_01: you know, empires, there's a ton of the world that is not.
[00:25:33] SPEAKER_01: And what's more, even the world that does,
[00:25:36] SPEAKER_01: can you get the depth and sophistication of it?
[00:25:38] SPEAKER_01: You know, there's a reason why you,
[00:25:40] SPEAKER_01: like the translations were,
[00:25:41] SPEAKER_01: and that's just the beginning of all kinds of things you can do.
[00:25:44] SPEAKER_01: And that's part of like, was, wait a minute,
[00:25:46] SPEAKER_01: we have the world's best translators,
[00:25:48] SPEAKER_01: like if you did it with humans,
[00:25:50] SPEAKER_01: it'd be just huge amount of work,
[00:25:52] SPEAKER_01: but we could set it up as an industrial process with AI,
[00:25:55] SPEAKER_01: and we can be in every language before too long.
[00:25:58] SPEAKER_01: Right, and this is stuff that it can already do.
[00:26:00] SPEAKER_02: I mean, maybe, I know we kind of need transition
[00:26:02] SPEAKER_02: to other stuff, so maybe this could be like the last section,
[00:26:05] SPEAKER_02: you know, kind of question about AI,
[00:26:07] SPEAKER_02: but like, if you look at the spending
[00:26:10] SPEAKER_02: that's going into these things right now,
[00:26:12] SPEAKER_02: and you know, Sam Altman has sort of created this incredible
[00:26:15] SPEAKER_02: kind of cycle where he will announce,
[00:26:17] SPEAKER_02: and open AI will announce these deals
[00:26:18] SPEAKER_02: with these chip manufacturers,
[00:26:20] SPEAKER_02: or cloud computing companies, or stock shoot up,
[00:26:23] SPEAKER_02: there's a huge amount of debt that gets involved in,
[00:26:25] SPEAKER_02: you know, building the infrastructure needed
[00:26:28] SPEAKER_02: to make this technology work.
[00:26:29] SPEAKER_02: I mean, does this feel sustainable to you?
[00:26:32] SPEAKER_02: Do you like this current set of AI being so intertwined
[00:26:35] SPEAKER_02: into the economy that it's a huge percentage of GDP?
[00:26:38] SPEAKER_02: It's, you know, this massive economy fueling process
[00:26:44] SPEAKER_02: that has so many kind of, you know,
[00:26:47] SPEAKER_02: load bearing beams built into it?
[00:26:48] SPEAKER_02: Like what's your sense in the fact that AI has become
[00:26:50] SPEAKER_02: this thing now that's so expensive,
[00:26:52] SPEAKER_02: and so, you know, economically motivated?
[00:26:56] Well, so, you know, I'll translate this a little bit
[00:26:58] SPEAKER_01: into what I think about is kind of like a framing I have,
[00:27:02] SPEAKER_01: which is AI bubble versus AI correction.
[00:27:05] SPEAKER_01: And I don't even sure we're gonna have a correction,
[00:27:07] SPEAKER_01: but I'm pretty sure we don't have an AI bubble,
[00:27:09] SPEAKER_01: because the bubble thing tends to be that this investment cycle
[00:27:13] SPEAKER_01: is ahead of the economy,
[00:27:15] SPEAKER_01: and there is no there, there on the other side of it.
[00:27:17] SPEAKER_01: So it's kind of like the simplistic way
[00:27:18] SPEAKER_01: of making this argument is,
[00:27:20] SPEAKER_01: Nvidia invests in companies,
[00:27:22] SPEAKER_01: companies buy the chips that you're training,
[00:27:24] SPEAKER_01: and then Nvidia invests another company,
[00:27:26] SPEAKER_01: is the buy more chips that you're training,
[00:27:27] SPEAKER_01: and that's the cycle of this.
[00:27:29] SPEAKER_01: The thing is, actually, in fact,
[00:27:30] SPEAKER_01: we're gonna get to where intelligence and compute
[00:27:34] SPEAKER_01: has this massive scale and always on 24.7 of electricity.
[00:27:40] SPEAKER_01: And that is actually, in fact, useful.
[00:27:42] SPEAKER_01: This is the undercapabilities thing.
[00:27:43] SPEAKER_01: It may be some retooling,
[00:27:45] SPEAKER_01: and it may be that we paid over too much premium prices
[00:27:49] SPEAKER_01: on timing and other kinds of things for this,
[00:27:51] SPEAKER_01: and you get some correction on pricing, market caps,
[00:27:55] SPEAKER_01: you know, economic flows and so forth.
[00:27:57] SPEAKER_01: So some people might have said,
[00:27:58] SPEAKER_01: hey, I paid $60 billion for the data center,
[00:28:00] SPEAKER_01: and that's really worth $30 billion,
[00:28:02] SPEAKER_01: so it's a $30 billion loss.
[00:28:03] SPEAKER_01: But by the way, if you started running that data center now,
[00:28:07] SPEAKER_01: you can actually run it operationally privately,
[00:28:09] SPEAKER_01: just like, for example,
[00:28:11] SPEAKER_01: the use of these AI models in inference
[00:28:13] SPEAKER_01: is actually already profitable.
[00:28:14] SPEAKER_01: It's the training costs that cause the issues.
[00:28:19] SPEAKER_01: And so that's the reason I say,
[00:28:20] SPEAKER_01: look, the bubble is a nice language,
[00:28:22] SPEAKER_01: like his kind of semi hysterical, but incorrect, I think.
[00:28:26] SPEAKER_01: What I think you might get is corrections,
[00:28:29] SPEAKER_01: because you might go, oh my God,
[00:28:31] SPEAKER_01: the actual real economic flourishing is at five years,
[00:28:35] SPEAKER_01: and that's too far from this really massive escalation,
[00:28:38] SPEAKER_01: and it's really, really, really expensive,
[00:28:40] SPEAKER_01: and there's kind of bumpiness in between, right,
[00:28:43] SPEAKER_01: with an overturn of capital to that,
[00:28:46] SPEAKER_01: and it gets you pricing corrections,
[00:28:48] SPEAKER_01: and some companies go out of business,
[00:28:50] SPEAKER_01: and some things go in bankrupt and get bought out of bankruptcy,
[00:28:53] SPEAKER_01: or assets get bought for dimes on the dollar,
[00:28:57] SPEAKER_01: but I think that doesn't create the kind of reset
[00:29:00] SPEAKER_01: that bubble collapses do.
[00:29:01] SPEAKER_01: I think it's just correction,
[00:29:02] SPEAKER_01: and that's what I think the worst possible outcome is,
[00:29:05] SPEAKER_01: and not even sure that that is enough.
[00:29:07] SPEAKER_01: Right, just some longer.
[00:29:07] SPEAKER_01: You're ruling out a bubble bursting.
[00:29:10] Yes.
[00:29:11] Yes.
[00:29:12] SPEAKER_03: Oh my God.
[00:29:13] SPEAKER_03: Great.
[00:29:14] SPEAKER_03: You're at your first.
[00:29:15] SPEAKER_03: I like to hear it.
[00:29:16] SPEAKER_03: It's a relief for everybody.
[00:29:17] SPEAKER_03: That's your 401K,
[00:29:19] SPEAKER_03: tied up, tied up and everything.
[00:29:20] SPEAKER_03: But obviously, they're pricing change.
[00:29:22] SPEAKER_03: They could be real corrections.
[00:29:24] SPEAKER_01: I'm not saying it's like,
[00:29:25] SPEAKER_01: it's a smooth sailing guaranteed from here,
[00:29:29] SPEAKER_01: November 20th, sticks on.
[00:29:32] We got it.
[00:29:33] SPEAKER_03: He said it.
[00:29:33] SPEAKER_03: He promised.
[00:29:34] SPEAKER_03: He promised it.
[00:29:36] I think one of the reasons we initiate,
[00:29:39] SPEAKER_03: like, oh, we need to have you back on the podcast,
[00:29:41] SPEAKER_03: was a couple of weeks ago,
[00:29:43] SPEAKER_03: you, I didn't came to the defense of Anthropic.
[00:29:46] SPEAKER_03: You know, Anthropic had sort of said,
[00:29:50] SPEAKER_03: you know, they're worried about safety.
[00:29:51] SPEAKER_03: They're like, oh, we're a corporation,
[00:29:52] SPEAKER_03: but we care about doing good.
[00:29:54] SPEAKER_03: You know, we want to make sure this AI thing,
[00:29:56] SPEAKER_03: it doesn't get out of our control.
[00:29:58] SPEAKER_03: And I feel like David Sacks, you know,
[00:30:01] SPEAKER_03: who obviously now represents the Trump administration on AI,
[00:30:05] SPEAKER_03: has basically said, you know,
[00:30:08] SPEAKER_03: it's purely cynical.
[00:30:10] SPEAKER_03: He seems to see it as a totally cynical sort of government,
[00:30:13] SPEAKER_03: capture, strategy instead of a sincerely held belief
[00:30:17] SPEAKER_03: that Anthropic seems to think we should be cautious.
[00:30:21] SPEAKER_03: Like, I don't know, it's a big question.
[00:30:24] SPEAKER_03: I guess to, you know, that's where we were.
[00:30:27] SPEAKER_03: The newest thing is, you know,
[00:30:28] SPEAKER_03: the Trump administration trying to say, you know,
[00:30:30] SPEAKER_03: no state bills.
[00:30:31] SPEAKER_03: Like I'm curious is you're at once the AI bull
[00:30:36] SPEAKER_03: and, you know, you share this sort of,
[00:30:38] SPEAKER_03: I don't know, democratic sensibility
[00:30:40] SPEAKER_03: that a tech should still be governed.
[00:30:41] SPEAKER_03: Some, yeah, what do you make of an effort
[00:30:43] SPEAKER_03: to stop state bills on AI?
[00:30:46] SPEAKER_01: Well, so two things.
[00:30:46] SPEAKER_01: One is, you know, I think Sacks is showing his own true colors
[00:30:51] SPEAKER_01: because it was like, if it was him in the Anthropic thing,
[00:30:53] SPEAKER_01: that's why he would be doing it.
[00:30:55] SPEAKER_01: They would always be instant seer.
[00:30:56] SPEAKER_03: That's literally, I think, when I tweeted something,
[00:30:58] SPEAKER_03: he doesn't know a sincere belief when he sees it.
[00:31:01] SPEAKER_03: Yeah.
[00:31:01] SPEAKER_03: Yeah.
[00:31:02] SPEAKER_01: So, and I think that the, you know,
[00:31:06] SPEAKER_01: I think anyone who has met Dario understands he is nothing,
[00:31:11] SPEAKER_01: but not exactly transparent, maybe too much so
[00:31:15] SPEAKER_01: for sometimes his own, his own, you know, good health.
[00:31:19] SPEAKER_01: And certainly an veteran administration, probably doesn't
[00:31:22] SPEAKER_02: have to be that transparent.
[00:31:23] SPEAKER_02: Yes.
[00:31:25] SPEAKER_01: And then on the state's things, you know,
[00:31:27] SPEAKER_01: I thought, you know, Gavin Newsom did a very good job
[00:31:29] SPEAKER_01: of saying, you know, the earlier one, the earlier SB, you know,
[00:31:35] SPEAKER_01: it was 1047, was overly burdensome,
[00:31:40] SPEAKER_01: but 53 was good.
[00:31:42] SPEAKER_01: And the 53 was good because it was like, look,
[00:31:44] SPEAKER_01: let's actually opt for, you know, kind of some transparency,
[00:31:50] SPEAKER_01: some measurement, some accountability.
[00:31:52] SPEAKER_01: That's what allows us to learn, have society in the process
[00:31:56] SPEAKER_01: as we're doing is not too burdensome.
[00:31:59] SPEAKER_01: Either large companies or small, doesn't prevent channels
[00:32:03] SPEAKER_01: of invention and risk-taking, doesn't slow things down
[00:32:09] SPEAKER_01: in global competition.
[00:32:10] SPEAKER_01: Because, you know, I think, you know,
[00:32:12] SPEAKER_01: focused on competition with China,
[00:32:13] SPEAKER_01: which obviously the current administration
[00:32:15] SPEAKER_01: should be more focused on.
[00:32:17] SPEAKER_01: They seem to want to sell a lot of chips to China for.
[00:32:19] SPEAKER_01: Now they're making China to buy soybeans.
[00:32:22] SPEAKER_03: It's like for, yeah, yeah, yeah, it's a lot of fun.
[00:32:25] SPEAKER_03: It's like, yeah, if you could make this stuff up,
[00:32:29] SPEAKER_01: like if it was playing on a television show,
[00:32:31] SPEAKER_01: you'd think, oh, no, it's not not believable.
[00:32:34] I know you want to answer everything in detail.
[00:32:37] SPEAKER_03: Are you worried about water consumption?
[00:32:40] Oh.
[00:32:41] Oh.
[00:32:45] SPEAKER_01: Ultimately no.
[00:32:48] SPEAKER_01: Partially because, you know, I think we may get to a point
[00:32:52] SPEAKER_01: where we're, for example, doing a lot of, like, you know,
[00:32:56] SPEAKER_01: we may have to end up using saline water
[00:32:58] SPEAKER_01: and other kinds of things as ways of doing this.
[00:33:01] SPEAKER_01: I do think that clean, drinkable, potable water
[00:33:03] SPEAKER_01: is one of things we should pay attention to on a global basis.
[00:33:08] Now, it's funny.
[00:33:09] SPEAKER_01: I haven't really thought about the water stuff
[00:33:10] SPEAKER_01: and I need to think about how AI would help with that
[00:33:13] SPEAKER_01: because one of the mistakes that people frequently make
[00:33:15] SPEAKER_01: when it's talking about electricity is they go,
[00:33:17] SPEAKER_01: oh, it's going to consume a lot of electricity.
[00:33:18] SPEAKER_01: It's like, well, actually, in fact,
[00:33:19] SPEAKER_01: if you're using intelligence at the scale
[00:33:23] SPEAKER_01: and depth that intelligence becomes available,
[00:33:26] SPEAKER_01: you can actually make consumption of electricity
[00:33:28] SPEAKER_01: a lot more efficient.
[00:33:29] SPEAKER_01: And I'm not talking about invention fusion.
[00:33:31] SPEAKER_01: I'm not the AGI and S.I.
[00:33:33] SPEAKER_01: I'm just using the AGI to solve all electrical needs.
[00:33:36] SPEAKER_00: Not that.
[00:33:36] SPEAKER_00: You're not saying that.
[00:33:37] SPEAKER_00: Yeah, not that.
[00:33:38] SPEAKER_01: It's just grid management.
[00:33:39] SPEAKER_01: It's appliance management.
[00:33:41] SPEAKER_01: It's all of those kinds of things.
[00:33:43] SPEAKER_01: Part of what makes electric cars really work
[00:33:45] SPEAKER_01: is a lot of battery management through software.
[00:33:48] SPEAKER_01: And so that kind of thing, adding intelligence to that
[00:33:51] SPEAKER_01: is the thing that now it's interesting.
[00:33:53] SPEAKER_01: I haven't thought about it.
[00:33:54] SPEAKER_01: I will after this question, good, very specific question is like,
[00:33:58] SPEAKER_01: all right, how would you apply AI to be helping
[00:33:59] SPEAKER_01: with water issues?
[00:34:01] SPEAKER_01: Because if you could, then whatever growth you have,
[00:34:06] SPEAKER_01: you have a good solution.
[00:34:09] SPEAKER_01: And my instinct is there is a there there,
[00:34:11] SPEAKER_01: but I don't have a clean answer today.
[00:34:14] Uh-huh.
[00:34:14] Yeah.
[00:34:16] SPEAKER_00: One quick regulation question.
[00:34:17] SPEAKER_00: You mentioned the California bill rewritten as it is,
[00:34:20] SPEAKER_00: this way of looking at it is better than the first edition
[00:34:23] SPEAKER_00: that was a little bit too stringent.
[00:34:25] SPEAKER_00: Is it going to, should it be up to the state's state by state?
[00:34:28] SPEAKER_00: Should we have some federal policy going in?
[00:34:30] SPEAKER_00: Because doing business in each different state
[00:34:32] SPEAKER_00: with different AI regulations seems like it would cause
[00:34:35] SPEAKER_00: a lot of headaches.
[00:34:37] Clearly, it's much better for technology
[00:34:43] to be more, kind of, kind of, kind of,
[00:34:46] SPEAKER_01: kind of, nationally and globally,
[00:34:49] SPEAKER_01: so that you don't have all the nuance and, you know,
[00:34:53] SPEAKER_01: state by state, it's like, you know, having,
[00:34:55] SPEAKER_01: you know, Rhode Island, you know, having its own
[00:34:57] SPEAKER_01: AI regulatory agency.
[00:34:59] SPEAKER_01: And, and so I think that's clearly better.
[00:35:03] SPEAKER_01: Now, the problem, of course, is the current administration,
[00:35:07] SPEAKER_01: if you kind of listen to them, you know,
[00:35:10] SPEAKER_01: the biggest issue seems to be woke AI,
[00:35:14] SPEAKER_01: which I don't even think is really an issue
[00:35:16] SPEAKER_01: when you look at all this stuff,
[00:35:18] SPEAKER_01: unless you want to look at, you know, the,
[00:35:20] SPEAKER_01: the GROC AI saying, you know, Elon is Superman, you know,
[00:35:24] SPEAKER_01: you know, more, more, more physically capable.
[00:35:27] SPEAKER_01: That is a matter of Lika Hitler.
[00:35:29] SPEAKER_01: Yeah, exactly.
[00:35:30] SPEAKER_01: No vulgar race for him.
[00:35:32] SPEAKER_01: But it's like, no, the, the, the, the,
[00:35:35] SPEAKER_01: the kind of the issues here are, you know,
[00:35:37] SPEAKER_01: how do we make AI American intelligence?
[00:35:41] SPEAKER_01: And it's both startups and scale companies.
[00:35:44] SPEAKER_01: And part of the reason why, like, you know,
[00:35:46] SPEAKER_01: if I were administration, I'd be supporting every,
[00:35:49] SPEAKER_01: you know, kind of frontier model effort,
[00:35:51] SPEAKER_01: not trying to single out and thropic
[00:35:54] SPEAKER_01: for personal political reasons.
[00:35:56] SPEAKER_01: And so, you know, because you want AI,
[00:36:00] SPEAKER_01: you want America to succeed with AI in terms of,
[00:36:03] SPEAKER_01: in terms of how it's operating.
[00:36:04] SPEAKER_01: And so, now being said, you wanted to be federal,
[00:36:08] SPEAKER_01: but the thing back to your question,
[00:36:10] SPEAKER_01: my line is, I think that the reason I was,
[00:36:14] SPEAKER_01: I thought Gavin got to the right answer here,
[00:36:18] was that I think it's fine if states say,
[00:36:20] SPEAKER_01: you help, here's some monitoring regimes,
[00:36:22] SPEAKER_01: because if you added up all the monitor regimes,
[00:36:24] SPEAKER_01: there might be a few of them that are,
[00:36:26] SPEAKER_01: that are, that are like strange and weird
[00:36:28] SPEAKER_01: and lightly burdened, some because you're monitoring
[00:36:30] SPEAKER_01: this other weird thing.
[00:36:32] SPEAKER_01: But like monitoring and data and transparency
[00:36:34] SPEAKER_01: is a good basis for getting a collective understanding
[00:36:37] SPEAKER_01: about what's actually going on, what might happen,
[00:36:40] and what are the places that, if you did need
[00:36:42] SPEAKER_01: more detailed regulation, for example,
[00:36:45] SPEAKER_01: the kinds of things that the anthropic people
[00:36:46] SPEAKER_01: are care about is like cyber attack.
[00:36:48] SPEAKER_01: They've been doing a bunch of work
[00:36:50] SPEAKER_01: that's been revealed on that recently.
[00:36:52] SPEAKER_01: Like, you know, and cyber security
[00:36:53] SPEAKER_01: is a really important thing for, you know,
[00:36:55] SPEAKER_01: all American citizens, all American industry,
[00:36:58] SPEAKER_01: all global citizens and industry as well.
[00:37:01] SPEAKER_01: And, you know, that kind of thing is important to do,
[00:37:03] SPEAKER_01: and you go, which, which things are important here?
[00:37:06] SPEAKER_01: And I think that's starting with kind of monitoring
[00:37:09] SPEAKER_01: and transparency and, you know, red team plans
[00:37:11] SPEAKER_01: or good, or is a excellent way to start,
[00:37:14] SPEAKER_01: and if it ends up being 15 different states
[00:37:16] SPEAKER_01: with 15 overlapping things on just that, you know,
[00:37:19] SPEAKER_01: that's, that's, that's, that's not a disaster.
[00:37:22] SPEAKER_01: A real quick question.
[00:37:24] SPEAKER_03: I, I, I certain California governors name
[00:37:26] SPEAKER_03: has no come up a couple times.
[00:37:28] SPEAKER_03: Are you supporting him in 2028?
[00:37:31] SPEAKER_03: Where, where are you on that?
[00:37:34] SPEAKER_03: Well, I mean, look, I think Gavin has done a very good job.
[00:37:39] SPEAKER_01: I mean, the California is, if it was,
[00:37:41] SPEAKER_01: if it was rated as an economy,
[00:37:43] SPEAKER_01: would be the fourth largest economy in the world.
[00:37:45] SPEAKER_01: You know, I think the growth of the economy
[00:37:48] SPEAKER_01: and the timing is governor is very good.
[00:37:50] SPEAKER_01: I think he tries to balance kind of a pro business
[00:37:53] SPEAKER_01: and pro technology approach with also society
[00:37:55] SPEAKER_01: and, and workers.
[00:37:56] SPEAKER_01: I mean, I've had lots of different interactions with him
[00:37:58] SPEAKER_01: when he's trying to figure out things like, you know,
[00:38:00] SPEAKER_01: how does California continue to lead Navy?
[00:38:02] SPEAKER_01: But we deal with, you know, the concerns of unions
[00:38:05] SPEAKER_01: and other folks in terms of doing this.
[00:38:07] SPEAKER_01: So I think Gavin, if he chooses to be a candidate,
[00:38:10] SPEAKER_01: is a, would be a strong candidate.
[00:38:12] SPEAKER_01: You know, I think it's a little early to be, you know,
[00:38:15] SPEAKER_01: calm.
[00:38:16] SPEAKER_01: There's a lot of virtues.
[00:38:17] SPEAKER_03: You're, yeah, okay, interesting.
[00:38:19] SPEAKER_03: Yeah.
[00:38:20] SPEAKER_03: I mean, I mean, look, like, like being pro Gavin puts you,
[00:38:23] SPEAKER_02: you know, in, against a lot of very vocal people
[00:38:27] SPEAKER_02: in the tech world these days and sort of the rise
[00:38:30] SPEAKER_02: of the, the mega right, which was this kind of phenomenon
[00:38:34] SPEAKER_02: during the 2024 election.
[00:38:35] SPEAKER_02: And you stuck by,
[00:38:36] SPEAKER_02: Did you try to make a Trump pilgrimage?
[00:38:38] SPEAKER_03: Did you, yeah, you can see it in the White House?
[00:38:41] No.
[00:38:42] SPEAKER_02: Didn't, didn't quite, didn't quite go across to do this.
[00:38:44] SPEAKER_02: Well, actually, the weirdest one,
[00:38:45] SPEAKER_02: let me take the weirdest look,
[00:38:46] SPEAKER_01: because you guys are gonna kick out of this.
[00:38:49] SPEAKER_01: So there was this meeting in Riyadh,
[00:38:54] SPEAKER_01: very early this year.
[00:38:55] SPEAKER_01: And they, some folks in the Saudis sent me a note saying,
[00:38:58] SPEAKER_01: hey, we like it a come and President Trump will be coming
[00:39:00] SPEAKER_01: and they're done.
[00:39:01] SPEAKER_01: It's like, look, I would come normally,
[00:39:03] SPEAKER_01: but unfortunately I got prior commitments in London
[00:39:05] SPEAKER_01: and I'm really busy.
[00:39:06] SPEAKER_01: And so I won't be able to make it,
[00:39:07] SPEAKER_01: but I anticipate you guys will have a great event.
[00:39:10] SPEAKER_01: Then whatever reason that got lost,
[00:39:12] SPEAKER_01: so the list that they provided to the White House
[00:39:16] SPEAKER_01: was of the people who were coming, included me.
[00:39:18] SPEAKER_01: And the White House published that list.
[00:39:20] SPEAKER_01: And then there was a whole new cycle on how I was going.
[00:39:22] SPEAKER_01: I mean, there's so many scandals.
[00:39:23] SPEAKER_01: We didn't even keep track.
[00:39:24] SPEAKER_01: You're like, if you remember,
[00:39:25] SPEAKER_01: I was accused of wanting to go.
[00:39:28] SPEAKER_01: And I know what to say,
[00:39:29] SPEAKER_01: because I didn't want to,
[00:39:30] SPEAKER_01: like I was like, it was great
[00:39:31] SPEAKER_01: they were having this event, didn't want to diss them.
[00:39:33] SPEAKER_01: So I just, like, took a picture with my chief of staff
[00:39:35] SPEAKER_01: in London and said,
[00:39:36] SPEAKER_01: hard to be in London in Riyadh, a PCTOT.
[00:39:38] SPEAKER_01: Right.
[00:39:39] SPEAKER_01: Yeah.
[00:39:40] SPEAKER_02: Well, that's the least of it.
[00:39:42] SPEAKER_02: I mean, how do you feel right now about the Democrats?
[00:39:44] SPEAKER_02: And specifically, you're sticking with them
[00:39:47] SPEAKER_02: as so many people, friends of yours kind of jump to Republicans,
[00:39:52] SPEAKER_02: to Trump.
[00:39:52] SPEAKER_02: I mean, Mark Pinkis,
[00:39:53] SPEAKER_02: who I know you were close with, kind of very vocally,
[00:39:56] SPEAKER_02: took the red pill.
[00:39:57] SPEAKER_02: I think he did it live on air with all in.
[00:40:00] SPEAKER_02: Like this Twitter was a strong signal.
[00:40:02] SPEAKER_03: I think I muted or blocked.
[00:40:04] SPEAKER_03: I like him, but like it was too long in the,
[00:40:08] I'm a Democrat,
[00:40:09] SPEAKER_03: but I hate everything about the Democrats.
[00:40:10] SPEAKER_03: It's like one thing to be a Republican.
[00:40:12] SPEAKER_03: It's another to just like endlessly claim to be a Democrat.
[00:40:16] SPEAKER_03: And anyway, sorry, read, I've answered for you.
[00:40:18] SPEAKER_03: But what was the question?
[00:40:20] SPEAKER_03: Well, I mean, you see what I'm getting at, read here.
[00:40:22] SPEAKER_02: Like you've stuck by the party and you know,
[00:40:25] SPEAKER_02: you didn't find reasons to kind of shift your,
[00:40:27] SPEAKER_02: at least, allegiances.
[00:40:29] SPEAKER_02: Trump, I mean, I'd like your take on his approval ratings,
[00:40:32] SPEAKER_02: but also just the standing of, you know, tech and Democrats
[00:40:36] SPEAKER_02: and, you know, how you feel about your decision
[00:40:37] SPEAKER_02: to kind of remain stalwart.
[00:40:40] Well, so look, I think you try to make the decisions
[00:40:43] SPEAKER_01: that are best for the broad range of American people.
[00:40:47] SPEAKER_01: And, you know, I don't, like,
[00:40:49] SPEAKER_01: is I find it a little uncomfortable when people say,
[00:40:51] SPEAKER_01: well, because you're a Democrat.
[00:40:53] SPEAKER_01: And it's like, well, look, I try to make the judgments
[00:40:56] SPEAKER_01: that I think are best for the American people.
[00:40:57] SPEAKER_01: And so, for example, let's take, you know, Trump's economy.
[00:41:01] SPEAKER_01: If we didn't have this massive AI boom,
[00:41:03] SPEAKER_01: I think we'd be feeling the effects of the tariffs
[00:41:06] SPEAKER_01: and the stupidity of that much more intensely
[00:41:09] SPEAKER_01: than we all know.
[00:41:10] SPEAKER_01: Right.
[00:41:11] SPEAKER_01: And so, and so it's like, it's like, you know,
[00:41:14] SPEAKER_01: part of what I was kind of things I was saying, you know,
[00:41:17] SPEAKER_01: to try to get people to pay attention,
[00:41:19] SPEAKER_01: is here is a businessman who lost money running a casino.
[00:41:24] Right. Right.
[00:41:25] SPEAKER_01: You know, it's kind of like, yes, we should fix the debt.
[00:41:29] SPEAKER_01: Fixing the debt requires doing major expenditures.
[00:41:33] SPEAKER_01: Like killing USAID does not fix the debt.
[00:41:37] Right. So there's all of these things.
[00:41:40] SPEAKER_01: Like, for example,
[00:41:41] SPEAKER_01: yes, we should deal with immigration.
[00:41:43] A lot of that's borders massively,
[00:41:46] SPEAKER_01: like increasing the spend on ice,
[00:41:48] SPEAKER_01: you know, for political purposes,
[00:41:50] SPEAKER_01: or spending a whole bunch of money deploying national guard
[00:41:54] SPEAKER_01: to picking up trash in DC is not good economics.
[00:41:57] SPEAKER_01: And then you get to like, okay, health and safety
[00:42:00] SPEAKER_01: and health concerns, not just the cutting of kind of,
[00:42:05] SPEAKER_01: health, you know, kind of care for seniors and everything else,
[00:42:09] SPEAKER_01: but also like, you know, RFK and being anti-vaccine
[00:42:14] SPEAKER_01: and all of the kind of things.
[00:42:15] SPEAKER_01: I would like science research cuts.
[00:42:16] SPEAKER_00: And science research, I just like,
[00:42:18] SPEAKER_01: it's a disaster all over the place.
[00:42:19] SPEAKER_01: And so that's the reason it's not really for me,
[00:42:21] SPEAKER_01: Democrat Republican.
[00:42:23] SPEAKER_01: Right. You know, there's a lot of people
[00:42:25] SPEAKER_01: who are traditional Republicans who, you know,
[00:42:28] SPEAKER_01: I'm friends with and I've had long talks with.
[00:42:30] SPEAKER_01: And so when I argue with the folks who are kind of magepilled,
[00:42:34] SPEAKER_01: you know, usually their arguments are,
[00:42:37] SPEAKER_01: Democrats are as bad or worse.
[00:42:40] SPEAKER_01: Like, like, bond closed doors.
[00:42:41] SPEAKER_01: They'll never say this publicly, of course,
[00:42:43] SPEAKER_01: because you have to be living in a regime
[00:42:45] SPEAKER_01: where it's all praise the king,
[00:42:46] SPEAKER_01: which I think it's own deep criticism.
[00:42:49] SPEAKER_01: Right. Like, like, for example,
[00:42:51] SPEAKER_01: here's one of the naughty things.
[00:42:53] To be a part of the Trump administration,
[00:42:55] SPEAKER_01: you have to publicly state that they,
[00:42:57] SPEAKER_01: that Biden stole the 2020 election.
[00:43:00] SPEAKER_01: Right.
[00:43:01] SPEAKER_01: And let's just look at that at face value.
[00:43:04] So when Trump was president with a Republican Senate,
[00:43:07] SPEAKER_01: in some states with Republican electorate states,
[00:43:10] SPEAKER_01: sleepy Joe Biden stole the election.
[00:43:12] SPEAKER_01: But when Biden was president in 2024, he didn't.
[00:43:16] SPEAKER_01: Right. Right. Right.
[00:43:17] SPEAKER_01: What? It's like, you have to state like the moon
[00:43:20] SPEAKER_01: is made out of blue cheese.
[00:43:21] SPEAKER_01: A premise of it is saying the emperor has, you know,
[00:43:23] SPEAKER_03: the emperor no clothes has clothes.
[00:43:25] SPEAKER_03: It's like, and so this is the thing,
[00:43:27] SPEAKER_01: like the people say, no, no, but the Democrats are,
[00:43:29] SPEAKER_01: it's like, no, no, we live in an environment
[00:43:31] SPEAKER_01: where we're speaking truth and understanding,
[00:43:33] SPEAKER_01: like, what is actually in fact best for business?
[00:43:34] SPEAKER_01: What is in fact best for technology?
[00:43:36] SPEAKER_01: What is in fact best for the welfare of citizens?
[00:43:38] SPEAKER_01: Is in fact the really good thing?
[00:43:41] SPEAKER_01: And you said, well, what, you know,
[00:43:42] SPEAKER_01: the Democrats had woke in speech.
[00:43:44] SPEAKER_01: And like, well, there's some issues there.
[00:43:46] But the same people who are saying,
[00:43:48] SPEAKER_01: oh, woke in speech, like the issue of freedom of speech
[00:43:50] SPEAKER_01: and freedom of assembly speaking truth to power.
[00:43:52] SPEAKER_01: Now when people are speaking truth of this power,
[00:43:54] SPEAKER_01: like, no, that's bad.
[00:43:55] SPEAKER_01: Right. I mean, I think one of the hallmarks
[00:43:57] SPEAKER_02: of this era is like cognitive dissonance
[00:43:59] SPEAKER_02: and being able to kind of ally yourself with the side
[00:44:01] SPEAKER_02: while accepting the fact that this person is widely betraying
[00:44:04] SPEAKER_02: a lot of your most held beliefs.
[00:44:05] SPEAKER_02: And to me, the most clear example of that
[00:44:07] SPEAKER_02: has been with Epstein.
[00:44:10] SPEAKER_02: And the fact that, you know, he,
[00:44:11] SPEAKER_02: Trump came into office under the, you know,
[00:44:14] SPEAKER_02: a lot of pressure from his base
[00:44:15] SPEAKER_02: that this was going to be a huge revelation
[00:44:17] SPEAKER_02: about, you know, the ties between Epstein and all elites,
[00:44:20] SPEAKER_02: you know, not just Democrats, but this was a whole conspiracy.
[00:44:23] SPEAKER_02: And, you know, you've talked about this read,
[00:44:25] SPEAKER_02: but it's one that you are wrapped up in as well.
[00:44:27] SPEAKER_02: I mean, you had a relationship,
[00:44:29] SPEAKER_02: professional relationship with Jeffrey Epstein.
[00:44:32] SPEAKER_02: You've tweeted about this.
[00:44:33] SPEAKER_02: You have recently called for Trump to release
[00:44:35] SPEAKER_02: the full Epstein files.
[00:44:37] SPEAKER_02: I mean, it's such a complicated topic
[00:44:39] SPEAKER_02: that inner we have so many aspects of how the economy
[00:44:41] SPEAKER_02: and the, you know, the global systems work.
[00:44:43] SPEAKER_02: But I mean, explain to us here kind of your stance
[00:44:47] SPEAKER_02: on Epstein and why you're asking for the full files
[00:44:49] SPEAKER_02: to be released.
[00:44:50] SPEAKER_02: So, you know, part of, once I realized kind of what
[00:44:55] SPEAKER_01: the issue was, because it was originally when MIT's
[00:44:58] SPEAKER_01: gave approval, said, okay, to fundraise,
[00:45:00] SPEAKER_01: and said, look, he, he served his time for his crimes.
[00:45:02] SPEAKER_01: And, you know, we believe in reform and re-bethlation
[00:45:05] SPEAKER_01: and has money and, you know, it's a good fundraising topic.
[00:45:08] SPEAKER_01: I was a great university's understand this better than I do.
[00:45:10] SPEAKER_01: And then as I started, like getting in
[00:45:12] SPEAKER_01: a little bit, I was like, oh, wait, there's,
[00:45:14] SPEAKER_01: there's a lot of issues where the victims don't feel
[00:45:18] with legitimacy that they've actually gotten,
[00:45:21] SPEAKER_01: you know, kind of truth and justice in this.
[00:45:23] SPEAKER_01: And that's basically when I kind of wrapped my,
[00:45:26] SPEAKER_01: you know, relationship, you know, down to, you know,
[00:45:28] SPEAKER_01: zero with him, like, no, like, like, didn't meet with him
[00:45:32] SPEAKER_01: again, et cetera, so I was like,
[00:45:33] SPEAKER_01: how'd you been worrying?
[00:45:34] SPEAKER_01: Loomers about him prior to that point.
[00:45:35] SPEAKER_02: I mean, what's this, this is suspicious.
[00:45:36] SPEAKER_02: No, I didn't actually, it was kind of one of those things
[00:45:38] SPEAKER_01: where like, like, you know, you Googled
[00:45:41] SPEAKER_01: and it looked very salacious and it was kind of like,
[00:45:43] SPEAKER_01: wow, that sounds kind of terrible,
[00:45:44] SPEAKER_01: but looks like he got prosecuted and put in jail and, you know,
[00:45:49] SPEAKER_01: you know, like, he's like a nexus.
[00:45:52] SPEAKER_03: It's like, okay, you're raising money for MIT,
[00:45:54] SPEAKER_03: or like an MIT foundation in the idea is like, okay,
[00:45:58] SPEAKER_03: he's a nexus for a ton of money.
[00:45:59] SPEAKER_03: We like spend some face time with,
[00:46:01] SPEAKER_03: or like, what is the like,
[00:46:03] SPEAKER_03: I think part of why people are fascinating is like,
[00:46:05] SPEAKER_03: they want to actually know like, what's,
[00:46:07] how does this guy connect to as so many people and like,
[00:46:10] SPEAKER_01: well, he was a masterful networker.
[00:46:11] SPEAKER_01: So like, for example, the thing that I think pissed off,
[00:46:15] SPEAKER_01: Elon with me and then started like,
[00:46:17] SPEAKER_01: Elon's own relationship with Epstein,
[00:46:19] SPEAKER_01: and the reason why he made a whole bunch of lies about me
[00:46:21] SPEAKER_01: on this was I was hosting a dinner in Palo Alto
[00:46:24] SPEAKER_01: for a MIT Media Lab researcher.
[00:46:28] And, you know, the director of the Media Lab, Joe Edo called me
[00:46:32] SPEAKER_01: and said, do you mind if Jeffery Epstein comes?
[00:46:34] SPEAKER_01: And I said, well, it depends on what MIT thinks.
[00:46:36] SPEAKER_01: He's a well-checked with MIT and I said,
[00:46:37] SPEAKER_01: okay, great, you know, fine,
[00:46:38] SPEAKER_01: because I was just little in not paying that much attention.
[00:46:41] SPEAKER_01: Because the thing that I then later learned that Epstein said,
[00:46:44] SPEAKER_01: well, I might be able to give a lot more money
[00:46:45] SPEAKER_01: if you invited me to this dinner,
[00:46:47] SPEAKER_01: because he's kind of like going through the network,
[00:46:49] SPEAKER_01: trying to meet people and so forth.
[00:46:51] SPEAKER_01: And, you know, I was just like, hey, look,
[00:46:53] SPEAKER_01: we're just trying to make, like, like, you know,
[00:46:56] SPEAKER_01: people who do in great science research
[00:46:58] SPEAKER_01: are trying to raise money from them.
[00:47:00] SPEAKER_01: These are a bunch of people who like, you know,
[00:47:02] SPEAKER_01: scientists and technology and everything else,
[00:47:04] SPEAKER_01: so we'll do that.
[00:47:05] SPEAKER_01: And I think it, you know, made, you know, Elon angry,
[00:47:10] SPEAKER_01: so that later he is inventing these fictions of,
[00:47:12] SPEAKER_01: you know, read off and it was a client of Jeffery Epstein's.
[00:47:15] SPEAKER_01: And it's like, where did, which asked did you pull that out of?
[00:47:18] SPEAKER_01: Right?
[00:47:19] SPEAKER_01: Because like, like, the only place,
[00:47:21] SPEAKER_01: everything that I've done with Epstein
[00:47:23] SPEAKER_01: has been public and apologized for.
[00:47:25] SPEAKER_01: Now, but I do think it's important to say one thing here.
[00:47:27] SPEAKER_01: The reason I started speaking up to Tom's regional question
[00:47:30] SPEAKER_01: is because, look, the thing that most matters first and foremost
[00:47:32] SPEAKER_01: is, is some justice to the victims.
[00:47:35] SPEAKER_01: It's super painful and it's important
[00:47:37] SPEAKER_01: that it's not just everything else.
[00:47:39] SPEAKER_01: And so the reason why after I made the apologies,
[00:47:41] SPEAKER_01: I was just broadly pretty quiet on it
[00:47:44] SPEAKER_01: because it was like, that was the thing that mattered.
[00:47:46] SPEAKER_01: And however that played out, it isn't a, you know,
[00:47:50] SPEAKER_01: Bob versus, you know, Frank kind of thing, you know,
[00:47:54] SPEAKER_01: blah, blah, blah, blah, but it's that kind of thing.
[00:47:56] SPEAKER_01: Now, obviously it got heavily politicized.
[00:47:59] SPEAKER_01: You got Elon inventing lies about me.
[00:48:01] SPEAKER_01: You got Pam Bondi, you know, saying,
[00:48:03] SPEAKER_01: well, because of Democratic Senator,
[00:48:05] SPEAKER_01: you were taking money from the close associate, you know,
[00:48:09] SPEAKER_01: of Jeffery Epstein, Reed Hoffman.
[00:48:11] And it's like, okay, well, you've seen the files.
[00:48:13] SPEAKER_01: You know that's lying.
[00:48:14] SPEAKER_01: Look at the, look at the documents that are already out.
[00:48:16] SPEAKER_01: Like there's almost nothing for me in those documents
[00:48:19] SPEAKER_01: relative to a lot of other people.
[00:48:20] SPEAKER_01: So, you know, I don't know if that counts as lying
[00:48:22] SPEAKER_01: under oath or not.
[00:48:23] SPEAKER_01: I'm not a legal expert on this.
[00:48:25] SPEAKER_01: Just to be clear here, you're saying that your relationship
[00:48:27] SPEAKER_02: with him came strictly through fundraising
[00:48:30] SPEAKER_02: for the MIT Foundation.
[00:48:32] SPEAKER_02: Is it accurate to say, and I don't have the article up
[00:48:35] SPEAKER_02: in front of me that you've also as part of that,
[00:48:37] SPEAKER_02: went to the island that you were on, Jeffery's island.
[00:48:39] SPEAKER_02: Well, because this is part, yes,
[00:48:41] SPEAKER_02: because Perra would Joey asked me to do is say,
[00:48:43] SPEAKER_01: hey, Jeffery said that he is more likely to give money
[00:48:45] SPEAKER_01: if you would come and visit with me.
[00:48:47] SPEAKER_01: Well, I'm coming to Boston.
[00:48:48] SPEAKER_01: I'll go through wherever this place is on the way to Boston.
[00:48:52] SPEAKER_01: Right.
[00:48:53] SPEAKER_01: To go there in the SP.
[00:48:54] SPEAKER_01: What was the island like?
[00:48:56] Like I was there for a night.
[00:48:59] SPEAKER_01: It was like, okay, there was a courtyard.
[00:49:01] SPEAKER_01: There was a set of boardroom there.
[00:49:03] SPEAKER_01: There was like a bunch of guest rooms.
[00:49:04] SPEAKER_01: There was a pool.
[00:49:06] You go to a lot of like, like are there a lot of private,
[00:49:08] SPEAKER_03: like you're going to fancy random people's islands,
[00:49:12] SPEAKER_03: like frequently or like how unusual.
[00:49:15] Well, that was the first time I've gone on island.
[00:49:17] SPEAKER_01: I've gone to a couple since then.
[00:49:20] SPEAKER_01: That was again part of the reason why like,
[00:49:22] SPEAKER_01: note to self, Google before going.
[00:49:24] SPEAKER_01: Right.
[00:49:26] Well, part of the challenge with the Epstein thing is like,
[00:49:28] SPEAKER_03: you just don't want your name and Epstein, right?
[00:49:30] SPEAKER_03: But like now Trump and Bondy have like
[00:49:33] SPEAKER_03: shouted your name and Epstein and where, you know,
[00:49:35] SPEAKER_03: not the largest podcast in the world.
[00:49:36] SPEAKER_03: So we're not like drawing the link.
[00:49:38] SPEAKER_03: So I think some of what I'm trying to get at is just like,
[00:49:41] SPEAKER_03: I think, do you think there will be like an answer
[00:49:44] SPEAKER_03: to like who, like we talk about Epstein files, right?
[00:49:47] SPEAKER_03: And even in prepping for this interview,
[00:49:49] SPEAKER_03: in some ways like, the Epstein files
[00:49:51] SPEAKER_03: are just like, if your name's in the email and Epstein,
[00:49:54] SPEAKER_03: they're, you know, you're like in the files.
[00:49:55] SPEAKER_03: Like do you think there will be a list of like,
[00:49:57] SPEAKER_03: these are the people in power who did something like
[00:50:01] SPEAKER_03: untoward with Epstein and these are just random people
[00:50:04] SPEAKER_03: who met him or do you think we'll never really have this
[00:50:06] SPEAKER_03: sort of sorting of like where,
[00:50:09] SPEAKER_03: where there was really problematic interaction?
[00:50:12] SPEAKER_03: So I don't know what's in the files
[00:50:18] SPEAKER_01: because I don't know what the level of like is it
[00:50:20] SPEAKER_01: simply all the email communications
[00:50:23] SPEAKER_01: and all of the plain logs.
[00:50:25] SPEAKER_01: Is it, did they run surveillance?
[00:50:27] SPEAKER_01: And did they have a bunch of different information
[00:50:29] SPEAKER_01: and different things like this?
[00:50:31] SPEAKER_01: Right. And it's unknown.
[00:50:32] SPEAKER_01: I do think that given the degree to which it's created
[00:50:36] SPEAKER_01: a polarization of it being used as the basis of lies,
[00:50:41] SPEAKER_01: political persecution attacks,
[00:50:42] SPEAKER_01: because, you know, at the beginning of this year,
[00:50:43] SPEAKER_01: I said, look, I anticipate the Trump administration
[00:50:46] SPEAKER_01: will make up shit and try to attack me
[00:50:48] SPEAKER_01: because, you know, I've been a political opponent
[00:50:50] SPEAKER_01: and here's an example, because you go, okay,
[00:50:52] SPEAKER_01: you have one person who's the president
[00:50:54] SPEAKER_01: who is like known Epstein for decades,
[00:50:57] SPEAKER_01: gone parties with him, you know,
[00:50:59] SPEAKER_01: we did a whole bunch of stuff with him in New York
[00:51:01] SPEAKER_01: and you got another person who did a couple of fundraising
[00:51:03] SPEAKER_01: events for MIT, right?
[00:51:05] SPEAKER_01: And which one is the one that Trump is calling
[00:51:07] SPEAKER_01: for investigation?
[00:51:08] SPEAKER_01: Because you're like, well,
[00:51:09] SPEAKER_01: there'd be an obvious one based on the raw amount of data,
[00:51:12] SPEAKER_01: pictures and all the rest of the stuff.
[00:51:14] SPEAKER_01: Right.
[00:51:15] SPEAKER_01: But the so political persecution,
[00:51:17] SPEAKER_01: given that, I think that it's,
[00:51:19] SPEAKER_01: that what they should do,
[00:51:21] SPEAKER_01: justice for the victims and truth for the American people,
[00:51:24] SPEAKER_01: is literally take every single piece of intel
[00:51:26] SPEAKER_01: that they have about Epstein and release it.
[00:51:29] SPEAKER_01: Unredacted, completely released, right?
[00:51:32] SPEAKER_01: Yeah.
[00:51:33] SPEAKER_01: And make files as well, like,
[00:51:35] SPEAKER_00: yeah, the ongoing investigation.
[00:51:37] Yeah, because then you can tell, like,
[00:51:39] SPEAKER_01: who's lying and who's not?
[00:51:41] SPEAKER_01: Right, among other things,
[00:51:43] SPEAKER_01: the political,
[00:51:44] SPEAKER_01: do you think it was like an original
[00:51:46] SPEAKER_01: asset or anything?
[00:51:47] SPEAKER_03: Like, sorry to say that?
[00:51:49] SPEAKER_03: Do you think it was like part of some intelligence agency
[00:51:52] SPEAKER_03: or like, I feel like, you know,
[00:51:53] SPEAKER_03: you're, you're a sort of master of the universe.
[00:51:57] SPEAKER_03: You know how things work.
[00:51:58] SPEAKER_03: And now you're being tagged with this by the president,
[00:52:00] SPEAKER_03: like you must have like paid whoever could tell you
[00:52:03] SPEAKER_03: anything, like what do we think actually happened here?
[00:52:06] SPEAKER_03: Or are you able to not get any answers?
[00:52:09] SPEAKER_03: Like do you feel like you've gotten compelling answers
[00:52:11] SPEAKER_03: on what you think is going on?
[00:52:14] SPEAKER_01: You know, I had a handful of interactions with a man,
[00:52:19] SPEAKER_01: entirely within these academic circles.
[00:52:21] SPEAKER_01: He was actually followed academic slot,
[00:52:23] SPEAKER_01: went to a lot of academic conferences,
[00:52:25] SPEAKER_01: did a bunch of stuff.
[00:52:26] SPEAKER_01: You know, he was talking about like, you know,
[00:52:28] SPEAKER_01: evolution and selection theory and free energy
[00:52:32] SPEAKER_01: and all the rest of this stuff.
[00:52:33] SPEAKER_01: And so, but, you know, it's obvious
[00:52:38] SPEAKER_01: that there was something going on because like,
[00:52:42] SPEAKER_01: he was obviously obvious when, or obviously,
[00:52:45] SPEAKER_02: when, like by conversation three I had with him
[00:52:48] SPEAKER_01: because he's like, oh, you're doing this thing
[00:52:51] SPEAKER_01: with Bitcoin, will you fly over with me and meet Putin
[00:52:56] SPEAKER_01: to talk about Bitcoin?
[00:52:58] SPEAKER_01: I'm like, oh really?
[00:52:58] SPEAKER_01: Interesting.
[00:52:59] SPEAKER_01: No, like holy shit.
[00:53:01] SPEAKER_01: Right.
[00:53:02] Like, you know, like, like, I understand
[00:53:07] SPEAKER_01: that you're trying to say, hey, I could go be part
[00:53:09] SPEAKER_01: of the Movers and Shakers.
[00:53:10] SPEAKER_01: Like, you know, like in no universe,
[00:53:14] SPEAKER_01: like there is zero quantum universe of this.
[00:53:16] SPEAKER_01: Is that one of the emails or that he asked you
[00:53:19] SPEAKER_03: to go see Putin?
[00:53:20] SPEAKER_03: Is that out there?
[00:53:20] SPEAKER_03: Or I don't know.
[00:53:22] SPEAKER_03: But that happened.
[00:53:24] SPEAKER_03: He asked you to go see Putin.
[00:53:25] SPEAKER_03: He was trying to build network with me.
[00:53:27] SPEAKER_01: And at that point, this was probably the first trigger
[00:53:30] SPEAKER_01: I was going, oh, okay, this is not good.
[00:53:35] SPEAKER_01: Right.
[00:53:36] SPEAKER_01: I should look at this a little more carefully.
[00:53:38] Right.
[00:53:39] SPEAKER_02: You know, and maybe we can close out this section.
[00:53:41] SPEAKER_02: Like the thing that tends to kind of keep the public
[00:53:44] SPEAKER_02: fascinated and kind of horrified about the situation
[00:53:46] SPEAKER_02: is that not only does it not seem to go away,
[00:53:48] SPEAKER_02: but the more information that comes out,
[00:53:49] SPEAKER_02: the worse it looks for a lot of the people
[00:53:52] SPEAKER_02: that were in a circle.
[00:53:53] SPEAKER_02: Like someone like Larry Summers was known to be kind
[00:53:55] SPEAKER_02: of associated with him.
[00:53:56] SPEAKER_02: But the fact that it got to a point
[00:53:58] SPEAKER_02: where there were specific emails that came out
[00:54:00] SPEAKER_02: that showed that he had been talking about.
[00:54:02] SPEAKER_02: The very thing that people thought was going on
[00:54:04] SPEAKER_02: between, you know, to some degree,
[00:54:07] SPEAKER_02: between Epstein and the wealthiest,
[00:54:09] SPEAKER_02: most connected people in this country,
[00:54:12] it kind of confirmed a lot of that.
[00:54:13] SPEAKER_02: And so you have these sort of waves of scandal
[00:54:16] SPEAKER_02: that have happened with it.
[00:54:17] SPEAKER_02: Bill Gates obviously was implicated with that
[00:54:19] SPEAKER_02: and either directly or indirectly has a step down
[00:54:22] SPEAKER_02: from the board of Microsoft.
[00:54:23] SPEAKER_02: Larry Summers is now basically a persona non-grata,
[00:54:26] SPEAKER_02: you know, and as a public figure,
[00:54:27] SPEAKER_02: you're in kind of con for the three of the files
[00:54:30] SPEAKER_02: that just everything can be out there.
[00:54:32] SPEAKER_02: And insinuation in you window can just go away.
[00:54:35] SPEAKER_02: And either you have the emails in which you did things
[00:54:37] SPEAKER_02: that are exactly what people thought was happening
[00:54:39] SPEAKER_02: or you didn't and we cannot kind of move on from it.
[00:54:41] SPEAKER_02: That's sort of where you're coming at.
[00:54:43] SPEAKER_02: Yeah, I think it's it then.
[00:54:45] SPEAKER_01: And your company, there's nothing that we can out
[00:54:47] SPEAKER_01: that says that you were doing any of these things
[00:54:49] SPEAKER_02: that Larry Summers was kind of like.
[00:54:51] SPEAKER_02: Oh, for sure.
[00:54:52] SPEAKER_02: Well, unless someone invents shit,
[00:54:55] SPEAKER_01: because like I know I was on the other side.
[00:54:57] SPEAKER_01: Like part of, you guys know this.
[00:55:00] SPEAKER_01: Like part of when I realized this was part of the ring
[00:55:02] SPEAKER_01: when I went and kind of made a, you know,
[00:55:04] SPEAKER_01: very public apology because it was like, okay,
[00:55:06] SPEAKER_01: I realized this and I'd already, at that point,
[00:55:09] SPEAKER_01: had ramped down, you know, connection with him, right?
[00:55:16] SPEAKER_01: To like no meetings and all the rest of the stuff
[00:55:19] SPEAKER_01: under any context.
[00:55:20] SPEAKER_01: And, you know, I think he still would drop me an email
[00:55:24] SPEAKER_01: every so often and say, hey, can we go on the phone?
[00:55:25] SPEAKER_01: I'd say, oh, maybe sometime, which is code four.
[00:55:29] SPEAKER_01: Never. Right, right.
[00:55:31] SPEAKER_01: And then, you know, as it gets politicized
[00:55:33] SPEAKER_01: into an attack, attack me or attack Democrats
[00:55:37] SPEAKER_01: or anything else, it's like it's completely
[00:55:39] SPEAKER_01: invented, slander and fiction.
[00:55:41] SPEAKER_01: Right.
[00:55:42] SPEAKER_01: And so, so.
[00:55:44] SPEAKER_01: Do you feel like Elon and Peter Teal
[00:55:46] SPEAKER_03: are like plain dirty or like you know them?
[00:55:49] SPEAKER_03: Like do you, I mean, they're allowing this
[00:55:51] SPEAKER_03: or infueling it and I don't know.
[00:55:53] SPEAKER_03: So, you know, it's more from Elon who, you know,
[00:55:59] kind of every so often said reads a client of Epstein's
[00:56:02] SPEAKER_01: and you say, okay, well, do you have any impressions?
[00:56:04] SPEAKER_01: Yes, it's like, okay, will you win the administration
[00:56:06] SPEAKER_01: for a couple of months?
[00:56:08] SPEAKER_01: I don't know.
[00:56:09] SPEAKER_01: Right. Yeah.
[00:56:09] SPEAKER_01: Zero has come out about this.
[00:56:11] SPEAKER_01: And another failure of Doge.
[00:56:13] SPEAKER_01: Yes, you're cheating.
[00:56:14] SPEAKER_03: You know, yeah.
[00:56:15] SPEAKER_01: You know, you know, Doge is actually should be E-dog,
[00:56:19] SPEAKER_01: you know, Elon's dog.
[00:56:22] SPEAKER_01: But the, you know, anyway, so the simplest way
[00:56:26] SPEAKER_01: to tell the world and look again, victims first,
[00:56:30] SPEAKER_01: but the telewords like, look, all of this slander
[00:56:35] SPEAKER_01: and you end up political persecution attack,
[00:56:38] SPEAKER_01: I'm innocent of, so this way is let's have it all out.
[00:56:41] SPEAKER_01: All of it.
[00:56:42] SPEAKER_01: I didn't want to end on this.
[00:56:44] SPEAKER_03: You bought an NFT that we saw.
[00:56:46] SPEAKER_03: We noted recently.
[00:56:48] Yes, it late.
[00:56:48] We were like, why is that still going on?
[00:56:50] SPEAKER_03: Like a reporter friend of my Senate to me,
[00:56:52] SPEAKER_02: your tweet and they were like, oh, read's been hacked.
[00:56:55] It's a classic tweet of someone who just got here
[00:56:59] SPEAKER_02: account taken over by like a crypto whale
[00:57:01] SPEAKER_02: that's trying to like offload their NFTs.
[00:57:03] SPEAKER_02: That was not the case.
[00:57:04] SPEAKER_02: You truly did buy a cyberpunk.
[00:57:06] SPEAKER_02: Yes.
[00:57:07] SPEAKER_01: What timing?
[00:57:08] SPEAKER_01: What timing?
[00:57:09] SPEAKER_01: What timing?
[00:57:10] SPEAKER_02: Well, my timing was good.
[00:57:11] SPEAKER_01: Sometimes better be lucky to smart.
[00:57:14] SPEAKER_01: The, look, so part of the thing that I've been,
[00:57:17] SPEAKER_01: I've been thinking about crypto somewhat this year
[00:57:19] SPEAKER_01: because it's like, look, there's got to be a good middle path
[00:57:22] SPEAKER_01: between the, what the, you know,
[00:57:26] SPEAKER_01: the democratic administration was doing,
[00:57:27] SPEAKER_01: which is kind of this, you know, extra legal kind of attack,
[00:57:31] SPEAKER_01: unbanking, etc., etc., and the kleptocracy
[00:57:36] SPEAKER_01: that is kind of running the current administration
[00:57:38] SPEAKER_01: with all this stuff.
[00:57:39] SPEAKER_01: The Trump family crypto companies.
[00:57:41] SPEAKER_00: Donate Timollon, yeah.
[00:57:43] SPEAKER_00: Yes.
[00:57:44] SPEAKER_00: Yeah.
[00:57:45] SPEAKER_01: And so there's got to be something,
[00:57:47] SPEAKER_01: literally stuff in the middle and I was like,
[00:57:48] SPEAKER_01: okay, I think there's going to be a bunch of ways
[00:57:51] SPEAKER_01: to actually show things that are positive for society,
[00:57:54] SPEAKER_01: positive for the financial system that work that way,
[00:57:57] SPEAKER_01: that that's what we should be steering towards.
[00:57:59] SPEAKER_01: It's the general purpose with technology is to,
[00:58:03] SPEAKER_01: is to steer towards how do we shake technology
[00:58:06] SPEAKER_01: to be great free matter, great for society,
[00:58:08] SPEAKER_01: great for industry.
[00:58:09] SPEAKER_01: And, and so I started thinking about crypto again.
[00:58:13] SPEAKER_01: And so I was like, and when I looked at it, I went, okay,
[00:58:16] SPEAKER_01: I think the crypto punks, you know,
[00:58:17] SPEAKER_01: are kind of a good intersection for one part of this
[00:58:21] SPEAKER_01: with kind of art and community and everything else.
[00:58:23] SPEAKER_01: And so, you know, did some research about, you know,
[00:58:26] SPEAKER_01: what are the different kinds of, of, of, you know,
[00:58:30] SPEAKER_01: punks and thought, you know, a hoodie would be particularly
[00:58:33] SPEAKER_01: valuable, you know, for, for, for a tech guy.
[00:58:37] SPEAKER_01: And so did that.
[00:58:38] SPEAKER_01: And, and, and then kind of that,
[00:58:39] SPEAKER_01: oh, you fun to kind of, you know,
[00:58:41] SPEAKER_01: I did it a few months ago before I did the tween.
[00:58:43] SPEAKER_01: It'd be fun to do a reveal.
[00:58:45] SPEAKER_01: And then, you know, maybe do some crypto stuff
[00:58:47] SPEAKER_01: in, you know, in next year.
[00:58:49] SPEAKER_01: Are you making investments in crypto companies?
[00:58:53] SPEAKER_01: I'm willing to, but most of my time in the investment side
[00:58:57] SPEAKER_01: right now is working on the companies that I'm, you know,
[00:59:00] SPEAKER_01: getting going like monos AI for carrying cancer and all
[00:59:03] SPEAKER_01: the rest.
[00:59:03] SPEAKER_01: And so it isn't, it isn't, it isn't that I don't.
[00:59:07] SPEAKER_01: Most of the stuff I've done with crypto is through
[00:59:09] SPEAKER_01: Greylock and with Greylock because of people bringing
[00:59:12] SPEAKER_01: stuff to me, but it's like, it's, it's, it, it, it
[00:59:15] SPEAKER_01: ends up be along with other people versus the direct
[00:59:18] SPEAKER_01: stuff that I'm doing in AI.
[00:59:19] SPEAKER_01: What are you, you seem like a guy you could never stop
[00:59:22] SPEAKER_03: doing deals or, yeah, are you?
[00:59:23] SPEAKER_03: What's, what's, you know, what is your, it's only
[00:59:26] SPEAKER_03: co-founding really or why not?
[00:59:28] SPEAKER_03: Why not?
[00:59:29] SPEAKER_03: I've been doing investing.
[00:59:30] SPEAKER_01: I just haven't been doing like the classic investor thing
[00:59:32] SPEAKER_01: as I meet, you know, you'd meet with dozens of entrepreneurs
[00:59:36] SPEAKER_01: every week.
[00:59:36] SPEAKER_01: And then you'd kind of sort that out.
[00:59:38] SPEAKER_01: That's kind of a classic.
[00:59:40] SPEAKER_01: Right.
[00:59:40] SPEAKER_01: Like, you know, one of the phrases that I use to simplify
[00:59:43] SPEAKER_01: kind of the venture job is your average general partner
[00:59:46] SPEAKER_01: meets with 600 to 800 companies per year and says, yes,
[00:59:50] SPEAKER_01: to zero to two of them.
[00:59:52] SPEAKER_01: Right.
[00:59:53] SPEAKER_01: That's the classic, you know, GP.
[00:59:57] And I'm probably more meet with like 150 companies a year,
[01:00:00] SPEAKER_01: maybe 100.
[01:00:02] SPEAKER_01: And it's mostly through, you know, kind of the various,
[01:00:07] SPEAKER_01: the people I work with, Greylock and, you know,
[01:00:10] SPEAKER_01: those global and other places.
[01:00:12] SPEAKER_01: And, you know, will sometimes lead a deal.
[01:00:18] SPEAKER_01: But most often, like the, what I've been doing the last couple
[01:00:20] SPEAKER_01: of years is because I've got these areas that I think, you know,
[01:00:24] SPEAKER_01: essentially Silicon Valley has a blind spot too, like, you know,
[01:00:28] SPEAKER_01: how you would do AI and drug discovery.
[01:00:31] SPEAKER_01: It isn't just create an AGI drug researcher
[01:00:33] SPEAKER_01: or put everything in Silicon.
[01:00:36] SPEAKER_01: And do something here that could make a massive difference
[01:00:39] SPEAKER_01: for society, then going and getting those companies going
[01:00:42] SPEAKER_01: has been more of the recent, you know,
[01:00:46] SPEAKER_01: the days of activity versus the occasional meeting
[01:00:49] SPEAKER_01: with entrepreneurs.
[01:00:50] SPEAKER_03: Right.
[01:00:50] SPEAKER_03: This is M-A-N-A-S, not M-A-N-U-S,
[01:00:55] SPEAKER_03: which is a drug discovery company.
[01:00:58] SPEAKER_03: Yes.
[01:00:59] SPEAKER_03: Yeah.
[01:01:00] SPEAKER_03: This is depend on AI coming up with a novel discovery.
[01:01:02] SPEAKER_03: It's obviously novel to you.
[01:01:03] SPEAKER_03: Oh, ultimately yes.
[01:01:05] SPEAKER_01: But the theory is, how do we get AI to generate
[01:01:08] SPEAKER_01: those novel discoveries is essentially what the,
[01:01:11] SPEAKER_01: as it were, the secret sauce and development is?
[01:01:15] SPEAKER_01: The $100 billion question.
[01:01:17] SPEAKER_00: Yes.
[01:01:17] SPEAKER_00: But it's basically thinking about it's a combination
[01:01:20] SPEAKER_01: of software and intelligent use of biology.
[01:01:24] SPEAKER_01: Has it seen novel ideas so far?
[01:01:27] Well, we've actually discovered some things
[01:01:29] SPEAKER_01: that we think are interesting.
[01:01:30] SPEAKER_01: We've got some, even in the early days of developing
[01:01:34] SPEAKER_01: the technology, there's a few drug filings
[01:01:37] SPEAKER_01: that we've made because we're like, oh, this could be really good.
[01:01:40] SPEAKER_01: And that's just in our prototyping work.
[01:01:42] SPEAKER_01: OK, we're recording this on Wednesday
[01:01:45] SPEAKER_03: before Thanksgiving.
[01:01:47] SPEAKER_03: So I feel like I have to do it.
[01:01:48] SPEAKER_03: It's the most clich thing ever.
[01:01:50] SPEAKER_03: But it's a good ending.
[01:01:51] SPEAKER_03: What are you grateful for?
[01:01:56] SPEAKER_01: Well, there's many things I'm grateful for.
[01:02:00] SPEAKER_01: Why don't I do three?
[01:02:02] SPEAKER_01: So one is, this is for all of the global chaos
[01:02:06] SPEAKER_01: and the turmoil of times we're living in.
[01:02:08] SPEAKER_01: With AI, this is a time to be alive.
[01:02:11] SPEAKER_01: The kind of significant moments in human history.
[01:02:14] SPEAKER_01: We are building it.
[01:02:17] SPEAKER_01: We are watching it.
[01:02:18] SPEAKER_01: We're shaping it.
[01:02:19] SPEAKER_01: Two is friends.
[01:02:21] SPEAKER_01: I think part of what lakes life valuable as friends.
[01:02:24] SPEAKER_01: I give a commencement speech in Vanderbilt a few years back on that,
[01:02:29] SPEAKER_01: because I think it's the kind of thing
[01:02:30] SPEAKER_01: to always pay attention to.
[01:02:32] SPEAKER_01: And then third is, I tend to call Thanksgiving
[01:02:39] SPEAKER_01: friendsgiving, because what I try to do is spend time with.
[01:02:43] SPEAKER_01: Of all the holidays, it's the gather people
[01:02:46] SPEAKER_01: at the table that matter to you and talk about life.
[01:02:53] Great.
[01:02:53] We'll read things to the things you're coming on the podcast.
[01:02:56] SPEAKER_03: My pleasure.
[01:02:57] SPEAKER_03: Thank you for tuning into this week's episode of The Podcast.
[01:03:00] SPEAKER_03: If you're new here, please like and subscribe.
[01:03:02] SPEAKER_03: It really helps the channel.
[01:03:03] SPEAKER_03: We're building a YouTube channel.
[01:03:05] SPEAKER_03: I think you can tell we're investing a lot more in our production.
[01:03:08] SPEAKER_03: And we appreciate your support.
[01:03:09] SPEAKER_03: And if you want the data, insider takes real reporting,
[01:03:14] SPEAKER_03: go to newcomer.co and subscribe to the sub-stack as well.
[01:03:18] SPEAKER_03: Thanks for following all.