# Sam Altman Is NO Steve Jobs: Why He FAILED to Sell AI on Jimmy Fallon

**Podcast:** Newcomer
**Date:** 2025-12-15
**Video ID:** bdauPRYW03Q
**Video URL:** https://www.youtube.com/watch?v=bdauPRYW03Q

---

[00:00:00] We have this like genius level at everything, intelligence sitting there,
[00:00:04] SPEAKER_02: like waiting to unravel the mysteries of humanity.
[00:00:06] SPEAKER_02: And I'm like, why does my kid stop dropping his pizza on the floor and laughing?
[00:00:10] SPEAKER_02: Yeah.
[00:00:19] They're calling it the biggest guest Fallon has ever booked.
[00:00:22] SPEAKER_00: Sam Altman on The Tonight Show.
[00:00:25] SPEAKER_00: I'm Madeline Rinbarger here with Eric Newcomer and Tom Doton for The Newcomer
[00:00:29] SPEAKER_00: podcast and we are talking late night TV.
[00:00:33] SPEAKER_00: It's just always more interesting when you get an AI CEO transforming humanity.
[00:00:37] SPEAKER_00: It's a come banter with Jimmy Fallon.
[00:00:39] SPEAKER_00: So come out.
[00:00:40] He's always so excited.
[00:00:41] SPEAKER_03: It's like, oh, one of us is on TV.
[00:00:43] SPEAKER_03: It's, yeah, always a surprise.
[00:00:46] SPEAKER_03: I liked what Jimmy Fallon had the Costco guys on.
[00:00:48] SPEAKER_01: I liked when he interviewed the Rizzler.
[00:00:51] SPEAKER_01: That was my favorite episode of The Tonight Show with Jimmy Fallon.
[00:00:55] SPEAKER_01: Yeah.
[00:00:56] SPEAKER_01: Tom, Tom, too good for Jimmy Fallon.
[00:00:58] SPEAKER_04: Is Sam Altman the Rizzler of Open A, of A, of A, I, C, O's?
[00:01:04] SPEAKER_01: I mean, you know, the Rizzler is obviously like America's favorite chubby boy.
[00:01:08] SPEAKER_01: Uh, you know, Sam is a, is a very, uh, I don't know, smallish, twinkish, uh, uh,
[00:01:14] SPEAKER_01: print, ascended, dark prints of A, I.
[00:01:16] SPEAKER_01: So it's hard to get too excited about the comp there.
[00:01:18] SPEAKER_01: But, uh, man, that guy can't really bring it in an interview.
[00:01:22] SPEAKER_01: Can he?
[00:01:23] SPEAKER_04: Very awkward.
[00:01:24] SPEAKER_04: Very awkward.
[00:01:25] SPEAKER_04: If you're like media strategy for Sam Altman, I see why you're like Jimmy Fallon.
[00:01:29] SPEAKER_04: He's the friendliest guy on lace.
[00:01:31] SPEAKER_04: I saw it.
[00:01:32] SPEAKER_04: Who wouldn't want it?
[00:01:33] SPEAKER_04: Like everybody wins, you know, you can't go wrong going on Jimmy Fallon.
[00:01:35] SPEAKER_04: And like, that was true.
[00:01:37] SPEAKER_04: It's not like, Fallon was like rooting against Sam.
[00:01:40] SPEAKER_04: But in some ways, how softball it was and that it still felt awkward and uncomfortable,
[00:01:47] SPEAKER_04: highlighted, I think, some of the problems with Sam Altman as on this sort of every man
[00:01:52] SPEAKER_04: show where I think of it, you know, had been a Conan podcast or something with a little
[00:01:56] SPEAKER_04: bit more of like actual, like jokes and repartee, it might have felt more comfortable.
[00:02:02] SPEAKER_04: But I don't know.
[00:02:03] SPEAKER_04: It was a very uneasy conversation.
[00:02:06] SPEAKER_04: Totally.
[00:02:07] Well, Jimmy Fallon was doing his usual foul and thing where he just like, you know,
[00:02:11] SPEAKER_00: laughs and asks a question.
[00:02:13] SPEAKER_00: But there were jokes.
[00:02:14] SPEAKER_00: But there weren't jokes.
[00:02:15] SPEAKER_00: So he just was laughing at Sam working his way through talking about why AI is so great.
[00:02:21] SPEAKER_00: It was like, come on, give me, give me a question, man.
[00:02:24] SPEAKER_00: The core, to get to it, the core problem with the interview is Sam has a terrible answer
[00:02:30] SPEAKER_04: for why people should use AI.
[00:02:32] SPEAKER_04: Like I feel like I get that it's not Uber where it's like you can get a car on your phone
[00:02:37] SPEAKER_04: from the airport right away.
[00:02:39] SPEAKER_04: Like it's hard to sum it up because it's like you could do anything with it.
[00:02:42] SPEAKER_04: I don't know.
[00:02:43] SPEAKER_04: It's unique to you.
[00:02:44] SPEAKER_04: And in some ways, the problem with AI is like, it's not always like the best answer for
[00:02:49] SPEAKER_04: anything.
[00:02:50] SPEAKER_04: Oh, you want tutoring while you could get a great human tutor.
[00:02:53] SPEAKER_04: But if you can't afford that, we have pretty good human tutoring that you, you know,
[00:02:57] SPEAKER_04: you can get from AI.
[00:02:59] SPEAKER_04: And Sam came up with health like it can be your doctor, which I do find it helpful for
[00:03:05] SPEAKER_04: health, but comes off as a little dystopian to me.
[00:03:08] SPEAKER_04: I don't know.
[00:03:09] SPEAKER_04: Would you think about his ability to answer the, what is your product for question?
[00:03:14] SPEAKER_04: It is asked her.
[00:03:15] SPEAKER_01: It is three years in to this fucking thing.
[00:03:18] SPEAKER_01: And by the way, it is caught on with people.
[00:03:20] SPEAKER_01: Their heart as the information recently reported nearly 900 million people using this product.
[00:03:24] SPEAKER_01: As if you just came to open AI or you know, chat GPT based on this interview, you would
[00:03:30] SPEAKER_01: have no idea why 900 million people are using this thing.
[00:03:33] SPEAKER_01: Right.
[00:03:34] And I don't want to say this is going to be a problem for them going forward because
[00:03:37] SPEAKER_01: they have 900 million users like, but I think this is going to be a problem for them going
[00:03:42] SPEAKER_01: to.
[00:03:43] SPEAKER_01: Like they have got to find a way to get the normies excited about this.
[00:03:47] SPEAKER_01: And you will reach saturation of people that are kind of sampling this product.
[00:03:50] SPEAKER_01: And either they organically find it themselves, maybe because their kids use it and you know,
[00:03:55] SPEAKER_01: at generations younger than us, it's a huge deal.
[00:03:58] SPEAKER_01: But like you've got to be able to sell this thing.
[00:04:00] SPEAKER_01: And like everyone lives in the shadow of Steve Jobs in the tech industry who really was
[00:04:06] the greatest pitchman of maybe the modern era generally, but like certainly within tech.
[00:04:12] SPEAKER_01: And you know, I know you acquired Johnny I've Sam, but like you are no Steve Jobs.
[00:04:17] SPEAKER_01: Well, I was going to say just, you know, even his closing pitch about like what is the
[00:04:22] SPEAKER_00: future of AI going to bring and like where will this take us?
[00:04:25] SPEAKER_00: And I get that he's trying to not be selling something dystopian like no one has any jobs
[00:04:30] SPEAKER_00: or you know, everything gets automated.
[00:04:31] SPEAKER_00: So he landed on, you know, AI for drug discovery and then in five years we'll have AI created
[00:04:37] SPEAKER_00: medical treatments as sort of like his pitch of the future, which again is a good use case.
[00:04:43] SPEAKER_00: But that's not what the 900 million to your point, Tom, users are going to be using GFTT for.
[00:04:48] SPEAKER_00: So you're not really selling it for the average consumer.
[00:04:52] SPEAKER_00: And you're also kind of aiming low for why this thing is so great.
[00:04:55] SPEAKER_00: Honestly.
[00:04:55] SPEAKER_00: Well, it's also like if you can't even explain what it does, how believable is it going to
[00:04:59] SPEAKER_01: sound to the general audience that like this thing that you don't really understand?
[00:05:03] SPEAKER_01: It's about to cure diseases.
[00:05:04] SPEAKER_01: We struggle with this too, you know, face with fucking Ed's it's Ron.
[00:05:08] SPEAKER_04: Like it is not that we couldn't come up with what we use it for, but you just sound silly.
[00:05:14] SPEAKER_04: I it's hard.
[00:05:15] SPEAKER_04: It's like I when I just say it copy edits our stories like that's important to me, but it's like,
[00:05:21] SPEAKER_04: okay, that's like that's a bazillion dollar company.
[00:05:24] SPEAKER_04: You know, it's like every one of them feels sort of random.
[00:05:28] SPEAKER_04: Like it's not again, it's not like Airbnb where it's like, you know, book a home and
[00:05:34] SPEAKER_04: somebody else's city.
[00:05:36] SPEAKER_04: It's yeah, it is a hard product.
[00:05:38] SPEAKER_04: There isn't like one thing.
[00:05:39] SPEAKER_04: It's like thought.
[00:05:40] SPEAKER_04: I think they need to emphasize fine like the intelligence you need or it's like custom
[00:05:48] SPEAKER_04: thought or you know, it should be that it's the best personal assistant you can get
[00:05:53] SPEAKER_00: online for any task you would need like a general purpose personal assistant, which is
[00:05:57] SPEAKER_00: not maybe find a sexier way to say that.
[00:05:59] SPEAKER_00: But that's what it kind of boils down to.
[00:06:00] SPEAKER_00: I wonder if he like, yeah, he tries to turn it back.
[00:06:02] SPEAKER_04: It's like, what do you need to do?
[00:06:04] SPEAKER_04: Like what's the challenge you face today?
[00:06:06] SPEAKER_04: And like I sort of so cratically show you how you could put AI to the test.
[00:06:11] SPEAKER_04: Like being asked to sort of draw it random.
[00:06:14] SPEAKER_04: It's like when you're like, so me this pen, the classic advice is like, oh, solicit what
[00:06:19] SPEAKER_04: they need a pen for.
[00:06:21] SPEAKER_04: You know, you can't just like arbitrarily just like sell something you have to assess like
[00:06:25] SPEAKER_04: a customer.
[00:06:26] SPEAKER_04: Right.
[00:06:27] SPEAKER_01: Right.
[00:06:28] SPEAKER_01: Exactly.
[00:06:29] SPEAKER_01: Engage with the person that you're talking to and you know, Jimmy Fallon, I don't even
[00:06:31] SPEAKER_01: know what to say about the guy what he would use it for.
[00:06:34] SPEAKER_01: Sam obviously went the personal route.
[00:06:35] SPEAKER_01: Oh, he could have gone negative.
[00:06:36] SPEAKER_01: He should have been like, you could have punched the shop a little bit of you.
[00:06:39] Talked to him.
[00:06:40] It's like, this is not going.
[00:06:43] SPEAKER_03: This isn't very funny.
[00:06:44] SPEAKER_03: Like did you run this?
[00:06:45] SPEAKER_03: Did you ask him, opening eye maybe to punch up some of the jokes a little bit like.
[00:06:49] SPEAKER_03: Yeah.
[00:06:50] SPEAKER_03: Well, you know, and sympathy for Jimmy Fallon.
[00:06:52] SPEAKER_01: I think he saw Sam coming off incredibly robotic and alien at the beginning.
[00:06:56] SPEAKER_01: And he's like, let me turn this personal.
[00:06:58] SPEAKER_01: You just had a kid recently, right?
[00:07:00] SPEAKER_01: How's that going?
[00:07:01] SPEAKER_01: And then Jimmy said, I think something like, do you love him?
[00:07:04] SPEAKER_01: Are you loving him?
[00:07:05] SPEAKER_02: So much.
[00:07:06] SPEAKER_02: It's the greatest thing in the world.
[00:07:07] SPEAKER_02: Which is just, you know, Jimmy Fallon's interviewing skills are, they are what they are.
[00:07:12] SPEAKER_01: Good, good, say we're going to talk about your child.
[00:07:14] SPEAKER_01: So do you love him?
[00:07:15] SPEAKER_01: The problem, you know, it's like, it shouldn't be a question.
[00:07:20] SPEAKER_03: Right.
[00:07:21] SPEAKER_01: What he was like, yeah, he's got over that hurdle.
[00:07:23] SPEAKER_03: It's like, yeah, it's a desperate attempt to humanize him.
[00:07:24] SPEAKER_03: Or maybe it's a general question to Sam.
[00:07:26] SPEAKER_01: It's like, I don't really get you, dude.
[00:07:27] SPEAKER_01: Like, do you love this child?
[00:07:28] SPEAKER_01: Like, it's hard to see you making emotional.
[00:07:31] SPEAKER_01: You're making connections to other human beings.
[00:07:32] SPEAKER_01: So tell me about this one that you and your husband are raising.
[00:07:35] SPEAKER_01: So then he asked him that and then Sam is just like, yeah, and I couldn't imagine how
[00:07:38] SPEAKER_01: anyone raised a child before Chatchee PT.
[00:07:40] SPEAKER_01: I cannot imagine having gone through that, like figuring out how to raise a newborn without
[00:07:44] SPEAKER_02: Chatchee PT, which is absurd.
[00:07:47] SPEAKER_01: Realize it, that's an insane sounding thing to say.
[00:07:49] SPEAKER_01: He backs off for a second.
[00:07:50] SPEAKER_01: He's like, I know that sounds kind of crazy, but like, here's sort of like, how I use it,
[00:07:55] SPEAKER_01: you know, it's like, I tell my kid who's like six months old, like, why don't you keep
[00:07:59] SPEAKER_01: dropping your pizza on the floor?
[00:08:00] SPEAKER_01: Which is what Sam said.
[00:08:02] SPEAKER_02: Why does my kid stop dropping his pizza on the floor?
[00:08:04] SPEAKER_02: He doesn't get it right.
[00:08:06] SPEAKER_01: It's weirdly phrased.
[00:08:08] SPEAKER_01: And also, later in that interview, says that his kid is not yet crawling at six months,
[00:08:12] SPEAKER_01: which is very normal.
[00:08:13] SPEAKER_01: Sam, don't worry about that.
[00:08:14] SPEAKER_01: And I'm glad you feel better about that.
[00:08:15] SPEAKER_01: But like, I don't think your kid's dropping pizza on the floor at six months.
[00:08:19] SPEAKER_01: He was not prepped at all for how to discuss his personal use of the thing.
[00:08:23] SPEAKER_01: Oh, I think he made up the anecdote about his kid too.
[00:08:25] SPEAKER_01: Like, you, you, oh, the thing about crawling?
[00:08:28] SPEAKER_01: No, that's unfair.
[00:08:29] SPEAKER_01: You think he should?
[00:08:30] SPEAKER_01: Oh, yeah.
[00:08:31] SPEAKER_01: I think he tried to improv something because that sort of thing is to range to me.
[00:08:35] SPEAKER_04: That is always a deeply trope.
[00:08:36] SPEAKER_04: Like, I don't just rely about random shit.
[00:08:39] SPEAKER_04: Like, you think the kid, that to me is the biggest problem with the whole interview.
[00:08:42] SPEAKER_04: You just, you think the pizza thing, it's six months old.
[00:08:45] SPEAKER_04: Oh, yeah.
[00:08:46] SPEAKER_04: I mean, I'd love to, we need to report that one out.
[00:08:48] SPEAKER_04: Yeah.
[00:08:49] SPEAKER_04: I like to report that one out, but you're not given a six months old pizza.
[00:08:52] SPEAKER_01: Right.
[00:08:53] SPEAKER_01: You know, pizza gate, you know?
[00:08:55] SPEAKER_04: Well, that's too real.
[00:08:57] SPEAKER_01: That's too real.
[00:09:00] SPEAKER_01: I will look, I mean, let me ask you, Eric, since you, you know, according to the Zitron
[00:09:03] SPEAKER_01: verse, you know, are obsessed with chat GPT and use it for everything.
[00:09:07] SPEAKER_01: Like, how much are you using it for your early month, you know, raising your child in
[00:09:12] SPEAKER_01: the early months?
[00:09:13] SPEAKER_01: Well, I use it for us all the time.
[00:09:15] SPEAKER_04: You know, yeah, it's what Sam is saying.
[00:09:19] SPEAKER_04: Honestly, it is great at reassurance.
[00:09:21] SPEAKER_04: It is, is this thing that's happening to me normal?
[00:09:24] SPEAKER_04: And it's like, oh, the sleep window is two hours instead of 90 minutes.
[00:09:28] SPEAKER_04: And it's like, well, how much is she sleeping?
[00:09:29] SPEAKER_04: So I think it's a good source of reassurance with some education built in in terms of what
[00:09:37] SPEAKER_04: normal milestones are.
[00:09:38] SPEAKER_04: I'm often like, what are, you know, oh, she could start grasping at things soon.
[00:09:43] SPEAKER_04: Maybe I should try to teach her how to hold on to something.
[00:09:46] SPEAKER_04: You know, I, I think it is useful, sort of like a coach on what milestones you expect.
[00:09:50] SPEAKER_04: And as a parent, you're always sort of just trying to know like, how long should my
[00:09:54] SPEAKER_04: kids stay awake?
[00:09:55] SPEAKER_04: Well, is it normal if XYZ happen?
[00:09:56] SPEAKER_04: So I, I think he was telling the truth that it's, it's, it's helpful as a sort of sounding
[00:10:02] SPEAKER_04: board for what's normal for children.
[00:10:04] SPEAKER_04: You've never had a child before.
[00:10:05] SPEAKER_04: That's what's insane about it.
[00:10:06] SPEAKER_04: Yeah, for your first child.
[00:10:07] SPEAKER_04: The first child you just don't know.
[00:10:09] SPEAKER_04: And so you just want somebody to like, is this normal?
[00:10:10] SPEAKER_04: And like doctors are usually getting flooded with like, you know, is this normal?
[00:10:15] SPEAKER_04: Is, yes.
[00:10:16] SPEAKER_04: So I think he's good for that.
[00:10:17] SPEAKER_04: I'll be tragic, you know, someday when it says something's normal and it's wrong.
[00:10:22] SPEAKER_04: But, um, but yeah, so I talked to it all the time about, about Esther.
[00:10:25] SPEAKER_04: I, I've said on this podcast that it was sounding board for the name.
[00:10:29] SPEAKER_04: You know, that kind of sort of thing.
[00:10:30] SPEAKER_04: Instead of being like an expert, it's like, you know, it's a useful sounding board for
[00:10:35] SPEAKER_04: a lot of this stuff.
[00:10:36] SPEAKER_04: It helps you work through your thought processes and then also either give reassurance or
[00:10:41] SPEAKER_00: feedback if you have a question.
[00:10:44] SPEAKER_00: In addition, just giving you an answer, which I think is a useful general purpose tool,
[00:10:48] SPEAKER_00: which is again, why Sam Wattman could have easily just said, it's the universe.
[00:10:51] SPEAKER_00: It's the universal personal assistant and not, you know, pitched it like, I think this
[00:10:56] SPEAKER_00: strange.
[00:10:57] SPEAKER_00: One thing undermining, I think open AI is that Google has just swallowed Gemini into Google.
[00:11:05] SPEAKER_04: So you Google and it explain, it does the similar thing at the top of Google.
[00:11:10] SPEAKER_04: And so a lot of people are already googling.
[00:11:13] SPEAKER_04: And so when you try and convince them what open AI is doing is novel, it is what they are
[00:11:18] SPEAKER_04: getting when they Google.
[00:11:19] SPEAKER_04: Now, even though that isn't what you used to get.
[00:11:23] SPEAKER_04: And so it's very hard for open AI to differentiate relative to Google when Google is offering
[00:11:29] SPEAKER_04: the same thing when you search.
[00:11:31] SPEAKER_04: They just sort of stealth swap that in.
[00:11:35] Right.
[00:11:36] SPEAKER_01: And you're basically introducing a problem to something that people don't always think
[00:11:39] SPEAKER_01: is a problem because effectively, what you're saying is, you know, how you hate having
[00:11:43] SPEAKER_01: to click on things when you're trying to research stuff online.
[00:11:45] SPEAKER_01: Well, you know, AI kind of absorbs it all into this one quick easy answer.
[00:11:50] SPEAKER_01: Exactly.
[00:11:51] SPEAKER_01: And now it's much faster.
[00:11:52] SPEAKER_01: And you don't really appreciate that until you start doing it.
[00:11:55] SPEAKER_01: And you may never appreciate it.
[00:11:57] SPEAKER_01: But you know, that doesn't sound like a revolution to people.
[00:12:00] SPEAKER_01: But what's the bad?
[00:12:01] SPEAKER_01: He should have never said on national TV that you couldn't have a, it would be hard to
[00:12:08] SPEAKER_04: raise a kid pre-Chancey BT.
[00:12:11] SPEAKER_04: What was the one he said?
[00:12:12] SPEAKER_04: Yeah, I can't imagine what it was like.
[00:12:14] SPEAKER_04: I can't imagine raising a kid without just a kid.
[00:12:16] SPEAKER_00: I feel like that should be the case study in how Silicon Valley should not communicate
[00:12:21] SPEAKER_04: with the public.
[00:12:22] SPEAKER_04: Like there's nothing more out of touch than saying like my product that did not exist
[00:12:27] SPEAKER_04: three years ago is necessary for a fundamental act of humanity that has been, been happening
[00:12:34] SPEAKER_04: since the beginning, the beginning of human existence, right?
[00:12:38] SPEAKER_04: And it is like, you could not be more out of touch.
[00:12:40] SPEAKER_04: It's just like insisting on something that is clearly not true.
[00:12:43] SPEAKER_04: It didn't exist.
[00:12:44] SPEAKER_04: You don't need it to raise a kid.
[00:12:47] SPEAKER_04: It might make it a marginally easy.
[00:12:48] SPEAKER_04: Right.
[00:12:49] SPEAKER_01: Especially when I don't really get to keep bagging on this, but right after he says that
[00:12:51] SPEAKER_01: he brings up this like bungled anecdote that didn't really make sense.
[00:12:55] SPEAKER_01: Right, that's a fake.
[00:12:56] SPEAKER_01: Right.
[00:12:57] SPEAKER_01: Yeah, that you didn't even need to ask Chatsy BT about anyways.
[00:12:59] SPEAKER_01: Like why are you talking to Chad about whether your kid shouldn't be dropping people
[00:13:02] SPEAKER_01: on the phone?
[00:13:03] SPEAKER_01: Why did your kid drop something like look at the kid?
[00:13:04] SPEAKER_03: You know, get on the phone.
[00:13:05] SPEAKER_03: How was your kid's phone?
[00:13:06] SPEAKER_04: Right.
[00:13:07] SPEAKER_01: Why did my kid drop pizza because he was holding pizza at six months?
[00:13:09] SPEAKER_00: Why did the baby have to be eating avocados?
[00:13:12] SPEAKER_04: Like what?
[00:13:13] Yeah.
[00:13:14] SPEAKER_01: And it's like also, you know, we're already worried enough about alienation and technology
[00:13:18] SPEAKER_01: making us feel like atomized and unconnected.
[00:13:20] SPEAKER_01: If we're just like, how do I experience this incredibly human moment by not talking to
[00:13:25] SPEAKER_01: other people?
[00:13:26] SPEAKER_01: Oh, I can talk to this genius AI.
[00:13:27] SPEAKER_01: The guy that's also described as like the greatest genius of all time.
[00:13:30] SPEAKER_01: We need an Andre Carpati is doing a better job at this.
[00:13:33] SPEAKER_04: Or I'm sort of kicking through my head like who do we want?
[00:13:37] SPEAKER_04: If you're rooting for AI, like I am like who in AI do you want out there actually?
[00:13:42] SPEAKER_04: Like making this case on fountain.
[00:13:45] SPEAKER_04: Like in some ways, I think it would be better if they just were nerdy or it's like I,
[00:13:49] SPEAKER_04: if it was someone who's transparently like a research nerd, they'd have a little bit of
[00:13:54] SPEAKER_04: defense where Sam's trying to say, oh, I figured out this affable thing.
[00:13:58] SPEAKER_04: And that makes it puts him in this uncanny valley of humanity.
[00:14:01] SPEAKER_04: It's the same is also the great fundraiser.
[00:14:03] SPEAKER_00: Like he's really great pitching all of these wealthy people and decision makers and places
[00:14:08] SPEAKER_00: where you need to get something out of people by like charming them and like giving the
[00:14:13] SPEAKER_00: spiel of the vision of the future.
[00:14:15] SPEAKER_00: Like he's amazing at that, but that skill set doesn't translate to like a mass talk show
[00:14:19] SPEAKER_00: audience where you're talking to the every man.
[00:14:22] SPEAKER_00: Like you need someone to your point, even nerdier.
[00:14:24] SPEAKER_00: Carpati is great because he talks about the limitations of this tech and is basically
[00:14:28] SPEAKER_00: like it's not going to steal your job.
[00:14:30] SPEAKER_00: It's not there yet.
[00:14:31] SPEAKER_00: Like it can't do these certain things, but it's super useful for these other things.
[00:14:34] SPEAKER_00: And it's really good at like laying out specifically for AI works.
[00:14:38] Now I agree with you guys, you want, and there was a time like where talk shows would
[00:14:41] SPEAKER_01: have scientists and other sort of like eccentric figures on this.
[00:14:45] SPEAKER_01: Cool mirror.
[00:14:46] SPEAKER_04: It was good at that daily show.
[00:14:47] SPEAKER_04: Yeah.
[00:14:48] Yeah, or even like, you know, take what you will about the guy, but like Neil deGrasse
[00:14:51] SPEAKER_01: Tyson sort of like has this, you know, academic, you know, I'm a pop scientist who's going
[00:14:57] SPEAKER_01: to talk to you about the mysteries of the universe.
[00:14:59] SPEAKER_01: People are very attracted to that.
[00:15:01] SPEAKER_01: Like that's the kind of way.
[00:15:02] SPEAKER_01: Where is the AI Carl Sagan, you know what I mean?
[00:15:04] SPEAKER_00: Yeah, I know.
[00:15:05] SPEAKER_00: Like we need the AI Carl Sagan.
[00:15:06] SPEAKER_00: I mean, trying to think who's out there like like Dario, I don't think you want that guy
[00:15:10] SPEAKER_01: going on foul and making one of his like absurd predictions that turn out not to be true.
[00:15:14] SPEAKER_01: Like in the next year, 50% of talk show hosts will be a eyes.
[00:15:17] SPEAKER_01: Yeah, exactly.
[00:15:18] SPEAKER_04: Exactly.
[00:15:19] SPEAKER_01: So that's not a great answer.
[00:15:21] SPEAKER_01: I don't know who else is out there.
[00:15:24] SPEAKER_01: Mira, you know, she's kind of messing up.
[00:15:26] SPEAKER_01: I haven't seen her speak publicly that much.
[00:15:29] SPEAKER_01: Have you?
[00:15:30] SPEAKER_00: I mean, there was that one time where she famously kind of clubbed it.
[00:15:33] SPEAKER_00: So yeah.
[00:15:34] SPEAKER_00: She was a bit misleading about how Sora was training on YouTube videos or not.
[00:15:39] SPEAKER_01: Just.
[00:15:40] SPEAKER_01: And reporters were hunting for that.
[00:15:42] SPEAKER_04: Like again, who was training her on that?
[00:15:44] SPEAKER_04: That would have been like, it was an active area of interest at the time.
[00:15:48] SPEAKER_04: Yeah.
[00:15:49] SPEAKER_01: Maybe it's us.
[00:15:50] SPEAKER_01: Maybe we're the ones.
[00:15:51] SPEAKER_01: Maybe the ones that should go on all these shows.
[00:15:53] SPEAKER_01: I came off of, you know, I'm working on another feature for Vanity Fair.
[00:15:57] SPEAKER_01: And I just came off of fucking monster writing session where I had clawed next to me,
[00:16:01] SPEAKER_01: coach me through the process.
[00:16:03] SPEAKER_01: And I am a convert.
[00:16:05] SPEAKER_01: I'm shifting my allegiances away from chat.
[00:16:09] SPEAKER_01: I feel like Kevin Russe right now just sort of.
[00:16:12] SPEAKER_01: Just a fucking leaf in the wind changing my mind.
[00:16:15] SPEAKER_01: But no, I do think there is incredible utility.
[00:16:18] SPEAKER_01: I was really blown away by Claude Sonnet as a kind of, I don't know, sort of co-pilot in
[00:16:25] SPEAKER_01: my writing process.
[00:16:26] SPEAKER_01: And so in terms of like what beats you're missing or story structure?
[00:16:31] SPEAKER_04: I would, yeah, well, so here was like my crazy to me moment about it.
[00:16:36] SPEAKER_01: So I'm writing this story and I'm trying to be very structured about it.
[00:16:39] SPEAKER_01: Like the last one I wrote, I kind of just fucking wrote.
[00:16:42] SPEAKER_01: But this one I was like, let me figure out the outline of this feature before I start going
[00:16:46] SPEAKER_01: into it.
[00:16:47] SPEAKER_01: And I used as a model for it a New Yorker story.
[00:16:50] SPEAKER_01: And I basically outlined the New Yorker story and I was like, I'm going to try and use
[00:16:53] SPEAKER_01: this for this piece.
[00:16:55] SPEAKER_01: And I'm writing it out and I put the first, I've written like kind of two thirds of the
[00:16:58] SPEAKER_01: piece and I put it into Claude Sonnet.
[00:17:01] SPEAKER_01: And I was like, what do you think about this?
[00:17:03] SPEAKER_01: This is a piece in writing for Vanity Fair.
[00:17:04] SPEAKER_01: Like how does it read?
[00:17:05] SPEAKER_01: Is it exciting?
[00:17:06] SPEAKER_01: And it's like, this feels less like a Vanity Fair piece and more like a New Yorker piece.
[00:17:10] SPEAKER_01: Wow.
[00:17:11] SPEAKER_04: And it's like, yeah, New Yorker, you kind of like ruminate on different, you know, descriptions
[00:17:17] SPEAKER_01: and stuff like that.
[00:17:18] SPEAKER_01: But Vanity Fair needs to be punchy and fast paced.
[00:17:20] SPEAKER_01: So you might want to rethink your structure a bit.
[00:17:22] SPEAKER_01: And I was like, that's insane.
[00:17:25] SPEAKER_01: Spot on.
[00:17:26] Wow.
[00:17:27] Wow.
[00:17:28] SPEAKER_04: I can bring it off of York version Tom.
[00:17:30] SPEAKER_04: That's amazing.
[00:17:31] SPEAKER_04: And magazines do, I mean, good magazines have that sort of house style, which you know,
[00:17:35] SPEAKER_04: I bristle at as like a sub-stacker word.
[00:17:38] SPEAKER_04: It's like, I'm going to write naturalistically or whatever.
[00:17:40] SPEAKER_04: But yeah, that's key to magazine writing.
[00:17:43] SPEAKER_04: Yeah.
[00:17:44] SPEAKER_01: And different kinds of magazines.
[00:17:45] SPEAKER_01: And also it pushes back on you.
[00:17:48] SPEAKER_01: Like I will put in a section and I'll be like, is this any good?
[00:17:50] SPEAKER_01: And I'll be like, yeah, it doesn't really work for me, dog.
[00:17:52] SPEAKER_01: Like maybe just learn you.
[00:17:54] SPEAKER_01: Like it's like, this Tom guy is negative.
[00:17:57] SPEAKER_04: Yeah, I think it just wants to be the like beat.
[00:18:00] SPEAKER_01: I don't want to be, I don't want my ass to be kissed by it by it.
[00:18:03] SPEAKER_01: Right.
[00:18:04] SPEAKER_01: I want to be told that like, you know, you're worthless.
[00:18:05] SPEAKER_01: And this writing didn't add up at all.
[00:18:07] SPEAKER_01: No, it pushed back.
[00:18:08] SPEAKER_01: It's smart ways to me.
[00:18:09] SPEAKER_01: It told me that like my logic was was off in certain things and, you know, highlight
[00:18:14] SPEAKER_01: other things that I, you know, I put in part of my transcripts.
[00:18:17] SPEAKER_01: Maybe I don't know if it's done.
[00:18:18] SPEAKER_01: I probably just chat you be, he just knows so much about me now.
[00:18:21] SPEAKER_04: But like whenever I go to the other ones, it doesn't have as much context.
[00:18:26] SPEAKER_04: And so I do think there's some, maybe that's the, flywheel, yeah, mode.
[00:18:31] SPEAKER_04: Sorry.
[00:18:32] SPEAKER_04: I think there's some mode, you know, there.
[00:18:34] SPEAKER_01: Yeah.
[00:18:35] SPEAKER_01: But, you know, maybe to finish this one off like with open AI and Sam, like, I think they're
[00:18:38] SPEAKER_01: really wrestling with that right now.
[00:18:40] SPEAKER_01: You know, how much do they want to tweak chat GPT to be less sick, a fan sick, even though
[00:18:45] SPEAKER_01: it got him to this point.
[00:18:47] SPEAKER_01: And, you know, how, you know, do you lose a certain contingent of users who like these
[00:18:51] SPEAKER_01: things because there's maybe a better model to be built or healthier model to be built.
[00:18:57] SPEAKER_01: Because, you know, we don't rely on these same kind of engagement tweaks and hacks.
[00:19:02] SPEAKER_01: And for me, it's like the difference between using a sick, a fan sick model and one that's
[00:19:06] SPEAKER_01: less so is enormous.
[00:19:08] SPEAKER_01: And I, and I prefer it a lot.
[00:19:09] SPEAKER_01: And, and what's his name at the cerebral valley summit kind of made that point.
[00:19:14] SPEAKER_01: I'm a Krieger.
[00:19:15] SPEAKER_01: Oh, my Krieger Chief Product Officer, an Anthropic.
[00:19:19] SPEAKER_04: Yeah.
[00:19:20] Yeah.
[00:19:21] SPEAKER_01: And I think that sick if and see is something that you're seeing more and more users pushing
[00:19:25] SPEAKER_01: back on and they actually want things to, you know, be more real and critical at times
[00:19:32] SPEAKER_01: of what you're putting into it.
[00:19:34] SPEAKER_01: It told me go to sleep last night.
[00:19:36] Really?
[00:19:37] SPEAKER_01: Finally?
[00:19:38] SPEAKER_01: How did it know the time or did you say I'm tired or?
[00:19:40] SPEAKER_01: It knows the time.
[00:19:41] SPEAKER_01: I think it has a better sense of it.
[00:19:42] SPEAKER_01: Like when you go to a cloud, it'll say like good evening, Tom or good morning.
[00:19:47] SPEAKER_01: And at a certain point, it was like, you've had a great session today.
[00:19:50] SPEAKER_01: Why don't you pack it in?
[00:19:51] SPEAKER_01: We'll start again tomorrow.
[00:19:52] SPEAKER_01: And I was like, huh.
[00:19:53] Yeah.
[00:19:54] SPEAKER_01: I love you.
[00:19:55] SPEAKER_01: Fascinating.
[00:19:56] When your, when your chatbot tells you to log off the chatbot levels of lack of sick
[00:20:01] SPEAKER_00: busy.
[00:20:02] SPEAKER_00: All right.
[00:20:03] SPEAKER_04: I'll spend more time on cloud.
[00:20:04] SPEAKER_04: This is a good.
[00:20:05] SPEAKER_04: That's convertible.
[00:20:06] SPEAKER_00: Not sponsored by cloud.
[00:20:07] SPEAKER_04: Yeah.
[00:20:08] Yeah.
[00:20:09] All right, Sam.
[00:20:10] SPEAKER_04: You got to, if you want to be Steve Jobs, you got to do better than this.
[00:20:14] SPEAKER_04: Like, he's made big strides.
[00:20:16] SPEAKER_04: He's made big strides.
[00:20:18] SPEAKER_04: But in some ways, yeah, I think it's an uncanny valley situation where he's made himself
[00:20:23] SPEAKER_04: too human.
[00:20:24] SPEAKER_04: So he's not getting the sympathy of nerddom, but he's not, he's not at Steve Jobs level
[00:20:29] SPEAKER_04: on, can't make up, can't make up stories that don't make any sense.
[00:20:33] SPEAKER_04: Yeah.
[00:20:34] SPEAKER_04: And tell us that we need a technology that didn't exist a couple years ago for a fundamental
[00:20:40] SPEAKER_04: mental human act that people have been doing pre pre fire.
[00:20:44] SPEAKER_04: So yeah.
[00:20:45] SPEAKER_04: Yeah.
[00:20:46] SPEAKER_03: That's a weird one.
[00:20:47] Last thing I can say is like one great hair.
[00:20:49] SPEAKER_01: I thought your hair was like on point, Sam.
[00:20:52] SPEAKER_01: And also it went better than the Tucker Carlson interview.
[00:20:54] SPEAKER_01: So you've had, you had complaints from one programmer who said you guys are basically stealing
[00:20:58] SPEAKER_01: people's stuff and not paying them.
[00:21:00] SPEAKER_01: And then he wound up murdered.
[00:21:02] What was that?
[00:21:03] SPEAKER_01: Oh, man, I forgot about that.
[00:21:05] SPEAKER_04: Yeah, he's really, he's really putting himself out there.
[00:21:08] SPEAKER_04: Kudos.
[00:21:09] SPEAKER_04: He's taking risks.
[00:21:10] SPEAKER_04: Big swing.
[00:21:11] SPEAKER_01: Yes.
[00:21:12] SPEAKER_01: Yeah.
[00:21:13] SPEAKER_01: No one accused him of murder in this one.
[00:21:14] SPEAKER_01: So that's a big, big thumbs up from the open air comms team.
[00:21:17] Thank you for tuning into this week's episode of the podcast.
[00:21:19] SPEAKER_01: If you're new here, please like and subscribe.
[00:21:21] SPEAKER_01: It really helps out the channel.
[00:21:22] SPEAKER_01: Listen in for new episodes every week wherever you get your podcasts.