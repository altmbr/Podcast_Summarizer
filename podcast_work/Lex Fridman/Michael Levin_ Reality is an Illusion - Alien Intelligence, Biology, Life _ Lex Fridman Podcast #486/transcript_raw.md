# Michael Levin: Reality is an Illusion - Alien Intelligence, Biology, Life | Lex Fridman Podcast #486

**Podcast:** Lex Fridman
**Date:** 2025-11-30
**Video ID:** Qp0rCU49lMs
**Video URL:** https://www.youtube.com/watch?v=Qp0rCU49lMs

---

[00:00:00] The following is a conversation with Michael Levin,
[00:00:02] SPEAKER_A: his second time on the podcast.
[00:00:04] SPEAKER_A: He is one of the most fascinating and brilliant biologists
[00:00:08] SPEAKER_A: and scientists I've ever had the pleasure of speaking with.
[00:00:12] SPEAKER_A: He and his labs at Tufts University
[00:00:14] SPEAKER_A: study and build biological systems
[00:00:16] SPEAKER_A: that help us understand the nature of intelligence,
[00:00:19] SPEAKER_A: agency, memory, consciousness, and life
[00:00:24] SPEAKER_A: in all of its forms here on Earth and beyond.
[00:00:28] SPEAKER_A: This is the Lex Friedman Podcast.
[00:00:30] SPEAKER_A: To support it, please check out our sponsors
[00:00:32] SPEAKER_A: in the description where you can also find
[00:00:34] SPEAKER_A: links to contact me, ask questions,
[00:00:37] SPEAKER_A: get feedback, and so on.
[00:00:40] SPEAKER_A: And now, dear friends, here's Michael Levin.
[00:00:44] SPEAKER_A: You write that the central question
[00:00:46] SPEAKER_A: at the heart of your work,
[00:00:48] SPEAKER_A: from biological systems to computational ones,
[00:00:51] SPEAKER_A: is how do embodied minds arise in the physical world
[00:00:56] SPEAKER_A: and what determines the capabilities
[00:00:58] SPEAKER_A: and properties of those minds?
[00:01:00] SPEAKER_A: Can you unpack that question for us
[00:01:02] SPEAKER_A: and maybe begin to answer it?
[00:01:04] Well, the fundamental tension is in both the first person,
[00:01:08] SPEAKER_B: the second person, and third person descriptions of mind.
[00:01:11] SPEAKER_B: So in third person, we want to understand
[00:01:14] SPEAKER_B: how do we recognize them and how do we know,
[00:01:16] SPEAKER_B: looking out into the world,
[00:01:17] SPEAKER_B: what degree of agency there is
[00:01:19] SPEAKER_B: and how best to relate to the different systems
[00:01:22] SPEAKER_B: that we find and are our intuitions any good
[00:01:25] SPEAKER_B: when we look at something
[00:01:26] SPEAKER_B: and it looks really stupid and mechanical
[00:01:28] SPEAKER_B: versus it really looks like
[00:01:30] SPEAKER_B: there's something cognitive going on there.
[00:01:32] SPEAKER_B: How do we get good at recognizing them?
[00:01:34] SPEAKER_B: Then there's the second person, which is the control,
[00:01:36] SPEAKER_B: and that's both for engineering
[00:01:38] SPEAKER_B: but also for regenerative medicine.
[00:01:40] SPEAKER_B: When you want to tell the system to do something,
[00:01:42] SPEAKER_B: what kind of tools are you going to use?
[00:01:45] SPEAKER_B: And this is a major part of my framework
[00:01:46] SPEAKER_B: is that all of these kinds of things are operational claims.
[00:01:49] SPEAKER_B: Are you going to use the tools of hardware rewiring,
[00:01:52] SPEAKER_B: of control theory and cybernetics,
[00:01:54] SPEAKER_B: of behavior science, of psychoanalysis
[00:01:57] SPEAKER_B: and love and friendship?
[00:01:58] SPEAKER_B: Like what are the interaction protocols that you bring?
[00:02:01] SPEAKER_B: And then in first person,
[00:02:02] SPEAKER_B: it's this notion of having an inner perspective
[00:02:04] SPEAKER_B: and being a system that has valence
[00:02:07] SPEAKER_B: and cares about the outcome of things,
[00:02:09] SPEAKER_B: makes decisions and has memories
[00:02:10] SPEAKER_B: and tells a story about itself and the outside world.
[00:02:13] SPEAKER_B: And how can all of that exist
[00:02:15] SPEAKER_B: and still be consistent with the laws of physics
[00:02:17] SPEAKER_B: and chemistry and various other things
[00:02:18] SPEAKER_B: that we see around us?
[00:02:20] SPEAKER_B: So that I find to be maybe the most interesting
[00:02:23] SPEAKER_B: and the most important mystery for all of us
[00:02:25] SPEAKER_B: to go on the science and also on the personal level.
[00:02:28] SPEAKER_B: So that's what I'm interested in.
[00:02:30] SPEAKER_B: So your work is focused on starting at the physics,
[00:02:34] SPEAKER_A: going all the way to friendship and love and psychoanalysis.
[00:02:37] SPEAKER_A: Yeah, although actually I would turn that upside down.
[00:02:39] SPEAKER_B: I think that pyramid is backwards
[00:02:41] SPEAKER_B: and I think it's behavior science at the bottom.
[00:02:43] SPEAKER_B: I think it's behavior science all the way.
[00:02:45] SPEAKER_B: I think in certain ways,
[00:02:47] SPEAKER_B: even math is the behavior of a certain kind of being
[00:02:50] SPEAKER_B: that lives in a latent space
[00:02:52] SPEAKER_B: and physics is what we call systems
[00:02:55] SPEAKER_B: that at least look to be amenable
[00:02:57] SPEAKER_B: to a very simple low agency kind of model and so on.
[00:03:00] SPEAKER_B: But that's what I'm interested in is understanding that
[00:03:03] SPEAKER_B: and developing applications
[00:03:05] SPEAKER_B: because it's very important to me
[00:03:07] SPEAKER_B: that what we do is transition deep ideas and philosophy
[00:03:12] SPEAKER_B: into actual practical applications
[00:03:14] SPEAKER_B: that not only make it clear
[00:03:15] SPEAKER_B: whether we're making any progress or not,
[00:03:18] SPEAKER_B: but also allow us to relieve suffering and make life better
[00:03:21] SPEAKER_B: for all sentient beings and enable us and others
[00:03:25] SPEAKER_B: to reach their full potential.
[00:03:26] SPEAKER_B: So these are very practical things, I think.
[00:03:28] SPEAKER_B: Behavioral science, I suppose, is more subjective
[00:03:31] SPEAKER_A: and mathematics and physics is more objective.
[00:03:33] SPEAKER_A: Would that be the clear difference?
[00:03:35] SPEAKER_A: The idea basically is that
[00:03:37] SPEAKER_B: where something is on that spectrum,
[00:03:40] SPEAKER_B: and I've called it the spectrum of persuadability.
[00:03:42] SPEAKER_B: You could call it the spectrum of intelligence or agency
[00:03:44] SPEAKER_B: or something like that.
[00:03:45] SPEAKER_B: I like the notion of the spectrum of persuadability
[00:03:47] SPEAKER_B: because it's an engineering approach.
[00:03:50] SPEAKER_B: It means that these are not things you can decide
[00:03:54] SPEAKER_B: or have feelings about from a philosophical armchair.
[00:03:57] SPEAKER_B: You have to make a hypothesis about which tools,
[00:04:00] SPEAKER_B: which interaction protocols
[00:04:02] SPEAKER_B: you're gonna bring to a given system.
[00:04:03] SPEAKER_B: And then we all get to find out
[00:04:04] SPEAKER_B: how that worked out for you, right?
[00:04:06] SPEAKER_B: So you could be wrong in many ways.
[00:04:08] SPEAKER_B: In both directions, you can guess too high or too low
[00:04:10] SPEAKER_B: or wrong in various ways.
[00:04:11] SPEAKER_B: And then we can all find out how that's working out.
[00:04:14] SPEAKER_B: And so I do think that the behavior of certain objects
[00:04:18] SPEAKER_B: is well-described by specific formal rules.
[00:04:21] SPEAKER_B: And we call those things the subject of mathematics.
[00:04:24] SPEAKER_B: And then there are some other things
[00:04:25] SPEAKER_B: whose behavior really requires the kinds of tools
[00:04:29] SPEAKER_B: that we use in behavioral cognitive neuroscience.
[00:04:32] SPEAKER_B: And those are other kinds of minds
[00:04:34] SPEAKER_B: that we think we study in biology
[00:04:37] SPEAKER_B: or in psychology or other sciences.
[00:04:39] SPEAKER_B: Why are you using the term persuadability?
[00:04:41] SPEAKER_A: Who are you persuading and of what in this context?
[00:04:45] SPEAKER_B: Yeah, the beginning of my work
[00:04:47] SPEAKER_B: is very much in regenerative medicine,
[00:04:50] SPEAKER_B: in bioengineering, things like that.
[00:04:53] SPEAKER_B: So for those kinds of systems, the question is always,
[00:04:56] SPEAKER_B: how do you get the system to do what you want it to do?
[00:04:58] SPEAKER_B: So there are cells, there are molecular networks,
[00:05:01] SPEAKER_B: there are materials, there are organs and tissues
[00:05:04] SPEAKER_B: and synthetic beings and biobots and whatever.
[00:05:06] SPEAKER_B: And so the idea is if I want your cells to regrow a limb,
[00:05:10] SPEAKER_B: for example, if you're injured
[00:05:11] SPEAKER_B: and I want your cells to regrow a limb,
[00:05:13] SPEAKER_B: I have many options.
[00:05:14] SPEAKER_B: Some of those options are,
[00:05:15] SPEAKER_B: I'm going to micromanage all of the molecular events
[00:05:18] SPEAKER_B: that have to happen, right?
[00:05:20] SPEAKER_B: And there's an incredible number of those.
[00:05:21] SPEAKER_B: Or maybe I just have to micromanage the cells
[00:05:24] SPEAKER_B: and the stem cell kinds of signaling factors.
[00:05:27] SPEAKER_B: Or maybe actually I can give the cells
[00:05:30] SPEAKER_B: a very high level prompt that says,
[00:05:33] SPEAKER_B: you really should build a limb
[00:05:34] SPEAKER_B: and convince them to do it, right?
[00:05:36] SPEAKER_B: And so which of those is possible?
[00:05:40] SPEAKER_B: I mean, clearly people have a lot of intuitions about that.
[00:05:42] SPEAKER_B: If you ask standard people in regenerative medicine
[00:05:44] SPEAKER_B: and molecular biology, they're going to say,
[00:05:46] SPEAKER_B: well, that convincing thing is crazy.
[00:05:48] SPEAKER_B: What we really should be doing is talking to the cells
[00:05:50] SPEAKER_B: or better yet, the molecular networks.
[00:05:52] SPEAKER_B: And in fact, all the excitement
[00:05:53] SPEAKER_B: of the biological sciences today
[00:05:55] SPEAKER_B: are at single molecule approaches
[00:05:57] SPEAKER_B: and big data and genomics and all of that.
[00:06:00] SPEAKER_B: The assumption is that going down
[00:06:02] SPEAKER_B: is where the action's going to be, going down in scale.
[00:06:05] SPEAKER_B: And I think that's wrong,
[00:06:07] SPEAKER_B: but the thing that we can say for sure
[00:06:10] SPEAKER_B: is that you can't guess that.
[00:06:12] SPEAKER_B: You have to do experiments and you have to see
[00:06:14] SPEAKER_B: because you don't know where any given system is
[00:06:16] SPEAKER_B: on that spectrum of persuadability.
[00:06:18] SPEAKER_B: And it turns out that every time we look
[00:06:20] SPEAKER_B: and we take tools from behavioral science,
[00:06:22] SPEAKER_B: so learning different kinds of training,
[00:06:24] SPEAKER_B: different kinds of models that are used in active inference
[00:06:28] SPEAKER_B: and surprise minimization and perceptual multistability
[00:06:32] SPEAKER_B: and visual illusions
[00:06:33] SPEAKER_B: and all these kinds of interesting things,
[00:06:35] SPEAKER_B: stress perception and active memory reconstruction,
[00:06:40] SPEAKER_B: all these interesting things,
[00:06:41] SPEAKER_B: when we apply them outside the brain
[00:06:43] SPEAKER_B: to other kinds of living systems,
[00:06:45] SPEAKER_B: we find novel discoveries and novel capabilities,
[00:06:48] SPEAKER_B: actually being able to get the material to do new things
[00:06:51] SPEAKER_B: that nobody had ever found before.
[00:06:52] SPEAKER_B: And precisely because I think that people
[00:06:56] SPEAKER_B: didn't look at it from those perspectives,
[00:06:58] SPEAKER_B: they assumed that it was a low-level kind of thing.
[00:07:01] SPEAKER_B: So when I say persuadability,
[00:07:02] SPEAKER_B: I mean different types of approaches, right?
[00:07:05] SPEAKER_B: And we all know if you want to persuade
[00:07:07] SPEAKER_B: your wind-up clock to do something,
[00:07:09] SPEAKER_B: you're not going to argue with it
[00:07:10] SPEAKER_B: or make it feel guilty or anything.
[00:07:12] SPEAKER_B: You're going to have to get in there with a wrench
[00:07:13] SPEAKER_B: and you're going to have to tune it up and do whatever.
[00:07:15] SPEAKER_B: If you want to do that same thing to a cell
[00:07:18] SPEAKER_B: or a thermostat or an animal or a human,
[00:07:20] SPEAKER_B: you're going to be using other sets of tools
[00:07:22] SPEAKER_B: that we've given other names to.
[00:07:24] SPEAKER_B: And so now, of course, that spectrum,
[00:07:26] SPEAKER_B: the important thing is that
[00:07:27] SPEAKER_B: as you get to the right of that spectrum,
[00:07:29] SPEAKER_B: whereas the agency of the system goes up,
[00:07:31] SPEAKER_B: it is no longer just about persuading it to do things.
[00:07:33] SPEAKER_B: It's a bi-directional relationship,
[00:07:35] SPEAKER_B: what Richard Watson would call
[00:07:36] SPEAKER_B: a mutual vulnerable knowing.
[00:07:38] SPEAKER_B: So the idea is that on the right side of that spectrum,
[00:07:41] SPEAKER_B: when systems reach the higher levels of agency,
[00:07:44] SPEAKER_B: the idea is that you're willing to let that system
[00:07:46] SPEAKER_B: persuade you of things as well.
[00:07:48] SPEAKER_B: You know, in molecular biology, you do things,
[00:07:50] SPEAKER_B: hopefully the system does what you want to do,
[00:07:52] SPEAKER_B: but you haven't changed.
[00:07:53] SPEAKER_B: You're still exactly the way you came in.
[00:07:55] SPEAKER_B: But on the right side of that spectrum,
[00:07:56] SPEAKER_B: if you're having interactions with even cells,
[00:07:59] SPEAKER_B: but certainly, you know, dogs, other animals,
[00:08:02] SPEAKER_B: maybe other creatures soon,
[00:08:04] SPEAKER_B: you're not the same at the end of that interaction
[00:08:06] SPEAKER_B: as you were going in.
[00:08:07] SPEAKER_B: It's a mutual bi-directional relationship.
[00:08:09] SPEAKER_B: So it's not just you persuading something else.
[00:08:11] SPEAKER_B: It's not you pushing things.
[00:08:13] SPEAKER_B: It's a mutual bi-directional set of persuasions,
[00:08:17] SPEAKER_B: whether those are purely intellectual or of other kinds.
[00:08:20] So in order to be effective at persuading
[00:08:23] SPEAKER_A: an intelligent being, you yourself have to be persuadable.
[00:08:27] SPEAKER_A: So the closer in intelligence you are
[00:08:29] SPEAKER_A: to the thing you're trying to persuade,
[00:08:31] SPEAKER_A: the more persuadable you have to become.
[00:08:33] SPEAKER_A: Hence the mutual vulnerable knowing.
[00:08:36] SPEAKER_A: What a term.
[00:08:37] Yeah, yeah, Richard, yeah,
[00:08:38] SPEAKER_B: you should talk to Richard as well.
[00:08:39] SPEAKER_B: He's an amazing guy,
[00:08:40] SPEAKER_B: and he's got some very interesting ideas
[00:08:42] SPEAKER_B: about the intersection of cognition and evolution.
[00:08:46] SPEAKER_B: But I think what you bring up is very important
[00:08:48] SPEAKER_B: because there has to be a kind of impedance match
[00:08:52] SPEAKER_B: between what you're looking for
[00:08:53] SPEAKER_B: and the tools that you're using.
[00:08:55] SPEAKER_B: I think the reason physics always sees mechanism
[00:08:58] SPEAKER_B: and not minds is that physics uses low agency tools.
[00:09:01] SPEAKER_B: You've got voltmeters and rulers and things like this.
[00:09:04] SPEAKER_B: And if you use those tools as your interface,
[00:09:06] SPEAKER_B: all you're ever going to see is mechanisms
[00:09:09] SPEAKER_B: and those kinds of things.
[00:09:10] SPEAKER_B: If you want to see minds, you have to use a mind, right?
[00:09:13] SPEAKER_B: You have to have,
[00:09:14] SPEAKER_B: there has to be some degree of resonance
[00:09:15] SPEAKER_B: between your interface and the thing you're hoping to find.
[00:09:18] SPEAKER_A: You've said this about physics before.
[00:09:19] SPEAKER_A: Can you just linger on that,
[00:09:21] SPEAKER_A: like expand on it, what you mean,
[00:09:22] SPEAKER_A: why physics is not enough to understand life,
[00:09:26] SPEAKER_A: to understand mind, to understand intelligence?
[00:09:29] SPEAKER_A: You make a lot of controversial statements with your work.
[00:09:31] SPEAKER_A: That's one of them.
[00:09:32] SPEAKER_A: Because there are a lot of physicists
[00:09:33] SPEAKER_A: that believe they can understand life,
[00:09:35] SPEAKER_A: the emergence of life, the origin of life,
[00:09:37] SPEAKER_A: the origin of intelligence, using the tools of physics.
[00:09:41] SPEAKER_A: In fact, all of the other tools
[00:09:43] SPEAKER_A: are a distraction to those folks.
[00:09:45] SPEAKER_A: If you want to understand fundamentally anything,
[00:09:47] SPEAKER_A: you have to start a physics to them.
[00:09:49] SPEAKER_A: And you're saying, no, physics is not enough.
[00:09:52] Here's the issue.
[00:09:53] SPEAKER_B: Everything here hangs on what it means to understand, okay?
[00:09:56] SPEAKER_B: In, for me, because understand doesn't just.
[00:10:00] SPEAKER_B: mean have some sort of pleasing model that seems to capture some important aspect of what's going
[00:10:06] SPEAKER_B: on. It also means that you have to be generative and creative in terms of capabilities. And so,
[00:10:12] SPEAKER_B: for me, that means if I tell you this is what I think about cognition in cells and tissues,
[00:10:16] SPEAKER_B: it means, for example, that I think we're going to be able to take those ideas and use them to
[00:10:22] SPEAKER_B: produce new regenerative medicine that actually helps people in various ways, right? It's just
[00:10:25] SPEAKER_B: an example. So, if you think as a physicist, you're going to have a complete understanding
[00:10:30] SPEAKER_B: of what's going on from that perspective of fields and particles and then, you know,
[00:10:36] SPEAKER_B: who knows what else is at the bottom there. Does that mean then that when somebody is missing a
[00:10:42] SPEAKER_B: finger or has a psychological problem or, you know, has these other high-level issues, that
[00:10:48] SPEAKER_B: you have something for them, that you're going to be able to do something? Because my claim is that
[00:10:51] SPEAKER_B: you're not going to. And even if you have some theory of physics that is completely compatible
[00:10:57] SPEAKER_B: with everything that's going on, it's not enough. That's not specific enough to enable you to solve
[00:11:02] SPEAKER_B: the problems you need to solve. In the end, when you need to solve those problems, the person
[00:11:06] SPEAKER_B: you're going to go to is not a physicist. It's going to be either a biologist or a psychiatrist
[00:11:12] SPEAKER_B: or who knows, but it's not going to be a physicist. And the simple example is this.
[00:11:17] SPEAKER_B: You know, let's say someone comes in here and tells you a beautiful mathematical proof. Okay,
[00:11:22] SPEAKER_B: it's just really, you know, deep and beautiful. And there's a physicist nearby and he says,
[00:11:26] SPEAKER_B: well, I know exactly what happened. There were some air particles that moved from
[00:11:30] SPEAKER_B: that guy's mouth to your ear. I see what goes on. It moved the cilia in your ear and the electrical
[00:11:36] SPEAKER_B: signals went up to your brain. I mean, we have a complete accounting of what happened, done and
[00:11:40] SPEAKER_B: done. But if you want to understand what's the more important aspect of that interaction,
[00:11:45] SPEAKER_B: it's not going to be found in the physics department. It's going to be found in the
[00:11:47] SPEAKER_B: math department. So that's my only claim is that is that physics is an amazing lens with which to
[00:11:51] SPEAKER_B: view the world, but you're capturing certain things. And if you want to stretch to sort of
[00:11:57] SPEAKER_B: encompass these other things, it's just, we just don't call that physics anymore, right? That's,
[00:12:01] SPEAKER_B: we call that something else. Okay. But you're kind of speaking about the
[00:12:06] SPEAKER_A: super complex organisms. Can we go to the simplest possible thing where you first take a step over
[00:12:12] SPEAKER_A: the line, the Cartesian cut, as you've called it from the non-mind to mind from the non-living to
[00:12:20] SPEAKER_A: live is simplest possible thing. Isn't that in the realm of physics to understand how to
[00:12:27] SPEAKER_A: understand that first step where you're like, that thing is no mind, probably non-living.
[00:12:34] SPEAKER_A: And here's a living thing that has a mind that line. I think that's a really interesting line.
[00:12:40] SPEAKER_A: Maybe you can speak to the line as well. And can physics help us understand it?
[00:12:43] SPEAKER_A: Yeah, let's talk about, well, first of all, of course it can mean it can help meaning that I'm
[00:12:48] SPEAKER_B: not saying physics is not helpful. Of course it's helpful. It's, it's a very important lens on one
[00:12:52] SPEAKER_B: slice of what's going on in any of these systems. But I think the most important thing I can say
[00:12:56] SPEAKER_B: about that question is I don't believe in any such line. I don't believe any of that exists.
[00:13:01] SPEAKER_B: I think I think there is a I think it's a continuum. I think we as humans like to demarcate
[00:13:08] SPEAKER_B: areas on that continuum and give them names because it makes life easier. And then we have
[00:13:12] SPEAKER_B: a lot of battles over, uh, you know, so-called category errors when people transgress those,
[00:13:17] SPEAKER_B: those categories. I think most of those categories at this point, they, they may have done some,
[00:13:22] SPEAKER_B: some good service at the beginning of when the scientific method was getting started and so on.
[00:13:26] SPEAKER_B: I think at this point, uh, they mostly hold back science. Many, many categories that we can talk
[00:13:30] SPEAKER_B: about are at this point, very harmful to progress because what those categories do is they prevent
[00:13:35] SPEAKER_B: you from porting tools. If you think that, uh, living things are fundamentally different from
[00:13:41] SPEAKER_B: non-living things, or if you think that cognitive things are these like advanced brainy things that
[00:13:46] SPEAKER_B: are very different from other kinds of systems, what you're not going to do is take the tools
[00:13:51] SPEAKER_B: that are appropriate to these, uh, to, to these kinds of, uh, cognitive systems, right? So the,
[00:13:55] SPEAKER_B: so the tools that have been developed in, in behavioral science and so on, you're never going
[00:13:58] SPEAKER_B: to try them in other contexts because, because you've already decided that there's a categorical
[00:14:02] SPEAKER_B: difference, that it would be a categorical error to apply them. And people say this to me all the
[00:14:07] SPEAKER_B: time is that you're making a category error. And as, as if these categories were given to us,
[00:14:11] SPEAKER_B: you know, from, from, from on high, and we have to, we have to obey them forevermore. The category
[00:14:16] SPEAKER_B: should change with the science. So, um, yeah, I don't believe in any such line. And I think,
[00:14:20] SPEAKER_B: I think a physics story is very often a useful part of the story, but for most interesting things,
[00:14:27] SPEAKER_B: it's not the entire story. Okay. So if there's no line, is it still useful to talk about things
[00:14:34] SPEAKER_A: like the origin of life? That's the, the, one of the big open mysteries before us as a human
[00:14:41] SPEAKER_A: civilization, as a scientifically minded, curious homo sapiens, how did this whole thing start?
[00:14:51] SPEAKER_A: Are you saying there is no start? Is there a point where you could say that invention right
[00:14:58] SPEAKER_A: there was the start of it all on earth? My suggestion is that much better than trying to,
[00:15:06] SPEAKER_B: in my experience, much better than trying to define any kind of a line. Okay. Because,
[00:15:11] SPEAKER_B: because inevitably I've never, I've never found, and people try to, you know, we play this game
[00:15:15] SPEAKER_B: all the time. When I make my continuum claim, then people try to come up, okay, well, what about this?
[00:15:19] SPEAKER_B: You know, what about this? And I haven't found one yet that really shoots that down that, that
[00:15:23] SPEAKER_B: you can't zoom in and say, yeah, okay, but right before then this happened. And then if we really
[00:15:26] SPEAKER_B: look close, like here's a bunch of steps in between, right? Pretty much everything ends up
[00:15:30] SPEAKER_B: being a continuum, but here's what I think is much more interesting than trying to make that line.
[00:15:34] SPEAKER_B: I think what's, what's really more useful is trying to understand the transformation process.
[00:15:39] SPEAKER_B: What is it that happened to scale up? And I'll give you a really dumb example. And we all,
[00:15:44] SPEAKER_B: and we always get into this because people, people often really, really don't like this
[00:15:48] SPEAKER_B: continuum view, the word adult, right? Everybody's going to say, look, I know what a baby is. I know
[00:15:53] SPEAKER_B: what an adult is. You're crazy to say that there's no difference. Not saying there's no difference.
[00:15:57] SPEAKER_B: What I'm saying is the word adult is really helpful in court because, because, because you
[00:16:02] SPEAKER_B: just need to move things along. And so we've decided that, uh, if you're 18, you're an adult.
[00:16:07] SPEAKER_B: However, what it hides is, is what it completely conceals is the fact that first of all, nothing
[00:16:13] SPEAKER_B: happens on your 18th birthday, right? That's, that's special. Second, if you actually look at
[00:16:18] SPEAKER_B: the data, the car rental companies actually have a much better estimate because they actually look
[00:16:22] SPEAKER_B: at the accident statistics and they'll say it's about 25 is, is really what you're looking for,
[00:16:26] SPEAKER_B: right? So theirs is a little better. It's less arbitrary, but in either case, what it's hiding
[00:16:31] SPEAKER_B: is the fact that we do not have a good story of what happened from the time that you were an egg
[00:16:36] SPEAKER_B: to the time that you're the supposed adult. And what is the scaling of personal responsibility,
[00:16:42] SPEAKER_B: decision-making judgment, like these are deep, fundamental content, you know, questions.
[00:16:46] SPEAKER_B: Nobody wants to get into that every time somebody, uh, you know, has a traffic ticket.
[00:16:50] SPEAKER_B: And so, okay. So, so we've just decided that there's this adult idea how, and, and, and of
[00:16:54] SPEAKER_B: course it does come up in court because then somebody has a brain tumor or somebody's eaten
[00:16:57] SPEAKER_B: too many Twinkies or, or something has happened and you say, look, that wasn't me. Whoever did
[00:17:01] SPEAKER_B: that, I was on drugs. Well, why'd you take the drugs? Well, that was, you know, that was yesterday
[00:17:05] SPEAKER_B: me today. This is right. So, so we get into these very deep questions that are completely glossed
[00:17:10] SPEAKER_B: over by this idea of an adult. So, so I think once you start scratching the surface, most of
[00:17:15] SPEAKER_B: these categories are like that they're convenient and they're good. It's, you know, I get into this
[00:17:20] SPEAKER_B: with neurons all the time. I'll ask people what's, what's a neuron, like what's really a neuron.
[00:17:24] SPEAKER_B: And yes, if you're, if you're in a neurobiology one-on-one, of course you just say like,
[00:17:29] SPEAKER_B: these are what neurons look like. Let's just study the neuroanatomy and we're done.
[00:17:32] SPEAKER_B: But if you really want to understand what's going on, well, neurons develop from other types of
[00:17:38] SPEAKER_B: cells. And that was a slow and gradual process. And most of the cells in your body do the things
[00:17:43] SPEAKER_B: that neurons do. So what really is a neuron, right? So, so once you start scratching this,
[00:17:46] SPEAKER_B: this, this happens. And I have some things that I think are coming out of our lab and others that
[00:17:51] SPEAKER_B: are, I think, very interesting about the origin of life, but I don't think it's about finding that
[00:17:55] SPEAKER_B: one boom, like this is, yeah, there'll be there, there are innovations, right? There are, there
[00:17:58] SPEAKER_B: are innovations that, that allow you to scale in a, in an amazing way for sure. For sure.
[00:18:04] SPEAKER_B: And there are lots of people that study those, right? So, so things that thermodynamic kind of
[00:18:08] SPEAKER_B: metabolic things and, and, and all kinds of architectures and so on. But I don't think
[00:18:12] SPEAKER_B: it's about finding a line. I think it's about finding a scaling process.
[00:18:16] SPEAKER_A: The scaling process, but then there's more rapid scaling and there's slower scaling. So
[00:18:22] SPEAKER_A: innovation invention, I think is useful to understand. So you can predict how likely it
[00:18:28] SPEAKER_A: is on other planets, for example, or to be able to describe the likelihood of these kinds of
[00:18:36] SPEAKER_A: phenomena happening in certain kinds of environments, again, specifically in answering how many alien
[00:18:42] SPEAKER_A: civilizations there are. That's why it's useful, but it's also useful on a scientific level
[00:18:49] SPEAKER_A: to have categories, not just because it makes us feel good and fuzzy inside, but because it
[00:18:54] SPEAKER_A: makes conversation possible and productive. I think if everything is a spectrum is, it becomes
[00:19:02] SPEAKER_A: difficult to make concrete statements, I think. But we even use the terms of biology and physics.
[00:19:07] SPEAKER_A: Those are categories. Technically it's all the same thing, really. Fundamentally, it's all the
[00:19:12] SPEAKER_A: same. There's no difference in biology and physics, but it's a useful category. If you
[00:19:17] SPEAKER_A: go to the physics department and the biology department, those people are different in,
[00:19:22] SPEAKER_A: in some kind of categorical way. So somehow, I don't know what the chicken or the egg is,
[00:19:27] SPEAKER_A: but the categories, maybe the categories create themselves because of the way we think about them
[00:19:32] SPEAKER_A: and use them in language, but it does seem useful. Let me make the opposite argument. They're
[00:19:36] SPEAKER_B: absolutely useful. They're useful specifically when you want to gloss over certain things.
[00:19:41] SPEAKER_B: The categories are exactly useful when there's a whole bunch of stuff. And this is, this is what's
[00:19:45] SPEAKER_B: important about science is like the art of being able to say something without first having to say
[00:19:49] SPEAKER_B: everything, right? We should make it impossible. So, so categories are great when you, when you
[00:19:53] SPEAKER_B: want to say, look, I know there's a bunch of stuff hidden here. I'm going to ignore all that. And
[00:19:57] SPEAKER_B: we're just going to like, let's get on with this particular.
[00:20:00] SPEAKER_B: thing. And all of that is great as long as you
[00:20:03] SPEAKER_B: don't lose track of the stuff that you glossed
[00:20:05] SPEAKER_B: over. And that was what I'm afraid is happening
[00:20:07] SPEAKER_B: in a lot of different ways. And in terms of look,
[00:20:10] SPEAKER_B: I'm, I'm, I'm very interested in, in, in life,
[00:20:13] SPEAKER_B: uh, you know, beyond earth and all, all of these
[00:20:15] SPEAKER_B: kinds of things. Although we should also talk
[00:20:16] SPEAKER_B: about what I call SUTI, S-U-T-I, the search for
[00:20:19] SPEAKER_B: unconventional terrestrial intelligences. I
[00:20:22] SPEAKER_B: think, I think, I think we got much bigger issues
[00:20:24] SPEAKER_B: than, than actually recognizing aliens off earth.
[00:20:26] SPEAKER_B: But I'll make this claim. I think the categorical
[00:20:29] SPEAKER_B: stuff is actually hurting that search because,
[00:20:32] SPEAKER_B: because if we try to define categories, uh, with
[00:20:36] SPEAKER_B: the kinds of criteria that we've gotten used to,
[00:20:38] SPEAKER_B: we are going to be very poorly set up to recognize
[00:20:42] SPEAKER_B: life in novel embodiments. I think we have a kind
[00:20:44] SPEAKER_B: of mind blindness. I think this is really key.
[00:20:46] SPEAKER_B: It's much, to me, to me, um, the cognitive
[00:20:49] SPEAKER_B: spectrum is much more interesting than the
[00:20:51] SPEAKER_B: spectrum of life. I think really what we're
[00:20:52] SPEAKER_B: talking about is a spectrum of cognition. And,
[00:20:56] SPEAKER_B: uh, it's, I know it's weird as a biologist to
[00:20:58] SPEAKER_B: say, I don't think life is all that interesting
[00:21:00] SPEAKER_B: in category. I think the categories of, of
[00:21:02] SPEAKER_B: different types of minds, I think is extremely
[00:21:05] SPEAKER_B: interesting. And to the extent that we think our
[00:21:07] SPEAKER_B: categories are complete and are cutting nature
[00:21:10] SPEAKER_B: at its joints, we are going to be very poorly
[00:21:13] SPEAKER_B: placed to recognize novel systems. So for example,
[00:21:16] SPEAKER_B: a lot of people will say, well, this is
[00:21:17] SPEAKER_B: intelligent and this isn't right. And there's a
[00:21:20] SPEAKER_B: binary thing. And, and, and that's useful in
[00:21:22] SPEAKER_B: occasionally that's useful for some things. I
[00:21:24] SPEAKER_B: would like to say, instead of that, let's make
[00:21:27] SPEAKER_B: us, let's, let's, let's admit that we have a
[00:21:28] SPEAKER_B: spectrum, but instead of just saying, oh, look,
[00:21:31] SPEAKER_B: everything's intelligent, right? Because if you
[00:21:33] SPEAKER_B: do that, you're right. You can't, you can't do
[00:21:34] SPEAKER_B: anything after that. What I'd like to say instead
[00:21:37] SPEAKER_B: is no, no, you have to be very specific as to
[00:21:39] SPEAKER_B: what kind and how much, in other words, what
[00:21:41] SPEAKER_B: problem space is it operating in? What kind of
[00:21:44] SPEAKER_B: mind does it have? What kind of cognitive
[00:21:45] SPEAKER_B: capacities does it have? You have to actually
[00:21:47] SPEAKER_B: be much more specific and, and we can even name,
[00:21:50] SPEAKER_B: right? That's fine. We can name different types
[00:21:51] SPEAKER_B: of, I mean, this is doing predictive processing.
[00:21:54] SPEAKER_B: This can't do that, but it can form memories.
[00:21:55] SPEAKER_B: What kind? Well, habituation and sensitization,
[00:21:58] SPEAKER_B: but not associative conditioning. Like it's
[00:22:00] SPEAKER_B: fine to have categories for specific capabilities,
[00:22:03] SPEAKER_B: but it's, it's, uh, it, it actually, I think it
[00:22:06] SPEAKER_B: actually makes, makes for much more rigorous
[00:22:07] SPEAKER_B: discussions because it makes you say, what is
[00:22:10] SPEAKER_B: it that you are claiming this thing does? And
[00:22:12] SPEAKER_B: it works in both directions. So, and so some
[00:22:14] SPEAKER_B: people will say, well, that's a, that's a cell
[00:22:16] SPEAKER_B: that can't be intelligent. And I'll say, well,
[00:22:18] SPEAKER_B: let's be very specific. Here are some claims
[00:22:20] SPEAKER_B: about, here's some problem solving that it's
[00:22:22] SPEAKER_B: doing. Tell me why that doesn't, you know, why
[00:22:24] SPEAKER_B: doesn't that match? Or in the opposite direction,
[00:22:26] SPEAKER_B: somebody comes to me and says, you're right,
[00:22:28] SPEAKER_B: you're right. You know, the whole, the whole
[00:22:29] SPEAKER_B: solar system, man, it's just like this amazing,
[00:22:32] SPEAKER_B: like, okay, what is it doing? Like, tell me,
[00:22:34] SPEAKER_B: tell me what, what tools of cognitive and
[00:22:36] SPEAKER_B: behavioral science are you using to, to, to
[00:22:38] SPEAKER_B: reach that conclusion, right? And so I think,
[00:22:40] SPEAKER_B: I think it's actually much more productive to
[00:22:42] SPEAKER_B: take this operational stance and say, tell, tell
[00:22:44] SPEAKER_B: me what protocols you think you can deploy with
[00:22:46] SPEAKER_B: this thing that would lead you to, to, to use
[00:22:48] SPEAKER_B: these terms.
[00:22:49] SPEAKER_A: To have a bit of a meta conversation about the
[00:22:51] SPEAKER_A: conversation. I should say that part of the
[00:22:53] SPEAKER_A: persuadability argument that we two intelligent
[00:22:57] SPEAKER_A: creatures are doing is me playing devil's
[00:22:59] SPEAKER_A: advocate every once in a while. And you did the
[00:23:01] SPEAKER_A: same, which is kind of interesting, taking the
[00:23:03] SPEAKER_A: opposite view and see what comes out. Because
[00:23:06] SPEAKER_A: you don't know the result of the argument until
[00:23:08] SPEAKER_A: you have the argument. And it's, seems productive
[00:23:12] SPEAKER_A: to just take the other side of the argument.
[00:23:14] SPEAKER_A: For sure. It's a very important thinking aid
[00:23:18] SPEAKER_B: to, first of all, you know, what they call
[00:23:20] SPEAKER_B: steel manning, right? To try to, try to make
[00:23:22] SPEAKER_B: the strongest possible case for the other side
[00:23:24] SPEAKER_B: and to ask yourself, okay, what are all the,
[00:23:26] SPEAKER_B: what are all the places that I sort of glossing
[00:23:29] SPEAKER_B: over because I don't know exactly what to say
[00:23:32] SPEAKER_B: and where all the, where all the holes in the
[00:23:33] SPEAKER_B: argument and what would, what would, you know,
[00:23:35] SPEAKER_B: a really good critique really look like? Yeah.
[00:23:38] SPEAKER_A: Sorry to go back there just to linger on the
[00:23:39] SPEAKER_A: term because it's so interesting, persuadability.
[00:23:42] SPEAKER_A: Did I understand correctly that you mean that
[00:23:44] SPEAKER_A: it's kind of synonymous with intelligence? So
[00:23:47] SPEAKER_A: it's an engineering centric view of an intelligence
[00:23:52] SPEAKER_A: system because if it's persuadable, you're more
[00:23:54] SPEAKER_A: focused on how can I steer the goals of the
[00:23:58] SPEAKER_A: system, the behaviors of the system, which meaning
[00:24:04] SPEAKER_A: an intelligence system maybe is a goal oriented,
[00:24:07] SPEAKER_A: goal driven system with agency. And when you
[00:24:12] SPEAKER_A: call it persuadable, you're thinking more like,
[00:24:16] SPEAKER_A: okay, here's an intelligence system that I'm
[00:24:17] SPEAKER_A: interacting with that I would like to get it
[00:24:20] SPEAKER_A: to accomplish certain things. But fundamentally
[00:24:23] SPEAKER_A: they're synonymous or correlated, persuadability
[00:24:27] SPEAKER_A: and intelligence. They're definitely correlated.
[00:24:29] SPEAKER_A: So, so let me, I want to, I want to preface this
[00:24:32] SPEAKER_B: with, with one thing. When I say it's an
[00:24:34] SPEAKER_B: engineering perspective, I don't mean that the
[00:24:37] SPEAKER_B: standard tools that we use in engineering and
[00:24:40] SPEAKER_B: this idea of, of enforced control and steering
[00:24:43] SPEAKER_B: is how we should view all of the world. I'm not
[00:24:46] SPEAKER_B: saying that at all. And, and I want to be very
[00:24:48] SPEAKER_B: clear on the, because, because, because people do
[00:24:51] SPEAKER_B: email me and say that is engineering thing. You're
[00:24:53] SPEAKER_B: going to drain the, you know, the life and the
[00:24:54] SPEAKER_B: majesty out of these high end, like human
[00:24:56] SPEAKER_B: conversation. My whole, my whole point is not that
[00:24:58] SPEAKER_B: at all. It's that of course, at the right side
[00:25:02] SPEAKER_B: of the spectrum, it doesn't look like engineering
[00:25:03] SPEAKER_B: anymore, right? It looks like, it looks like
[00:25:05] SPEAKER_B: friendship and love and psychoanalysis and all
[00:25:07] SPEAKER_B: these other tools that we have. But here's what
[00:25:10] SPEAKER_B: I want to do. I want to be very specific to my
[00:25:12] SPEAKER_B: colleagues in regenerative medicine. And just
[00:25:14] SPEAKER_B: imagine if I, you know, if I, if I went to a
[00:25:16] SPEAKER_B: bioengineering department or a genetics
[00:25:18] SPEAKER_B: department, and I started talking about high
[00:25:20] SPEAKER_B: level, you know, cognition and psychoanalysis,
[00:25:22] SPEAKER_B: right? They didn't want to hear that. So, so I
[00:25:25] SPEAKER_B: bring my, I focus on the engineering approach
[00:25:28] SPEAKER_B: because I want to say, look, this is not a
[00:25:31] SPEAKER_B: philosophical problem. This is not a linguistics
[00:25:33] SPEAKER_B: problem. We are not trying to define terms in
[00:25:36] SPEAKER_B: different ways to make anybody feel fuzzy. What
[00:25:37] SPEAKER_B: I'm telling you is if you want to reach certain
[00:25:40] SPEAKER_B: capabilities, if you want to reprogram cancer,
[00:25:42] SPEAKER_B: if you want to regrow new organs, you want to
[00:25:44] SPEAKER_B: defeat aging, you want to do these specific
[00:25:46] SPEAKER_B: things, you are leaving too much on the table by
[00:25:49] SPEAKER_B: making an unwarranted assumption that the low
[00:25:51] SPEAKER_B: level tools that we have. So these are the rules
[00:25:54] SPEAKER_B: of chemistry and the kind of remolecular rewiring
[00:25:57] SPEAKER_B: that those are going to be sufficient to get to
[00:25:59] SPEAKER_B: where you want to go. It's a, it's a, it's an
[00:26:00] SPEAKER_B: assumption only, and it's an unwarranted
[00:26:02] SPEAKER_B: assumption. And actually we've done experiments
[00:26:04] SPEAKER_B: now. So, so not philosophy, but real experiments
[00:26:06] SPEAKER_B: that if you take these other tools, you can in
[00:26:09] SPEAKER_B: fact persuade the system in ways that has never
[00:26:11] SPEAKER_B: been done before. And, and, and we can, we can
[00:26:13] SPEAKER_B: unpack all of that, but it is, it is absolutely
[00:26:16] SPEAKER_B: correlated with intelligence. So let me flesh
[00:26:18] SPEAKER_B: that out a little bit. What I think is scaling
[00:26:21] SPEAKER_B: in all of these things, right? Because I keep
[00:26:23] SPEAKER_B: talking about the scaling. So what is it that's
[00:26:24] SPEAKER_B: scaling? What I think is scaling is something I
[00:26:28] SPEAKER_B: call the cognitive light cone. And the cognitive
[00:26:30] SPEAKER_B: light cone is the size of the biggest goal state
[00:26:34] SPEAKER_B: that you can pursue. This doesn't mean how far
[00:26:37] SPEAKER_B: do your senses reach. This doesn't mean how far
[00:26:40] SPEAKER_B: can you affect it. So the James Webb telescope
[00:26:42] SPEAKER_B: has enormous sensory reach, but that doesn't
[00:26:44] SPEAKER_B: mean that's, that's the size of its cognitive
[00:26:46] SPEAKER_B: light cone. The size of the cognitive light cone
[00:26:48] SPEAKER_B: is the scale of the biggest goal you can
[00:26:50] SPEAKER_B: actively pursue. But I do think it's a useful
[00:26:53] SPEAKER_B: concept to enable us to think about very different
[00:26:55] SPEAKER_B: types of agents of different composition,
[00:26:57] SPEAKER_B: different provenance, you know, engineered,
[00:26:59] SPEAKER_B: evolved, hybrid, whatever, all in the same
[00:27:02] SPEAKER_B: framework. And by the way, the reason I use
[00:27:04] SPEAKER_B: light cone is that it has this idea from physics
[00:27:07] SPEAKER_B: that you're putting space and time kind of in
[00:27:08] SPEAKER_B: the same diagram, which is which, which I like
[00:27:10] SPEAKER_B: here. So if you tell me that all your goals
[00:27:14] SPEAKER_B: revolve around maximizing the amount of sugar
[00:27:18] SPEAKER_B: con, the amount of sugar in this, in this, you
[00:27:20] SPEAKER_B: know, 10, 20 micron radius of space time, and
[00:27:24] SPEAKER_B: that you have, you know, 20 minutes memory going
[00:27:26] SPEAKER_B: back and maybe five minutes predictive capacity
[00:27:28] SPEAKER_B: going forward, that tiny little cognitive light
[00:27:30] SPEAKER_B: cone, I'm going to see probably a bacterium. And
[00:27:32] SPEAKER_B: if you say to me that, well, I, I'm able to
[00:27:35] SPEAKER_B: care about several hundred yards sort of scale,
[00:27:40] SPEAKER_B: I could never care about what happens three
[00:27:42] SPEAKER_B: weeks from now, two towns over just impossible.
[00:27:44] SPEAKER_B: I'm saying you might be a dog. And if, and if
[00:27:47] SPEAKER_B: you say to me, okay, I care about really what
[00:27:50] SPEAKER_B: happens, you know, the financial markets on
[00:27:52] SPEAKER_B: earth, you know, long after I'm dead and this
[00:27:55] SPEAKER_B: and that, so you're probably a human. And if
[00:27:57] SPEAKER_B: you say to me, I care in the linear range, I
[00:28:00] SPEAKER_B: actively not, I'm not just saying that I can
[00:28:01] SPEAKER_B: actively care in the linear range about all
[00:28:04] SPEAKER_B: the living beings on this planet. I'm going to
[00:28:06] SPEAKER_B: say, well, you're not a standard human, you must
[00:28:08] SPEAKER_B: be something else because humans, I don't know,
[00:28:10] SPEAKER_B: these standard humans today, I don't think can
[00:28:12] SPEAKER_B: do that. You, you must be some kind of a body
[00:28:14] SPEAKER_B: soft or some other thing that has these massive
[00:28:16] SPEAKER_B: cognitive light cones. So I think what's scaling
[00:28:18] SPEAKER_B: from zero, and I do think it goes all the way
[00:28:20] SPEAKER_B: down. I think we can talk about, um, uh, even
[00:28:23] SPEAKER_B: even particles doing something like this. I
[00:28:25] SPEAKER_B: think what scales is the size of the cognitive
[00:28:28] SPEAKER_B: light cone. And so now this is an interesting
[00:28:30] SPEAKER_B: here. I'll, I'll try for a definition of life
[00:28:32] SPEAKER_B: or whatever, for whatever it's worth. I spend
[00:28:33] SPEAKER_B: no time trying to make that stick, but if we
[00:28:36] SPEAKER_B: wanted to, uh, I think we call things alive to
[00:28:40] SPEAKER_B: the extent that the cognitive light cone of
[00:28:44] SPEAKER_B: that thing is bigger than that of its parts. So
[00:28:47] SPEAKER_B: in other words, rocks, aren't very exciting
[00:28:49] SPEAKER_B: because the things it knows how to do are the
[00:28:51] SPEAKER_B: things that it's parts already know how to do,
[00:28:53] SPEAKER_B: which is follow gradients and things like that.
[00:28:55] SPEAKER_B: But living things are amazing at aligning their,
[00:28:59] SPEAKER_B: but there are competent parts so that the
[00:29:01] SPEAKER_B: collective has a larger cognitive light cone
[00:29:03] SPEAKER_B: than the parts. I'll give you a very simple
[00:29:05] SPEAKER_B: example that comes up in biology and it comes
[00:29:07] SPEAKER_B: up in our cancer program all the time. Individual
[00:29:10] SPEAKER_B: cells have little tiny cognitive light cones.
[00:29:13] SPEAKER_B: They, what are their goals? Well, they're trying
[00:29:15] SPEAKER_B: to manage pH, metabolic state, some other things.
[00:29:19] SPEAKER_B: There are some goals in transcriptional space,
[00:29:21] SPEAKER_B: some goals and, uh, metabolic space, some goals
[00:29:24] SPEAKER_B: and, uh, physiological state space, but, but
[00:29:26] SPEAKER_B: they, they're generally very tiny goals. One
[00:29:29] SPEAKER_B: thing evolution did was to provide a kind of
[00:29:31] SPEAKER_B: cognitive glue, which we can also talk about
[00:29:34] SPEAKER_B: that ties them together into a multicellular
[00:29:37] SPEAKER_B: system. And those systems have grandiose goals.
[00:29:40] SPEAKER_B: They're making limbs. And, and if you're a
[00:29:42] SPEAKER_B: salamander limb and you chop it off, they will
[00:29:44] SPEAKER_B: regrow that limb with the right number of fingers.
[00:29:46] SPEAKER_B: Then they'll stop when it's done. The goal has
[00:29:48] SPEAKER_B: been achieved. No individual cell knows what
[00:29:50] SPEAKER_B: a finger is or how many fingers you're supposed
[00:29:52] SPEAKER_B: to have, but the collective absolutely does.
[00:29:54] SPEAKER_B: And that process of growing that cognitive
[00:29:56] SPEAKER_B: light cone from a single cell to something much
[00:29:59] SPEAKER_B: bigger. And of course,
[00:30:00] SPEAKER_B: the failure mode of that process.
[00:30:02] SPEAKER_B: So cancer, right?
[00:30:02] SPEAKER_B: When cells disconnect, they physiologically
[00:30:05] SPEAKER_B: disconnect from the other cells, their cognitive
[00:30:07] SPEAKER_B: lycone shrinks, the boundary between self and world,
[00:30:09] SPEAKER_B: which is what the cognitive lycone defines, shrinks.
[00:30:12] SPEAKER_B: Now they're back to an amoeba.
[00:30:13] SPEAKER_B: As far as they're concerned, the rest of the body is
[00:30:15] SPEAKER_B: just an external environment and they do what amoebas
[00:30:17] SPEAKER_B: do, they go where life is good, they reproduce as
[00:30:19] SPEAKER_B: much as they can, right?
[00:30:20] SPEAKER_B: So that cognitive lycone, that is the thing that
[00:30:23] SPEAKER_B: I'm talking about that scales.
[00:30:25] SPEAKER_B: And so when we're looking for life, I don't think
[00:30:28] SPEAKER_B: we're looking for specific materials.
[00:30:30] SPEAKER_B: I don't think we're looking for specific metabolic
[00:30:32] SPEAKER_B: states.
[00:30:33] SPEAKER_B: I think we're looking for scales of cognitive
[00:30:35] SPEAKER_B: lycone.
[00:30:35] SPEAKER_B: We're looking for alignment of parts towards bigger
[00:30:39] SPEAKER_B: goals in spaces that the parts could not comprehend.
[00:30:42] And so cognitive lycone, just to make clear, is
[00:30:47] SPEAKER_A: about goals that you can actively pursue now.
[00:30:51] SPEAKER_A: You said linear, like within reach immediately.
[00:30:54] SPEAKER_A: No, I didn't.
[00:30:54] SPEAKER_A: Sorry.
[00:30:55] SPEAKER_B: I didn't mean that.
[00:30:55] SPEAKER_B: First of all, the goal necessarily is often removed
[00:30:59] SPEAKER_B: in time.
[00:30:59] SPEAKER_B: So in other words, when you're pursuing a goal,
[00:31:02] SPEAKER_B: it means that you have a separation between current
[00:31:04] SPEAKER_B: state and target state at minimum.
[00:31:06] SPEAKER_B: Your thermostat, right?
[00:31:07] SPEAKER_B: Let's just think about that.
[00:31:08] SPEAKER_B: There is a separation in time because the thing
[00:31:11] SPEAKER_B: you're trying to make happen so that the temperature
[00:31:13] SPEAKER_B: goes to a certain level is not true right now.
[00:31:15] SPEAKER_B: And all your actions are going to be around reducing
[00:31:17] SPEAKER_B: that error, right?
[00:31:17] SPEAKER_B: That basic homeostatic loop is all about closing
[00:31:20] SPEAKER_B: that gap.
[00:31:21] SPEAKER_B: When I said linear range, this is what I meant.
[00:31:23] SPEAKER_B: If I say to you, this terrible thing happened
[00:31:27] SPEAKER_B: to, you know, 10 people and, you know, you have
[00:31:31] SPEAKER_B: some degree of activation about it.
[00:31:33] SPEAKER_B: And then they say, no, no, actually it was 100, you
[00:31:36] SPEAKER_B: know, 10,000 people.
[00:31:37] SPEAKER_B: You're not a thousand times more activated about
[00:31:40] SPEAKER_B: it. You're somewhat more activated, but it's not
[00:31:42] SPEAKER_B: a thousand. And if I say, oh my God, it was actually
[00:31:44] SPEAKER_B: 10 million people.
[00:31:45] SPEAKER_B: You're not a million times more activated.
[00:31:47] SPEAKER_B: You don't have that capacity in the linear range.
[00:31:49] SPEAKER_B: You're sort of, right?
[00:31:50] SPEAKER_B: If you think about that curve, we sort of, we reach
[00:31:52] SPEAKER_B: a saturation point.
[00:31:54] SPEAKER_B: I have some amazing colleagues in the Buddhist
[00:31:55] SPEAKER_B: community with whom we've written some papers about
[00:31:57] SPEAKER_B: this. The radius of compassion is like, can you
[00:32:00] SPEAKER_B: grow your cognitive system to the point that, yeah,
[00:32:03] SPEAKER_B: it really isn't just your family group.
[00:32:05] SPEAKER_B: It really isn't just a hundred people, you know,
[00:32:07] SPEAKER_B: in your, in your, you know, circle.
[00:32:09] SPEAKER_B: Can you grow your cognitive light cone to the
[00:32:12] SPEAKER_B: point where, no, no, we care about the whole, whether
[00:32:14] SPEAKER_B: it's all of humanity or the whole ecosystem or the
[00:32:16] SPEAKER_B: whole whatever.
[00:32:17] SPEAKER_B: Can you actually care about that the exact same
[00:32:20] SPEAKER_B: way that we now care about a much smaller set of
[00:32:23] SPEAKER_B: people? That's what I mean by linear range.
[00:32:25] SPEAKER_B: But you said separated by time, like a thermostat,
[00:32:28] SPEAKER_A: but a bacteria.
[00:32:30] SPEAKER_A: I mean, if you zoom out far enough, a bacteria could
[00:32:34] SPEAKER_A: be formulated to have a goal state of creating human
[00:32:37] SPEAKER_A: civilization. Because if you look at the, you know,
[00:32:40] SPEAKER_A: bacteria has a role to play in the whole history
[00:32:45] SPEAKER_A: of earth. And so if you anthropomorphize the goals
[00:32:50] SPEAKER_A: of a bacteria enough, I mean, it has a concrete
[00:32:55] SPEAKER_A: role to play in the history of the evolution of
[00:32:58] SPEAKER_A: human civilization.
[00:33:00] SPEAKER_A: So you do need to, when you define a cognitive
[00:33:03] SPEAKER_A: light cone, you're looking at directly short-term
[00:33:07] SPEAKER_A: behavior.
[00:33:08] Well, no, how do you know what the cognitive
[00:33:11] SPEAKER_B: light cone of something is? Because as you've
[00:33:12] SPEAKER_B: said, it could be, it could be almost anything.
[00:33:15] SPEAKER_B: The key is you have to do experiments. And the way
[00:33:17] SPEAKER_B: you do experiments is you put barrier, you have
[00:33:19] SPEAKER_B: to do interventional experiments. You have to put
[00:33:21] SPEAKER_B: barriers between it and its goal, and you have to
[00:33:23] SPEAKER_B: ask what happens. And intelligence is the degree
[00:33:26] SPEAKER_B: of ingenuity that it has in overcoming barriers
[00:33:30] SPEAKER_B: between it and its goal. Now, if it were to be
[00:33:32] SPEAKER_B: that, now this is the, this is I think totally
[00:33:36] SPEAKER_B: doable, but impractical and very expensive
[00:33:38] SPEAKER_B: experiment. But you could imagine setting up a
[00:33:41] SPEAKER_B: scenario where the bacteria were blocked from
[00:33:44] SPEAKER_B: becoming more complex and you can ask if they
[00:33:47] SPEAKER_B: would try to find ways around it or whether
[00:33:49] SPEAKER_B: it's actually, nah, their goals are actually
[00:33:51] SPEAKER_B: metabolic. And as long as those goals are met,
[00:33:53] SPEAKER_B: they're not going to actually get around your
[00:33:54] SPEAKER_B: barrier. This business of putting barriers
[00:33:57] SPEAKER_B: between things and their goals is actually
[00:33:59] SPEAKER_B: extremely powerful because we've deployed it
[00:34:03] SPEAKER_B: in all kinds of, and I'm sure we'll get to
[00:34:05] SPEAKER_B: this later, but we've deployed it in all kinds
[00:34:07] SPEAKER_B: of weird systems that you wouldn't think are
[00:34:09] SPEAKER_B: goal-driven systems. And what it allows us to
[00:34:11] SPEAKER_B: do is to get beyond just the, the, the, what
[00:34:15] SPEAKER_B: you call the anthropomorphizing claims of
[00:34:17] SPEAKER_B: say, you know, saying, oh yeah, I think, you
[00:34:18] SPEAKER_B: know, I think this is thing is trying to do
[00:34:20] SPEAKER_B: this or that. The question is, well, let's do
[00:34:21] SPEAKER_B: the experiment. And one other thing I want to
[00:34:24] SPEAKER_B: say about anthropomorphizing is people, people
[00:34:26] SPEAKER_B: say this to me all the time. Um, I, I don't
[00:34:29] SPEAKER_B: think that exists. I think that's kind of like,
[00:34:32] SPEAKER_B: you know, uh, and I'll, I'll tell you why I
[00:34:35] SPEAKER_B: think it's like heresy or like, uh, other, other
[00:34:39] SPEAKER_B: terms that aren't really a thing because if
[00:34:42] SPEAKER_B: you, if you unpack it, here's, here's what
[00:34:44] SPEAKER_B: anthropomorphism means. Humans have a certain
[00:34:46] SPEAKER_B: magic and you're making a category error by
[00:34:49] SPEAKER_B: attributing that magic somewhere else. My point
[00:34:51] SPEAKER_B: is we have the same magic that everything has.
[00:34:54] SPEAKER_B: We have a couple of interesting things besides
[00:34:56] SPEAKER_B: the cognitive icon and some other stuff. And
[00:34:58] SPEAKER_B: it isn't that you have to keep the humans
[00:35:00] SPEAKER_B: separate because there's some bright line. It's
[00:35:03] SPEAKER_B: just, it's, it's that same old, uh, all, all I'm,
[00:35:07] SPEAKER_B: all I'm arguing for is the scientific method
[00:35:09] SPEAKER_B: really, that's really all this is. All I'm saying
[00:35:11] SPEAKER_B: is you can't just make pronouncements such as
[00:35:14] SPEAKER_B: the humans are this and let's not sort of push
[00:35:18] SPEAKER_B: that. You have to do experiments after you've
[00:35:20] SPEAKER_B: done your experiments. You can say either I've
[00:35:23] SPEAKER_B: done it and I found, look at that, that thing
[00:35:24] SPEAKER_B: actually can predict the future for the next,
[00:35:26] SPEAKER_B: you know, 12 minutes. Amazing. Or you say, you
[00:35:28] SPEAKER_B: know what, I've tried all the things in the
[00:35:30] SPEAKER_B: behaviorist handbook. They just don't help me
[00:35:32] SPEAKER_B: with this. It's a very low level of like, that's
[00:35:34] SPEAKER_B: it. It's a, it's a very low level of intelligence.
[00:35:36] SPEAKER_B: Fine. Right. Done. So that's really all I'm
[00:35:38] SPEAKER_B: arguing for is an empirical approach and then
[00:35:40] SPEAKER_B: things like anthropomorphism go away. It's just
[00:35:42] SPEAKER_B: a matter of have you done the experiment and
[00:35:44] SPEAKER_B: what did you find? And that's actually one of
[00:35:46] SPEAKER_A: the things you're saying that, uh, if you remove
[00:35:49] SPEAKER_A: the categorization of things, you can use the
[00:35:52] SPEAKER_A: tools of one discipline on everything. You can
[00:35:56] SPEAKER_A: try to try and then see that's the underpinnings
[00:36:00] SPEAKER_A: of the criticism anthropomorphization because,
[00:36:04] SPEAKER_A: uh, what is that? That's like psychoanalysis of
[00:36:07] SPEAKER_A: another human could technically be applied to
[00:36:11] SPEAKER_A: robots, to AI systems, to more primitive
[00:36:15] SPEAKER_A: biological systems and so on. Try. Yeah. We've
[00:36:18] used everything from basic habituation conditioning
[00:36:23] SPEAKER_B: all the way through anxiolytics, hallucinogens,
[00:36:26] SPEAKER_B: all kinds of cognitive modification on the range
[00:36:29] SPEAKER_B: of things that you wouldn't believe. And by the
[00:36:31] SPEAKER_B: way, I'm not the first person to come up with
[00:36:33] SPEAKER_B: this. So there was a guy named Bose well over
[00:36:35] SPEAKER_B: 100 years ago who was studying how anesthesia
[00:36:39] SPEAKER_B: affected animals and animal cells and drawing
[00:36:41] SPEAKER_B: specific curves around electrical excitability.
[00:36:44] SPEAKER_B: And he then went and did it with plants and
[00:36:48] SPEAKER_B: saw some very similar phenomena and being the
[00:36:50] SPEAKER_B: genius that he was, he then said, well, how do
[00:36:52] SPEAKER_B: I don't know when to stop, but there's no,
[00:36:54] SPEAKER_B: there's no, you know, everybody thinks we should
[00:36:56] SPEAKER_B: have stopped long before plants because people
[00:36:58] SPEAKER_B: made fun of him for that. And he's like, yeah,
[00:36:59] SPEAKER_B: but, but the science doesn't tell us where to
[00:37:01] SPEAKER_B: stop. The tool is working. Let's keep going. And
[00:37:04] SPEAKER_B: he showed interesting phenomena on materials,
[00:37:06] SPEAKER_B: metals and, and, and other kinds of materials.
[00:37:08] SPEAKER_B: Right. And so, uh, the interesting thing is that
[00:37:12] SPEAKER_B: yeah, there is no, there is no, uh, you know,
[00:37:15] SPEAKER_B: generic rule that tells you when, uh, when do
[00:37:18] SPEAKER_B: you need to stop? We make those up. Those are
[00:37:19] SPEAKER_B: completely made up. You have to just, uh, you
[00:37:21] SPEAKER_B: have to do the science and find out.
[00:37:23] SPEAKER_B: Yeah. You, uh, we'll probably get to it. Uh,
[00:37:26] SPEAKER_A: you've been doing recent work on looking at
[00:37:28] SPEAKER_A: computational systems, even trivial ones like
[00:37:30] SPEAKER_A: algorithms and sorting algorithms and analyzing
[00:37:33] SPEAKER_A: the behavioral kind of way. See if there's minds
[00:37:35] SPEAKER_A: inside those sorting algorithms. And of course,
[00:37:39] SPEAKER_A: they make a pothead statement question here that
[00:37:43] SPEAKER_A: you can start to do things like, uh, trying to
[00:37:47] SPEAKER_A: do psychedelics with a sorting. Yeah. And what
[00:37:50] SPEAKER_A: does that even look like? It looks like a
[00:37:52] SPEAKER_A: ridiculous question. It'll get you fired from
[00:37:54] SPEAKER_A: most academic departments, but it may be, if
[00:37:56] SPEAKER_A: you take it seriously, you can try and see if
[00:37:59] SPEAKER_A: it applies. Yeah. If it has, if a thing could
[00:38:02] SPEAKER_A: be shown to have some kind of cognitive
[00:38:05] SPEAKER_A: complexity, some kind of mind, why not apply to
[00:38:09] SPEAKER_A: it the same kind of analysis and the same kind
[00:38:12] SPEAKER_A: of tools like psychedelics that you would to a
[00:38:15] SPEAKER_A: human mind? That's a complex human mind. At
[00:38:19] SPEAKER_A: least might be a productive question to ask
[00:38:21] SPEAKER_A: what, because you've seen like spiders on
[00:38:23] SPEAKER_A: psychedelics, like more primitive biological
[00:38:25] SPEAKER_A: organisms on psychedelics. Why not try to see
[00:38:28] SPEAKER_A: what an algorithm does on psychedelics? Well,
[00:38:33] yeah, because you see, the thing to remember
[00:38:35] SPEAKER_B: is we don't have a magic sense or really good
[00:38:39] SPEAKER_B: intuition for what the mapping is between the
[00:38:43] SPEAKER_B: embodiment of something and the degree of
[00:38:45] SPEAKER_B: intelligence it has. We think we do because we
[00:38:47] SPEAKER_B: have an N of one example on earth and we kind
[00:38:49] SPEAKER_B: of know what to expect from cells, snakes, you
[00:38:52] SPEAKER_B: know, primates. But we really don't, we don't
[00:38:56] SPEAKER_B: have, and this is what we'll get into more of
[00:38:57] SPEAKER_B: the stuff on the platonic space, but our
[00:39:00] SPEAKER_B: intuitions around that stuff is so bad that to
[00:39:03] SPEAKER_B: really think that we know enough not to try
[00:39:05] SPEAKER_B: things at this point is, I think, really
[00:39:07] SPEAKER_B: short-sighted. Before we talk about the platonic
[00:39:10] SPEAKER_A: space, let's lay out some foundations. I
[00:39:14] SPEAKER_A: think one useful one comes from the paper
[00:39:16] SPEAKER_A: technological approach to mind everywhere. An
[00:39:20] SPEAKER_A: experimentally grounded framework for
[00:39:22] SPEAKER_A: understanding diverse bodies and minds. Could
[00:39:26] SPEAKER_A: you tell me about this framework and maybe can
[00:39:28] SPEAKER_A: you tell me about figure one from this paper
[00:39:32] SPEAKER_A: that has a few components. One is the tiers of
[00:39:34] SPEAKER_A: biological cognition that goes from group to
[00:39:37] SPEAKER_A: whole organism to whole tissue organ, down to
[00:39:40] SPEAKER_A: neural network, down to cytoskeleton, down to
[00:39:43] SPEAKER_A: genetic network, and then there's layers of
[00:39:46] SPEAKER_A: biological systems from ecosystem down to
[00:39:50] SPEAKER_A: swarm, down to organism, tissue, and finally
[00:39:55] SPEAKER_A: cell. So can you explain this figure and can
[00:39:58] SPEAKER_A: you explain this?
[00:40:00] SPEAKER_A: tame so-called framework.
[00:40:02] SPEAKER_A: So this is the version 1.0, and there's a kind of update at 2.0 that I'm writing at
[00:40:07] SPEAKER_B: the moment, trying to formalize in a careful way all the things that we've been talking
[00:40:14] SPEAKER_B: about here, and in particular, this notion of having to do experiments to figure out
[00:40:19] SPEAKER_B: where any given system is on a continuum. And we can, let's just start with figure two
[00:40:24] SPEAKER_B: maybe for a second, and then we'll come back to figure one. And first, just to unpack the
[00:40:28] SPEAKER_B: acronym. I like the idea that it spells out tame because the central focus of this is
[00:40:33] SPEAKER_B: interactions and how do you interact with a system to have a productive interaction
[00:40:38] SPEAKER_B: with it. And the idea is that cognitive claims are really protocol claims. When you tell
[00:40:43] SPEAKER_B: me that something has some degree of intelligence, what you're really saying is, this is the
[00:40:47] SPEAKER_B: set of tools I'm going to deploy, and we can all find out how that worked out for you.
[00:40:51] SPEAKER_B: And so technological, because I wanted to be clear with my colleagues that this was
[00:40:56] SPEAKER_B: not a project in just philosophy. This had very specific empirical implications that
[00:41:02] SPEAKER_B: are going to play out in engineering and regenerative medicine and so on. Technological approach
[00:41:06] SPEAKER_B: to mind everywhere, this idea that we don't know yet where different kinds of minds are
[00:41:11] SPEAKER_B: to be found, and we have to empirically figure that out. And so what you see here in figure
[00:41:16] SPEAKER_B: two is basically this idea that there is a spectrum, and I'm just showing four waypoints
[00:41:20] SPEAKER_B: along that spectrum. And as you move to the right of that spectrum, a couple of things
[00:41:24] SPEAKER_B: happen. Persuadability goes up, meaning that the systems become more reprogrammable, more
[00:41:28] SPEAKER_B: plastic, more able to do different things than whatever they're standardly doing. So
[00:41:33] SPEAKER_B: you have more ability to get them to do new and interesting things. The effort needed
[00:41:37] SPEAKER_B: to exert influence goes down. That is, autonomy goes up. And to the extent that you are good
[00:41:42] SPEAKER_B: at convincing or motivating the system to do things, you don't have to sweat the details
[00:41:46] SPEAKER_B: as much, right? And this also has to do with what I call engineering agential materials.
[00:41:51] SPEAKER_B: So when you engineer wood, metal, plastic, things like that, you are responsible for
[00:41:55] SPEAKER_B: absolutely everything because the material is not going to do anything other than hopefully
[00:41:58] SPEAKER_B: hold its shape. If you're engineering active matter or you're engineering computational
[00:42:04] SPEAKER_B: materials, or better yet, agential materials like living matter, you can do some very high
[00:42:10] SPEAKER_B: level prompting and let the system then do very complicated things that you don't need
[00:42:16] SPEAKER_B: to micromanage. And we all know that that increases when you're starting to work with
[00:42:20] SPEAKER_B: intelligent systems like animals and humans and so on. And the other thing that goes down
[00:42:25] SPEAKER_B: as you get to the right is the amount of mechanism or physics that you need to exert the
[00:42:30] SPEAKER_B: influence goes down. So if you know how your thermostat is to be set as far as its set
[00:42:35] SPEAKER_B: point, you really don't need to know much of anything else, right? You just need to
[00:42:38] SPEAKER_B: know that it is a homeostatic system and that this is how I change the set point. You don't
[00:42:42] SPEAKER_B: need to know how the cooling and heating plant works in order to get it to do complex
[00:42:45] SPEAKER_B: things.
[00:42:46] SPEAKER_B: By the way, a quick pause just for people who are listening. Let me describe what's in
[00:42:49] SPEAKER_A: the figure. So there's four different systems going up the scale of persuadability. So the
[00:42:55] SPEAKER_A: first system is a mechanical clock, then it's a thermostat, then it's a dog that gets rewards
[00:43:00] SPEAKER_A: and punishments, Pavlov's dog. And then finally, a bunch of very smart looking humans
[00:43:05] SPEAKER_A: communicate with each other and arguing, persuading each other using hashtag reasons. And then
[00:43:13] SPEAKER_A: there's arrows below that showing persuadability going up as you go up these systems from the
[00:43:19] SPEAKER_A: mechanical clock to a bunch of Greeks arguing and then going down as the effort needed to
[00:43:24] SPEAKER_A: exert influence. And once again, going down as mechanism knowledge needed to exert that
[00:43:29] SPEAKER_A: influence.
[00:43:30] SPEAKER_A: Yeah. I'll give you an example about that panel C here with the dog. Isn't it amazing
[00:43:35] SPEAKER_B: that humans have been training dogs and horses for thousands of years knowing zero
[00:43:40] SPEAKER_B: neuroscience? Also amazing is that when I'm talking to you right now, I don't need to
[00:43:45] SPEAKER_B: worry about manipulating all of the synaptic proteins in your brain to make you understand
[00:43:49] SPEAKER_B: what I'm saying and hopefully remember it. You're going to do that all on your own. I'm
[00:43:52] SPEAKER_B: giving you very thin in terms of information content, very thin prompt. And I'm counting
[00:43:57] SPEAKER_B: on you as a, as a multi-scale agential material to take care of the chemistry underneath.
[00:44:02] SPEAKER_B: So you don't need a wrench to convince me.
[00:44:05] SPEAKER_A: Correct. I don't need, and I don't need physics to convince you. And I don't need to know
[00:44:08] SPEAKER_B: how you work. Like I don't need to understand all of the steps. What I do need to have is
[00:44:13] SPEAKER_B: trust that you are a multi-scale cognitive system that already does that for yourself.
[00:44:17] SPEAKER_B: And you do like, this is an amazing thing. I don't, people don't think about this enough. I
[00:44:20] SPEAKER_B: think, uh, when you wake up in the morning and you have social goals, research goals,
[00:44:26] SPEAKER_B: financial goals, whatever, whatever it is that you have in order for you to act on those
[00:44:29] SPEAKER_B: goals, sodium and calcium and other ions have to cross your muscle membranes. Those
[00:44:34] SPEAKER_B: incredibly abstract goal states ultimately have to make the chemistry dance in a very
[00:44:39] SPEAKER_B: particular way, right? Our entire body is, is, is a transducer of, of very abstract things.
[00:44:46] SPEAKER_B: And, and by the way, not just our brains, but other, you know, our organs have, um, uh,
[00:44:52] SPEAKER_B: anatomical goals and other things that we can talk about because all of this, uh, plays
[00:44:55] SPEAKER_B: out in, uh, in, in, in regeneration and development and so on, but that the scaling,
[00:45:00] SPEAKER_B: right, of all of these things, the way that the way you regulate yourself is not by, oh
[00:45:04] SPEAKER_B: my God, you don't have to sit there and think, wow, I really have to push some, some,
[00:45:07] SPEAKER_B: you know, some sodiums across this membrane.
[00:45:10] SPEAKER_B: All of that happens automatically. And that's the, that's the incredible benefit of these
[00:45:14] SPEAKER_B: multiscale materials. So what I was trying to do in this paper is a couple of things.
[00:45:18] SPEAKER_B: All of these were, by the way, drawn by Jeremy Gay, who's this amazing graphic artist
[00:45:22] SPEAKER_B: that works with me. First of all, in panel a, which is the spiral I was trying to point
[00:45:26] SPEAKER_B: out is that at every level of biological organization, like we all know we're sort of
[00:45:30] SPEAKER_B: nested dolls of, uh, you know, organs and tissues and cells and molecules and whatever.
[00:45:35] SPEAKER_B: But what I was trying to point out is that this is not just structural. Every one of
[00:45:38] SPEAKER_B: those layers is competent and is doing problem solving in different spaces and spaces
[00:45:44] SPEAKER_B: that are very hard for us to imagine.
[00:45:45] SPEAKER_B: We humans are, because of our own evolutionary history, we are so obsessed with movement
[00:45:49] SPEAKER_B: in three dimensional space that even, even in AI, you see this all the time.
[00:45:53] SPEAKER_B: They say, well, this thing doesn't have a robotic body.
[00:45:55] SPEAKER_B: It's not embodied.
[00:45:57] SPEAKER_B: Yeah.
[00:45:57] SPEAKER_B: It's not embodied by moving around in 3d space, but biology has embodiments in all
[00:46:01] SPEAKER_B: kinds of spaces that are hard for us to imagine, right?
[00:46:04] SPEAKER_B: So your cells and tissues are moving in high dimensional physiological state spaces and
[00:46:08] SPEAKER_B: in, uh, gene expression, state spaces in anatomical state spaces.
[00:46:14] SPEAKER_B: They're doing that perception, decision-making action loop that we do in 3d space.
[00:46:19] SPEAKER_B: When we think about robots wandering around your kitchen, they're doing those loops in
[00:46:23] SPEAKER_B: these other spaces.
[00:46:24] SPEAKER_B: And so the first thing I was trying to point out is that, yeah, every layer of your body
[00:46:27] SPEAKER_B: has its own ability to solve problems in those spaces.
[00:46:31] SPEAKER_B: And then, um, on the right, what I was saying is that this distinction between, you know,
[00:46:35] SPEAKER_B: people say, well, there are living beings and then there are engineered machines.
[00:46:39] SPEAKER_B: And then they often follow up with all the things machines are never going to be able
[00:46:41] SPEAKER_B: to do and whatever.
[00:46:42] SPEAKER_B: And so what I was trying to point out here is that it is very difficult to maintain those
[00:46:47] SPEAKER_B: kinds of distinctions because life is incredibly interoperable.
[00:46:50] SPEAKER_B: Uh, life doesn't really care if, if, um, the thing it's working with was evolved through
[00:46:57] SPEAKER_B: a random trial and error, or was engineered with a higher degree of agency because at
[00:47:01] SPEAKER_B: every level within the cell, within the tissue, within the organism, within the
[00:47:05] SPEAKER_B: collective, you, you can replace and substitute engineered systems with the
[00:47:10] SPEAKER_B: naturally evolved systems.
[00:47:12] SPEAKER_B: And that question of, is it really, you know, is it biology or is it technology?
[00:47:16] SPEAKER_B: I don't think it's a useful question anymore.
[00:47:18] SPEAKER_B: So I was trying to warm people up with this idea that what we're going to do now is
[00:47:21] SPEAKER_B: talk about minds in general, regardless of their history or their composition.
[00:47:26] SPEAKER_B: It doesn't matter what you're made of.
[00:47:27] SPEAKER_B: It doesn't matter how you got here.
[00:47:28] SPEAKER_B: Let's talk about what you're able to do and what your inner world looks like.
[00:47:32] SPEAKER_B: That was the goal of that.
[00:47:34] Is it useful to, as a thought experiment, as an experiment of radical empathy to try
[00:47:40] SPEAKER_A: to put ourselves in the space of the different minds at each stage of the spiral, like
[00:47:47] SPEAKER_A: what state space is human civilization as a collective embodied?
[00:47:54] SPEAKER_A: What does it operate in?
[00:47:56] SPEAKER_A: So humans, individual organisms operate in 3D space.
[00:48:01] SPEAKER_A: That's what we understand.
[00:48:03] SPEAKER_A: But when there's a bunch of us together, what are we doing together?
[00:48:07] SPEAKER_B: It's really hard and you have to do experiments, which at larger
[00:48:10] SPEAKER_B: scales are really difficult.
[00:48:12] SPEAKER_B: But there is such a thing.
[00:48:13] SPEAKER_A: There may well be, we have to do experiments.
[00:48:16] SPEAKER_B: I don't know.
[00:48:17] SPEAKER_B: There's an example.
[00:48:18] SPEAKER_B: Somebody will say to me, well, you know, with your, with your kind of panpsychist
[00:48:21] SPEAKER_B: view, you might as you, you probably think the weather is, uh, is, is a gentle too.
[00:48:25] SPEAKER_B: It's like, well, I can't say that, but we don't know.
[00:48:28] SPEAKER_B: But have you ever tried to see if a hurricane has habituation or sensitization?
[00:48:32] SPEAKER_B: Maybe we haven't done the experiment.
[00:48:34] SPEAKER_B: It's hard, but you could, right.
[00:48:36] SPEAKER_B: And maybe, maybe weather systems can have certain kinds of memories.
[00:48:38] SPEAKER_B: I have no idea.
[00:48:39] SPEAKER_B: We have to do experiments.
[00:48:41] SPEAKER_B: So I don't know what the entire human society is doing, but, but I'll just give
[00:48:44] SPEAKER_B: you a simple example of, um, uh, the kinds of tools and we're, we're actively trying
[00:48:48] SPEAKER_B: to build tools now to enable radically different agents to communicate.
[00:48:52] SPEAKER_B: So, so we, we, we are doing this using, using AI and other, uh, other tools to
[00:48:57] SPEAKER_B: try and, uh, try and get this kind of communication going across very different
[00:49:01] SPEAKER_B: spaces.
[00:49:01] SPEAKER_B: I'll just give you a very kind of dumb example of, of, of how, how that might be.
[00:49:06] SPEAKER_B: Imagine that, um, you're playing tic-tac-toe against an alien.
[00:49:09] SPEAKER_B: So you're in a room, you don't see him.
[00:49:11] SPEAKER_B: Uh, so, so you, you, you draw the tic-tac-toe thing on the board, on the
[00:49:15] SPEAKER_B: floor and, uh, and you know what you're doing, you're trying to, uh, you're trying
[00:49:19] SPEAKER_B: to make straight lines with X's and O's and you're having a nice game.
[00:49:21] SPEAKER_B: It's obvious that he understands the process.
[00:49:23] SPEAKER_B: Like sometimes you win, sometimes you lose, like it's obvious in that, in that
[00:49:26] SPEAKER_B: one little, little segment of activity, you guys are sharing a world.
[00:49:31] SPEAKER_B: What, what's happening in the other room next door?
[00:49:33] SPEAKER_B: Well, let's say the alien doesn't know anything about geometry.
[00:49:37] SPEAKER_B: They don't understand straight lines.
[00:49:39] SPEAKER_B: What he's doing is he's got a, he's got a box and it's full of, uh, basically
[00:49:43] SPEAKER_B: billiard balls, each one of which has a number on it.
[00:49:45] SPEAKER_B: And all he's looking, he's doing is he's looking through the box to find
[00:49:48] SPEAKER_B: billiard balls, whose numbers add up to 15.
[00:49:52] SPEAKER_B: He doesn't understand geometry at all.
[00:49:53] SPEAKER_B: All he understands is arithmetic.
[00:49:55] SPEAKER_B: You don't think about arithmetic.
[00:49:57] SPEAKER_B: You think geometry.
[00:49:57] SPEAKER_B: The reason you guys are playing the same game is.
[00:50:00] SPEAKER_B: that there's this magic square, right?
[00:50:02] SPEAKER_B: That somebody could construct it,
[00:50:03] SPEAKER_B: that basically is a three by three square,
[00:50:06] SPEAKER_B: where if you pick the numbers right, they add up to 15.
[00:50:09] SPEAKER_B: He has no idea that there's a geometric interpretation
[00:50:11] SPEAKER_B: to this.
[00:50:12] SPEAKER_B: He is solving the problem that he sees,
[00:50:15] SPEAKER_B: which is totally algebra.
[00:50:16] SPEAKER_B: You don't know anything about that.
[00:50:17] SPEAKER_B: But if there is an appropriate interface
[00:50:19] SPEAKER_B: like this magic square,
[00:50:20] SPEAKER_B: you guys can share that experience.
[00:50:22] SPEAKER_B: You can have an experience.
[00:50:23] SPEAKER_B: It doesn't mean you start to think like him.
[00:50:25] SPEAKER_B: It means that you guys are able to interact
[00:50:27] SPEAKER_B: in a particular way.
[00:50:27] SPEAKER_B: Okay, so there's a mapping between the two different ways
[00:50:31] SPEAKER_A: of seeing the world that allows you to communicate
[00:50:34] SPEAKER_A: with each other.
[00:50:35] SPEAKER_B: Of seeing a thin slice of the world.
[00:50:36] SPEAKER_B: Thin slice of the world.
[00:50:37] SPEAKER_A: How do you find that mapping?
[00:50:39] SPEAKER_A: So you're saying we're trying to figure out ways
[00:50:42] SPEAKER_A: of finding that mapping for different kinds of systems.
[00:50:46] SPEAKER_A: So what's the process for doing that?
[00:50:48] SPEAKER_A: So the process is twofold.
[00:50:51] SPEAKER_B: One is to get a better understanding
[00:50:54] SPEAKER_B: of what the system, what space is the system navigating?
[00:50:58] SPEAKER_B: What goals does it have?
[00:50:59] SPEAKER_B: What level of ingenuity does it have to reach those goals?
[00:51:01] SPEAKER_B: For example, xenobots, right?
[00:51:03] SPEAKER_B: We make xenobots, this is, or anthrobots.
[00:51:05] SPEAKER_B: These are biological systems that have never existed
[00:51:07] SPEAKER_B: on earth before.
[00:51:08] SPEAKER_B: We have no idea what their cognitive properties are.
[00:51:12] SPEAKER_B: We're learning, we found some things.
[00:51:13] SPEAKER_B: But you can't predict that from first principles
[00:51:15] SPEAKER_B: because they're not at all what their past history
[00:51:18] SPEAKER_B: would inform you of.
[00:51:19] SPEAKER_B: Can you actually explain briefly what a xenobot is
[00:51:22] SPEAKER_A: and what an anthrobot is?
[00:51:24] So one of the things that we've been doing
[00:51:25] SPEAKER_B: is trying to create novel beings
[00:51:28] SPEAKER_B: that have never been here before.
[00:51:29] SPEAKER_B: The reason is that typically when you have
[00:51:32] SPEAKER_B: a biological system, an animal or a plant,
[00:51:35] SPEAKER_B: and you say, hey, why does it have certain forms
[00:51:38] SPEAKER_B: of behavior, certain forms of anatomy,
[00:51:40] SPEAKER_B: certain forms of physiology?
[00:51:42] SPEAKER_B: Why does it have those?
[00:51:43] SPEAKER_B: The answer is always the same.
[00:51:45] SPEAKER_B: Well, there's a history of evolutionary selection
[00:51:48] SPEAKER_B: and there's a long history going back of adaptation
[00:51:52] SPEAKER_B: and there are certain environments
[00:51:53] SPEAKER_B: and this is what survived and so that's why it has.
[00:51:55] SPEAKER_B: So what I wanted to do was break out of that mold
[00:51:59] SPEAKER_B: and to basically force us as a community
[00:52:04] SPEAKER_B: to dig deeper into where these things come from.
[00:52:07] SPEAKER_B: And that means taking away the crutch
[00:52:09] SPEAKER_B: where you just say, well, it's evolutionary selection.
[00:52:11] SPEAKER_B: That's why it looks like that.
[00:52:13] SPEAKER_B: So in order to do that,
[00:52:14] SPEAKER_B: we have to make artificial synthetic beings.
[00:52:16] SPEAKER_B: Now, to be clear, we are starting with living cells.
[00:52:19] SPEAKER_B: So it's not that they had no evolutionary history.
[00:52:22] SPEAKER_B: The cells do, they had evolutionary history
[00:52:23] SPEAKER_B: in frogs or humans or whatever,
[00:52:25] SPEAKER_B: but the creatures they make and the capabilities
[00:52:27] SPEAKER_B: that these creatures have were never directly selected for.
[00:52:30] SPEAKER_B: And in fact, they never existed.
[00:52:31] SPEAKER_B: So you can't tell the same kind of story.
[00:52:33] SPEAKER_B: And what I mean is we can take epithelial cells
[00:52:36] SPEAKER_B: off of an early frog embryo and we don't change the DNA,
[00:52:40] SPEAKER_B: no synthetic biology circuits, no material scaffolds,
[00:52:43] SPEAKER_B: no nanomaterials, no weird drugs, none of that.
[00:52:46] SPEAKER_B: What we're mostly doing is liberating them
[00:52:49] SPEAKER_B: from the instructive influences of the rest of the cells
[00:52:53] SPEAKER_B: that they were in in their bodies.
[00:52:54] SPEAKER_B: And so when you do that, right,
[00:52:56] SPEAKER_B: normally the cells are bullied by their neighboring cells
[00:52:59] SPEAKER_B: into having a very boring life.
[00:53:01] SPEAKER_B: They become a two-dimensional outer covering
[00:53:02] SPEAKER_B: for the embryo and they keep out the bacteria and that's that.
[00:53:06] SPEAKER_B: So you might ask, well, what are these cells capable of
[00:53:08] SPEAKER_B: when you take them away from that influence?
[00:53:10] SPEAKER_B: So when you do that,
[00:53:11] SPEAKER_B: they form another little life form we call a xenobot.
[00:53:15] SPEAKER_B: And it's this self-motile little thing
[00:53:17] SPEAKER_B: that has a cilia covering its surface.
[00:53:19] SPEAKER_B: The cilia are coordinated so they row against the water
[00:53:22] SPEAKER_B: and then the thing starts to move
[00:53:23] SPEAKER_B: and has all kinds of amazing properties.
[00:53:25] SPEAKER_B: It has different gene expression,
[00:53:27] SPEAKER_B: so it has its own novel transcriptome.
[00:53:29] SPEAKER_B: It's able to do things like kinematic self-replication,
[00:53:32] SPEAKER_B: meaning make copies of itself from loose cells
[00:53:35] SPEAKER_B: that you put in its environment.
[00:53:37] SPEAKER_B: It has the ability to respond to sound,
[00:53:39] SPEAKER_B: which normal embryos don't do.
[00:53:42] SPEAKER_B: It has these novel capacities and we did that
[00:53:44] SPEAKER_B: and we said, look, here are some amazing features
[00:53:46] SPEAKER_B: of this novel system.
[00:53:47] SPEAKER_B: Let's try to understand where they came from.
[00:53:49] SPEAKER_B: And some people said, well,
[00:53:51] SPEAKER_B: maybe it's a frog specific thing, you know?
[00:53:54] SPEAKER_B: Maybe this is just something unique to frog cells.
[00:53:57] SPEAKER_B: And so we said, okay,
[00:53:58] SPEAKER_B: what's the furthest you can get from frog embryonic cells?
[00:54:00] SPEAKER_B: How about human adult cells?
[00:54:02] SPEAKER_B: And so we took cells from adult human patients
[00:54:05] SPEAKER_B: who were donating tracheal epithelia
[00:54:07] SPEAKER_B: for biopsies and things like that.
[00:54:09] SPEAKER_B: And those cells in, again, no genetic change,
[00:54:12] SPEAKER_B: nothing like that.
[00:54:13] SPEAKER_B: They self-organize into something we call anthrobots.
[00:54:16] SPEAKER_B: Again, self-multi little creature,
[00:54:18] SPEAKER_B: 9,000 different gene expressions.
[00:54:21] SPEAKER_B: So about half the genome is now different
[00:54:23] SPEAKER_B: and they have interesting abilities.
[00:54:26] SPEAKER_B: For example, they can heal human neural wounds.
[00:54:29] SPEAKER_B: So in vitro, if you plate some neurons
[00:54:32] SPEAKER_B: and you put a big scratch through it,
[00:54:33] SPEAKER_B: so you damage them,
[00:54:34] SPEAKER_B: anthrobots can sit down and they will try,
[00:54:36] SPEAKER_B: they will spontaneously,
[00:54:37] SPEAKER_B: without us having to teach them to do it,
[00:54:39] SPEAKER_B: they will spontaneously try to knit the neurons across.
[00:54:42] SPEAKER_B: Well, what is this video that we're looking at here?
[00:54:43] SPEAKER_A: So this is an anthrobot.
[00:54:44] SPEAKER_B: So often when I give talks about this,
[00:54:46] SPEAKER_B: I show people this video and I say,
[00:54:48] SPEAKER_B: what do you think this is?
[00:54:49] SPEAKER_B: And people will say,
[00:54:50] SPEAKER_B: well, it looks like some primitive organism
[00:54:52] SPEAKER_B: you got from the bottom of a pond somewhere.
[00:54:54] SPEAKER_B: And I'll say,
[00:54:55] SPEAKER_B: well, what do you think the genome would look like?
[00:54:57] SPEAKER_B: And is it, well,
[00:54:58] SPEAKER_B: the genome would look like some primitive creature, right?
[00:55:00] SPEAKER_B: If you sequence that thing,
[00:55:01] SPEAKER_B: you'll get a hundred percent Homo sapiens.
[00:55:03] SPEAKER_B: And that doesn't look like any stage
[00:55:05] SPEAKER_B: of normal human development.
[00:55:06] SPEAKER_B: It doesn't act like any stage of human development.
[00:55:09] SPEAKER_B: It has the ability to move around.
[00:55:11] SPEAKER_B: It has, as I said,
[00:55:12] SPEAKER_B: over 9,000 differential gene expressions.
[00:55:15] SPEAKER_B: Also, interestingly,
[00:55:16] SPEAKER_B: it is younger than the cells that it comes from.
[00:55:20] SPEAKER_B: So it actually has the ability to roll back its age.
[00:55:22] SPEAKER_B: And we can talk about that
[00:55:24] SPEAKER_B: and what the implications of that are.
[00:55:25] SPEAKER_B: But to go back to your original question,
[00:55:28] SPEAKER_B: what we're doing with these kinds of systems-
[00:55:30] SPEAKER_A: Try and talk to it.
[00:55:31] SPEAKER_A: We're trying to talk to it.
[00:55:31] SPEAKER_A: That's exactly right.
[00:55:32] SPEAKER_B: And not just to this,
[00:55:33] SPEAKER_B: we're trying to talk to molecular networks.
[00:55:35] SPEAKER_B: So we found a couple of years ago,
[00:55:37] SPEAKER_B: we found that gene regulatory networks,
[00:55:38] SPEAKER_B: nevermind the cells,
[00:55:39] SPEAKER_B: but the molecular pathways inside of cells
[00:55:42] SPEAKER_B: can have several different kinds of learning,
[00:55:44] SPEAKER_B: including Pavlovian conditioning.
[00:55:45] SPEAKER_B: And what we're doing now is trying to talk to it.
[00:55:47] SPEAKER_B: The biomedical applications are obvious.
[00:55:49] SPEAKER_B: Instead of, hey Siri, you want, hey liver,
[00:55:52] SPEAKER_B: why do I feel like crap today?
[00:55:53] SPEAKER_B: And you want an answer.
[00:55:54] SPEAKER_B: Well, you know, your potassium levels are this and that.
[00:55:56] SPEAKER_B: And I don't feel, you know,
[00:55:58] SPEAKER_B: I don't feel good for these reasons.
[00:55:59] SPEAKER_B: And you should be able to talk to these things.
[00:56:01] SPEAKER_B: And there should be able to be an interface
[00:56:03] SPEAKER_B: that allows us to communicate, right?
[00:56:06] SPEAKER_B: And I think AI is going to be a huge component
[00:56:08] SPEAKER_B: of that interface,
[00:56:09] SPEAKER_B: of allowing us to talk to these systems.
[00:56:11] SPEAKER_B: It's a tool to combat our mind blindness,
[00:56:14] SPEAKER_B: to help us see diverse,
[00:56:15] SPEAKER_B: other very unconventional minds that are all around us.
[00:56:18] SPEAKER_B: Can you generalize that?
[00:56:20] SPEAKER_A: Let's say we meet an alien
[00:56:22] SPEAKER_A: or an unconventional mind
[00:56:27] SPEAKER_A: here on earth.
[00:56:29] SPEAKER_A: Think of it as a black box.
[00:56:32] SPEAKER_A: You show up,
[00:56:34] SPEAKER_A: what's the procedure for trying to get some hooks
[00:56:39] SPEAKER_A: into a communication protocol with the thing?
[00:56:43] SPEAKER_B: Yeah, that is exactly the mission of my lab.
[00:56:46] SPEAKER_B: It is to enable us to develop tools,
[00:56:48] SPEAKER_B: to recognize these things,
[00:56:50] SPEAKER_B: to learn to communicate with them,
[00:56:52] SPEAKER_B: to ethically relate to them.
[00:56:54] SPEAKER_B: And in general, to expand our ability to do this
[00:56:58] SPEAKER_B: in the world around us.
[00:57:00] SPEAKER_B: I specifically chose these kinds of things
[00:57:02] SPEAKER_B: because they're not as alien as proper aliens would be.
[00:57:06] SPEAKER_B: So we have some hope.
[00:57:07] SPEAKER_B: I mean, we're made of them.
[00:57:09] SPEAKER_B: We have many things in common.
[00:57:10] SPEAKER_B: There's some hope of understanding them.
[00:57:11] SPEAKER_B: You're talking about xenobots and antibodies.
[00:57:12] SPEAKER_A: Xenobots and antibodies and cells and everything else.
[00:57:14] SPEAKER_B: But they're alien in a couple of important ways.
[00:57:17] SPEAKER_B: One is the space they live in
[00:57:19] SPEAKER_B: is very hard for us to imagine.
[00:57:21] SPEAKER_B: What space do they live in?
[00:57:22] SPEAKER_B: Well, your body, your body cells,
[00:57:26] SPEAKER_B: long before we had a brain
[00:57:27] SPEAKER_B: that was good for navigating three-dimensional space,
[00:57:30] SPEAKER_B: was navigating the space of anatomical possibilities.
[00:57:33] SPEAKER_B: It was going from,
[00:57:34] SPEAKER_B: you start as an egg and you have to become a snake
[00:57:38] SPEAKER_B: or a giraffe or whatever, or a human, whatever,
[00:57:41] SPEAKER_B: whatever we're gonna be.
[00:57:42] SPEAKER_B: And I specifically am telling you that
[00:57:46] SPEAKER_B: this general idea when people model that
[00:57:49] SPEAKER_B: with kind of cellular automata type of ideas,
[00:57:52] SPEAKER_B: this open loop kind of thing where,
[00:57:54] SPEAKER_B: well, everything just follows local rules
[00:57:55] SPEAKER_B: and eventually there's complexity and here you go.
[00:57:58] SPEAKER_B: Now you've got a giraffe or a human.
[00:58:01] SPEAKER_B: I'm specifically telling you that that model
[00:58:02] SPEAKER_B: is totally insufficient to grasp what's actually going on.
[00:58:06] SPEAKER_B: What's actually going on,
[00:58:07] SPEAKER_B: and there've been many, many experiments on this,
[00:58:09] SPEAKER_B: is that the system is navigating a space.
[00:58:11] SPEAKER_B: It is navigating a space of anatomical possibilities.
[00:58:14] SPEAKER_B: If you try to block where it's going,
[00:58:15] SPEAKER_B: it will try to get around you.
[00:58:17] SPEAKER_B: If you try to challenge it
[00:58:18] SPEAKER_B: with things it's never seen before,
[00:58:19] SPEAKER_B: it will try to come up with a solution.
[00:58:21] SPEAKER_B: If you really defeat its ability to do that, which you can,
[00:58:26] SPEAKER_B: they're not infinitely intelligent, so you can defeat them,
[00:58:29] SPEAKER_B: you will either get birth defects
[00:58:30] SPEAKER_B: or you will get creative problem solving
[00:58:32] SPEAKER_B: such as what you're seeing here
[00:58:34] SPEAKER_B: with xenobots and anthropots.
[00:58:35] SPEAKER_B: If you can't be a human, you'll find another way to be in.
[00:58:39] SPEAKER_B: You can be an anthropot, for example,
[00:58:41] SPEAKER_B: or you'll be something else.
[00:58:42] SPEAKER_B: Just to clarify, what's the difference
[00:58:44] SPEAKER_A: between cellular automata type of action
[00:58:46] SPEAKER_A: where you're just responding to your local environment
[00:58:48] SPEAKER_A: and creating some kind of complex behavior
[00:58:51] SPEAKER_A: and operating in the space of anatomical possibilities?
[00:58:56] SPEAKER_A: Sure.
[00:58:57] SPEAKER_A: So there's a kind of goal, I guess, you're articulating.
[00:58:59] SPEAKER_A: There is some kind of thing.
[00:59:03] SPEAKER_A: There's a will to X something.
[00:59:06] SPEAKER_A: The will thing, let's put that aside.
[00:59:08] SPEAKER_B: Okay, sorry.
[00:59:09] SPEAKER_B: Well, it's fine.
[00:59:10] SPEAKER_B: There I go, anthropomaniac.
[00:59:11] SPEAKER_A: I just always love to quote Nietzsche, so.
[00:59:12] SPEAKER_A: Yeah, yeah, yeah.
[00:59:13] SPEAKER_B: And I'm not saying that's wrong.
[00:59:15] SPEAKER_B: I'm just saying I don't have data for that one,
[00:59:16] SPEAKER_B: but I'll tell you the stuff that I'm quite certain of.
[00:59:19] SPEAKER_B: There are a couple of different formalisms
[00:59:20] SPEAKER_B: that we have in control theory.
[00:59:23] SPEAKER_B: One of those formalisms is open loop complexity.
[00:59:27] SPEAKER_B: In other words, I've got a bunch of subunits
[00:59:29] SPEAKER_B: like a cellular automaton.
[00:59:31] SPEAKER_B: They follow certain rules and you turn the crank,
[00:59:35] SPEAKER_B: time goes forward, whatever happens, happens.
[00:59:37] SPEAKER_B: Now, clearly you can get complexity from this.
[00:59:40] SPEAKER_B: Clearly you can get some very interesting looking things,
[00:59:42] SPEAKER_B: right, so the game of life,
[00:59:43] SPEAKER_B: all those kinds of cool things, right?
[00:59:45] SPEAKER_B: You can get complexity, no problem.
[00:59:47] SPEAKER_B: But the idea that that model is going to be sufficient
[00:59:51] SPEAKER_B: to explain and control things like morphogenesis
[00:59:56] SPEAKER_B: is a hypothesis.
[00:59:57] SPEAKER_B: It's okay to make that hypothesis,
[00:59:59] SPEAKER_B: but it...
[01:00:00] SPEAKER_B: we know, we know it's false, despite the fact that that is what, what we learn, you
[01:00:04] SPEAKER_B: know, in, in, in basic, um, uh, uh, cell biology and developmental biology classes.
[01:00:09] SPEAKER_B: When the first time you see something like this inevitably, especially if you're an
[01:00:13] SPEAKER_B: engineer in those classes, you raise your hand and go, Hey, how does it know to do
[01:00:16] SPEAKER_B: that?
[01:00:16] SPEAKER_B: How does it know, uh, you know, four fingers instead of seven, what they tell
[01:00:20] SPEAKER_B: you is it doesn't know anything.
[01:00:21] SPEAKER_B: Make sure that that's, that's very clear.
[01:00:23] SPEAKER_B: They all insist that when we learn these things, they insist not none, nothing
[01:00:26] SPEAKER_B: here knows anything.
[01:00:27] SPEAKER_B: There are rules of chemistry.
[01:00:28] SPEAKER_B: They roll forward and this is what happens.
[01:00:29] SPEAKER_B: Okay.
[01:00:30] SPEAKER_B: Now that model is testable.
[01:00:32] SPEAKER_B: We can ask, does that model explain what happens?
[01:00:35] SPEAKER_B: Here's where that model falls down.
[01:00:36] SPEAKER_B: If you have that model and situations change, either, either there's damage or
[01:00:43] SPEAKER_B: something, something in the environment that that's happened, those kinds of open
[01:00:47] SPEAKER_B: loop models do not adjust to give you, uh, to give you the same goal by different
[01:00:54] SPEAKER_B: means, this is William James's definition of intelligence is same goal by different
[01:00:57] SPEAKER_B: means.
[01:00:58] SPEAKER_B: And in particular, working them backwards, let's say you're in regenerative
[01:01:02] SPEAKER_B: medicine and you say, okay, but this is the situation now I want it to be
[01:01:05] SPEAKER_B: different.
[01:01:06] SPEAKER_B: What should the rules be?
[01:01:07] SPEAKER_B: It's not reversible.
[01:01:08] SPEAKER_B: So the thing with those kinds of open loop models is they're not reversible.
[01:01:11] SPEAKER_B: You don't know what to do to make the outcome that you want.
[01:01:14] SPEAKER_B: All you know how to do is roll them forward.
[01:01:16] SPEAKER_B: Right now in biology, we see the following.
[01:01:19] SPEAKER_B: Uh, if, if you have a developmental system and you put barriers between, so I'm
[01:01:26] SPEAKER_B: going to give you two pieces of evidence that suggests that there is a goal.
[01:01:29] SPEAKER_B: One piece of evidence is that if you try to block these things from the outcome
[01:01:34] SPEAKER_B: that they normally have, they will do some amazing things.
[01:01:38] SPEAKER_B: Uh, sometimes very clever things, sometimes not at all the way that they
[01:01:41] SPEAKER_B: normally do it, right?
[01:01:43] SPEAKER_B: So this is William James's definition by different means, by following
[01:01:46] SPEAKER_B: different trajectories, they will go around various local maxima and minima
[01:01:50] SPEAKER_B: to get to where they need to go.
[01:01:51] SPEAKER_B: It is navigation of a space.
[01:01:53] SPEAKER_B: It is not blind, turn the crank and wherever we end up is where we end up.
[01:01:56] SPEAKER_B: That is not what we see experimentally.
[01:01:59] SPEAKER_B: And more importantly, I think what we've shown, and this is, this is, um, uh,
[01:02:03] SPEAKER_B: this is something that I'm particularly happy with in our lab for over the last
[01:02:06] SPEAKER_B: 20 years, we've shown the following.
[01:02:08] SPEAKER_B: We can actually rewrite the goal states because we found them.
[01:02:11] SPEAKER_B: We, we have shown through, uh, through our work on bioelectric imaging
[01:02:15] SPEAKER_B: and bioelectric reprogramming.
[01:02:17] SPEAKER_B: We have actually shown how those goal memories are encoded, at least in some
[01:02:21] SPEAKER_B: cases, we certainly haven't got them all, but we have some.
[01:02:24] SPEAKER_B: If you can find where the goal state is encoded, read it out and reset it.
[01:02:30] SPEAKER_B: And the system will now implement a new goal based on what you just reset.
[01:02:34] SPEAKER_B: That is the ultimate, uh, evidence that, that your goal, uh, directed model is
[01:02:39] SPEAKER_B: working because if there was no goal, that shouldn't be possible, you shouldn't be.
[01:02:43] SPEAKER_B: Right.
[01:02:43] SPEAKER_B: Once, once you can find it, read it, uh, interpret it and rewrite it, it means
[01:02:48] SPEAKER_B: that by, by any engineering standard, it means that you're dealing
[01:02:51] SPEAKER_B: with a homeostatic mechanism.
[01:02:53] SPEAKER_B: How do you find where the goal is encoded?
[01:02:55] SPEAKER_A: So through lots and lots of hard work, the barrier thing is part of
[01:02:59] SPEAKER_A: that, creating barriers and observing.
[01:03:01] SPEAKER_A: The barrier thing tells you that it, you should be looking for a goal.
[01:03:04] SPEAKER_B: So step one, when you approach the agentic system is create a barrier of
[01:03:07] SPEAKER_A: different kinds until you see how persistent it is at pursuing the thing
[01:03:12] SPEAKER_A: it seemed to have been pursuing originally.
[01:03:14] SPEAKER_A: And then, you know, okay, cool.
[01:03:16] SPEAKER_A: This is a, this thing has agency first of all.
[01:03:19] SPEAKER_A: And then the second of all, like you started to build an intuition
[01:03:22] SPEAKER_A: about exactly which goal is pursuing.
[01:03:24] SPEAKER_A: Yes.
[01:03:24] SPEAKER_A: The first couple of steps are all imagination.
[01:03:26] SPEAKER_B: You have to ask yourself, what space is this thing even working in?
[01:03:29] SPEAKER_B: And, and you really have to stretch your mind because, because we can't imagine
[01:03:33] SPEAKER_B: all the spaces that systems work in.
[01:03:34] SPEAKER_B: Right.
[01:03:35] SPEAKER_B: So, so step one is what space is it?
[01:03:37] SPEAKER_B: Step two, what do I think the goal is?
[01:03:39] SPEAKER_B: And let's not mistake step two, you're not done just because you
[01:03:41] SPEAKER_B: haven't made a hypothesis.
[01:03:42] SPEAKER_B: That doesn't mean you can say, well, there, there, I see it doing this.
[01:03:45] SPEAKER_B: Therefore that's the goal.
[01:03:45] SPEAKER_B: You don't know that you have to actually do experiments.
[01:03:48] SPEAKER_B: Now, once you've made those hypotheses, now you do the experiments.
[01:03:50] SPEAKER_B: You say, okay, if I want to block it from reaching its goal, how do I do that?
[01:03:53] SPEAKER_B: And this, by the way, is exactly the approach we took with the sorting
[01:03:56] SPEAKER_B: algorithms and with everything else.
[01:03:58] SPEAKER_B: You, you, you hypothesize the goal, you put a barrier in, and then you get to
[01:04:02] SPEAKER_B: find out what level of ingenuity it has.
[01:04:04] SPEAKER_B: Maybe what you see is, well, that derailed everything.
[01:04:07] SPEAKER_B: So probably this thing isn't very smart or you see, oh, wow, it can
[01:04:10] SPEAKER_B: go around and do these things.
[01:04:11] SPEAKER_B: Or you might say, wow, it's taking a completely different approach using
[01:04:15] SPEAKER_B: its affordances in novel ways.
[01:04:17] SPEAKER_B: Like that's a high level of intelligence.
[01:04:18] SPEAKER_B: You, you will find out what the, what the answer is.
[01:04:21] SPEAKER_A: Another part, I had a question.
[01:04:22] SPEAKER_A: Is it possible to look at, uh, speaking of unconventional organisms and
[01:04:28] SPEAKER_A: going to Richard Dawkins, for example, with memes, is it possible to think
[01:04:32] SPEAKER_A: of things like ideas, like how weird can we get, can we look at ideas as
[01:04:37] SPEAKER_A: organisms, then creating barriers for those ideas and seeing are the ideas
[01:04:41] SPEAKER_A: themselves as you take the actual individual ideas and trying to empathize
[01:04:48] SPEAKER_A: and visualize what kind of space they might be operating in, can they be
[01:04:54] SPEAKER_A: seen as organisms that have a mind?
[01:04:58] Yeah.
[01:04:58] Um, okay.
[01:04:59] SPEAKER_B: If you want to get really weird, we can, we can get, we can get really weird here.
[01:05:03] SPEAKER_B: Uh, think about the, uh, caterpillar butterfly transition.
[01:05:06] SPEAKER_B: Okay.
[01:05:07] SPEAKER_B: So you've got a caterpillar soft bodied kind of creature as a particular
[01:05:10] SPEAKER_B: controller that's suitable for running a soft body, you know, kind of robot.
[01:05:14] SPEAKER_B: It has a brain for that task and then it has to become this butterfly
[01:05:17] SPEAKER_B: hard-bodied creature flies around.
[01:05:19] SPEAKER_B: During the process of metamorphosis, its brain is basically ripped
[01:05:23] SPEAKER_B: up and rebuilt from scratch.
[01:05:26] SPEAKER_B: Right now, what's been found is that if you train the caterpillar, so you give
[01:05:30] SPEAKER_B: it a new memory, meaning that if you, if the caterpillar sees this color disc,
[01:05:33] SPEAKER_B: then it crawls over and eat some leaves.
[01:05:36] SPEAKER_B: Turns out the butterfly retains that memory.
[01:05:39] SPEAKER_B: Now, the obvious question is how the hell do you retain memories when the
[01:05:42] SPEAKER_B: medium is being refactored like that?
[01:05:44] SPEAKER_B: Let's put that aside.
[01:05:45] SPEAKER_B: That's I'm going to get somewhere even weirder than that.
[01:05:48] SPEAKER_B: There's something else that's even more interesting than that.
[01:05:50] SPEAKER_B: It's not just that you have to, uh, retain the memory.
[01:05:54] SPEAKER_B: You have to remap that memory onto a completely new context because guess
[01:05:58] SPEAKER_B: what, the butterfly doesn't move the way the caterpillar moves
[01:06:01] SPEAKER_B: and it doesn't care about leaves.
[01:06:02] SPEAKER_B: It wants nectar from, from flowers.
[01:06:04] SPEAKER_B: And so if you're going, if that memory is going to survive, it can't just persist.
[01:06:09] SPEAKER_B: It has to be remapped, be remapped into a novel context.
[01:06:13] SPEAKER_B: Now here's what I, now here's, here's where things get weird.
[01:06:15] SPEAKER_B: We can take a couple of different perspectives here.
[01:06:18] SPEAKER_B: We can take the perspective of the caterpillar facing some sort of crazy
[01:06:22] SPEAKER_B: singularity and say, my God, I'm going to, I'm going to cease to exist.
[01:06:25] SPEAKER_B: But, but you know, I'll sort of be reborn in this new higher
[01:06:28] SPEAKER_B: dimensional world where I'll fly.
[01:06:29] SPEAKER_B: Okay.
[01:06:29] SPEAKER_B: So that's one thing.
[01:06:31] SPEAKER_B: We can take the perspective of the butterfly and say that, well, here I am,
[01:06:36] SPEAKER_B: but you know, I seem to be saddled with some, some tendencies and some memories
[01:06:40] SPEAKER_B: and I don't know where the hell they came from and, and, and I don't
[01:06:43] SPEAKER_B: remember exactly how I got them.
[01:06:45] SPEAKER_B: And they seem to be a core part of my psychological makeup.
[01:06:48] SPEAKER_B: And, and, you know, they're, they're, they come from somewhere.
[01:06:50] SPEAKER_B: I don't know where they come from.
[01:06:51] SPEAKER_B: Right.
[01:06:51] SPEAKER_B: So you can take that perspective, but there's a third perspective that I
[01:06:54] SPEAKER_B: think is really interesting and useful.
[01:06:56] SPEAKER_B: And the third perspective is that of the memory itself.
[01:06:59] SPEAKER_B: If you take a perspective of the memory, which, which what's a, what is a memory?
[01:07:02] SPEAKER_B: It is a pattern.
[01:07:03] SPEAKER_B: It is an informational pattern that was continuously reinforced
[01:07:07] SPEAKER_B: within one cognitive system.
[01:07:09] SPEAKER_B: And now here I am, I'm this memory.
[01:07:12] SPEAKER_B: What do I need to do to persist into the future?
[01:07:16] SPEAKER_B: Well, now I'm facing the paradox of change.
[01:07:17] SPEAKER_B: If I, if I try to remain the same, I'm gone.
[01:07:20] SPEAKER_B: There's no way the butterfly is going to retain me in, in the
[01:07:22] SPEAKER_B: original form that I'm in now.
[01:07:24] SPEAKER_B: What I need to do is, is change, adapt and morph.
[01:07:28] SPEAKER_B: Now you might say, well, that's kind of crazy.
[01:07:30] SPEAKER_B: Uh, well, how are you taking the perspective of a, of a, of a pattern
[01:07:34] SPEAKER_B: within an excitable medium, right?
[01:07:35] SPEAKER_B: Agents are physical things.
[01:07:37] SPEAKER_B: You're talking about the, you're talking about information, right?
[01:07:40] SPEAKER_B: So, so let me, let me tell you another quick science fiction story.
[01:07:44] SPEAKER_B: Imagine that, uh, some creatures come out from the center of the earth.
[01:07:47] SPEAKER_B: They live down in the core.
[01:07:48] SPEAKER_B: They're super dense.
[01:07:50] SPEAKER_B: Okay.
[01:07:50] SPEAKER_B: They're incredibly dense because they live down in the core.
[01:07:52] SPEAKER_B: They have gamma ray vision for, you know, for, and so on.
[01:07:56] SPEAKER_B: So they come out to the surface.
[01:07:57] SPEAKER_B: What do they see?
[01:07:58] SPEAKER_B: Well, all of this stuff that we're seeing here, this is
[01:08:01] SPEAKER_B: like a thin plasma to them.
[01:08:03] SPEAKER_B: They, they are so dense.
[01:08:04] SPEAKER_B: None of this is, is, is, is solid to them.
[01:08:06] SPEAKER_B: They don't, they don't see any of this stuff.
[01:08:08] SPEAKER_B: So they're walking around, you know, they're, the planet is sort of, uh, you
[01:08:12] SPEAKER_B: know, covered in this like thin gas, you know, and one of them is a scientist
[01:08:15] SPEAKER_B: and he's, and he's taking measurements of the gas and he says to the others,
[01:08:17] SPEAKER_B: you know, I've been watching this gas and they're like little whirlpools in
[01:08:21] SPEAKER_B: this gas and they almost look like agents.
[01:08:23] SPEAKER_B: They almost look like they are doing things.
[01:08:24] SPEAKER_B: They, they're moving around.
[01:08:25] SPEAKER_B: They kind of hold themselves together for a little bit and they're
[01:08:28] SPEAKER_B: trying to make stuff happen.
[01:08:29] SPEAKER_B: And, and the, the others say, well, that's crazy patterns in the gas.
[01:08:34] SPEAKER_B: Can't be agents.
[01:08:34] SPEAKER_B: We are agents.
[01:08:35] SPEAKER_B: We're, we're solid.
[01:08:36] SPEAKER_B: This is just patterns in an excitable medium.
[01:08:38] SPEAKER_B: And by the way, how long do they hold together?
[01:08:40] SPEAKER_B: He says, well, about a hundred years.
[01:08:41] SPEAKER_B: That's crazy.
[01:08:42] SPEAKER_B: Nothing, you know, no, no real agent can, can exist to be, be that dissipate that
[01:08:45] SPEAKER_B: fast.
[01:08:46] SPEAKER_B: Okay.
[01:08:46] SPEAKER_B: We are all metabolic patterns among other things.
[01:08:49] SPEAKER_B: Right.
[01:08:49] SPEAKER_B: And so one of the things that, and so you see what I'm warming up to, to here.
[01:08:53] SPEAKER_B: So, so one of the things that we've been trying to dissolve, and this is like
[01:08:56] SPEAKER_B: some work that I've done with Chris Fields and others is this distinction
[01:08:59] SPEAKER_B: between thoughts and thinkers.
[01:09:01] SPEAKER_B: So, uh, all agents are patterns within some excitable medium.
[01:09:07] SPEAKER_B: We could talk about what, what that is and they can spawn off others.
[01:09:11] SPEAKER_B: And now you can have a really interesting spectrum.
[01:09:13] SPEAKER_B: Here's the, here's the spectrum.
[01:09:15] SPEAKER_B: Um, you can have fleeting thoughts, which are like waves in, in, in the
[01:09:21] SPEAKER_B: ocean, when you throw a rock in, you know, they sort of, they sort of go
[01:09:24] SPEAKER_B: through the excitable medium and then they're gone, they pass through and
[01:09:27] SPEAKER_B: they're gone, right?
[01:09:28] SPEAKER_B: So those are, those are kind of fleeting thoughts.
[01:09:30] SPEAKER_B: Then you can have patterns that have a degree of persistence.
[01:09:33] SPEAKER_B: So they might be hurricanes or solitons or persistent thoughts or
[01:09:40] SPEAKER_B: earworms or depressive thoughts.
[01:09:42] SPEAKER_B: Those are harder to get rid of.
[01:09:44] SPEAKER_B: They, they stick around for a little while.
[01:09:46] SPEAKER_B: They often do a little bit of niche construction.
[01:09:48] SPEAKER_B: So they change the actual brain to have, to make it easier to have
[01:09:51] SPEAKER_B: more of those thoughts, right?
[01:09:52] SPEAKER_B: Like that's a, that's a thing.
[01:09:54] SPEAKER_B: And so they, they, they stay around longer now.
[01:09:57] SPEAKER_B: Uh, what's, what's further.
[01:10:00] SPEAKER_B: than that, well, fragments, personality
[01:10:02] SPEAKER_B: fragments of a dissociative personality
[01:10:04] SPEAKER_B: disorder, they're more, more stable.
[01:10:07] SPEAKER_B: And they're not just on autopilot.
[01:10:09] SPEAKER_B: They have goals and they can do things.
[01:10:12] SPEAKER_B: And then past that as a full-blown human
[01:10:14] SPEAKER_B: personality and who the hell knows what's
[01:10:16] SPEAKER_B: past that, maybe some sort of transhuman,
[01:10:18] SPEAKER_B: you know, transpersonal, like, I don't
[01:10:20] SPEAKER_B: know, right.
[01:10:21] SPEAKER_B: But, but this idea, again, I'm back to
[01:10:22] SPEAKER_B: this notion of a spectrum.
[01:10:23] SPEAKER_B: It's, there is not a sharp distinction
[01:10:26] SPEAKER_B: between, you know, we are real agents and
[01:10:28] SPEAKER_B: then we have these, these, these thoughts.
[01:10:30] SPEAKER_B: Yeah.
[01:10:30] SPEAKER_B: Patterns can be agents too, but again,
[01:10:33] SPEAKER_B: you don't know until you do the experiment.
[01:10:35] SPEAKER_B: So if you want to know whether a
[01:10:36] SPEAKER_B: soliton or a hurricane or a thought
[01:10:38] SPEAKER_B: within a cognitive system is its own
[01:10:40] SPEAKER_B: agent, do the experiment, see what it can
[01:10:43] SPEAKER_B: do.
[01:10:43] SPEAKER_B: Does it, can it learn from experience?
[01:10:45] SPEAKER_B: Does it have memories?
[01:10:45] SPEAKER_B: Does it have goal states?
[01:10:46] SPEAKER_B: Does it, you know, what, what can it do?
[01:10:48] SPEAKER_B: Right.
[01:10:48] SPEAKER_B: Does it have language?
[01:10:49] SPEAKER_B: So, so, uh, coming back to then here in
[01:10:52] SPEAKER_B: the original question, yeah, we can
[01:10:54] SPEAKER_B: definitely apply this methodology to
[01:10:57] SPEAKER_B: ideas and concepts and, and, and social,
[01:10:59] SPEAKER_B: um, uh, you know, whatever's, but you've
[01:11:02] SPEAKER_B: got to do the experiment.
[01:11:04] That's such a challenging thought
[01:11:07] SPEAKER_A: experiment of like thinking about memories
[01:11:10] SPEAKER_A: from the caterpillar to the
[01:11:11] SPEAKER_A: butterflies and organism.
[01:11:13] SPEAKER_A: I think at the very basic level,
[01:11:15] SPEAKER_A: intuitively, we think of organisms as
[01:11:18] SPEAKER_A: hardware and, uh, software is not possibly
[01:11:23] SPEAKER_A: being able to be organisms, but what
[01:11:26] SPEAKER_A: you're saying is that it's all just
[01:11:28] SPEAKER_A: patterns in an excitable medium and it
[01:11:31] SPEAKER_A: doesn't really matter what the pattern
[01:11:34] SPEAKER_A: is, we need to, and what the
[01:11:37] SPEAKER_A: excitable medium is, we need to do the
[01:11:39] SPEAKER_A: testing of what, how persistent is it?
[01:11:43] SPEAKER_A: How goal oriented is it?
[01:11:45] SPEAKER_A: And there's certain kind of tests to do
[01:11:48] SPEAKER_A: that, and you can apply that to memories.
[01:11:50] SPEAKER_A: You can apply that to ideas.
[01:11:51] SPEAKER_A: You can apply that to anything really.
[01:11:54] SPEAKER_A: I mean, you could probably think about
[01:11:55] SPEAKER_A: like consciousness, you could, there's
[01:11:59] SPEAKER_A: really no, um, boundary to what you can
[01:12:02] SPEAKER_A: imagine, probably really, really wild
[01:12:05] SPEAKER_A: things could be, could be minds.
[01:12:08] SPEAKER_A: Yeah.
[01:12:08] SPEAKER_A: Stay tuned.
[01:12:09] SPEAKER_B: I mean, this is exactly what we're doing.
[01:12:11] SPEAKER_B: We're getting progressively like more
[01:12:13] SPEAKER_B: and more unconventional.
[01:12:14] SPEAKER_B: I mean, so, so this, so this whole
[01:12:15] SPEAKER_B: distinction between software and hardware,
[01:12:17] SPEAKER_B: I think, I think it's a super important,
[01:12:20] SPEAKER_B: uh, concept to think about.
[01:12:23] SPEAKER_B: And, and yet the way we've mapped it onto
[01:12:25] SPEAKER_B: the world, I, I, I w I would like to blow
[01:12:28] SPEAKER_B: that up in, in the, in the following way.
[01:12:30] SPEAKER_B: Um, and, and again, I want to point out
[01:12:32] SPEAKER_B: that, so, so I'll tell you what the, what
[01:12:33] SPEAKER_B: the practical, um, consequences are,
[01:12:35] SPEAKER_B: because this is not just, you know, fun
[01:12:38] SPEAKER_B: stories that we tell each other.
[01:12:39] SPEAKER_B: These have really important
[01:12:41] SPEAKER_B: research, um, implications.
[01:12:43] SPEAKER_B: Think about a Turing machine.
[01:12:44] SPEAKER_B: So one thing you can say is the
[01:12:46] SPEAKER_B: machine's the agent, it has passive data
[01:12:49] SPEAKER_B: and it operates on the data and that's it.
[01:12:51] SPEAKER_B: The story of agency is the story of
[01:12:53] SPEAKER_B: whatever that machine can and can't do.
[01:12:54] SPEAKER_B: The data is passive and it moves it around.
[01:12:56] SPEAKER_B: You can tell the opposite story.
[01:12:57] SPEAKER_B: You can say, look, the patterns
[01:12:59] SPEAKER_B: on the data are the agent.
[01:13:00] SPEAKER_B: The machine is a stigmergic scratch pad
[01:13:02] SPEAKER_B: in the world of the data doing what data does.
[01:13:05] SPEAKER_B: The machine is just the consequences,
[01:13:07] SPEAKER_B: the scratch pad of it, working itself out.
[01:13:09] SPEAKER_B: And both of those stories make sense
[01:13:10] SPEAKER_B: depending on what you're trying to do.
[01:13:12] SPEAKER_B: Here's the, um, the biomedical side of things.
[01:13:14] SPEAKER_B: So our bio, our, our program
[01:13:16] SPEAKER_B: in bioelectrics and aging.
[01:13:18] SPEAKER_B: Okay.
[01:13:19] SPEAKER_B: One model you could have is the physical
[01:13:22] SPEAKER_B: organism is the agent and the cellular
[01:13:26] SPEAKER_B: collective has pattern memories.
[01:13:30] SPEAKER_B: Specifically what I was saying
[01:13:31] SPEAKER_B: before goals, anatomical goals.
[01:13:33] SPEAKER_B: If you want to, if you want to
[01:13:34] SPEAKER_B: persist for a hundred plus years,
[01:13:36] SPEAKER_B: your cells better remember what
[01:13:38] SPEAKER_B: your correct shape is and
[01:13:39] SPEAKER_B: where the new cells go.
[01:13:40] SPEAKER_B: Right?
[01:13:40] SPEAKER_B: So there are these pattern memories
[01:13:42] SPEAKER_B: that exist during embryogenesis,
[01:13:43] SPEAKER_B: during regeneration, during
[01:13:44] SPEAKER_B: resistance to aging, we can see them.
[01:13:46] SPEAKER_B: We can visualize them.
[01:13:47] SPEAKER_B: One thing you can imagine is fine.
[01:13:49] SPEAKER_B: The physical body, the cells are the agent.
[01:13:52] SPEAKER_B: The electrical pattern
[01:13:53] SPEAKER_B: memories are just data.
[01:13:55] SPEAKER_B: And what might happen during aging
[01:13:57] SPEAKER_B: is that the data might get degraded.
[01:14:00] SPEAKER_B: They might get fuzzy.
[01:14:01] SPEAKER_B: And so what we need to do is
[01:14:02] SPEAKER_B: reinforce the data, reinforce the
[01:14:03] SPEAKER_B: memories, reinforce the pattern memories.
[01:14:05] SPEAKER_B: That's one, that's one specific
[01:14:07] SPEAKER_B: research program, and we're doing that.
[01:14:10] SPEAKER_B: But that's not the only research program
[01:14:11] SPEAKER_B: because the other thing you might
[01:14:13] SPEAKER_B: imagine is that what if the patterns
[01:14:16] SPEAKER_B: are the agent in exactly the same
[01:14:18] SPEAKER_B: sense as we think in our brains, it's
[01:14:21] SPEAKER_B: the, uh, patterns of, uh,
[01:14:23] SPEAKER_B: electrophysiological, um, you know,
[01:14:26] SPEAKER_B: computations and whatever else that
[01:14:28] SPEAKER_B: is the agent, right?
[01:14:30] SPEAKER_B: And that what they're doing in the
[01:14:31] SPEAKER_B: brain are the side effects of the
[01:14:33] SPEAKER_B: patterns working themselves out.
[01:14:34] SPEAKER_B: And those side effects might be to
[01:14:36] SPEAKER_B: fire off some muscles and some glands
[01:14:37] SPEAKER_B: and some other things from that
[01:14:39] SPEAKER_B: perspective, maybe what's actually
[01:14:42] SPEAKER_B: happening is maybe the agents finding
[01:14:43] SPEAKER_B: it harder and harder to be embodied
[01:14:45] SPEAKER_B: in the physical world.
[01:14:46] SPEAKER_B: Why?
[01:14:47] SPEAKER_B: Because the cells might get less.
[01:14:49] SPEAKER_B: Um, responsive.
[01:14:51] SPEAKER_B: In other words, there's,
[01:14:52] SPEAKER_B: the cells are sluggish.
[01:14:53] SPEAKER_B: The patterns are fine.
[01:14:54] SPEAKER_B: They're having a harder time making
[01:14:56] SPEAKER_B: the cells do what they need to do.
[01:14:58] SPEAKER_B: And that may be what you need to
[01:14:59] SPEAKER_B: do is not reinforce the memories.
[01:15:00] SPEAKER_B: Maybe what you need to do is make the
[01:15:02] SPEAKER_B: cells more responsive to them.
[01:15:03] SPEAKER_B: And that is a different research
[01:15:05] SPEAKER_B: agenda, so which, which we are also
[01:15:06] SPEAKER_B: doing, we have evidence for that
[01:15:08] SPEAKER_B: as well, actually now.
[01:15:09] SPEAKER_B: And then we've, we published it recently.
[01:15:11] SPEAKER_B: And so my point here is when we tell
[01:15:12] SPEAKER_B: these crazy sci-fi stories, the only
[01:15:15] SPEAKER_B: worth to them, and the only reason
[01:15:16] SPEAKER_B: I'm talking about them now, and I
[01:15:17] SPEAKER_B: hadn't been up, you know, a year ago,
[01:15:18] SPEAKER_B: I wasn't talking about this stuff is
[01:15:20] SPEAKER_B: because these are now actionable in
[01:15:21] SPEAKER_B: terms of specific experimental research
[01:15:23] SPEAKER_B: agendas that are heading to the clinic.
[01:15:24] SPEAKER_B: I hope in, uh, in some of
[01:15:26] SPEAKER_B: these biomedical approaches.
[01:15:28] SPEAKER_B: And so now here we can go beyond
[01:15:29] SPEAKER_B: this and we can say, okay, so up until
[01:15:31] SPEAKER_B: now, we've considered what, what are
[01:15:34] SPEAKER_B: disease states?
[01:15:35] SPEAKER_B: Well, we know there's organic disease.
[01:15:37] SPEAKER_B: Something is physically broken.
[01:15:38] SPEAKER_B: We can see the tissue is breaking down.
[01:15:40] SPEAKER_B: There's this damage in the joint, you
[01:15:41] SPEAKER_B: know, but the liver is doing what
[01:15:43] SPEAKER_B: everything, and we can see these things.
[01:15:45] SPEAKER_B: But what about disease states
[01:15:47] SPEAKER_B: that are not physical states?
[01:15:49] SPEAKER_B: They're physiological states or
[01:15:52] SPEAKER_B: informational states or cognitive problems.
[01:15:55] SPEAKER_B: So in other words, in all of these
[01:15:56] SPEAKER_B: other spaces in that you can start to
[01:15:58] SPEAKER_B: ask what's a barrier in gene expression
[01:16:00] SPEAKER_B: space, what's a local minimum that
[01:16:03] SPEAKER_B: traps you in physiological state space.
[01:16:05] SPEAKER_B: And what is a stress pattern that
[01:16:07] SPEAKER_B: keeps itself together, moves around
[01:16:09] SPEAKER_B: the body, causes damage, tries
[01:16:11] SPEAKER_B: to keep itself going, right?
[01:16:12] SPEAKER_B: What, what level of agency does it have?
[01:16:15] SPEAKER_B: This suggests an entirely different
[01:16:17] SPEAKER_B: set of approaches to, to biomedicine.
[01:16:20] SPEAKER_B: And, you know, anybody who's, let's
[01:16:23] SPEAKER_B: say in the alternative medicine
[01:16:24] SPEAKER_B: community is, is probably yelling
[01:16:26] SPEAKER_B: at the screen and saying, we've been
[01:16:27] SPEAKER_B: saying this for hundreds of years.
[01:16:29] SPEAKER_B: And yeah, but, but, and I'm, I'm
[01:16:32] SPEAKER_B: well aware these are not, the ideas
[01:16:34] SPEAKER_B: are not new, what's new is being able
[01:16:36] SPEAKER_B: to now take this and make them
[01:16:37] SPEAKER_B: actionable and say, yeah, but we
[01:16:38] SPEAKER_B: can image this now.
[01:16:39] SPEAKER_B: I can now actually see the
[01:16:41] SPEAKER_B: bioelectric patterns and why they
[01:16:42] SPEAKER_B: go here and not there.
[01:16:44] SPEAKER_B: And we have the tools that now
[01:16:45] SPEAKER_B: hopefully will get us to, to, to
[01:16:46] SPEAKER_B: therapeutics.
[01:16:48] SPEAKER_B: So this is, this is very actionable
[01:16:50] SPEAKER_B: stuff and it all leans on not assuming
[01:16:53] SPEAKER_B: we know minds when we see them because
[01:16:55] SPEAKER_B: we don't, and we have to do experiments.
[01:16:57] To return back to the software
[01:16:58] SPEAKER_A: hardware distinction, you're saying
[01:17:00] SPEAKER_A: that we can see the software as the
[01:17:03] SPEAKER_A: organism and the hardware is just the
[01:17:06] SPEAKER_A: scratch pad, or you can see the hardware
[01:17:10] SPEAKER_A: as the organism and the software is
[01:17:12] SPEAKER_A: the thing that the hardware generates.
[01:17:14] SPEAKER_A: And in so doing, we can decrease the
[01:17:18] SPEAKER_A: amount of importance we assign to
[01:17:20] SPEAKER_A: something like the human brain, where
[01:17:22] SPEAKER_A: it could be the activations, it could
[01:17:23] SPEAKER_A: be the electrical signals that are the
[01:17:25] SPEAKER_A: organisms, and the brain is the scratch pad.
[01:17:30] And by saying scratch pad, I don't
[01:17:31] SPEAKER_B: mean it's not important.
[01:17:32] SPEAKER_B: When we get to talking about the
[01:17:34] SPEAKER_B: platonic space, we, we have to talk
[01:17:35] SPEAKER_B: about how important the interface
[01:17:37] SPEAKER_B: actually is.
[01:17:38] SPEAKER_B: It's, it's, the scratch pad isn't
[01:17:39] SPEAKER_B: unimportant, the scratch pad is critical.
[01:17:41] SPEAKER_B: It's just that my only point is that
[01:17:43] SPEAKER_B: when we have these, uh, formalisms of
[01:17:45] SPEAKER_B: software, of hardware, of other things,
[01:17:47] SPEAKER_B: the way we map those formalisms onto
[01:17:49] SPEAKER_B: the world is not obvious.
[01:17:51] SPEAKER_B: It's not given to us.
[01:17:52] SPEAKER_B: We, we get used to certain things, right?
[01:17:54] SPEAKER_B: But, but who's the hardware,
[01:17:56] SPEAKER_B: who's the software, who's the agent,
[01:17:57] SPEAKER_B: and who's the, who's the excitable
[01:17:59] SPEAKER_B: medium is, is to be determined.
[01:18:02] So this is the good place to talk
[01:18:04] SPEAKER_A: about the increasingly radical, weird
[01:18:07] SPEAKER_A: ideas that you've been writing about.
[01:18:09] SPEAKER_A: You've mentioned it a few times,
[01:18:11] SPEAKER_A: the platonic space.
[01:18:14] SPEAKER_A: So there's this ingressing minds paper
[01:18:16] SPEAKER_A: where you described the platonic space.
[01:18:19] SPEAKER_A: You mentioned there's an asynchronous
[01:18:22] SPEAKER_A: conference happening, uh, which
[01:18:24] SPEAKER_A: is a fascinating concept because
[01:18:26] SPEAKER_A: it's asynchronous, people are just
[01:18:28] SPEAKER_A: contributing asynchronously.
[01:18:30] SPEAKER_A: So what happened was this crazy notion,
[01:18:31] SPEAKER_B: which I'll describe momentarily.
[01:18:33] SPEAKER_B: I have given a couple of talks on it.
[01:18:35] SPEAKER_B: I then found a couple of papers in the
[01:18:38] SPEAKER_B: machine learning community called, uh,
[01:18:40] SPEAKER_B: the platonic representation hypothesis.
[01:18:42] SPEAKER_B: And I said, that's pretty cool.
[01:18:43] SPEAKER_B: These guys are climbing up to the
[01:18:45] SPEAKER_B: same point where I'm getting at it
[01:18:46] SPEAKER_B: from biology and philosophy and whatever.
[01:18:48] SPEAKER_B: They're getting there from computer
[01:18:50] SPEAKER_B: science and machine learning.
[01:18:51] SPEAKER_B: We'll take a couple hours.
[01:18:52] SPEAKER_B: I'll give a talk.
[01:18:53] SPEAKER_B: They'll give a talk.
[01:18:53] SPEAKER_B: We'll talk about it.
[01:18:54] SPEAKER_B: I thought there were going to
[01:18:55] SPEAKER_B: be three talks at this thing.
[01:18:56] SPEAKER_B: Once I started reaching out to people
[01:18:59] SPEAKER_B: for this, everybody sort of said, you
[01:19:02] SPEAKER_B: know, I know somebody who's really
[01:19:04] SPEAKER_B: into this stuff, but they never talk
[01:19:05] SPEAKER_B: about it because there's no audience for this.
[01:19:07] SPEAKER_B: So I reached out to them and then they said,
[01:19:09] SPEAKER_B: yeah, well, yeah, I know this, this
[01:19:10] SPEAKER_B: mathematician, or I know this, uh, you
[01:19:12] SPEAKER_B: know, uh, economist, whatever, who has
[01:19:14] SPEAKER_B: these ideas and there's nowhere we
[01:19:15] SPEAKER_B: can ever talk about them.
[01:19:16] SPEAKER_B: So I got this whole list and it became
[01:19:18] SPEAKER_B: completely obvious that we can't do this
[01:19:20] SPEAKER_B: in a normal, you know, it's, we're now
[01:19:22] SPEAKER_B: booked up through, through December.
[01:19:24] SPEAKER_B: So every week in our, in our center,
[01:19:26] SPEAKER_B: somebody gives a talk, we, we kind of
[01:19:28] SPEAKER_B: discuss it, it all goes on this thing.
[01:19:29] SPEAKER_B: I'll give you a link to it.
[01:19:30] SPEAKER_B: And then there's a, there's a huge
[01:19:32] SPEAKER_B: running discussion after that.
[01:19:33] SPEAKER_B: And then in the end, we're all going
[01:19:34] SPEAKER_B: to get together for an actual real
[01:19:36] SPEAKER_B: time discussion section and talk about it.
[01:19:38] SPEAKER_B: But there's going to be probably 15 or
[01:19:41] SPEAKER_B: so talks about this from, from
[01:19:42] SPEAKER_B: all kinds of disciplines.
[01:19:43] SPEAKER_B: It's blown up in a way that I didn't
[01:19:46] SPEAKER_B: realize how much undercurrent of these
[01:19:50] SPEAKER_B: ideas had already existed that were ready.
[01:19:53] SPEAKER_B: Like now, now is the time.
[01:19:54] SPEAKER_B: And I think this is like, I've been
[01:19:56] SPEAKER_B: thinking about these things for, I
[01:19:57] SPEAKER_B: don't know, 30 plus years, I never.
[01:20:00] SPEAKER_B: talked about them before because they weren't actionable before. There wasn't a way to actually
[01:20:05] SPEAKER_B: make empirical progress with this now. You know, this is something that Pythagoras and
[01:20:09] SPEAKER_B: Plato and probably many people before them talked about. But now we're to the point where
[01:20:15] SPEAKER_B: we can actually do experiments and they're making a difference in our research program.
[01:20:19] You can just look it up, Platonic Space Conference. There's a bunch of different fascinating talks,
[01:20:26] SPEAKER_A: first on the patterns of forms of behavior beyond emergence, then Radical Platonism and
[01:20:34] SPEAKER_A: Radical Empiricism from Joe Oditz, and Patterns and Explanatory Gaps in Psychotherapy, Does
[01:20:42] SPEAKER_A: God Play Dice? from Alexey Tolchinsky, and so on. So let's talk about it. What is it?
[01:20:48] SPEAKER_A: And it's fascinating that the origins of some of these ideas are connected to ML people
[01:20:56] SPEAKER_A: thinking about representation space.
[01:20:58] SPEAKER_A: Yeah. The first thing I want to say is that while I'm currently calling it the Platonic
[01:21:03] SPEAKER_B: Space, I am in no way trying to stick close to the things that Plato actually thought
[01:21:09] SPEAKER_B: about. In fact, to whatever extent we even know what that is, I think I depart from that
[01:21:13] SPEAKER_B: in quite in some ways. And I'm going to have to change the name at some point. The reason
[01:21:17] SPEAKER_B: I'm using the name now is because I wanted to be clear about a particular connection
[01:21:22] SPEAKER_B: to mathematics, which a lot of mathematicians would call themselves Platonists, because
[01:21:27] SPEAKER_B: what they think they're doing is discovering, not inventing as a human construction, but
[01:21:33] SPEAKER_B: discovering a structured ordered space of truths. Let's put it this way. In biology,
[01:21:40] SPEAKER_B: as in physics, there's something very curious that happens that if you keep asking why,
[01:21:47] SPEAKER_B: then something interesting goes on. I'll give you two examples. First of all, imagine
[01:21:53] SPEAKER_B: cicadas. So the cicadas come out at 13 years and 17 years. And so if you're a biologist
[01:22:00] SPEAKER_B: and you say, so why is that? And then you get this explanation for, well, it's because
[01:22:03] SPEAKER_B: they're trying to be off cycle from their predators. Because if it was 12 years, then
[01:22:07] SPEAKER_B: every two year, every three year, every four year, every six year, a predator would eat
[01:22:10] SPEAKER_B: you when you come out. And you say, okay, okay, cool. That makes sense. What's special
[01:22:14] SPEAKER_B: about 13 and 17? Oh, they're prime. Uh-huh. And why are they prime? Well, now you're in
[01:22:18] SPEAKER_B: the math department. You're no longer in the biology department. You're no longer in the
[01:22:21] SPEAKER_B: physics department. You're now in the math department to understand why the distribution
[01:22:26] SPEAKER_B: of primes is what it is. Another example, and I'm not a physicist, but what I see is
[01:22:30] SPEAKER_B: every time you talk to a physicist and you say, hey, why do the, you know, leptons do
[01:22:36] SPEAKER_B: this or that, or the fermions are doing whatever? Eventually the answer is, oh, because there's
[01:22:41] SPEAKER_B: this mathematical, you know, this SU8 group or whatever the heck it is. And it has certain
[01:22:45] SPEAKER_B: symmetries in these certain structures. Yeah. Great. Once again, you're in the math department.
[01:22:49] SPEAKER_B: So, so something interesting happens is that there are facts that you come across. Many
[01:22:54] SPEAKER_B: of them are very surprising. You don't get to design them. You get more out than you
[01:22:57] SPEAKER_B: put in, in a certain way, because you make very minimal assumptions and then certain
[01:23:01] SPEAKER_B: facts are thrust upon you. For example, the value of Feigenbaum's constant, the value
[01:23:07] SPEAKER_B: of natural logarithm E, these things you sort of discover, right? And the salient fact
[01:23:14] SPEAKER_B: is this. If those facts were different, then biology and physics would be different, right?
[01:23:21] SPEAKER_B: So they matter. They, they impact instructively, functionally, they impact the physical world.
[01:23:26] SPEAKER_B: If the distribution of primes was something else, well then the cicadas would have been
[01:23:29] SPEAKER_B: coming out at different times, but the reverse isn't true. What I mean is there is nothing
[01:23:33] SPEAKER_B: you can do in the physical world to change E, as far as I know, to change E or to change
[01:23:38] SPEAKER_B: Feigenbaum's constant. You could have swapped out all the constants at the Big Bang, right?
[01:23:43] SPEAKER_B: You can change all the different things. You are not going to change those things. So,
[01:23:47] SPEAKER_B: so this, I think Plato and Pythagoras understood very clearly that there is a set of truths
[01:23:54] SPEAKER_B: which impact the physical world, but they themselves are not defined by and determined
[01:23:59] SPEAKER_B: by what happens in the physical world. You can't change them by things you do in the
[01:24:02] SPEAKER_B: physical world, right? And so I'll make a couple of claims about that. One claim is,
[01:24:06] SPEAKER_B: I think we call physics, those things that are constrained by those patterns. When you
[01:24:11] SPEAKER_B: say, Hey, why is this the way it is? Ah, it's because this is how symmetry symmetries or,
[01:24:15] SPEAKER_B: or, you know, topology or whatever. Biology are the things that are enabled by those.
[01:24:23] SPEAKER_B: They're free lunches. They're biology exploits these kinds of truths. And, uh, and really
[01:24:28] SPEAKER_B: it enables biology and evolution to do amazing things without having to pay for it. I think
[01:24:32] SPEAKER_B: there's a lot of free, free lunches going on here. And so I show you a xenobot or an
[01:24:37] SPEAKER_B: anthropod and, uh, I say, Hey, look, here are some amazing things they're doing that
[01:24:41] SPEAKER_B: tissue has never done before in their history. I say, first of all, where did that come from?
[01:24:48] SPEAKER_B: And when did we pay the computational cost for it? Because we know when we pay the computational
[01:24:52] SPEAKER_B: cost to design a frog or a human, it was for the eons that the genome was bashing against
[01:24:56] SPEAKER_B: the environment getting selected, right? So you pay the computational cost of that. There's
[01:25:00] SPEAKER_B: never been any anthropods. There's never been any xenobots. When do we pay the computational
[01:25:03] SPEAKER_B: cost for designing kinematic self-replication and, you know, all these things that they're
[01:25:07] SPEAKER_B: able to do? So there's two things people say. One is, well, it's sort of, you got it at
[01:25:14] SPEAKER_B: the same time that they were being selected to be good humans and good frogs. Now, the
[01:25:18] SPEAKER_B: problem with that is it kind of undermines the point of evolution. The point of evolutionary
[01:25:22] SPEAKER_B: theory was to have a very tight specificity between what, how you are now in the history
[01:25:28] SPEAKER_B: of selection that got you here, right? The history of environments that got you to this
[01:25:31] SPEAKER_B: point. If you say, yeah, okay, so this is what your environmental history was. And by
[01:25:35] SPEAKER_B: the way, you got something completely different. You got, you got these other skills that you
[01:25:39] SPEAKER_B: didn't know about that. That's really strange, right? And so then what people say is, well,
[01:25:43] SPEAKER_B: it's emergent. And they say, what's that? What does that mean? And they say, well, besides
[01:25:47] SPEAKER_B: the fact that you got surprised, right? Emergence is often just means I didn't see it coming.
[01:25:50] SPEAKER_B: You know, there was something happened. I didn't know that was going to happen. So what
[01:25:55] SPEAKER_B: does it mean that it's emergent? And people say, well, and there are many emergent things
[01:25:58] SPEAKER_B: like this. For example, the fact that gene regulatory networks can do associative learning,
[01:26:02] SPEAKER_B: like that's amazing. And you don't need evolution for that. Even random genetic regulatory networks
[01:26:06] SPEAKER_B: can do associative learning. I say, why, why, why, why does that happen? And he said, well,
[01:26:10] SPEAKER_B: it's just a fact that holds in the world. Just the fact that holds. So, so now you have
[01:26:14] SPEAKER_B: a, you have, you have an option and you can go one of two ways. You can either say, okay,
[01:26:19] SPEAKER_B: look, I like my sparse ontology. I don't want to think about weird platonic spaces. I'm
[01:26:23] SPEAKER_B: a physicalist. I want it to physical world, nothing more. So what we're going to do is
[01:26:27] SPEAKER_B: when we come across these crazy things that are very specific, like, you know, anthropods
[01:26:31] SPEAKER_B: have four specific behaviors that they switch around. Why, why four? Why not 12? Why not
[01:26:34] SPEAKER_B: one? Like four? Why four? When we come across these things, just like when we come across
[01:26:38] SPEAKER_B: the value of E or Feigenbaum's number or whatever, what we're going to do is we're going to write
[01:26:42] SPEAKER_B: it down in our big book of emergence. And that, that, that, that's it. We're just gonna
[01:26:47] SPEAKER_B: have to live with it. This is, this is what happens. We're just, you know, there's some
[01:26:50] SPEAKER_B: cool surprises, you know, when we come across them, we're going to write them down. Great.
[01:26:53] SPEAKER_B: It's a random grab bag of stuff. And when we come across them, we'll write it down.
[01:26:56] SPEAKER_B: That's, that's one. The upside is you get to be a physicalist and you get to keep your,
[01:27:01] SPEAKER_B: your sparse ontology. The downside is I find it incredibly pessimistic and mysterian because
[01:27:09] SPEAKER_B: you're basically then just willing to make a catalog of these, of these amazing patterns.
[01:27:15] SPEAKER_B: Why not instead, and this is why I started with this, with this platonic terminology,
[01:27:20] SPEAKER_B: why not do what the mathematicians already do? A huge number of them say, we are going
[01:27:26] SPEAKER_B: to make the same optimistic assumption that science makes, that there's an underlying
[01:27:29] SPEAKER_B: structure to that latent space. It's not a random grab bag of stuff. There's a space
[01:27:34] SPEAKER_B: to it, which where these patterns come from. And by studying them systematically, we can
[01:27:38] SPEAKER_B: get from one to another. We can map out the space. We can, we can find out the relationships
[01:27:43] SPEAKER_B: between them. We can get an idea of what's in that space. And we're not going to assume
[01:27:46] SPEAKER_B: that it's just random. We're going to assume there's some kind of structure to it. And
[01:27:49] SPEAKER_B: you'll see all kinds of people. I mean, you know, well-known mathematicians that talk
[01:27:52] SPEAKER_B: about this stuff, you know, Penrose and then lots of other people who will say that, yeah,
[01:27:56] SPEAKER_B: there's another space physically, and it has, it has spatial structure. It has components
[01:28:00] SPEAKER_B: to it and so on. And we can traverse that space in various ways. And then, and then
[01:28:04] SPEAKER_B: there's the physical space. So I find, I find that much more appealing because it suggests
[01:28:11] SPEAKER_B: a research program, which we are now undergoing in our lab. The research program is everything
[01:28:16] SPEAKER_B: that we make cells, embryos, robots, biobots, language models, simple machines, all of it.
[01:28:23] SPEAKER_B: They are interfaces. They are all physical things are interfaces to these patterns. You
[01:28:28] SPEAKER_B: build an interface. Some of those patterns are going to come through that interface,
[01:28:31] SPEAKER_B: depending on what you build. Some patterns versus others are going to come through. The
[01:28:36] SPEAKER_B: research program is mapping out that relationship between the physical pointers that we make
[01:28:41] SPEAKER_B: and the patterns that come through it, right? Understanding what is the structure of that
[01:28:44] SPEAKER_B: space, what exists in that space and what do I need to make physically to make certain
[01:28:49] SPEAKER_B: patterns come through. Now, when I say patterns, now we have to ask what kinds of things live
[01:28:53] SPEAKER_B: in that space. Well, the mathematicians will tell what we already know. We have a whole
[01:28:56] SPEAKER_B: list of objects, you know, the amplitude, hedrons and the, you know, all this crazy
[01:28:59] SPEAKER_B: stuff that lives in that space. Yeah. I think that's one layer of stuff that lives in that
[01:29:04] SPEAKER_B: space, but I think those patterns are the lower agency kinds of things that are basically
[01:29:11] SPEAKER_B: studied by mathematicians. What also lives in that space are much more active, more complex,
[01:29:18] SPEAKER_B: higher agency patterns that we recognize as kinds of minds that behavioral scientists
[01:29:22] SPEAKER_B: would look at that pattern and say, well, I know what that is. That's the competency
[01:29:25] SPEAKER_B: for delayed gratification or problem solving of certain kinds or whatever. And so, so what
[01:29:30] SPEAKER_B: I end up with right now is a model in which that latent space contains things that come
[01:29:35] SPEAKER_B: through physical objects. So simple, simple patterns, right? So, so facts about triangles
[01:29:40] SPEAKER_B: and Fibonacci, you know, patterns and fractals and things like that. But also if you make
[01:29:45] SPEAKER_B: more complex interfaces such as biologicals and, and, but, but importantly, not just biologicals,
[01:29:51] SPEAKER_B: but let's say cells and embryos and tissues, what you will then pull down is much more
[01:29:55] SPEAKER_B: complex patterns that we say, ah, that's a, that's a, that's a mind, that's a human mind.
[01:30:00] SPEAKER_B: or that's a, you know, snake mind or whatever.
[01:30:02] SPEAKER_B: So I think the mind brain relationship is exactly the kind of thing that the math
[01:30:10] SPEAKER_B: physics relationship is that in some very interesting way, there are truths of
[01:30:14] SPEAKER_B: mathematics that become embodied and they kind of haunt physical objects, right.
[01:30:19] SPEAKER_B: In a very specific functional way.
[01:30:21] SPEAKER_B: And in the exact same way, there are other patterns that are much more
[01:30:26] SPEAKER_B: complex, higher agency patterns that basically inform in form living things
[01:30:32] SPEAKER_B: that we see as, as obvious embodied minds.
[01:30:35] SPEAKER_B: Okay.
[01:30:35] SPEAKER_B: Given how weird and complicated this we're describing is, we'll talk about it more,
[01:30:40] SPEAKER_A: but you got an ELI five, the basics to a person has never seen this.
[01:30:45] SPEAKER_A: So again, you mentioned things like pointers.
[01:30:49] SPEAKER_A: So the physical object themselves, or the brain is a pointer to that platonic
[01:30:55] SPEAKER_A: space, what is in that platonic space?
[01:30:58] SPEAKER_A: What is the platonic space?
[01:31:02] SPEAKER_A: What is the embodiment?
[01:31:03] SPEAKER_A: What is the pointer?
[01:31:05] SPEAKER_B: Yeah.
[01:31:05] SPEAKER_B: Um, okay.
[01:31:05] SPEAKER_B: Let's, let's try it.
[01:31:06] SPEAKER_B: Let's try it this way.
[01:31:07] SPEAKER_B: Um, there are certain facts of mathematics.
[01:31:12] SPEAKER_B: So the distribution of prime numbers, right.
[01:31:14] SPEAKER_B: That if you map them out, they make these nice spirals.
[01:31:16] SPEAKER_B: And there's an image that I often show, which is a very particular kind of, um,
[01:31:20] SPEAKER_B: fractal, uh, and that fractal is a Halley map, which is, it's, it's pretty awesome
[01:31:25] SPEAKER_B: that it actually looks very organic.
[01:31:27] SPEAKER_B: It looks very biological.
[01:31:29] SPEAKER_B: So if you look at that thing, that image, which has a very specific complex
[01:31:33] SPEAKER_B: structure, it's a map of a very compact mathematical object.
[01:31:37] SPEAKER_B: That formula is like, you know, Z cubed plus seven.
[01:31:40] SPEAKER_B: It's something like that.
[01:31:41] SPEAKER_B: That's it.
[01:31:42] SPEAKER_B: So now, so now you look at that structure and you say, where
[01:31:45] SPEAKER_B: does that actually come from?
[01:31:46] SPEAKER_B: It's definitely not packed into the ZQ plus seven.
[01:31:49] SPEAKER_B: It's not, there's not enough bits in that to give you all of that.
[01:31:52] SPEAKER_B: There's no fact of physics that determines this.
[01:31:54] SPEAKER_B: There's no evolutionary history.
[01:31:55] SPEAKER_B: It's not like we selected this based on some, you know, from,
[01:31:58] SPEAKER_B: from a larger set over time.
[01:31:59] SPEAKER_B: Where does this come from?
[01:32:01] SPEAKER_B: Or, or the fact that I think about, think about the way the
[01:32:04] SPEAKER_B: biology exploits these things.
[01:32:06] SPEAKER_B: Imagine, imagine a world in which the highest fitness belonged to
[01:32:10] SPEAKER_B: a certain kind of triangle, right?
[01:32:11] SPEAKER_B: So evolution cranks, a bunch of generations and it gets the first
[01:32:15] SPEAKER_B: angle, right, and cranks a bunch more generations gets a second angle, right?
[01:32:18] SPEAKER_B: Now there's, now there's something amazing that happens.
[01:32:20] SPEAKER_B: It doesn't need to look for the third angle because you already know, if
[01:32:23] SPEAKER_B: you know to, you get this magical free gift from geometry that says, why?
[01:32:25] SPEAKER_B: I already know what the third one should be.
[01:32:27] SPEAKER_B: You don't have to go look for it.
[01:32:28] SPEAKER_B: Or as evolution, if you invent a voltage gated ion channel, which
[01:32:32] SPEAKER_B: is basically a transistor, right?
[01:32:33] SPEAKER_B: And you can make a logic gate, then all the truth tables and the fact
[01:32:37] SPEAKER_B: that NAND is special and all these other things, you don't have
[01:32:40] SPEAKER_B: to evolve those things.
[01:32:41] SPEAKER_B: You get those for free.
[01:32:42] SPEAKER_B: You inherit those.
[01:32:43] SPEAKER_B: Where do all those things live?
[01:32:44] SPEAKER_B: These mathematical truths that you come across that you don't have any
[01:32:47] SPEAKER_B: choice about, you can't, you know, uh, once you've committed to certain
[01:32:51] SPEAKER_B: axioms, there's a whole bunch of other stuff that is now just, it is what it is.
[01:32:56] SPEAKER_B: And so what I'm saying is, and this is, this is what, what, what Pythagoras
[01:32:59] SPEAKER_B: was, was saying, I think that there is a whole space of these kinds of.
[01:33:03] SPEAKER_B: Uh, truths.
[01:33:04] SPEAKER_B: Now he was focused on, on mathematical ones, but, but he was embodying
[01:33:07] SPEAKER_B: them in music and in geometry and then things like that, there are the
[01:33:11] SPEAKER_B: space of patterns and, uh, and they make a difference in the physical
[01:33:15] SPEAKER_B: world to machines, to sound, to things like that, what I'm extending it.
[01:33:19] SPEAKER_B: And what I'm saying is, yeah.
[01:33:21] SPEAKER_B: And so far we've only been looking at the low agency inhabitants of that world.
[01:33:27] SPEAKER_B: There are other patterns that we would recognize as kinds of minds and that.
[01:33:31] SPEAKER_B: You don't see them in this space until there's an interface, until there's a way
[01:33:35] SPEAKER_B: for them to come through the physical world, that interface, the same, the same
[01:33:39] SPEAKER_B: way that you have to make a triangular object before you can actually see the
[01:33:43] SPEAKER_B: you know, what, what you're going to gain right out of, out of the rules of
[01:33:46] SPEAKER_B: geometry and whatever, or you have to actually do the computation on the
[01:33:48] SPEAKER_B: fractal before you actually see that pattern.
[01:33:51] SPEAKER_B: If you want to see some of those minds, you have to build an interface.
[01:33:54] SPEAKER_B: Right.
[01:33:54] SPEAKER_B: At least, at least if you're going to interact with them in the physical
[01:33:56] SPEAKER_B: world, the way we normally do science.
[01:33:58] SPEAKER_B: As Darwin said, mathematicians have their own new sense, like a
[01:34:01] SPEAKER_B: different sense than the rest of us.
[01:34:03] SPEAKER_B: And so that's right.
[01:34:04] SPEAKER_B: You, you know, mathematicians can, can perhaps interact with these, uh, with
[01:34:08] SPEAKER_B: these patterns directly in that space.
[01:34:10] SPEAKER_B: But for the rest of us, we have to make interfaces.
[01:34:13] SPEAKER_B: And when we make interfaces, which might be cells or robots or, you
[01:34:18] SPEAKER_B: know, embryos or whatever, what we are pulling down our minds that are
[01:34:22] SPEAKER_B: fundamentally not produced by physics.
[01:34:24] SPEAKER_B: So I don't believe that.
[01:34:25] SPEAKER_B: I don't know if we're going to get into the whole consciousness thing, but I don't
[01:34:28] SPEAKER_B: believe that we create consciousness, whether we make babies or whether we make
[01:34:32] SPEAKER_B: robots, nobody's creating consciousness.
[01:34:34] SPEAKER_B: What you create is an interface, a physical interface through which.
[01:34:38] SPEAKER_B: Specific patterns, which we call kinds of minds are going to ingress.
[01:34:43] SPEAKER_B: Right.
[01:34:44] SPEAKER_B: And, and, and consciousness is what it looks like from that
[01:34:46] SPEAKER_B: direction, looking out into the world.
[01:34:48] SPEAKER_B: It's, it's what we call the view from the perspective of the platonic patterns.
[01:34:52] SPEAKER_B: Just to clarify, what you're saying is a pretty radical idea here.
[01:34:59] SPEAKER_A: So if, uh, there's a mapping from mathematics to physics, okay, that's
[01:35:05] SPEAKER_A: understandable, intuitive, as you've described, but what you're suggesting is
[01:35:11] SPEAKER_A: there's a mapping from some kind of abstract mind object to an embodied brain
[01:35:21] SPEAKER_A: that we think of as a mind as us fellows, humans, what is that?
[01:35:27] SPEAKER_A: What exactly?
[01:35:28] SPEAKER_A: Cause you said interface, you've also said pointer.
[01:35:32] SPEAKER_A: So the brain, and I think you said somewhere a thin interface, a thin client.
[01:35:37] Yeah.
[01:35:38] SPEAKER_B: The brain, the brain, the brain is a thin client.
[01:35:40] SPEAKER_B: Yeah.
[01:35:40] SPEAKER_B: Thin client.
[01:35:41] SPEAKER_A: Okay.
[01:35:41] SPEAKER_A: So you're a brain is a thin client to this other world.
[01:35:46] SPEAKER_A: Yeah.
[01:35:47] SPEAKER_A: Can you just lay out very clearly how radical the idea is?
[01:35:52] SPEAKER_A: Sure.
[01:35:52] SPEAKER_A: Cause you're kind of dancing around.
[01:35:55] SPEAKER_A: I think you'd also, uh, point to Donald Hoffman and kind of who speaks of an
[01:36:01] SPEAKER_A: interface, uh, to a world.
[01:36:05] SPEAKER_A: So we've only interact with the quote unquote, real world through an interface.
[01:36:09] SPEAKER_A: What is the connection here?
[01:36:11] Yeah.
[01:36:11] Um, okay.
[01:36:12] SPEAKER_B: A couple of things.
[01:36:12] SPEAKER_B: First of all, when you said it makes sense for physics, I want to show that
[01:36:16] SPEAKER_B: it's not as simple as it sounds, because what it means is that even in Newton's
[01:36:23] SPEAKER_B: boring, uh, sort of classical universe, long before quantum, anything Newton's
[01:36:28] SPEAKER_B: world, physicalism was already dead in, in, by in Newton's world.
[01:36:32] SPEAKER_B: I mean, think about what that means.
[01:36:33] SPEAKER_B: This is, this is nuts because, because already he knew perfectly well.
[01:36:36] SPEAKER_B: I mean, Pythagoras and Plato knew that even in a, a totally classical
[01:36:41] SPEAKER_B: deterministic world already, you have the ingression of information that
[01:36:47] SPEAKER_B: determines what happens in what's possible and what's not possible in that
[01:36:50] SPEAKER_B: world from a space that is itself not physical.
[01:36:53] SPEAKER_B: In other words, it's something like the natural logarithm E, right?
[01:36:57] SPEAKER_B: Nothing in Newton's world is set to the value of E.
[01:37:00] SPEAKER_B: There was nothing you could do to set the value of E in that world.
[01:37:03] SPEAKER_B: And yet that fact that it was that and not something else governed all sorts
[01:37:07] SPEAKER_B: of properties of things that happen.
[01:37:08] SPEAKER_B: His way that classical world was already haunted by
[01:37:12] SPEAKER_B: patterns from outside that world.
[01:37:14] SPEAKER_B: That's this, this should be like, this is, this is, this is wild.
[01:37:17] SPEAKER_B: This is, this is not saying that, okay, everything was, was cool.
[01:37:21] SPEAKER_B: Physicalism was great up until, you know, maybe we got quantum interfaces
[01:37:25] SPEAKER_B: or we got the, you know, consciousness or whatever, but, but originally it was fine.
[01:37:29] SPEAKER_B: No, this is saying that it was that, that worldview was already, uh, impossible
[01:37:34] SPEAKER_B: really for, since from, from a very long time ago, we already knew that there
[01:37:39] SPEAKER_B: are non-physical properties that matter in the physical world.
[01:37:42] This is a chicken or the egg question.
[01:37:44] SPEAKER_A: You're saying Newton's laws are creating the physical world.
[01:37:51] SPEAKER_A: That is a, that is a very deep follow on question that, that I will,
[01:37:56] SPEAKER_B: we'll, we'll come back to in a minute.
[01:37:57] SPEAKER_B: What, what I, all I was saying about Newton is that in the law,
[01:38:00] SPEAKER_B: you don't need quantum anything.
[01:38:02] SPEAKER_B: You don't need to think about consciousness.
[01:38:04] SPEAKER_B: You already long before you get to any of that as, as Pythagoras, I think
[01:38:09] SPEAKER_B: knew already, we have the idea that this physical world is being strongly impacted
[01:38:15] SPEAKER_B: by truths that do not live in the physical world.
[01:38:18] SPEAKER_B: And when I, which truth that we're referring to, are we talking about
[01:38:21] SPEAKER_A: Newton's law, like mathematical equations?
[01:38:23] SPEAKER_B: Mathematical, mathematical facts.
[01:38:24] SPEAKER_B: So for example, the actual value of E or.
[01:38:27] SPEAKER_B: Oh, like very primitive mathematical facts.
[01:38:29] SPEAKER_A: Yeah.
[01:38:29] SPEAKER_A: I mean, some of them are, you know, I mean, if you, if you ask Don Hoffman,
[01:38:32] SPEAKER_B: there's this like amplitude Hedron thing that, that is a set of mathematical
[01:38:36] SPEAKER_B: objects that determines all the scattering amplitudes of the particles
[01:38:39] SPEAKER_B: and whatever, right.
[01:38:39] SPEAKER_B: They don't have to be simple.
[01:38:40] SPEAKER_B: I mean, the old ones were simple.
[01:38:42] SPEAKER_B: Now they're like crazy.
[01:38:42] SPEAKER_B: I can't, I can't imagine this amplitude Hedron thing, but maybe they can.
[01:38:46] SPEAKER_B: But, um, but, but all of these are mathematical structures that explain and
[01:38:51] SPEAKER_B: determine facts about the physical world, right?
[01:38:53] SPEAKER_B: If you ask physicists, Hey, why, you know, this many of this type of particle,
[01:38:57] SPEAKER_B: because this mathematical thing has the symmetries.
[01:38:59] SPEAKER_B: That's why.
[01:39:00] So Newton is discovering these things.
[01:39:02] SPEAKER_A: They're not, he's not inventing.
[01:39:04] SPEAKER_A: This is very controversial, right?
[01:39:05] SPEAKER_B: And there are of course, physicists and mathematicians who, who disagree
[01:39:09] SPEAKER_B: with what I'm saying for sure.
[01:39:11] SPEAKER_B: But what I'm leaning on is simply this.
[01:39:14] SPEAKER_B: I don't know of anything you can do in the physical world at the big, you're
[01:39:19] SPEAKER_B: around at the big bang, you get to set all the constants, set physics, however
[01:39:23] SPEAKER_B: you want, can you change E can you change Feigenbaum's constant?
[01:39:27] SPEAKER_B: I don't think you can.
[01:39:28] SPEAKER_B: Is that an obvious statement?
[01:39:30] SPEAKER_A: I don't even know what it means to change the parameters
[01:39:33] SPEAKER_A: at the start of the big bang.
[01:39:34] SPEAKER_A: So physicists do this.
[01:39:36] SPEAKER_B: They'll say, okay, you know, if we made the, if we made the, the ratio between
[01:39:39] SPEAKER_B: the, the, the, you know, the gravitation and, and the electromagnetic force
[01:39:44] SPEAKER_B: different, would we have matter?
[01:39:45] SPEAKER_B: Would we, how many dimensions would we have?
[01:39:47] SPEAKER_B: Would there be inflation?
[01:39:48] SPEAKER_B: Would there be this or that?
[01:39:50] SPEAKER_B: Right.
[01:39:50] SPEAKER_B: You can, you can imagine playing with there.
[01:39:52] SPEAKER_B: There are however many unitless constants of physics.
[01:39:55] SPEAKER_B: These are the kind of like knobs on the universe that, that, that you could have.
[01:40:00] SPEAKER_B: could in theory be different. And then you'd have different physics, you'd have
[01:40:03] SPEAKER_B: different physics, you'd have different physical properties.
[01:40:05] SPEAKER_B: You're saying that's not going to change the axiomatic systems that mathematics has?
[01:40:10] SPEAKER_A: What I'm not saying is that every alien everywhere is going to have the exact same
[01:40:13] SPEAKER_B: math that we have. That's not what I'm claiming, although maybe, but that's not
[01:40:16] SPEAKER_B: what I'm claiming. What I'm saying is you get more out than you put in. Once you've
[01:40:20] SPEAKER_B: made a choice and maybe some alien somewhere made a different choice of how
[01:40:23] SPEAKER_B: they're going to do their math. But once you've made your choice, then you get
[01:40:27] SPEAKER_B: saddled with a whole bunch of new truths that you discover that you can't do
[01:40:30] SPEAKER_B: anything about. They are given to you from somewhere and you can say they're
[01:40:34] SPEAKER_B: random or you can say, no, there's a space of these facts that they're pulled
[01:40:38] SPEAKER_B: from. There's a latent space of options that they come from. So when you get, so
[01:40:41] SPEAKER_B: when your E is exactly 2.718 and so on, there is nothing you can do in physics
[01:40:46] SPEAKER_B: to change it.
[01:40:46] SPEAKER_B: And you're saying that space is immutable.
[01:40:49] I'm not saying it's immutable. So I think Plato may or may not have thought that
[01:40:54] SPEAKER_B: these forms are eternal and unchanging. That's one place we differ. I actually
[01:40:57] SPEAKER_B: think that space has some action to it, maybe even some computation to it.
[01:41:01] SPEAKER_B: But we're, we're just pointers. Okay. That's
[01:41:05] SPEAKER_A: well, so, so let's, okay. So, so I'll, so I'll circle us. I'll circle back around
[01:41:09] SPEAKER_B: to that, to that whole thing. So, so the, the only thing I was trying to do is blow
[01:41:12] SPEAKER_B: up the idea that we're cool with how it works in physics. No problem there. I
[01:41:16] SPEAKER_B: like, I don't like, I think that's a much bigger deal than, than people normally
[01:41:20] SPEAKER_B: think it is. I think already there, you have this weird haunting of, of the
[01:41:25] SPEAKER_B: physical world by patterns that are not coming from the physical world. The
[01:41:28] SPEAKER_B: reason I emphasize this is because now what I'm going to, when I amplify this
[01:41:32] SPEAKER_B: into biology, I don't think it sort of jumps as a new thing. I think it's just a
[01:41:37] SPEAKER_B: much more, I think what we call biology is our systems that exploit the hell out
[01:41:41] SPEAKER_B: of it. I think physics is constrained by it, but we call biology, those things
[01:41:46] SPEAKER_B: that make, make use of those kinds of things and run with it. And so I, again,
[01:41:51] SPEAKER_B: I just think it's a scaling. I don't think it's a brand new thing that happens.
[01:41:54] SPEAKER_B: I think it's a, it's a scaling, right? So what I'm saying is we already know from
[01:41:59] SPEAKER_B: physics that there are non-physical patterns and these are generally patterns
[01:42:04] SPEAKER_B: of form, which is why I call them low agency, because they're like fractals
[01:42:07] SPEAKER_B: that stand still and they're like prime number distributions. Although there's a
[01:42:10] SPEAKER_B: mathematician that's talking in our symposium, that's telling me that
[01:42:13] SPEAKER_B: actually I'm too chauvinistic even there. Actually, even those things have
[01:42:17] SPEAKER_B: more, more oomph than even I gave them credit for, which I, which I love. So, so
[01:42:22] SPEAKER_B: what I'm saying is those kinds of static patterns are things that we typically
[01:42:26] SPEAKER_B: see in physics, but they're not the full extent of what lives in that space. That
[01:42:31] SPEAKER_B: space is also home to some patterns that are very high agency. And if we give
[01:42:35] SPEAKER_B: them a body, if we build a body that they can inhabit, then we get to see
[01:42:40] SPEAKER_B: different behavioral competencies that the behavior scientists say, oh, I know
[01:42:43] SPEAKER_B: what that looks like. That's a, this, this kind of behavioral, you know, this
[01:42:47] SPEAKER_B: kind of mind or that kind of mind in a certain sense. I mean, yes, what I'm
[01:42:51] SPEAKER_B: saying is extremely radical, but it is a very old idea. It's an old idea of a
[01:42:58] SPEAKER_B: dualistic worldview, right? Where the mind was not in the physical body and
[01:43:03] SPEAKER_B: that it in some way interacted with the physical brain. So I just want to be
[01:43:07] SPEAKER_B: clear. I'm not claiming that this is fundamentally a new idea. This has been
[01:43:11] SPEAKER_B: around for forever. However, it's mostly been discredited. And, uh, it's a very
[01:43:17] SPEAKER_B: unpopular view nowadays. There are very few people in the, for example,
[01:43:20] SPEAKER_B: cognitive science community or, or anywhere else in science that like this
[01:43:23] SPEAKER_B: kind of view primarily, and already Descartes was getting, getting crap for
[01:43:27] SPEAKER_B: this when he first tried it out as this interaction problem, right? So the idea
[01:43:31] SPEAKER_B: was, okay, well, if you have this non-physical mind and then you have this
[01:43:35] SPEAKER_B: brain that presumably obeys conservation of mass energy and things like that, how
[01:43:38] SPEAKER_B: are you supposed to, you know, how are you supposed to interact with it? There
[01:43:41] SPEAKER_B: are many other problems there. So, uh, what I'm trying to point out is that
[01:43:45] SPEAKER_B: first of all, physics already had this problem. You didn't have to wait until
[01:43:48] SPEAKER_B: you had biology and, and, and cognitive science to, to ask about it. And what I
[01:43:52] SPEAKER_B: think is happening in a way, the way we need to think about this is coming back
[01:43:58] SPEAKER_B: to the, to my, my point that I think the mind brain relationship is basically of
[01:44:02] SPEAKER_B: the same kind as the math physics relationship, the same way that
[01:44:07] SPEAKER_B: non-physical facts of physics haunt physical objects is basically how I
[01:44:11] SPEAKER_B: think different kinds of patterns that we call kinds of minds are manifesting
[01:44:17] SPEAKER_B: through our two interfaces, like brains.
[01:44:19] SPEAKER_B: How do we prove or disprove the existence of that world? Cause it's a
[01:44:24] SPEAKER_A: pretty radical one because this physical world can poke. It's there. It feels
[01:44:31] SPEAKER_A: like all the incredible things like consciousness and cognition and all the
[01:44:36] SPEAKER_A: goal oriented behavior and agency all seems to come from this 3d entity.
[01:44:43] SPEAKER_A: And so like we can test it, we can poke it, we can hit it with a stick.
[01:44:47] SPEAKER_A: Yeah. Sort of noises.
[01:44:49] SPEAKER_B: Sort of. I mean, so, so Descartes got some stuff wrong, I think, but one thing
[01:44:54] SPEAKER_B: that he did get right, the fact that you actually, you don't know what you can
[01:44:58] SPEAKER_B: poke and what you can't poke. The only thing you actually know are the contents
[01:45:00] SPEAKER_B: of your mind and everything else might be. And in fact, what we know from a Neil
[01:45:06] SPEAKER_B: Seth and Don Hoffman and various other people, it's definitely a construct.
[01:45:09] SPEAKER_B: You might be on drugs and you might wake up tomorrow and say, my God, I had the
[01:45:13] SPEAKER_B: craziest dream of being Lex Reedman and amazing.
[01:45:16] SPEAKER_A: It's a nightmare.
[01:45:17] SPEAKER_A: Yeah. Who knows?
[01:45:19] SPEAKER_B: It's a ride.
[01:45:19] SPEAKER_B: Right. But, um, but, but you see, uh, I, I, you know, it's, it's not clear at
[01:45:25] SPEAKER_B: all that, that the, that the physical poking is your primary reality.
[01:45:28] SPEAKER_B: That's not clear to me at all.
[01:45:30] I don't know.
[01:45:31] SPEAKER_A: That's a obvious thing that a lot of people can show.
[01:45:35] SPEAKER_A: It's true to take a step to the, the, the cart.
[01:45:38] SPEAKER_A: I think therefore I am.
[01:45:39] SPEAKER_A: That's the only thing, you know, for sure.
[01:45:40] SPEAKER_A: And everything else could be an illusion or a dream that's already a leap.
[01:45:45] SPEAKER_A: I think from a basic caveman science perspective, the repeatable experiment is
[01:45:52] SPEAKER_A: the one that most of intelligence comes from here, the reality is exactly as it
[01:45:56] SPEAKER_A: is to take a step towards the Donald Hoffman worldview takes a lot of guts
[01:46:04] SPEAKER_A: and imagination and, uh, stripping away of the ego and all these kinds of processes.
[01:46:11] SPEAKER_A: I think you can get there more easily by synthetic bioengineering
[01:46:15] SPEAKER_B: in the following, in the following sense.
[01:46:17] SPEAKER_B: Do you feel a lack of X-ray perception?
[01:46:19] SPEAKER_B: Do you feel blind in the X-ray spectrum or, or in the ultraviolet?
[01:46:23] SPEAKER_B: I mean, you don't, you have absolutely no clue that stuff is there.
[01:46:27] SPEAKER_B: And, uh, all of your, um, your, your reality, as you see it, it's
[01:46:33] SPEAKER_B: shaped by your evolutionary history.
[01:46:35] SPEAKER_B: It's shaped by the cognitive structure that you have, right?
[01:46:38] SPEAKER_B: There are tons of stuff going on around us right now that we, of
[01:46:41] SPEAKER_B: which we are completely oblivious.
[01:46:43] SPEAKER_B: There's equally all kinds of other stuff, which we construct.
[01:46:46] SPEAKER_B: And this is just, this is just modern cognitive science that says that a lot
[01:46:50] SPEAKER_B: of what we think is going on is, is, is a total fabrication constructed by us.
[01:46:54] SPEAKER_B: So I think this is not a, I don't think this is a philosophy.
[01:46:57] SPEAKER_B: I mean, Descartes got there through, from a philosophical point.
[01:46:59] SPEAKER_B: That's not what I'm, that's not the leap I'm asking us to make.
[01:47:02] SPEAKER_B: I'm saying that depending on your embodiment, depending on your interface,
[01:47:06] SPEAKER_B: and this is incredible, this is, um, increasingly going to be more relevant
[01:47:10] SPEAKER_B: as we make first augmented humans that have sensory substitution, you're
[01:47:14] SPEAKER_B: going to be walking around, your friend's going to be like, oh man, I
[01:47:16] SPEAKER_B: have this primary perception of the solar weather and the stock market
[01:47:19] SPEAKER_B: because I got those implants.
[01:47:21] SPEAKER_B: And what do you see?
[01:47:21] SPEAKER_B: Well, I see the, you know, the traffic of the internet through
[01:47:24] SPEAKER_B: the, you know, a trans-Pacific channel.
[01:47:26] SPEAKER_B: We're all going to be living in somewhat different worlds.
[01:47:28] SPEAKER_B: That's the first thing.
[01:47:29] SPEAKER_B: The second thing is we're going to become better attuned to other beings,
[01:47:33] SPEAKER_B: whether they be cells, tissues, you know, what's, what's it like to be a cell
[01:47:38] SPEAKER_B: living in a 20,000 dimensional transcriptional space, okay, to novel
[01:47:43] SPEAKER_B: beings that have never been here before that have all kinds of crazy spaces
[01:47:47] SPEAKER_B: that they live in, and that might be AIs.
[01:47:49] SPEAKER_B: It might be cyborgs.
[01:47:51] SPEAKER_B: It might be hybrids.
[01:47:52] SPEAKER_B: It might be all sorts of things.
[01:47:53] SPEAKER_B: So this idea that we have a consensus reality here, that's independent of
[01:47:59] SPEAKER_B: some very specifically chosen aspects of our brain and our interaction, we're
[01:48:04] SPEAKER_B: going to have to give that up no matter what to relate to these other beings.
[01:48:07] SPEAKER_B: I think the tension is, uh, and absolutely.
[01:48:09] SPEAKER_A: And this idea that you're talking about sort of almost, I think you've
[01:48:13] SPEAKER_A: termed it cognitive prosthetics, which is different ways of perceiving
[01:48:18] SPEAKER_A: and interacting with the world.
[01:48:20] SPEAKER_A: But I guess the question is, is our human experience, the direct human
[01:48:25] SPEAKER_A: experience, is that just a slice of the real world or is it a
[01:48:29] SPEAKER_A: pointer to a different world?
[01:48:32] SPEAKER_A: That that's what I'm trying to, uh, figure out because the claim you're
[01:48:36] SPEAKER_A: making is a really fascinating one.
[01:48:39] SPEAKER_A: Compelling one.
[01:48:40] SPEAKER_A: There's a pretty strong one, which is there's another world into which our
[01:48:46] SPEAKER_A: brain is an interface to, which means you could theoretically
[01:48:49] SPEAKER_A: map that world systematically.
[01:48:52] SPEAKER_A: Yeah.
[01:48:52] SPEAKER_A: Which, which is exactly what we're trying to do.
[01:48:54] SPEAKER_B: I mean, but it's not clear that that world exists.
[01:48:59] Yeah.
[01:48:59] Yeah.
[01:48:59] Okay.
[01:49:00] SPEAKER_B: I mean, so, so that's the beautiful part about this and this is why I'm talking
[01:49:04] SPEAKER_B: about this now where I wasn't, you know, about a year ago, up until a year ago, I
[01:49:08] SPEAKER_B: was never talking about this because I think this is now actionable.
[01:49:11] SPEAKER_B: So there's this diagram that's called a map of mathematics and they basically
[01:49:14] SPEAKER_B: try to, uh, show how all the different pieces of math linked together.
[01:49:18] SPEAKER_B: And then there's a bunch of different versions of it.
[01:49:20] SPEAKER_B: So there's two features to this.
[01:49:23] SPEAKER_B: One is that, what is it a map of?
[01:49:25] SPEAKER_B: Well, it's a map of various truths.
[01:49:27] SPEAKER_B: It's a map of facts that are, that are, that are thrust on you.
[01:49:30] SPEAKER_B: You don't have a choice.
[01:49:31] SPEAKER_B: Once you've picked some axioms, you just, you know, here's some surprising facts
[01:49:34] SPEAKER_B: that, that are just going to be given to you.
[01:49:37] SPEAKER_B: But the other key thing about this is that it has a metric.
[01:49:40] SPEAKER_B: It's not just a random heap of facts.
[01:49:42] SPEAKER_B: They are all connected to each other in a particular way.
[01:49:45] SPEAKER_B: They, they literally make a space.
[01:49:47] SPEAKER_B: And so when I say it's a space of patterns, what I mean is it is not just
[01:49:50] SPEAKER_B: a random bag of patterns such that when you have one pattern, you are no
[01:49:54] SPEAKER_B: closer to finding any other pattern.
[01:49:56] SPEAKER_B: I'm saying that there's a, some kind of a metric to it so that when you find one.
[01:50:00] SPEAKER_B: others are closer to it and then you can get there.
[01:50:03] SPEAKER_B: So that's the claim.
[01:50:04] SPEAKER_B: And obviously this is not everybody buys this and so on.
[01:50:08] SPEAKER_B: This is one idea.
[01:50:09] SPEAKER_B: Now, how do we know that this exists?
[01:50:11] SPEAKER_B: Well, I'll say a couple of things.
[01:50:12] SPEAKER_B: If that didn't exist, what is that a map of?
[01:50:18] SPEAKER_B: If there is no space, if you don't want to call it a space,
[01:50:21] SPEAKER_B: that's okay.
[01:50:22] SPEAKER_B: But you can't get away from the fact that as a matter of
[01:50:25] SPEAKER_B: research, there are patterns that relate to each other in a
[01:50:28] SPEAKER_B: particular way.
[01:50:29] SPEAKER_B: What's the final step of calling it a space is
[01:50:34] SPEAKER_B: minimal. The bigger issue is what the hell is it a map of
[01:50:38] SPEAKER_B: then if it's not a space?
[01:50:39] SPEAKER_B: So that's the first thing.
[01:50:40] SPEAKER_B: Now, that's how it plays out, I think, in math and physics.
[01:50:43] SPEAKER_B: Now in biology, here's how we're going to know if this makes
[01:50:46] SPEAKER_B: any sense.
[01:50:47] SPEAKER_B: What we are doing now is trying to map out that space by
[01:50:51] SPEAKER_B: saying, look, we know that the frog genome maps to one
[01:50:57] SPEAKER_B: thing and that's a frog.
[01:50:58] SPEAKER_B: Turns out that exact same genome, if you just
[01:51:02] SPEAKER_B: take the slightest step with the exact same genome,
[01:51:04] SPEAKER_B: which just takes themselves out of their environment, they
[01:51:07] SPEAKER_B: can also make xenobots with very specific, different
[01:51:10] SPEAKER_B: transcriptomes, very specific behaviors, very specific
[01:51:13] SPEAKER_B: shapes.
[01:51:14] SPEAKER_B: It's not just, oh, well, you know, they do whatever, and
[01:51:16] SPEAKER_B: that they have very specific behaviors, just like the frog
[01:51:18] SPEAKER_B: had very specific properties.
[01:51:19] SPEAKER_B: We can start to map out what all those are, right?
[01:51:23] SPEAKER_B: Basically, try to draw the latent space from which those
[01:51:28] SPEAKER_B: things are pulled, and one of two things is going to happen
[01:51:31] SPEAKER_B: in the future.
[01:51:31] SPEAKER_B: And so this is, you know, come back in 20 years and we'll
[01:51:34] SPEAKER_B: see how this worked out.
[01:51:34] SPEAKER_B: One thing that could happen is that we're going to see, oh
[01:51:38] SPEAKER_B: yeah, just like the map of mathematics, we made a
[01:51:42] SPEAKER_B: map of the space, and we know now that if I want a system
[01:51:46] SPEAKER_B: that acts like this and this, here's the kind of body I need
[01:51:49] SPEAKER_B: to make forward because those are the patterns that exist.
[01:51:51] SPEAKER_B: The anthrobots have four different behaviors, not seven and
[01:51:54] SPEAKER_B: not one, and so that's what I can pull from.
[01:51:57] SPEAKER_B: These are the options I have.
[01:51:58] SPEAKER_B: Is it possible that there's varying degrees of grandeur
[01:52:03] SPEAKER_A: to the space that you're thinking about mapping? Meaning
[01:52:07] SPEAKER_A: it could be just like with the space of mathematics, might
[01:52:10] SPEAKER_A: it strictly be just the space of biology versus a space
[01:52:14] SPEAKER_A: of like minds, which feels like it could encompass a lot
[01:52:21] SPEAKER_A: more than just biology.
[01:52:25] Yeah, except that I don't see how it would be separate
[01:52:29] SPEAKER_B: because I'm not just talking about an anatomical shape
[01:52:34] SPEAKER_B: and transcriptional profile.
[01:52:35] SPEAKER_B: I'm also talking about behavioral competencies.
[01:52:38] SPEAKER_B: So when we make something and we find out that, okay, it
[01:52:41] SPEAKER_B: does habituation sensitization.
[01:52:43] SPEAKER_B: It does not do Pavlovian conditioning and it does do delayed
[01:52:46] SPEAKER_B: gratification and it doesn't have language.
[01:52:48] SPEAKER_B: That is a very specific cognitive profile.
[01:52:51] SPEAKER_B: That's a region of that space and there's another region
[01:52:53] SPEAKER_B: that looks different because I don't make a sharp distinction
[01:52:55] SPEAKER_B: between biology and cognition.
[01:52:58] SPEAKER_B: If you want to explain behaviors, they are drawn from some
[01:53:02] SPEAKER_B: distribution as well.
[01:53:03] SPEAKER_B: So I think in 20 years or however long it's going to take,
[01:53:06] SPEAKER_B: one of two things will happen. Either we and other people
[01:53:10] SPEAKER_B: who are working on this are going to actually produce a
[01:53:12] SPEAKER_B: map of that space and say, here's why you've gotten systems
[01:53:18] SPEAKER_B: that work like this and like this and like this, but you've
[01:53:20] SPEAKER_B: never seen any that work like that, right?
[01:53:22] SPEAKER_B: Or we're going to find out that I'm wrong and that basically
[01:53:27] SPEAKER_B: it's not worth calling it a space because it is so random
[01:53:31] SPEAKER_B: and so jumbled up that we've been able to make zero progress
[01:53:35] SPEAKER_B: in linking the embodiments that we make to the patterns
[01:53:38] SPEAKER_B: that come through.
[01:53:40] Yeah, just to be clear, I mean from your blog post on this
[01:53:44] SPEAKER_A: from the paper.
[01:53:45] SPEAKER_A: I mean, we're talking about a space that includes a lot
[01:53:47] SPEAKER_A: of stuff.
[01:53:48] SPEAKER_A: Yeah, yeah.
[01:53:48] SPEAKER_A: Includes human.
[01:53:51] SPEAKER_A: What is it?
[01:53:51] SPEAKER_A: Meditating?
[01:53:52] SPEAKER_A: Steve.
[01:53:53] SPEAKER_A: Hello.
[01:53:53] SPEAKER_A: My name is Steve.
[01:53:54] SPEAKER_A: AI systems.
[01:53:55] SPEAKER_A: So all the space of computational systems, objects, biological
[01:54:00] SPEAKER_A: systems, concepts, it includes everything.
[01:54:04] SPEAKER_A: Well, it includes specific patterns that we have given names
[01:54:08] SPEAKER_B: to.
[01:54:08] SPEAKER_B: Right.
[01:54:08] SPEAKER_B: Some of those patterns we've named mathematical objects.
[01:54:11] SPEAKER_B: Some of those patterns we made, we've named anatomical
[01:54:14] SPEAKER_B: outcomes.
[01:54:14] SPEAKER_B: Some of those patterns we've made psychological types.
[01:54:17] SPEAKER_B: So every entry in an encyclopedia, old-school Britannica
[01:54:22] SPEAKER_A: is a pointer to this space.
[01:54:27] SPEAKER_A: There is a set of things that I feel very strongly about
[01:54:30] SPEAKER_B: because the research is telling us that's what's going
[01:54:33] SPEAKER_B: on.
[01:54:33] SPEAKER_B: And then there's a bunch of other stuff that I see as
[01:54:37] SPEAKER_B: hypotheses for next steps that guide experiment.
[01:54:40] SPEAKER_B: So what I'm about to tell you, I don't, you know, these
[01:54:42] SPEAKER_B: are things I don't actually know.
[01:54:43] SPEAKER_B: These are just guesses that, that, you know, you need
[01:54:47] SPEAKER_B: to make some guesses to make progress.
[01:54:49] SPEAKER_B: I don't think that there are specific, or I don't know,
[01:54:52] SPEAKER_B: but it doesn't mean that there are going to be specific
[01:54:55] SPEAKER_B: latonic patterns for this is the Titanic and this is
[01:54:58] SPEAKER_B: the sister of the Titanic.
[01:55:00] SPEAKER_B: And this is some other kind of boat.
[01:55:01] SPEAKER_B: This is not what I'm saying.
[01:55:02] SPEAKER_B: What I'm saying is in some way that we absolutely need
[01:55:06] SPEAKER_B: to work out when we make minimal interfaces, we get
[01:55:10] SPEAKER_B: more than we put in.
[01:55:12] SPEAKER_B: We get behaviors, we get shapes, we get mathematical
[01:55:15] SPEAKER_B: truths, and we get all kinds of patterns that we did
[01:55:18] SPEAKER_B: not have to create.
[01:55:20] SPEAKER_B: We didn't micromanage them.
[01:55:21] SPEAKER_B: We didn't know they were coming.
[01:55:22] SPEAKER_B: We didn't have to put any effort into making them.
[01:55:25] SPEAKER_B: They come from some distribution that seems to exist
[01:55:28] SPEAKER_B: that we don't have to create.
[01:55:30] SPEAKER_B: And exactly whether that space is sparse or dense,
[01:55:33] SPEAKER_B: I don't know.
[01:55:34] SPEAKER_B: So for example, if there is some kind of a platonic
[01:55:39] SPEAKER_B: form for the movie, The Godfather, if it's surrounded
[01:55:42] SPEAKER_B: by a bunch of crappy versions and then crappier versions
[01:55:44] SPEAKER_B: still, I have no idea, right?
[01:55:45] SPEAKER_B: I don't know if the space is sparse or not.
[01:55:48] SPEAKER_B: I don't know if it's finite or infinite.
[01:55:51] SPEAKER_B: These are all things I don't know.
[01:55:52] SPEAKER_B: What I do know is that it seems like physics and for
[01:55:56] SPEAKER_B: sure, biology and cognition are the benefits of
[01:55:59] SPEAKER_B: ingressions that are free lunches in some sense.
[01:56:02] SPEAKER_B: We did not make them.
[01:56:03] SPEAKER_B: Calling them emergent does nothing for a research
[01:56:06] SPEAKER_B: program.
[01:56:07] SPEAKER_B: Okay, that just means you got surprised.
[01:56:09] SPEAKER_B: I think it's much better if you say, if you make
[01:56:12] SPEAKER_B: the optimistic assumption that they come from a
[01:56:14] SPEAKER_B: structured space that we have a prayer in hell of
[01:56:17] SPEAKER_B: actually exploring and in some decades, if I'm wrong
[01:56:20] SPEAKER_B: and it says, you know what we tried, it looks like
[01:56:22] SPEAKER_B: it really is random too bad.
[01:56:24] SPEAKER_B: Fine.
[01:56:24] SPEAKER_B: Is there a difference between like, can we one day
[01:56:27] SPEAKER_A: prove the existence of this world?
[01:56:30] SPEAKER_A: And is there a difference between it being a really
[01:56:34] SPEAKER_A: effective model for connecting things, explaining
[01:56:40] SPEAKER_A: things versus like an actual place where the information
[01:56:46] SPEAKER_A: about these distributions that we're sampling actually
[01:56:48] SPEAKER_A: exists that we can hit with a stick.
[01:56:52] SPEAKER_A: You can, yeah, you can, you can try to make that
[01:56:54] SPEAKER_B: distinction.
[01:56:55] SPEAKER_B: Yeah, but I think, I think modern cognitive neuroscience
[01:56:59] SPEAKER_B: will tell you that whatever you think this is at
[01:57:03] SPEAKER_B: most, it is a very effective model for predicting
[01:57:07] SPEAKER_B: the future experiences you're going to have.
[01:57:09] SPEAKER_B: So all of this that we think about as physical
[01:57:10] SPEAKER_A: reality is just as a nice convenient model.
[01:57:13] SPEAKER_A: I mean, that's not me.
[01:57:14] SPEAKER_B: That's predictive processing and active inference.
[01:57:16] SPEAKER_B: Like that's modern neuroscience telling you this
[01:57:18] SPEAKER_B: that that this isn't anything that I, that I'm
[01:57:20] SPEAKER_B: particularly coming up with.
[01:57:21] SPEAKER_B: All I'm saying is the distinction, the distinction
[01:57:25] SPEAKER_B: you're trying to make, which is like an old school,
[01:57:26] SPEAKER_B: like realist, you know, kind of view that is it, is
[01:57:30] SPEAKER_B: it, is it, is it metaphorical or is it real?
[01:57:33] SPEAKER_B: All we have in science are metaphors, I think.
[01:57:36] SPEAKER_B: And the only question is how good are your metaphors?
[01:57:38] SPEAKER_B: And I think as agents act, living in a world, all we
[01:57:42] SPEAKER_B: have are models of what we are and what the outside
[01:57:45] SPEAKER_B: world is.
[01:57:46] SPEAKER_B: That's it.
[01:57:47] SPEAKER_B: And the question is, how good is it a model?
[01:57:49] SPEAKER_B: And, and, and my claim about this is in some small
[01:57:52] SPEAKER_B: number of decades, either this will either give
[01:57:54] SPEAKER_B: rise to a very enabling mapping of the space for,
[01:57:58] SPEAKER_B: for AI, for bioengineering, for, you know, biology,
[01:58:02] SPEAKER_B: whatever, or we're going to find out that it really
[01:58:06] SPEAKER_B: sucks because it really is a random grab bag of
[01:58:08] SPEAKER_B: stuff.
[01:58:08] SPEAKER_B: And we tried the optimistic research program.
[01:58:11] SPEAKER_B: It failed and we're just gonna have to live with
[01:58:12] SPEAKER_B: surprise.
[01:58:13] SPEAKER_B: I mean, I doubt that's going to happen, but it's
[01:58:15] SPEAKER_B: a possible outcome.
[01:58:16] SPEAKER_B: But do you think it's, there is some place where
[01:58:18] SPEAKER_A: the information is stored about these distributions
[01:58:23] SPEAKER_A: that were being sampled through the thin interfaces?
[01:58:26] SPEAKER_A: Like actual place?
[01:58:28] Place is weird because it isn't the same as our
[01:58:30] SPEAKER_B: physical space-time.
[01:58:32] SPEAKER_B: Okay.
[01:58:32] SPEAKER_B: I don't think it's that.
[01:58:33] SPEAKER_B: So calling it a place is a little, a little weird.
[01:58:35] SPEAKER_B: No, but like physics, general relativity describes
[01:58:39] SPEAKER_A: a space-time.
[01:58:40] SPEAKER_A: Could other physics theories be able to describe
[01:58:43] SPEAKER_A: this other space where information is stored that
[01:58:47] SPEAKER_A: we can apply, maybe different, but in the same
[01:58:50] SPEAKER_A: spirit, laws about information?
[01:58:53] I definitely think they're going to be systematic
[01:58:55] SPEAKER_B: laws.
[01:58:56] SPEAKER_B: I don't think they're going to look anything like
[01:58:57] SPEAKER_B: physics.
[01:58:58] SPEAKER_B: You can call it physics if you want, but I think
[01:59:00] SPEAKER_B: it's going to be so different than that probably
[01:59:02] SPEAKER_B: just, you know, cracks the word.
[01:59:05] SPEAKER_B: And whether information is going to survive that,
[01:59:09] SPEAKER_B: I'm not sure, but I, but I definitely think that
[01:59:11] SPEAKER_B: it's going to be, there are going to be laws, but
[01:59:14] SPEAKER_B: I think they're going to look a lot more like
[01:59:18] SPEAKER_B: aspects of, of, of psychology and cognitive science
[01:59:21] SPEAKER_B: than they're going to look like physics.
[01:59:22] SPEAKER_B: That's my guess.
[01:59:23] SPEAKER_B: So what does it look like to prove that world
[01:59:25] SPEAKER_A: exists?
[01:59:26] SPEAKER_A: What it looks like is a successful research
[01:59:29] SPEAKER_B: program that explains how you pull particular
[01:59:33] SPEAKER_B: patterns when you need them and why some patterns
[01:59:36] SPEAKER_B: come and others don't and show that they come
[01:59:38] SPEAKER_B: from an ordered space.
[01:59:40] SPEAKER_A: Across a large number of organisms?
[01:59:43] SPEAKER_A: Well, it's not just organisms.
[01:59:44] SPEAKER_B: I mean, I think, I think it's going to end up
[01:59:46] SPEAKER_B: and I mean, you can talk to the machine learning
[01:59:47] SPEAKER_B: people about how they got to this point again,
[01:59:50] SPEAKER_B: because this is, this is not just me.
[01:59:51] SPEAKER_B: There's a bunch of, there are a bunch of different
[01:59:53] SPEAKER_B: disciplines that are converging on this now
[01:59:54] SPEAKER_B: simultaneously.
[01:59:56] SPEAKER_B: You're going to, you're going to find, again,
[01:59:59] SPEAKER_B: just like in mathematics.
[02:00:00] SPEAKER_B: where from, from, from, from different directions, everybody sort of is looking
[02:00:04] SPEAKER_B: at different things.
[02:00:04] SPEAKER_B: Oh my God, this is one underlying structure that seems still I can
[02:00:07] SPEAKER_B: inform all of this.
[02:00:08] SPEAKER_B: Uh, so in, in physics, in mathematics, in, uh, computer science, machine
[02:00:13] SPEAKER_B: learning, possibly in economics, uh, certainly in biology, possibly in, you
[02:00:17] SPEAKER_B: know, cognitive science, we're going to find these structures.
[02:00:20] SPEAKER_B: It was already obvious in Pythagoras this time that, that there are these
[02:00:24] SPEAKER_B: patterns, the only remaining question is, are they part of an ordered structured
[02:00:30] SPEAKER_B: or, you know, um, uh, space and are we up to the task of mapping out the
[02:00:35] SPEAKER_B: relationship between what we build and the patterns that come through it.
[02:00:39] So from the machine learning perspective, is it then the case that, uh, even
[02:00:45] SPEAKER_A: something as simple as LLMs are sneaking up onto this world, that the representations
[02:00:51] SPEAKER_A: that they form are sneaking up to it?
[02:00:54] Well, when I I've given, I've given this talk to, to, to some audiences and
[02:00:57] SPEAKER_B: especially in the organicist community, people like the first part where it's
[02:01:04] SPEAKER_B: like, okay, now there's an idea for what the magic quote unquote is that's, uh,
[02:01:09] SPEAKER_B: that's special about the living things.
[02:01:11] SPEAKER_B: And then so on now, now, if we could just stop there, we would have dumb machines
[02:01:17] SPEAKER_B: that just do what the algorithm says.
[02:01:19] SPEAKER_B: And we have these magical living interfaces that can be the recipient
[02:01:23] SPEAKER_B: for these ingressions.
[02:01:24] SPEAKER_B: Cool.
[02:01:24] SPEAKER_B: Right.
[02:01:24] SPEAKER_B: We can cut up the world in this way.
[02:01:26] SPEAKER_B: Uh, unfortunately, or, or fortunately, um, I think that's not the case.
[02:01:31] SPEAKER_B: And I think that even, even simple, uh, minimal computational models are to some
[02:01:38] SPEAKER_B: extent beneficiaries of these free lunches.
[02:01:41] SPEAKER_B: I think that, um, the theories we have, and this, this goes back to the, to the
[02:01:47] SPEAKER_B: thin client interface kind of idea, the theories we have of both of physics and
[02:01:54] SPEAKER_B: computation.
[02:01:54] SPEAKER_B: So theory of algorithms, you know, Turing machines, all that, all that good stuff.
[02:01:58] SPEAKER_B: Those are all good theories of the front end interface.
[02:02:02] SPEAKER_B: And they're not complete theories of the whole thing.
[02:02:04] SPEAKER_B: They capture the front end, which is why they get surprised, which is why these
[02:02:08] SPEAKER_B: things are surprising when they happen.
[02:02:10] SPEAKER_B: I think that when we see embryos of different species, we are pulling from
[02:02:14] SPEAKER_B: well-trodden familiar regions of that space.
[02:02:17] SPEAKER_B: And we know what to expect frog, you know, snake, whatever.
[02:02:21] SPEAKER_B: When we make cyborgs and hybrids and biobots, we are pulling from new regions
[02:02:28] SPEAKER_B: of that space that look a little weird and they're unexpected, but you know, we
[02:02:32] SPEAKER_B: can still kind of get our, get our mind around them.
[02:02:34] SPEAKER_B: When we start making AI's like proper AI's, we are now fishing in a region of
[02:02:40] SPEAKER_B: that space that we may, that may never have had bodies before it may have
[02:02:43] SPEAKER_B: never been embodied before.
[02:02:45] SPEAKER_B: And what we get from that is going to be extremely surprising.
[02:02:50] SPEAKER_B: And the final thing I just to mention on that is that because of this, because of
[02:02:56] SPEAKER_B: the inputs from this platonic space, some of the really interesting things that
[02:03:00] SPEAKER_B: artificial constructs can do are not because of the algorithm they're in
[02:03:05] SPEAKER_B: spite of the algorithm, they are filling up the spaces in between there's what
[02:03:09] SPEAKER_B: the algorithm is forcing you to do.
[02:03:10] SPEAKER_B: And then there's the other cool stuff it's doing, which
[02:03:12] SPEAKER_B: is nowhere in the algorithm.
[02:03:14] SPEAKER_B: And if that's true, and we think it's true, even a very minimal systems,
[02:03:19] SPEAKER_B: then this whole business of, of, um, of language models and AI's in general.
[02:03:24] SPEAKER_B: Watching the language part, maybe a total red herring because the
[02:03:27] SPEAKER_B: language is what we force them to do.
[02:03:29] SPEAKER_B: The question is what, what, what else are they doing that we are not,
[02:03:33] SPEAKER_B: we are not good at noticing.
[02:03:35] SPEAKER_B: And this is, you know, this, this, this is something that we are, I think, um,
[02:03:39] SPEAKER_B: as a, as a kind of a existential, um, step for humanity is to, is to become
[02:03:45] SPEAKER_B: better at this because we are not good at recognizing these things now.
[02:03:49] SPEAKER_A: You got to tell me more about, uh, this behavior that is observable, that
[02:03:54] SPEAKER_A: is unrelated to the explicitly stated goal of a particular algorithm.
[02:03:59] SPEAKER_A: So you looked at a simple algorithm of, uh, sorting.
[02:04:02] SPEAKER_A: Can you explain what was done?
[02:04:04] SPEAKER_A: Sure.
[02:04:05] SPEAKER_B: First, just the goal of the study, there are two things
[02:04:07] SPEAKER_B: that people generally assume.
[02:04:08] SPEAKER_B: One is that we have a pretty good intuition about what kind of
[02:04:13] SPEAKER_B: systems are going to have competencies.
[02:04:15] SPEAKER_B: So from observing biologicals, we're not terribly surprised when
[02:04:19] SPEAKER_B: biology does interesting things.
[02:04:20] SPEAKER_B: Everybody always says, well, it's biology.
[02:04:22] SPEAKER_B: And of course it does all this cool stuff.
[02:04:25] SPEAKER_B: And yeah, but, but do we have these machines and the whole point of having
[02:04:28] SPEAKER_B: machines and dumb and algorithms and so on as they do exactly what you tell them
[02:04:32] SPEAKER_B: to do, right, and people feel pretty strongly that that's a binary distinction
[02:04:36] SPEAKER_B: and that that's what, uh, that's, we can carve up the world in that way.
[02:04:40] SPEAKER_B: So I wanted to do two things.
[02:04:42] SPEAKER_B: I wanted to, first of all, explore that and hopefully break the assumption
[02:04:46] SPEAKER_B: that we're good at seeing this because I think we're not, and I think it's
[02:04:49] SPEAKER_B: extremely important that we understand very soon that, uh, we need to get much
[02:04:53] SPEAKER_B: better at, uh, at, at knowing when to, uh, when to expect these things.
[02:04:58] SPEAKER_B: And the other thing I wanted to do was to find out, uh, you know, most, mostly
[02:05:03] SPEAKER_B: people assume that you need a lot of complexity for this.
[02:05:05] SPEAKER_B: So when somebody says, well, the capabilities of my mind are not, uh,
[02:05:12] SPEAKER_B: properly, um, encompassed by the rules of biochemistry, everybody's like, yeah,
[02:05:16] SPEAKER_B: that makes sense where, you know, you're very complex and okay, you're, you know,
[02:05:19] SPEAKER_B: your mind does things that, that you can't, you couldn't, you didn't see that
[02:05:22] SPEAKER_B: coming from the rules of biochemistry, right?
[02:05:24] SPEAKER_B: Like we, we know that.
[02:05:25] SPEAKER_B: Um, so mostly people think that has to do with complexity.
[02:05:28] SPEAKER_B: And, and what I would like to find out is as, as part of understanding what kind
[02:05:32] SPEAKER_B: of interfaces give rise to what kind of ingressions, is it really about complexity?
[02:05:36] SPEAKER_B: How much complexity do you actually need?
[02:05:38] SPEAKER_B: Is there some threshold after which this happens?
[02:05:41] SPEAKER_B: Is it really specific materials?
[02:05:42] SPEAKER_B: Is it biologicals?
[02:05:43] SPEAKER_B: Is it something about evolution?
[02:05:45] SPEAKER_B: Like, what is it about these kinds of things that allows this, this, this
[02:05:49] SPEAKER_B: surprise, right, allows this idea that we are more than the sum of our parts.
[02:05:52] SPEAKER_B: And so, and, and, and I had a strong intuition that none of those things are
[02:05:56] SPEAKER_B: actually required, that this is this kind of magic, so to speak seeps
[02:06:00] SPEAKER_B: into pretty much everything.
[02:06:03] SPEAKER_B: And, uh, and so to, to look at that, I wanted also to, uh, have an example
[02:06:07] SPEAKER_B: that had significant shock value because the thing with biology is.
[02:06:13] SPEAKER_B: There's always more mechanism to be discovered, right?
[02:06:15] SPEAKER_B: Like there's infinite depth of what the materials are doing, what the, you
[02:06:18] SPEAKER_B: know, somebody will always say, well, there's a, there's a mechanism,
[02:06:19] SPEAKER_B: but I just haven't found it yet.
[02:06:21] SPEAKER_B: So I wanted an example that was simple, transparent, so you could see all the
[02:06:25] SPEAKER_B: stuff, there was nowhere to hide.
[02:06:26] SPEAKER_B: I wanted it to be deterministic because I don't want it to be something around
[02:06:29] SPEAKER_B: unpredictability or stochasticity.
[02:06:32] SPEAKER_B: And, uh, and I want it to be something familiar to people, minimal.
[02:06:35] SPEAKER_B: And I wanted to use it as a model system for honing our abilities to take a new
[02:06:40] SPEAKER_B: system and looking at it with fresh eyes.
[02:06:42] SPEAKER_B: And that's because the sorting algorithms have been studied for over 60 years.
[02:06:47] SPEAKER_B: We all think we know what they do and what their properties are.
[02:06:50] SPEAKER_B: The algorithm itself is just a few lines of code.
[02:06:52] SPEAKER_B: You know, you can, you can see exactly what's there.
[02:06:55] SPEAKER_B: It's deterministic.
[02:06:56] SPEAKER_B: And that's, that's, so that, that's why, that's why, right.
[02:06:58] SPEAKER_B: I wanted, I wanted the most shock value out of a system like that.
[02:07:01] SPEAKER_B: If we were to find anything and to use it as an example of taking something
[02:07:05] SPEAKER_B: minimal and, and, and seeing what can be gotten out of it.
[02:07:08] SPEAKER_B: So I'll describe two interesting things about it.
[02:07:10] SPEAKER_B: And then we have lots of other work coming, uh, in the next, in the next
[02:07:14] SPEAKER_B: year, but even simpler systems.
[02:07:16] SPEAKER_B: I mean, it's actually crazy.
[02:07:18] SPEAKER_B: Um, so the, so the very first thing is this, the standard sorting.
[02:07:22] SPEAKER_B: So let's, let's take bubble sort, right.
[02:07:24] SPEAKER_B: And, and, and all these sorting algorithms, you know, what you're starting out with
[02:07:27] SPEAKER_B: is an array of jumbled up digits.
[02:07:30] SPEAKER_B: Okay.
[02:07:30] SPEAKER_B: So integers, it's an array of mixed up integers.
[02:07:33] SPEAKER_B: And what the algorithm is designed to do is to eventually
[02:07:38] SPEAKER_B: arrange them all into order.
[02:07:40] SPEAKER_B: And what it does generally is compare some pieces of that array and based on
[02:07:44] SPEAKER_B: which one is larger than which it swaps them around, and you can imagine that if
[02:07:46] SPEAKER_B: you just keep doing that and you just keep comparing and swapping, then
[02:07:49] SPEAKER_B: eventually you can get all the digits in the same order.
[02:07:52] SPEAKER_B: So the first thing I decided to do, and this is, uh, this is the work of my
[02:07:56] SPEAKER_B: student Tianning Zhang and then Adam Goldstein on this paper, this goes back
[02:08:00] SPEAKER_B: to our original discussion about putting a barrier between it and its goals.
[02:08:04] SPEAKER_B: And the first thing I said, okay, how do, how do we put a barrier in?
[02:08:06] SPEAKER_B: Well, how about this?
[02:08:07] SPEAKER_B: The traditional algorithm assumes that the hardware is working correctly.
[02:08:13] SPEAKER_B: So if you have a seven and then the five, and you tell them to swap the, the lines
[02:08:18] SPEAKER_B: of swap, the swap, the five and the seven, and then you go on, you never check.
[02:08:23] SPEAKER_B: Did it swap?
[02:08:23] SPEAKER_B: Because you assume that, that, that it's reliable hardware.
[02:08:27] SPEAKER_B: Okay.
[02:08:28] SPEAKER_B: So what we decided to do was to break one of the digits so that it doesn't move.
[02:08:32] SPEAKER_B: When you tell it to move, it doesn't move.
[02:08:35] SPEAKER_B: We don't change the algorithm.
[02:08:36] SPEAKER_B: That's really key.
[02:08:37] SPEAKER_B: We do not put anything new in the algorithm that says, what do you do
[02:08:40] SPEAKER_B: if the damn thing didn't move?
[02:08:41] SPEAKER_B: Okay.
[02:08:42] SPEAKER_B: Just run it exactly the same way.
[02:08:44] SPEAKER_B: What happens?
[02:08:45] SPEAKER_B: Turns out something very interesting happens.
[02:08:48] SPEAKER_B: It still works.
[02:08:49] SPEAKER_B: It's still, so it's still sorted.
[02:08:52] SPEAKER_B: Uh, but it, it eventually sorts, sorts it by moving all the stuff around the broken
[02:08:58] SPEAKER_B: number.
[02:08:59] SPEAKER_B: Okay.
[02:08:59] SPEAKER_B: And that makes sense.
[02:09:00] SPEAKER_B: But here's something interesting.
[02:09:01] SPEAKER_B: Suppose we, suppose we plot at any given moment, we plot the degree of sortedness
[02:09:06] SPEAKER_B: of the string as a function of time.
[02:09:08] SPEAKER_B: If you run the normal algorithm and sort it, it's guaranteed to get where it's
[02:09:13] SPEAKER_B: going, that's the, you know, it's gotta, it's gotta sort and it will always reach
[02:09:16] SPEAKER_B: the end, but when it encounters one of the broken digits.
[02:09:19] SPEAKER_B: What happens is the actual sortedness goes down in order to then recoup
[02:09:26] SPEAKER_B: and get better order later.
[02:09:29] SPEAKER_B: What it's able to do is to go against the thing that it's trying to do, to go around
[02:09:35] SPEAKER_B: in order to meet its goal later on.
[02:09:38] SPEAKER_B: Now, if I didn't, if, if, if I showed this to a behavior scientist and I didn't tell
[02:09:42] SPEAKER_B: them what this, what, what system was doing is they will say, well, we know what this
[02:09:45] SPEAKER_B: is, this is delayed gratification.
[02:09:47] SPEAKER_B: This is the ability of a system to go against its gradient
[02:09:51] SPEAKER_B: and get what it needs to do.
[02:09:52] SPEAKER_B: Now, imagine two magnets.
[02:09:54] SPEAKER_B: Imagine you take two magnets and you put a piece of wood between them.
[02:09:56] SPEAKER_B: And they're like this, what the magnet is not going to do is to.
[02:10:00] SPEAKER_B: go around the barrier and get to its goal.
[02:10:02] SPEAKER_B: The two, they're not smart enough to go against their gradient.
[02:10:05] SPEAKER_B: They're just going to keep doing this.
[02:10:06] SPEAKER_B: Some animals are smart enough, right?
[02:10:08] SPEAKER_B: They'll go around and the sorting algorithm is smart enough to do that.
[02:10:13] SPEAKER_B: But the trick is there are no steps in the algorithm for doing that.
[02:10:17] SPEAKER_B: You could stare at our algorithm all day long.
[02:10:19] SPEAKER_B: You would not see that this thing can do delay gratification.
[02:10:22] SPEAKER_B: It isn't there.
[02:10:23] SPEAKER_B: Now there's two ways to look at this.
[02:10:24] SPEAKER_B: On the one hand, you could say, so the reductionist physics approach, you
[02:10:27] SPEAKER_B: could say, did it, did it follow all the steps in the algorithms?
[02:10:31] SPEAKER_B: Yeah, it did.
[02:10:32] SPEAKER_B: Well then, uh, there's nothing to see here.
[02:10:35] SPEAKER_B: There's no magic.
[02:10:36] SPEAKER_B: This is, you know, it does what it does.
[02:10:38] SPEAKER_B: It didn't, it didn't disobey the algorithm.
[02:10:40] SPEAKER_B: Right.
[02:10:40] SPEAKER_B: I'm not claiming that this is a miracle.
[02:10:42] SPEAKER_B: I'm not saying it disobeys the algorithm.
[02:10:44] SPEAKER_B: I'm saying it's not failing to sort.
[02:10:46] SPEAKER_B: I'm saying it's not doing some sort of, you know, crazy quantum thing.
[02:10:49] SPEAKER_B: Not saying any of that.
[02:10:50] SPEAKER_B: What I'm saying is other people might call it emergent.
[02:10:53] SPEAKER_B: What it has is our properties that are not complexity, not unpredictability,
[02:10:58] SPEAKER_B: not perverse instantiation, as in sometimes in a life, what it has are
[02:11:03] SPEAKER_B: unexpected competencies recognizable by behavioral scientists, meaning,
[02:11:07] SPEAKER_B: meaning different types of cognition.
[02:11:10] SPEAKER_B: Primitive.
[02:11:10] SPEAKER_B: Well, we want it primitive.
[02:11:11] SPEAKER_B: So there you go.
[02:11:12] SPEAKER_B: It's simple, uh, that you didn't have to code into the algorithm.
[02:11:16] SPEAKER_B: That's very important.
[02:11:17] SPEAKER_B: You get more than you start with.
[02:11:19] SPEAKER_B: You then, then you put it and you didn't have to do that.
[02:11:21] SPEAKER_B: You get these surprising behavioral competencies, not just complexity.
[02:11:25] SPEAKER_B: That's the first thing.
[02:11:26] SPEAKER_B: The second thing, which, which is also crazy, but it requires a little
[02:11:30] SPEAKER_B: bit of a little bit of explanation.
[02:11:32] SPEAKER_B: The second thing that we said is, okay, what if instead of in the typical
[02:11:36] SPEAKER_B: sorting algorithm, you have a single controller top down, I'm, I'm sort of
[02:11:40] SPEAKER_B: godlike looking down at the numbers and I'm swapping them according to the
[02:11:42] SPEAKER_B: algorithm, what if, and this goes back to actually the title of the paper talks
[02:11:46] SPEAKER_B: about agential data, self-sorting algorithms.
[02:11:49] SPEAKER_B: This is back to like, what's, who's the pattern and who's the agent, right?
[02:11:52] SPEAKER_B: He said, what if we give the numbers a little bit of agency?
[02:11:54] SPEAKER_B: Here's what we're going to, we're not going to have any kind of top down sort.
[02:11:57] SPEAKER_B: Every single number knows the algorithm and he's just going to
[02:12:01] SPEAKER_B: do whatever the algorithm says.
[02:12:02] SPEAKER_B: So if I'm a five, I'm just going to execute the algorithm and the algorithm
[02:12:08] SPEAKER_B: will try to make sure that to my right is the six and to my left is a four.
[02:12:11] SPEAKER_B: That's it.
[02:12:11] SPEAKER_B: That's it.
[02:12:12] SPEAKER_B: So every digit is, so it's like a distributed as you know, it's like an ant colony.
[02:12:15] SPEAKER_B: There is no central planner.
[02:12:17] SPEAKER_B: Everybody just does their own algorithm.
[02:12:19] SPEAKER_B: Okay.
[02:12:19] SPEAKER_B: We're just going to do that once you've done that.
[02:12:21] SPEAKER_B: And by the way, one of the values of doing that is that you can simulate
[02:12:24] SPEAKER_B: biological processes because in biology, you know, if I have like a frog face and
[02:12:29] SPEAKER_B: I scramble it with all the different organs, every, every tissue is going to
[02:12:33] SPEAKER_B: rearrange itself so that ultimately you have, you know, nose, eyes, head, you
[02:12:35] SPEAKER_B: know, you're going to have an order, right?
[02:12:36] SPEAKER_B: So you can do that, but, um, okay, fine, but you can do something else.
[02:12:40] SPEAKER_B: Cool.
[02:12:40] SPEAKER_B: Once you've done that, you can do something cool that you can't
[02:12:42] SPEAKER_B: do with a standard algorithm.
[02:12:44] SPEAKER_B: You can make a chimeric algorithm.
[02:12:46] SPEAKER_B: What I mean is not all the cells have to follow the same algorithm.
[02:12:49] SPEAKER_B: Some of them might follow bubble sorts.
[02:12:51] SPEAKER_B: Some of them might follow selection sort.
[02:12:52] SPEAKER_B: It's like in biology.
[02:12:53] SPEAKER_B: What we do when we make chimeras, we make frog a lottles.
[02:12:56] SPEAKER_B: So frog a lottles have some frog cells.
[02:12:58] SPEAKER_B: They have some axolotl cells.
[02:12:59] SPEAKER_B: What is that going to look like?
[02:13:00] SPEAKER_B: Does anybody know what a frog a lottle is going to look like?
[02:13:03] SPEAKER_B: It's actually really interesting that despite all the genetics and the, and
[02:13:06] SPEAKER_B: the developmental biology, you have the genomes, you have the frog genome,
[02:13:09] SPEAKER_B: you have the axolotl genome.
[02:13:10] SPEAKER_B: Nobody can tell you what a frog a lottle is going to look like.
[02:13:12] SPEAKER_B: Even though you have, you have, this is, this is the, this is the back to your
[02:13:16] SPEAKER_B: question about physics and chemistry.
[02:13:17] SPEAKER_B: Like, yeah, you can know everything there is to know about how, you know, how the
[02:13:20] SPEAKER_B: physics and the, and the genetics work, but the decision making, right, is like
[02:13:25] SPEAKER_B: baby, baby axolotls have legs, tadpoles don't have legs.
[02:13:28] SPEAKER_B: So frog a lottle going to have legs, right?
[02:13:30] SPEAKER_B: Can you predict that from, from understanding the physics
[02:13:33] SPEAKER_B: of transcription and all of that?
[02:13:34] SPEAKER_B: Anyway, so, so we made some, so, so you see, you see, this is like an
[02:13:39] SPEAKER_B: intersection of biology, physics, cognition.
[02:13:41] SPEAKER_B: So we made chimeric algorithms and we said, okay, half the digits
[02:13:45] SPEAKER_B: randomly, we assign them randomly.
[02:13:47] SPEAKER_B: So half the digits are randomly doing bubble sort, half the digits are
[02:13:49] SPEAKER_B: randomly doing, I don't know, selection sort or something.
[02:13:51] SPEAKER_B: But that once you choose bubble sort, that digit is sticking with bubble sort.
[02:13:55] SPEAKER_A: It's sticking.
[02:13:56] SPEAKER_A: We haven't done the thing where they can swip, swap between, no,
[02:13:59] SPEAKER_B: but they're, they're, they're sticking to it, right?
[02:14:00] SPEAKER_B: You label them and they're sticking to it.
[02:14:02] SPEAKER_B: The first thing we learned is that, the first thing we learned is that
[02:14:05] SPEAKER_B: distributed sorting still works.
[02:14:06] SPEAKER_B: It's amazing.
[02:14:07] SPEAKER_B: You don't need a central planner when, when every, when every number is doing
[02:14:10] SPEAKER_B: its whole thing, still gets sorted.
[02:14:11] SPEAKER_B: That's cool.
[02:14:12] SPEAKER_B: The second thing we found is that when you make a chimeric algorithm,
[02:14:16] SPEAKER_B: where actually the algorithms are not even matching, that works too.
[02:14:20] SPEAKER_B: This is, the thing still gets sorted.
[02:14:22] SPEAKER_B: That's cool.
[02:14:23] SPEAKER_B: But the most amazing thing is when we looked at something that
[02:14:26] SPEAKER_B: had nothing to do with sorting.
[02:14:27] SPEAKER_B: And that is, we asked the following question, we defined, Adam Goldstein
[02:14:31] SPEAKER_B: actually named this property and I think it's, it's well-named, we define
[02:14:33] SPEAKER_B: the algo type of a single cell.
[02:14:35] SPEAKER_B: It's not the genotype, it's not the phenotype, it's the algo type.
[02:14:38] SPEAKER_B: The algo type is simply this, what algorithm are you following?
[02:14:40] SPEAKER_B: Which one are you?
[02:14:41] SPEAKER_B: Are you a, are you a selection sort or bubble sort, right?
[02:14:43] SPEAKER_B: That's it, there's two algo types.
[02:14:45] SPEAKER_B: And we simply asked the following question, during that process of sorting,
[02:14:51] SPEAKER_B: what are the odds that whatever algo type you are, the guys
[02:14:54] SPEAKER_B: next to you are your same type?
[02:14:58] SPEAKER_B: It's, it's not the same as asking how the numbers are sorted, because it's
[02:15:00] SPEAKER_B: got nothing to do with the numbers.
[02:15:01] SPEAKER_B: It's actually, it's just whatever type you are.
[02:15:03] SPEAKER_B: It's more about clustering than sorting.
[02:15:05] SPEAKER_A: Clustering.
[02:15:05] SPEAKER_A: Well, that's exactly what we call it.
[02:15:06] SPEAKER_B: We call it clustering.
[02:15:07] SPEAKER_B: And at first, so, so now think of what happens and that's, and you can
[02:15:10] SPEAKER_B: see this on that graph, it's the red.
[02:15:12] SPEAKER_B: You start off, the clustering is at 50% because as I told you, we
[02:15:16] SPEAKER_B: assign the algo types randomly.
[02:15:18] SPEAKER_B: So the odds that the guy next to you is the same as you is half, 50%, right?
[02:15:21] SPEAKER_B: There's only two algo types.
[02:15:23] SPEAKER_B: In the end, it is also 50% because the thing that dominates is actually
[02:15:28] SPEAKER_B: the sorting algorithm and the sorting algorithm doesn't care what type you are.
[02:15:31] SPEAKER_B: You've got to get the numbers in order.
[02:15:32] SPEAKER_B: So by the time you're done, you're back to random algo types because, because
[02:15:36] SPEAKER_B: you have to get the numbers sorted.
[02:15:39] SPEAKER_B: But in between, in between you get some amount of increased, very significant.
[02:15:45] SPEAKER_B: Cause look at, look at the control is in the middle.
[02:15:47] SPEAKER_B: The pink is in the middle.
[02:15:48] SPEAKER_B: Uh, in, in between you get significant amounts of clustering, meaning that
[02:15:53] SPEAKER_B: certain algo types like to hang out with their buddies for as long as they can.
[02:15:56] SPEAKER_B: Now, now, now here's, here's the one more thing and then I'll kind of give
[02:15:59] SPEAKER_B: a philosophical significance of this.
[02:16:02] SPEAKER_B: And so we saw this and I said, that's nuts because the algorithm doesn't
[02:16:06] SPEAKER_B: have any provisions for asking what algo type am I, what algo type is my, is my
[02:16:11] SPEAKER_B: neighbor, if we're not the same, I'm going to move to be next to, like, if
[02:16:13] SPEAKER_B: you wanted to implement this, you would have to write a whole bunch of extra
[02:16:16] SPEAKER_B: steps that would have to be a whole bunch of observations that you would
[02:16:19] SPEAKER_B: have to take of your neighbor to see how he's acting, then you would
[02:16:21] SPEAKER_B: infer what algo type he is.
[02:16:23] SPEAKER_B: Then you would go stand next to the one that seems to have the same algo type as
[02:16:26] SPEAKER_B: you, you would have to take a bunch of measurements to say, wait, is that guy
[02:16:29] SPEAKER_B: doing bubbles or is he doing selection?
[02:16:30] SPEAKER_B: Sorry.
[02:16:31] SPEAKER_B: Like if you want to implement this, it's a whole bunch of algorithmic steps.
[02:16:34] SPEAKER_B: None of that exists in our algorithm.
[02:16:36] SPEAKER_B: You don't have any way of knowing what algo type you are or what anybody else is.
[02:16:39] SPEAKER_B: Okay.
[02:16:39] SPEAKER_B: We didn't have to pay for that at all.
[02:16:41] SPEAKER_B: So notice, notice a couple of interesting things.
[02:16:43] SPEAKER_B: The first interesting thing is that this was not at all obvious from the,
[02:16:48] SPEAKER_B: uh, from the algorithm itself.
[02:16:50] SPEAKER_B: Algorithm doesn't say anything about algo types.
[02:16:52] SPEAKER_B: Second thing is we paid computationally for all the steps needed to have the
[02:16:57] SPEAKER_B: numbers sorted, right?
[02:16:58] SPEAKER_B: Because we know, you know, you pay, you pay for certain computation costs.
[02:17:02] SPEAKER_B: The clustering was free.
[02:17:04] SPEAKER_B: We didn't pay for that at all.
[02:17:05] SPEAKER_B: There were no extra steps.
[02:17:07] SPEAKER_B: So this gets back to your other question of how do we know there's a platonic
[02:17:10] SPEAKER_B: space, and this is kind of like one of the craziest things that we're doing.
[02:17:12] SPEAKER_B: I actually suspect we can get free compute out of it.
[02:17:15] SPEAKER_B: I suspect that one of the things that we can do here is use these
[02:17:19] SPEAKER_B: ingressions in a useful way that don't require you to pay costs to pay physical
[02:17:23] SPEAKER_B: costs, right?
[02:17:24] SPEAKER_B: Like we, we know every, every bit has a, has an energy cost that you have to get.
[02:17:28] SPEAKER_B: The clustering was free.
[02:17:29] SPEAKER_B: Nothing, nothing extra was done.
[02:17:30] SPEAKER_B: Yeah.
[02:17:31] SPEAKER_A: Just, uh, the, this plot for people who are just listening on the X axis is the
[02:17:34] SPEAKER_A: percentage of completion of the sorting process and the Y axis is the sortedness
[02:17:42] SPEAKER_A: of the list of numbers, and then also in the red line is basically the
[02:17:48] SPEAKER_A: degree to which they're clustered.
[02:17:50] SPEAKER_A: And, uh, you're saying that there's this unexpected competence of clustering.
[02:17:58] SPEAKER_A: And I should comment that I'm sure there's a theoretical computer
[02:18:02] SPEAKER_A: scientist listening to this saying, I can model exactly what is happening
[02:18:06] SPEAKER_A: here and prove that the clustering increasing decreases.
[02:18:09] SPEAKER_A: So taking the specific instantiation of the thing you've experimented with
[02:18:16] SPEAKER_A: and prove certain properties of this.
[02:18:18] SPEAKER_A: But the point is that there's a more general pattern here of probably other
[02:18:24] SPEAKER_A: that you haven't discovered unexpected competencies that emerged from this,
[02:18:28] SPEAKER_A: that you can, could get free computation out of this thing.
[02:18:32] So this goes back to the very first thing you said about physicists
[02:18:35] SPEAKER_B: thinking that physics is enough.
[02:18:37] SPEAKER_B: You're a hundred percent correct that somebody could look at this and say,
[02:18:41] SPEAKER_B: well, I see exactly why this is happening.
[02:18:43] SPEAKER_B: We can track, we can track through the algorithm.
[02:18:46] SPEAKER_B: Yeah, you can.
[02:18:46] SPEAKER_B: There's no miracle going on here, right?
[02:18:48] SPEAKER_B: I'm not, the hardware isn't doing some crazy thing that it wasn't supposed to do.
[02:18:51] SPEAKER_B: The point is that despite following the algorithm to do one thing, it is also at
[02:18:56] SPEAKER_B: the same time doing other things that are neither prescribed nor forbidden by the
[02:19:00] SPEAKER_B: algorithm, it's the space between, between the chance and necessity, which is how a
[02:19:05] SPEAKER_B: lot of people, you know, see these things.
[02:19:07] SPEAKER_B: It's that, it's that free space.
[02:19:08] SPEAKER_B: We don't really have a good vocabulary for it, where the
[02:19:10] SPEAKER_B: interesting things happen.
[02:19:12] SPEAKER_B: And to whatever extent it's doing other things that are useful, that stuff is,
[02:19:16] SPEAKER_B: is, is computationally without extra costs.
[02:19:19] SPEAKER_B: Now there's one other cool thing about this.
[02:19:21] SPEAKER_B: And this is the beginning of a lot of thinking that I've done about when this,
[02:19:24] SPEAKER_B: this relates to AI and stuff like that.
[02:19:26] SPEAKER_B: Intrinsic motivations, the sorting of the digits is what we forced it to do.
[02:19:34] SPEAKER_B: The clustering is an intrinsic motivation.
[02:19:36] SPEAKER_B: We didn't ask for it.
[02:19:37] SPEAKER_B: We didn't expect it to happen.
[02:19:39] SPEAKER_B: We didn't we didn't explicitly forbid it, but we didn't, you know, we didn't know.
[02:19:44] SPEAKER_B: This is a great definition of the intrinsic motivation of a system.
[02:19:47] SPEAKER_B: So when people say, Oh, that's a machine.
[02:19:49] SPEAKER_B: It only does what you programmed it to do.
[02:19:51] SPEAKER_B: I, you know, I, as a human have intrinsic motivation, you know, uh, I'm creative
[02:19:56] SPEAKER_B: and I have intrinsic motivation, machines don't do that, even, even, even this
[02:19:59] SPEAKER_B: minimal thing has intrinsic motivation.
[02:20:00] SPEAKER_B: has a minimal kind of intrinsic motivation, which is something that is not forbidden by
[02:20:05] SPEAKER_B: the algorithm, but isn't prescribed by the algorithm either.
[02:20:08] SPEAKER_B: And I think that's an important third thing besides chance and necessity.
[02:20:13] SPEAKER_B: Something else that's fun about this is when you think about intrinsic motivations, think
[02:20:18] SPEAKER_B: about a child.
[02:20:20] SPEAKER_B: If you make him sit in math class all day, you're never going to know what the other
[02:20:24] SPEAKER_B: intrinsic motivations are that he might be doing, right?
[02:20:27] SPEAKER_B: Who knows what else he might be interested in?
[02:20:29] SPEAKER_B: So we, so I wanted to ask this question, I want to say, if we let off the pressure on
[02:20:33] SPEAKER_B: the sorting, what would happen?
[02:20:37] SPEAKER_B: Now that's hard because, because if you mess with the algorithm, now it's no longer the
[02:20:41] SPEAKER_B: same algorithm.
[02:20:42] SPEAKER_B: So you don't want to do that.
[02:20:43] SPEAKER_B: So we did something that I think was, it was kind of clever.
[02:20:45] SPEAKER_B: We allowed repeat digits.
[02:20:48] SPEAKER_B: So if you allow repeat digits in your, in your array, you can still have all the fives
[02:20:53] SPEAKER_B: can still be after all the fours and after all the sixes, but you can keep them as clustered
[02:20:57] SPEAKER_B: as you want.
[02:20:58] SPEAKER_B: So this thing at the end where they have to get de-clustered in order for the sorting
[02:21:01] SPEAKER_B: to happen, we thought maybe we could let off the pressure a little bit.
[02:21:04] SPEAKER_B: If you do that, all you do is allow some extra repeat digits, the clustering gets bigger.
[02:21:10] SPEAKER_B: It will cluster as much as you let it.
[02:21:12] SPEAKER_B: The clustering is what it wants to do.
[02:21:14] SPEAKER_B: The sorting is what we're forcing it to do.
[02:21:17] SPEAKER_B: And my only point is if, if the, if the bubble sort, which has been gone over and gone over
[02:21:22] SPEAKER_B: how many times has these kinds of things that we didn't see coming, what about the AIs,
[02:21:28] SPEAKER_B: the language model, everything else?
[02:21:29] SPEAKER_B: Not because, not because they talk, not because they say that they're, you know, have an inner
[02:21:33] SPEAKER_B: perspective or any of that, but just from the fact that this thing is even, even the
[02:21:38] SPEAKER_B: most minimal system surprises with what happens.
[02:21:41] SPEAKER_B: And I, frankly, when I see this, tell me if this doesn't sound like all of our existential
[02:21:46] SPEAKER_B: story.
[02:21:47] SPEAKER_B: For the brief time that we're here, the universe is going to grind us into dust eventually.
[02:21:53] SPEAKER_B: But until then, we get to do some cool stuff that is intrinsically motivating to us, that
[02:21:58] SPEAKER_B: is neither forbidden by our, by the laws of physics, nor determined by the laws of physics,
[02:22:04] SPEAKER_B: but eventually it kind of comes to an end.
[02:22:07] SPEAKER_B: So I think that, that aspect of it, right, that there are spaces, even in algorithms,
[02:22:14] SPEAKER_B: there are spaces in which you can do other new things, not just random stuff, not just
[02:22:18] SPEAKER_B: complex stuff, but things that are easily recognizable to a behavior scientist.
[02:22:22] SPEAKER_B: You see, that's the point here.
[02:22:24] SPEAKER_B: And I think that kind of intrinsic motivation is what's telling us that this idea that we
[02:22:30] SPEAKER_B: can carve up the world, we can say, okay, look, biology is complex, cognition, who knows
[02:22:35] SPEAKER_B: what's responsible for that, but at least we can take a chunk of the world aside and
[02:22:40] SPEAKER_B: we can, we can cut it off and we can say, these are the dumb machines.
[02:22:44] SPEAKER_B: These are just this algorithms, whereas we know the rules of biochemistry don't explain
[02:22:50] SPEAKER_B: everything we want to know about how psychology is going to go, but at least the rules of
[02:22:53] SPEAKER_B: algorithms tell us exactly what the machines are going to do, right?
[02:22:57] SPEAKER_B: We have, we have some hope that we've, we've carved off a little part of the world and
[02:23:00] SPEAKER_B: everything is nice and simple and it is exactly what we said it was going to be.
[02:23:04] SPEAKER_B: I think that failed.
[02:23:05] SPEAKER_B: I think it was a good try.
[02:23:06] SPEAKER_B: I think we have good theories of interfaces, but even, even the simplest algorithms have,
[02:23:12] SPEAKER_B: have these kinds of things going on.
[02:23:14] SPEAKER_B: And so that's, that, that's why I think something like this is significant.
[02:23:17] Do you think that there is going to be in all kinds of systems of varying complexity,
[02:23:24] SPEAKER_A: things that the system wants to do and things that it's forced to do?
[02:23:29] SPEAKER_A: So are there these unexpected competencies to be discovered in basically all algorithms
[02:23:35] SPEAKER_A: and all systems?
[02:23:38] SPEAKER_A: That's my suspicion.
[02:23:39] SPEAKER_B: And I think that is extremely important for us as humans to have a research program to
[02:23:43] SPEAKER_B: learn to recognize and predict and recognize.
[02:23:46] SPEAKER_B: We make things, nevermind something as simple as this.
[02:23:48] SPEAKER_B: We make, we make, you know, social structures, financial structures, internet of things,
[02:23:52] SPEAKER_B: um, robotics, AI, so we make all this stuff.
[02:23:55] SPEAKER_B: And we think that the thing we make it do is the main show.
[02:23:59] SPEAKER_B: And I think it is very important for us to learn to recognize the, the, the kind of stuff
[02:24:03] SPEAKER_B: that, that sneaks in into the spaces.
[02:24:06] What, what, it's a very counterintuitive notion, by the way, I like the word emergent.
[02:24:13] SPEAKER_A: I hear your criticism, and it's a really strong one that emergent is like, you toss
[02:24:18] SPEAKER_A: your hands up, but I don't know the process, but it's just a beautiful word because it
[02:24:23] SPEAKER_A: is, I guess it's a synonym for surprising.
[02:24:27] SPEAKER_A: And I mean, this is very surprising, but just because it's surprising doesn't mean there's
[02:24:31] SPEAKER_A: not a mechanism that explains it.
[02:24:35] SPEAKER_B: Mechanism and explanation are both, uh, not all they're cracked up to be in the sense
[02:24:40] SPEAKER_B: that, you know, anything you and I do, we could, we could come up with the most beautiful
[02:24:45] SPEAKER_B: theory.
[02:24:46] SPEAKER_B: We paint a painting, anything we do.
[02:24:49] SPEAKER_B: Somebody could say, well, I was watching the biochemistry and the, and the, and the Schrodinger
[02:24:53] SPEAKER_B: equation playing out.
[02:24:55] SPEAKER_B: And it was, it totally described everything that was happening.
[02:24:58] SPEAKER_B: You didn't break, you didn't break even a single law of biochemistry, nothing to see
[02:25:02] SPEAKER_B: here, nothing to see, right?
[02:25:04] SPEAKER_B: Like, okay.
[02:25:05] SPEAKER_B: You know, consistent with the, with the low level rules, you can do the same thing here.
[02:25:09] SPEAKER_B: You can look at the machine code and say, yeah, this thing is just executing machine
[02:25:12] SPEAKER_B: code.
[02:25:13] SPEAKER_B: You can go further and say, well, it's quite, it's quantum foam.
[02:25:15] SPEAKER_B: It's just doing the thing that quantum foam does.
[02:25:17] SPEAKER_B: That you're saying that's what physicists miss.
[02:25:20] And I'm not saying they're unaware of that.
[02:25:22] SPEAKER_B: I'm saying, I mean, they're generally a pretty sophisticated bunch.
[02:25:25] SPEAKER_B: I just think they've picked a level and they're going to discover what is to be seen at that
[02:25:30] SPEAKER_B: level, which is a lot.
[02:25:32] SPEAKER_B: And my point is the stuff that the, the, the behavior scientists are interested in shows
[02:25:37] SPEAKER_B: up at a much lower level than you think.
[02:25:39] SPEAKER_B: How often do you think there's a misalignment of this kind between the thing that a system
[02:25:44] SPEAKER_A: is forced to do and what it wants to do?
[02:25:47] SPEAKER_A: And it's particularly, I'm thinking about various levels of complexity of AI systems.
[02:25:53] SPEAKER_A: So right now we've looked at like five other systems.
[02:25:57] SPEAKER_B: That's a small N okay.
[02:25:58] SPEAKER_B: But, but just looking at that, I would find it very surprising if bubble sort was able
[02:26:06] SPEAKER_B: to do this.
[02:26:07] SPEAKER_B: And then there was some sort of valley of death where nothing showed up and then blah,
[02:26:11] SPEAKER_B: blah, living things.
[02:26:12] SPEAKER_B: Like, I can't imagine that I, I'm going to say that if something, and we, and we actually
[02:26:15] SPEAKER_B: have a system that's even simpler than this, which is one D cellular automata, that's doing
[02:26:19] SPEAKER_B: some weird stuff.
[02:26:20] SPEAKER_B: Uh, if, if these things are to be found in this kind of simple system, I mean, they just
[02:26:25] SPEAKER_B: have to be showing up in, in these other more complex AIs and things like that.
[02:26:30] SPEAKER_B: The only thing, what, what we don't know, but we're going to find out is to what extent
[02:26:35] SPEAKER_B: there is interaction between these.
[02:26:37] SPEAKER_B: So I call these things side quests, you know, it's like, they're like, like, like in a game,
[02:26:41] SPEAKER_B: you know, whether it's the main thing you're supposed to do.
[02:26:43] SPEAKER_B: And then as long as, as long as you still do it, the thing about this is you have to
[02:26:45] SPEAKER_B: sort yet.
[02:26:46] SPEAKER_B: You have to sort, there's no miracle.
[02:26:48] SPEAKER_B: You're going to sort, but, but, but as long as you can do other stuff while you're sorting,
[02:26:53] SPEAKER_B: it's not forbidden.
[02:26:54] SPEAKER_B: And what we don't know is to what extent are the two things linked.
[02:26:57] SPEAKER_B: So if you do have a system that's very good at language, are the, are the others, the,
[02:27:02] SPEAKER_B: the side quests that it's capable of, do they have anything to do with language whatsoever?
[02:27:07] SPEAKER_B: We don't know the answer to that.
[02:27:08] SPEAKER_B: The answer might be no.
[02:27:09] SPEAKER_B: In which case, all of the stuff that we've been saying about language models, because
[02:27:13] SPEAKER_B: of what they're saying, all of that could be a total red herring and not really important.
[02:27:18] SPEAKER_B: And the really exciting stuff is what we never looked for.
[02:27:21] SPEAKER_B: Or in complex systems, maybe those things become linked.
[02:27:24] SPEAKER_B: In biology, they're linked.
[02:27:25] SPEAKER_B: In biology, evolution makes sure that, that the things you're capable of have a lot to
[02:27:29] SPEAKER_B: do with what you've actually been selected for.
[02:27:32] SPEAKER_B: And these things, I don't know.
[02:27:34] SPEAKER_B: And so we might find out that, that they actually do give the language some sort of leg up,
[02:27:38] SPEAKER_B: or we might find that the language is, is just, you know, that's not, that's not the
[02:27:42] SPEAKER_B: interesting part.
[02:27:43] SPEAKER_B: Also, it is an interesting question of this intrinsic motivation of clustering.
[02:27:49] SPEAKER_A: Is this a property of the particular sorting algorithms?
[02:27:54] SPEAKER_A: Is this a property of all sorting algorithms?
[02:27:58] SPEAKER_A: Is this a property of all algorithms operating on lists, on numbers?
[02:28:04] SPEAKER_A: How big is this?
[02:28:05] SPEAKER_A: So for example, with LLMs, is it a property of any algorithm that's trying to model language
[02:28:10] SPEAKER_A: or is it very specific to transformers?
[02:28:13] SPEAKER_A: And that's all to be discovered.
[02:28:14] SPEAKER_A: We're doing all that.
[02:28:15] SPEAKER_B: We're doing all that.
[02:28:16] SPEAKER_B: We're testing, we're testing the stuff in other algorithms we're looking for.
[02:28:18] SPEAKER_B: We're developing suites of code to look for other properties.
[02:28:21] SPEAKER_B: We, you know, to some extent it's very hard because we don't know what to look for, but
[02:28:26] SPEAKER_B: we do have a behaviorist handbook, which tells you what the, all kinds of things to
[02:28:29] SPEAKER_B: look for, the delay gratification, the, you know, problem solving, like we have, we have
[02:28:34] SPEAKER_B: all that.
[02:28:35] SPEAKER_B: I'll tell you an N of one of an interesting biological intrinsic motivation, because,
[02:28:39] SPEAKER_B: because people, so, so, so in, in like the alignment community and stuff, there's a lot
[02:28:43] SPEAKER_B: of discussion about like, what are the intrinsic motivations going to be of AIs?
[02:28:46] SPEAKER_B: What are their goals going to be?
[02:28:47] SPEAKER_B: Right.
[02:28:48] SPEAKER_B: They will, what are they going to want to do?
[02:28:49] SPEAKER_B: Just, just as an N of one observation, anthropods, the very first thing we checked for, so this
[02:28:55] SPEAKER_B: is not experiment number 972 out of a thousand things.
[02:28:59] SPEAKER_B: This is the very first thing we checked for.
[02:29:01] SPEAKER_B: We put them on a plate of neurons with a big wound through them, a big scratch.
[02:29:04] SPEAKER_B: First thing they did was heal the wound.
[02:29:06] SPEAKER_B: Okay.
[02:29:07] SPEAKER_B: So it's an N of one, but I, I liked the fact that the first intrinsic motivation that we
[02:29:11] SPEAKER_B: noticed out of that system was benevolent and healing.
[02:29:13] SPEAKER_B: Like I thought that was pretty cool.
[02:29:15] SPEAKER_B: And we don't know, maybe, you know, maybe the next 20 things we find are going to be
[02:29:18] SPEAKER_B: some sort of, you know, damaging effects.
[02:29:19] SPEAKER_B: I can't tell you that, but, but the first thing that we saw was, was kind of a positive
[02:29:24] SPEAKER_B: one.
[02:29:25] SPEAKER_B: And, and I don't know, that makes me feel better.
[02:29:27] SPEAKER_A: What was the thing you mentioned with the anthropods that they can reverse aging?
[02:29:31] SPEAKER_B: There's a procedure called an epigenetic clock, where what you can do is look at a particular
[02:29:37] SPEAKER_B: epigenetic states of cells and compare to a curve that was built from humans of known
[02:29:43] SPEAKER_B: age.
[02:29:44] SPEAKER_B: You can guess, you can guess what the age is.
[02:29:46] SPEAKER_B: Okay.
[02:29:47] SPEAKER_B: So, so, so we can take now, and this is Steve Havrath's work and many other people that
[02:29:51] SPEAKER_B: you take a set of cells, you can guess what their biological age is.
[02:29:55] SPEAKER_B: Okay.
[02:29:56] SPEAKER_B: So we make the anthropods from cells that we get from human trachea.
[02:30:00] SPEAKER_B: epithelium, we collaborated with with Steve's group, the
[02:30:04] SPEAKER_B: Clock Foundation, we sent them a bunch of cells. And we saw
[02:30:07] SPEAKER_B: that if you if you check the the anthropods themselves, they are
[02:30:13] SPEAKER_B: roughly 20% younger than the cells they come from. And so
[02:30:18] SPEAKER_B: that's amazing. And I can I can give you a theory of why that
[02:30:23] SPEAKER_B: happens, although we're still investigating. And then I can
[02:30:26] SPEAKER_B: tell you the implications for longevity and things like that.
[02:30:29] SPEAKER_B: My theory for why it happens, I call this, I call this age
[02:30:35] SPEAKER_B: evidencing. And I think that what's happening here, like with
[02:30:38] SPEAKER_B: a lot of biology, is that cells have to update their priors
[02:30:42] SPEAKER_B: based on experience. And so I think that they come from an old
[02:30:46] SPEAKER_B: body, they have a lot of priors about how many years they've
[02:30:48] SPEAKER_B: been around and all that. But their new environment screams,
[02:30:51] SPEAKER_B: I'm an embryo. Basically, there's no other cells around
[02:30:54] SPEAKER_B: you're being bent into a pretzel, they actually express
[02:30:56] SPEAKER_B: some embryonic genes. They say you're an embryo. And I think it
[02:31:02] SPEAKER_B: doesn't, it's not enough new evidence to roll them like all
[02:31:05] SPEAKER_B: the way back. But it's enough to update them to about 28% back.
[02:31:08] SPEAKER_B: Yeah, so similar to like, when older adult gives birth to a
[02:31:15] SPEAKER_A: child. So you're, you're saying you can just fake it till you
[02:31:20] SPEAKER_A: make it with with age, like the environment convinces the cell
[02:31:26] SPEAKER_A: that it's young?
[02:31:27] Well, first of all, yes, yes. And that's, that's, that's my
[02:31:31] SPEAKER_B: hypothesis. And we have a whole bunch of research being done on
[02:31:34] SPEAKER_B: this. There was a study where they went into a, an old age
[02:31:37] SPEAKER_B: home, and they redid the decor, like 60 style when all these
[02:31:41] SPEAKER_B: folks were really young. And they found all kinds of
[02:31:45] SPEAKER_B: improvements in blood chemistry and stuff like that. Because
[02:31:48] SPEAKER_B: they say it was sort of mentally taking them back to when you
[02:31:50] SPEAKER_B: know, when they were the way they were at that time. I think
[02:31:53] SPEAKER_B: this is a basal version of that, that basically, if if
[02:31:57] SPEAKER_B: you're finding yourself in an embryonic environment, what's
[02:32:00] SPEAKER_B: more plausible that that you're young or or what, what, you
[02:32:04] SPEAKER_B: know, what, like, I think, I think this is this is the basic
[02:32:07] SPEAKER_B: feature of biology is to is to update priors based on
[02:32:10] SPEAKER_B: experience.
[02:32:10] SPEAKER_B: Do you think that's actually actionable for longevity? Like
[02:32:15] SPEAKER_A: you can convince cells that they're younger and thereby
[02:32:17] SPEAKER_A: extend the lifespan?
[02:32:20] SPEAKER_A: This is what we're trying to do. Yeah.
[02:32:22] SPEAKER_B: Could it be as simple as that?
[02:32:24] SPEAKER_A: Why that's not? Well, I'm not claiming it's simple. That is in
[02:32:27] SPEAKER_B: no way simple. But because because again, you have to all
[02:32:31] SPEAKER_B: all of this, all of the regenerative medicine stuff that
[02:32:33] SPEAKER_B: we do, balances on one key thing, which is learning to
[02:32:37] SPEAKER_B: communicate to the system. We have to if you're going to
[02:32:40] SPEAKER_B: convince that system, you know, so so when we make gut tissue
[02:32:43] SPEAKER_B: into an eye, you have to convince those cells that their
[02:32:46] SPEAKER_B: priors about we are we are gut precursors, those priors are
[02:32:49] SPEAKER_B: wrong, and you should adopt this new worldview that you're
[02:32:52] SPEAKER_B: going to be you know, you're going to be an eye. So being
[02:32:54] SPEAKER_B: convincing and figuring out what what kind of messages are
[02:32:58] SPEAKER_B: convincing to to cells, and how to speak the language and how to
[02:33:01] SPEAKER_B: make them take on new new beliefs, literally, is is at the
[02:33:06] SPEAKER_B: root of all of these future advances in birth defects and
[02:33:10] SPEAKER_B: regenerative medicine and cancer. And that's that's what's
[02:33:12] SPEAKER_B: going on here. So I'm not saying it's simple, but I can see the
[02:33:15] SPEAKER_B: I can see the path.
[02:33:17] SPEAKER_A: Going back to the platonic space, I have to ask if if our
[02:33:22] SPEAKER_A: brains are indeed thin client interfaces to that space, what
[02:33:30] SPEAKER_A: does that mean for our mind? Like, can we upload the mind?
[02:33:36] SPEAKER_A: Can we copy it? Can we ship it over to other planets? Like how?
[02:33:44] SPEAKER_A: What does that mean for exactly where the mind is stored?
[02:33:49] SPEAKER_A: Yeah, a couple of things. So we so we are now beyond anything
[02:33:53] SPEAKER_B: that I can say with any certainty, this is total total
[02:33:55] SPEAKER_B: conjecture. Okay, so because we don't know yet, the whole point
[02:33:58] SPEAKER_B: of this is we actually don't really understand very well the
[02:34:00] SPEAKER_B: relationship between the interface and the thing.
[02:34:02] SPEAKER_B: And the thing you're currently working on is to map
[02:34:06] SPEAKER_A: Correct, correct. And we're and we are beginning to map it. But
[02:34:09] SPEAKER_B: you know, this is this is a massive effort. So, so so a
[02:34:13] SPEAKER_B: couple of a couple of conjectures here. One is that I
[02:34:18] SPEAKER_B: I strongly suspect that the majority of what we think of as
[02:34:25] SPEAKER_B: the mind is is the pattern in that space. Okay. And one of the
[02:34:30] SPEAKER_B: interesting predictions from that model, which is not a
[02:34:33] SPEAKER_B: prediction of modern neuroscience, is that there
[02:34:36] SPEAKER_B: should be cases where there is very minimal brain and yet
[02:34:41] SPEAKER_B: normal IQ function. This has been seen clinically, we just
[02:34:46] SPEAKER_B: Karina Kaufman, and I reviewed this in a paper recently, a
[02:34:48] SPEAKER_B: bunch of cases of humans, where there's very little brain
[02:34:51] SPEAKER_B: tissue, and they have normal or sometimes above normal
[02:34:54] SPEAKER_B: intelligence. Now, things are not simple, because that
[02:34:57] SPEAKER_B: obviously doesn't happen all the time, right? Most of the
[02:34:59] SPEAKER_B: time that doesn't happen. So So what's going on, we don't
[02:35:02] SPEAKER_B: understand. But it is a very curious thing. That is not a
[02:35:06] SPEAKER_B: prediction of not saying I'm not saying it can't, you know,
[02:35:08] SPEAKER_B: you can take modern neuroscience and sort of bend it
[02:35:11] SPEAKER_B: into a pretzel to accommodate it, you can say, well, there
[02:35:13] SPEAKER_B: are these, you know, kind of redundancies and things like
[02:35:16] SPEAKER_B: this, right? So you can accommodate it, but doesn't
[02:35:18] SPEAKER_B: predict this. So there are these incredibly curious cases.
[02:35:23] SPEAKER_B: Now, do I think you can copy it? No, I don't think you can
[02:35:28] SPEAKER_B: because what you're going to be copying is the is the
[02:35:31] SPEAKER_B: interface, the front end of the brain or the whatever you
[02:35:35] SPEAKER_B: the the action is actually the pattern in the platonic
[02:35:38] SPEAKER_B: space, you're going to be able to copy that, I doubt it.
[02:35:40] SPEAKER_B: But what you could do is produce another interface through
[02:35:44] SPEAKER_B: which that particular pattern is going to come through. I
[02:35:47] SPEAKER_B: think that's probably possible. I can't say anything about
[02:35:50] SPEAKER_B: at this point about what that would take. But my guess is
[02:35:53] SPEAKER_B: that that's that that's possible. Is your guess your gut
[02:35:56] SPEAKER_A: is that that process, if possible, is different than
[02:36:01] SPEAKER_A: copying? Like, it looks more like creating a new thing
[02:36:06] SPEAKER_A: versus copying for the interface. So if you could, so
[02:36:10] SPEAKER_B: so so so here's my prediction for Star Trek transporter.
[02:36:14] SPEAKER_B: For whatever reason, right now, your brain and body are
[02:36:18] SPEAKER_B: very attuned and attractive to a particular pattern, which
[02:36:21] SPEAKER_B: is your set of psychological propensities. If we could
[02:36:25] SPEAKER_B: if we could rebuild that exact same thing somewhere else.
[02:36:29] SPEAKER_B: I don't see any reason why that same pattern wouldn't
[02:36:32] SPEAKER_B: come through it the same way it comes through this one.
[02:36:34] SPEAKER_B: That's that would be a guess, you know, so so I think
[02:36:37] SPEAKER_B: what you what you will be copying is the physical interface
[02:36:40] SPEAKER_B: and hoping to maintain whatever it is about that interface
[02:36:44] SPEAKER_B: that was appropriate for that pattern. We don't really
[02:36:47] SPEAKER_B: know what that is at this point.
[02:36:48] SPEAKER_B: So when we've been talking about mind in this particular
[02:36:51] SPEAKER_A: case, it's the most important to me because I'm a human
[02:36:55] SPEAKER_A: The self come along with that.
[02:36:58] SPEAKER_A: This the feeling like this mind belongs to me.
[02:37:06] Yeah, so that come along with all minds the the subjective
[02:37:13] SPEAKER_A: not the subjective experience the subjective experience
[02:37:15] SPEAKER_A: is important to consciousness, but like the ownership
[02:37:19] I suspect so and I think so because of the way we come
[02:37:25] SPEAKER_B: into being so so one of the things that I should be
[02:37:28] SPEAKER_B: working on is this paper called booting up the agent
[02:37:32] SPEAKER_B: and it talks about the very earliest steps of becoming
[02:37:35] SPEAKER_B: a being in this world kind of like you can do this
[02:37:37] SPEAKER_B: for a computer right before you switch the power on it
[02:37:40] SPEAKER_B: belongs to the domain of physics, right? It obeys
[02:37:43] SPEAKER_B: the laws of physics. You switch the power on some
[02:37:45] SPEAKER_B: number of what nanoseconds microseconds. I don't
[02:37:48] SPEAKER_B: know later you have a thing that oh look it's taking
[02:37:51] SPEAKER_B: instructions off the stack and doing them right.
[02:37:54] SPEAKER_B: So now you're now it's executing an algorithm.
[02:37:56] SPEAKER_B: How did you get from from physics to executing an
[02:37:59] SPEAKER_B: algorithm? Like what what what what was happening
[02:38:01] SPEAKER_B: during the boot up exactly before it starts to run
[02:38:03] SPEAKER_B: code or whatever, right? And so we can ask that same
[02:38:06] SPEAKER_B: question in biology. What are the earliest steps
[02:38:10] SPEAKER_B: of of becoming a being?
[02:38:12] SPEAKER_B: It's a fascinating question through embryogenesis
[02:38:14] SPEAKER_A: at which point is the are you booting on? Yeah.
[02:38:17] SPEAKER_A: Yeah. Yeah. Yeah. Yeah, exactly. You have a hope
[02:38:19] SPEAKER_B: of an answer to that. Well, I think I think so. I
[02:38:22] SPEAKER_B: think so in two ways. The first thing is just
[02:38:25] SPEAKER_B: physically what what happens so I think that the
[02:38:29] SPEAKER_B: your first task as as a as a being and and I again
[02:38:34] SPEAKER_B: I don't think this is a binary thing. I think this
[02:38:36] SPEAKER_B: is a positive feedback loop that sort of cranks on
[02:38:39] SPEAKER_B: up and up your first task as a being coming into
[02:38:43] SPEAKER_B: this world is to tell a very compelling story to
[02:38:47] SPEAKER_B: your parts as a biological. You are made of a
[02:38:50] SPEAKER_B: gentle parts. Those parts need to be aligned
[02:38:53] SPEAKER_B: literally into a goal. They have no comprehension
[02:38:57] SPEAKER_B: of they if you're going to move through anatomical
[02:39:00] SPEAKER_B: space by means of a bunch of cells, which only
[02:39:02] SPEAKER_B: know physiological and you know metabolic spaces
[02:39:06] SPEAKER_B: and things like that. You are going to have to
[02:39:08] SPEAKER_B: develop a model and give them bend their actions
[02:39:12] SPEAKER_B: space. You're going to have to deform their option
[02:39:14] SPEAKER_B: space with signals with behavior shaping cues with
[02:39:18] SPEAKER_B: rewards and punishments, whatever you got your job
[02:39:21] SPEAKER_B: as a as an agent is ownership of your parts is
[02:39:24] SPEAKER_B: alignment of your parts. I think that fundamentally
[02:39:28] SPEAKER_B: is going to give rise to this this this ability
[02:39:30] SPEAKER_B: now now that also means having a boundary saying
[02:39:32] SPEAKER_B: okay, this is the stuff I control. This is me this
[02:39:35] SPEAKER_B: other stuff over here is outside world. I have to
[02:39:36] SPEAKER_B: figure out you don't know where that is by the way
[02:39:38] SPEAKER_B: you have to figure it out and an embryo Genesis.
[02:39:40] SPEAKER_B: It's really cool. You can as a as a as a grad
[02:39:43] SPEAKER_B: student. I used to do this experiment with duck
[02:39:45] SPEAKER_B: embryos with a flat blasted is you can take a needle
[02:39:48] SPEAKER_B: and put some scratches into it and every every
[02:39:51] SPEAKER_B: Island you make for a while until they heal up
[02:39:53] SPEAKER_B: thinks it's the only embryo. There's nothing else
[02:39:55] SPEAKER_B: around so it becomes an embryo and eventually you
[02:39:58] SPEAKER_B: get twins and triplets and quadruplets and
[02:40:00] SPEAKER_B: and things like that, but each one of them at the border,
[02:40:03] SPEAKER_B: you know, they're joined, well, where do I end
[02:40:05] SPEAKER_B: and where does he begin?
[02:40:07] SPEAKER_B: You have to, you know, you have to know
[02:40:08] SPEAKER_B: what your borders are.
[02:40:10] SPEAKER_B: So that action of aligning your parts
[02:40:13] SPEAKER_B: and coming to be this, I mean,
[02:40:16] SPEAKER_B: I'm even going to say this emergence,
[02:40:18] SPEAKER_B: we just don't have a good vocabulary for it,
[02:40:19] SPEAKER_B: this emergence of a model that aligns all the parts
[02:40:23] SPEAKER_B: is really critical to keep that thing going.
[02:40:25] SPEAKER_B: There's something else that's really interesting.
[02:40:27] SPEAKER_B: And I was thinking about this in the context
[02:40:30] SPEAKER_B: of this question of like, you know,
[02:40:32] SPEAKER_B: these beautiful kind of ideas, you know,
[02:40:35] SPEAKER_B: that there's this amazing thing that we found
[02:40:38] SPEAKER_B: and this is largely the work of Federico Pagosi in my group.
[02:40:42] SPEAKER_B: So a couple of years ago,
[02:40:43] SPEAKER_B: we saw that networks of chemicals can learn,
[02:40:47] SPEAKER_B: they have five or six different kinds of learning
[02:40:49] SPEAKER_B: that they can do.
[02:40:51] SPEAKER_B: And so what I asked them to do was to calculate
[02:40:55] SPEAKER_B: the causal emergence of those networks
[02:40:58] SPEAKER_B: while they're learning.
[02:40:59] SPEAKER_B: And what I mean by that is this,
[02:41:02] SPEAKER_B: if you're a rat and you learn to press a lever
[02:41:05] SPEAKER_B: and get a reward,
[02:41:07] SPEAKER_B: there's no individual cell that had both experiences,
[02:41:10] SPEAKER_B: the cells at your paw had touched the lever,
[02:41:13] SPEAKER_B: the cells in your gut got the delicious reward.
[02:41:16] SPEAKER_B: No individual cell has that both experiences.
[02:41:19] SPEAKER_B: Who owns that associative memory?
[02:41:20] SPEAKER_B: Well, the rat.
[02:41:21] SPEAKER_B: So that means you have to be integrated, right?
[02:41:23] SPEAKER_B: If you're going to learn associative memories
[02:41:25] SPEAKER_B: from different parts,
[02:41:26] SPEAKER_B: you have to be an integrated agent that can do that.
[02:41:28] SPEAKER_B: And so we can measure that now with metrics
[02:41:30] SPEAKER_B: of causal emergence like phi and things like that.
[02:41:33] SPEAKER_B: So we know that in order to learn,
[02:41:35] SPEAKER_B: you have to have significant phi,
[02:41:38] SPEAKER_B: but I wanted to ask the opposite question.
[02:41:40] SPEAKER_B: What does learning do for your phi level?
[02:41:43] SPEAKER_B: Does it do anything for your degree of being an agent
[02:41:45] SPEAKER_B: that is more than the sum of its parts?
[02:41:48] SPEAKER_B: So we trained the networks and sure enough,
[02:41:50] SPEAKER_B: some of them, not all of them, but some of them,
[02:41:52] SPEAKER_B: as you train them, their phi goes up, okay?
[02:41:57] SPEAKER_B: And so basically what we were able to find
[02:41:59] SPEAKER_B: is that there is this positive feedback loop
[02:42:05] SPEAKER_B: between every time you learn something,
[02:42:07] SPEAKER_B: you become more of an integrated agent.
[02:42:10] SPEAKER_B: And every time you do that, it becomes easier to learn.
[02:42:12] SPEAKER_B: And so it's this-
[02:42:13] SPEAKER_B: It's a virtuous cycle.
[02:42:14] SPEAKER_A: It's a virtuous cycle.
[02:42:15] SPEAKER_B: It's an asymmetry that points upwards
[02:42:17] SPEAKER_B: for agency and intelligence.
[02:42:19] SPEAKER_B: And now back to our platonic space stuff,
[02:42:23] SPEAKER_B: where does that come from?
[02:42:23] SPEAKER_B: It doesn't come from evolution.
[02:42:24] SPEAKER_B: You don't need to have any evolution for this.
[02:42:26] SPEAKER_B: Evolution will optimize the crap out of it for sure,
[02:42:28] SPEAKER_B: but you don't need evolution to have this.
[02:42:30] SPEAKER_B: It doesn't come from physics.
[02:42:31] SPEAKER_B: It comes from the rules of information,
[02:42:33] SPEAKER_B: the causal information theory,
[02:42:35] SPEAKER_B: and the behavior of networks, the mathematical objects.
[02:42:38] SPEAKER_B: This is not anything that was given to you by physics
[02:42:44] SPEAKER_B: or by a history of selection.
[02:42:46] SPEAKER_B: It's a free gift for math.
[02:42:47] SPEAKER_B: And those two free gifts for math
[02:42:51] SPEAKER_B: lock together into a spiral that I think causes,
[02:42:55] SPEAKER_B: simultaneously, a rise in intelligence
[02:42:57] SPEAKER_B: and a rise in collective agency.
[02:43:00] SPEAKER_B: And I think that's just, you know,
[02:43:01] SPEAKER_B: that's been, you know, just amazing to think about.
[02:43:04] SPEAKER_B: Well, that free gift from,
[02:43:06] SPEAKER_A: I think, is extremely useful in biology.
[02:43:10] SPEAKER_A: When you have small entities forming networks,
[02:43:14] SPEAKER_A: hierarchy that builds more and more complex organisms.
[02:43:17] SPEAKER_A: That's obvious.
[02:43:18] SPEAKER_A: I mean, this speaks to embryogenesis,
[02:43:21] SPEAKER_A: which I think is one of the coolest things in the universe.
[02:43:24] SPEAKER_A: And in fact, you acknowledge its coolness
[02:43:27] SPEAKER_A: in Ingressing Mind's paper, writing, quote,
[02:43:30] SPEAKER_A: most of the big questions of philosophy
[02:43:33] SPEAKER_A: are raised by the process of embryogenesis.
[02:43:36] SPEAKER_A: Right in front of our eyes,
[02:43:38] SPEAKER_A: a single cell multiplies and self assembles
[02:43:40] SPEAKER_A: into a complex organism,
[02:43:43] SPEAKER_A: with order on every scale of organization
[02:43:46] SPEAKER_A: and adaptive behavior.
[02:43:47] SPEAKER_A: Each of us takes the same journey
[02:43:49] SPEAKER_A: across the Cartesian cut,
[02:43:51] SPEAKER_A: starting off as a quiescent human oocyte,
[02:43:56] SPEAKER_A: a little blob thought to be well described
[02:43:59] SPEAKER_A: by chemistry and physics.
[02:44:00] SPEAKER_A: Gradually, it undergoes metamorphosis
[02:44:03] SPEAKER_A: and eventually becomes a mature human with hopes, dreams,
[02:44:07] SPEAKER_A: and a self-reflective metacognition
[02:44:12] SPEAKER_A: that can enable it to describe itself as not a machine.
[02:44:15] SPEAKER_A: It's more than its brain, body,
[02:44:17] SPEAKER_A: and underlying molecular mechanisms, and so on.
[02:44:21] SPEAKER_A: What in all of our discussion can we say
[02:44:25] SPEAKER_A: as the clear intuition,
[02:44:28] SPEAKER_A: how it's possible to take a leap
[02:44:31] SPEAKER_A: from a single cell to a fully functioning organism,
[02:44:38] SPEAKER_A: full of dreams and hopes and friends and love
[02:44:40] SPEAKER_A: and all that kind of stuff?
[02:44:42] SPEAKER_A: In everything we've been talking about,
[02:44:43] SPEAKER_A: which has been a little bit technical,
[02:44:47] SPEAKER_A: how do we understand?
[02:44:48] SPEAKER_A: Because that's one of the most magical things
[02:44:50] SPEAKER_A: the universe is able to create,
[02:44:51] SPEAKER_A: perhaps the most magical.
[02:44:54] SPEAKER_A: From simple physics and chemistry,
[02:44:56] SPEAKER_A: create this, us two talking about ourselves.
[02:45:02] I think we have to keep in mind
[02:45:05] SPEAKER_B: that physics and chemistry are not real things.
[02:45:08] SPEAKER_B: They are lenses that we put on the world,
[02:45:11] SPEAKER_B: that they are perspectives where we say we are,
[02:45:15] SPEAKER_B: for the time being,
[02:45:17] SPEAKER_B: for the duration of this chemistry class
[02:45:18] SPEAKER_B: or career or whatever,
[02:45:20] SPEAKER_B: we are going to put aside all the other levels
[02:45:22] SPEAKER_B: and we're gonna focus on this one level.
[02:45:24] SPEAKER_B: And that what is fundamentally going on
[02:45:27] SPEAKER_B: during that process is an amazing positive feedback loop
[02:45:32] SPEAKER_B: of collective intelligence for the interface.
[02:45:34] SPEAKER_B: It's the physical interface is scaling,
[02:45:38] SPEAKER_B: it's the cognitive light cone that it can support.
[02:45:41] SPEAKER_B: So it's going from a molecular network.
[02:45:43] SPEAKER_B: The molecular network can already do things
[02:45:45] SPEAKER_B: like Pavlovian conditioning.
[02:45:46] SPEAKER_B: You don't start with zero.
[02:45:47] SPEAKER_B: When you have a simple molecular network,
[02:45:50] SPEAKER_B: you are already hosting some patterns
[02:45:52] SPEAKER_B: from the platonic space
[02:45:53] SPEAKER_B: that look like Pavlovian conditioning.
[02:45:55] SPEAKER_B: You've already got that in starting out.
[02:45:57] SPEAKER_B: That's just the molecular network.
[02:45:58] SPEAKER_B: Then you become a cell and then you're many cells.
[02:46:02] SPEAKER_B: And now you're navigating anatomical morphous space
[02:46:04] SPEAKER_B: and you're hosting all kinds of other patterns.
[02:46:07] SPEAKER_B: And eventually you, and I think again,
[02:46:10] SPEAKER_B: I think there's, and this is like what,
[02:46:12] SPEAKER_B: all the stuff that we're trying to work out now,
[02:46:14] SPEAKER_B: there's a consistent feedback
[02:46:16] SPEAKER_B: between the ingressions you get
[02:46:19] SPEAKER_B: and the ability to have new ones.
[02:46:20] SPEAKER_B: Which again, I think it's this like positive feedback cycle
[02:46:23] SPEAKER_B: where the more of these free gifts you pull down,
[02:46:26] SPEAKER_B: they allow you physically to develop to a ways
[02:46:28] SPEAKER_B: where, oh look, now we're suitable for more and higher ones.
[02:46:32] SPEAKER_B: And this continuously goes and goes and goes
[02:46:34] SPEAKER_B: until you're able to pull down
[02:46:36] SPEAKER_B: a full human set of behavioral capacities.
[02:46:39] SPEAKER_B: What is the mechanism of such radical scaling
[02:46:42] SPEAKER_A: of the cognitive cone?
[02:46:45] SPEAKER_A: Is it just this kind of,
[02:46:46] SPEAKER_A: the same thing that you were talking about
[02:46:47] SPEAKER_A: with the network of chemicals being able to learn?
[02:46:49] SPEAKER_A: I'll give you two mechanisms that we found.
[02:46:52] SPEAKER_B: But again, just to be clear,
[02:46:54] SPEAKER_B: these are mechanisms of the physical interface.
[02:46:58] SPEAKER_B: What we haven't gotten is a mature theory
[02:47:01] SPEAKER_B: of how they map onto the space.
[02:47:02] SPEAKER_B: That's just like just beginning.
[02:47:04] SPEAKER_B: But I'll tell you what the physical side
[02:47:06] SPEAKER_B: of things look like.
[02:47:07] SPEAKER_B: The first one has to do with stress propagation.
[02:47:11] SPEAKER_B: So imagine that you've got a bunch of cells
[02:47:14] SPEAKER_B: and there's a cell down here that needs to be up there.
[02:47:17] SPEAKER_B: All of these cells are exactly where they need to go.
[02:47:18] SPEAKER_B: So they're happy, their stress is low.
[02:47:20] SPEAKER_B: This cell, now let's imagine stress is basically a,
[02:47:28] SPEAKER_B: it's a physical implementation of the error function.
[02:47:32] SPEAKER_B: It's basically, the amount of stress
[02:47:33] SPEAKER_B: is basically the delta between where you are now
[02:47:36] SPEAKER_B: and where you need to be.
[02:47:37] SPEAKER_B: Not necessarily in physical position.
[02:47:38] SPEAKER_B: This could be an anatomical space and physiological space
[02:47:41] SPEAKER_B: and in transcriptional space, whatever, right?
[02:47:43] SPEAKER_B: It's just the delta from your set point.
[02:47:46] SPEAKER_B: So you're stressed out, but these guys are happy.
[02:47:48] SPEAKER_B: They're not moving.
[02:47:49] SPEAKER_B: You can't get past them.
[02:47:50] SPEAKER_B: Now imagine if what you could do
[02:47:51] SPEAKER_B: is you could leak your stress,
[02:47:53] SPEAKER_B: whatever your stress molecule is.
[02:47:54] SPEAKER_B: And the cool thing is that evolution
[02:47:55] SPEAKER_B: has actually conserved these highly.
[02:47:57] SPEAKER_B: So these are all, and we're studying all of these things.
[02:47:59] SPEAKER_B: They're actually highly conserved.
[02:48:00] SPEAKER_B: If you start leaking your stress molecules,
[02:48:02] SPEAKER_B: then all of this stuff around here
[02:48:04] SPEAKER_B: is starting to get stressed out.
[02:48:06] SPEAKER_B: When things get stressed, starting to get stressed out,
[02:48:08] SPEAKER_B: their temperature in the, not physical temperature,
[02:48:10] SPEAKER_B: but in the sense of like simulated annealing or something,
[02:48:12] SPEAKER_B: their ability to, their plasticity goes up
[02:48:15] SPEAKER_B: because they're feeling stressed.
[02:48:16] SPEAKER_B: They need to relieve that stress.
[02:48:18] SPEAKER_B: And because all the stress molecules are the same,
[02:48:20] SPEAKER_B: they don't know it's not their stress.
[02:48:22] SPEAKER_B: They are equally irritated by them
[02:48:25] SPEAKER_B: as if it was their own stress.
[02:48:26] SPEAKER_B: So they become a little more plastic.
[02:48:28] SPEAKER_B: They become ready to kind of adopt different fates.
[02:48:31] SPEAKER_B: You get up to where you're going
[02:48:33] SPEAKER_B: and then everybody's stress can drop.
[02:48:35] SPEAKER_B: So notice what can happen by a very simple mechanism.
[02:48:38] SPEAKER_B: Just be leaky for your own stress.
[02:48:40] SPEAKER_B: My problems become your problems,
[02:48:43] SPEAKER_B: not because you're altruistic,
[02:48:44] SPEAKER_B: not because you actually care about my problems.
[02:48:45] SPEAKER_B: There's no mechanism for you
[02:48:46] SPEAKER_B: to actually care about my problems.
[02:48:47] SPEAKER_B: But just that simple mechanism means that
[02:48:51] SPEAKER_B: far away regions are now responsive
[02:48:53] SPEAKER_B: to the needs of other regions,
[02:48:55] SPEAKER_B: such that complex rearrangements
[02:48:57] SPEAKER_B: and things like that can happen.
[02:48:58] SPEAKER_B: It's alignment of everybody to the same goal
[02:49:01] SPEAKER_B: through this very dumb, simple stress sharing thing.
[02:49:03] SPEAKER_B: Via leaky stress.
[02:49:05] SPEAKER_A: Leaky stress, right?
[02:49:06] SPEAKER_B: So there's another one.
[02:49:07] SPEAKER_B: There's another one, which I call memory anonymization.
[02:49:10] SPEAKER_B: So imagine here are two cells
[02:49:13] SPEAKER_B: and imagine something happens to this cell
[02:49:15] SPEAKER_B: and it sends a signal over to this cell.
[02:49:19] SPEAKER_B: Traditionally, you send a signal over.
[02:49:21] SPEAKER_B: This cell receives it.
[02:49:22] SPEAKER_B: It's very clear that it came from outside.
[02:49:24] SPEAKER_B: So this cell can do many things.
[02:49:25] SPEAKER_B: It could ignore it.
[02:49:26] SPEAKER_B: It could take on the information.
[02:49:28] SPEAKER_B: It could just ignore it.
[02:49:29] SPEAKER_B: It could reinterpret it.
[02:49:30] SPEAKER_B: It could do whatever.
[02:49:31] SPEAKER_B: But it's very clear that came from outside.
[02:49:33] SPEAKER_B: Now imagine the kind of thing that we study,
[02:49:35] SPEAKER_B: which is called gap junctions.
[02:49:38] SPEAKER_B: These are electrical synapses
[02:49:40] SPEAKER_B: that could directly link the internal miliers of two cells.
[02:49:44] SPEAKER_B: If something happens to this cell,
[02:49:45] SPEAKER_B: let's say it gets poked
[02:49:46] SPEAKER_B: and there's a calcium spike or something,
[02:49:48] SPEAKER_B: that propagates through the gap junction here.
[02:49:51] SPEAKER_B: This cell now has the same information,
[02:49:54] SPEAKER_B: but this cell has no idea.
[02:49:55] SPEAKER_B: Wait a minute.
[02:49:56] SPEAKER_B: Is that my memory or is that his memory?
[02:49:57] SPEAKER_B: Because it's the same, right?
[02:49:59] SPEAKER_B: It's the same.
[02:50:00] SPEAKER_B: It's the same components.
[02:50:02] SPEAKER_B: And so what you're able to do now is to have a mind melt.
[02:50:04] SPEAKER_B: You can have a mind melt between the two cells
[02:50:07] SPEAKER_B: where nobody's quite sure whose memory it is.
[02:50:09] SPEAKER_B: And when you share memories like this,
[02:50:11] SPEAKER_B: it's harder to say that I'm separate from you.
[02:50:13] SPEAKER_B: If we share the same memories, we're kind of a,
[02:50:16] SPEAKER_B: and I don't mean every single memories, right?
[02:50:18] SPEAKER_B: So they still have some identity,
[02:50:19] SPEAKER_B: but to a large extent, they have a little bit of a mind melt
[02:50:22] SPEAKER_B: and there's many complexities you can lean on top of it.
[02:50:25] SPEAKER_B: But what it means is that if you have a large group of cells,
[02:50:29] SPEAKER_B: they now have joint memories of what happened to us,
[02:50:33] SPEAKER_B: as opposed to, you know what happened to you
[02:50:35] SPEAKER_B: and I know what happened to me.
[02:50:36] SPEAKER_B: And that enables a higher cognitive light cone
[02:50:39] SPEAKER_B: because you have greater computational capacity.
[02:50:41] SPEAKER_B: You have a greater area of concern
[02:50:43] SPEAKER_B: of things you want to manage.
[02:50:44] SPEAKER_B: I don't just want to manage my tiny little memory states
[02:50:47] SPEAKER_B: because I'm getting your memories.
[02:50:48] SPEAKER_B: Now I know I got to manage this whole thing.
[02:50:50] SPEAKER_B: So both of these things end up scaling
[02:50:53] SPEAKER_B: the size of things you care about.
[02:50:55] SPEAKER_B: And that is a major ladder for cognition
[02:50:58] SPEAKER_B: is scale the degree of, you know,
[02:51:00] SPEAKER_B: the size of concern that you have.
[02:51:02] SPEAKER_B: It'd be fascinating to be able to engineer that scaling.
[02:51:06] SPEAKER_A: Probably applicable to AI systems.
[02:51:07] SPEAKER_A: How do you rapidly scale the cognitive cone?
[02:51:12] SPEAKER_A: Yeah, we have some collaborators in a company called Softmax
[02:51:15] SPEAKER_B: that we're working with to do some of that stuff.
[02:51:18] SPEAKER_B: In biology, that's our cancer therapeutic,
[02:51:22] SPEAKER_B: which is that what you see in cancer, literally,
[02:51:25] SPEAKER_B: is cells electrically disconnect from their neighbors.
[02:51:29] SPEAKER_B: When they were part of a giant memory
[02:51:31] SPEAKER_B: that was working on making a nice organ,
[02:51:33] SPEAKER_B: well, now they can't remember any of that.
[02:51:35] SPEAKER_B: Now they're just amoebas and the rest of the body
[02:51:37] SPEAKER_B: is just external environment.
[02:51:38] SPEAKER_B: And what we found is if you then physically reconnect them
[02:51:42] SPEAKER_B: to the network, you don't have to fix the DNA.
[02:51:45] SPEAKER_B: You don't have to kill the cells with chemo.
[02:51:47] SPEAKER_B: You can just reconnect them and they go back to,
[02:51:50] SPEAKER_B: because they're now part of this larger collective,
[02:51:52] SPEAKER_B: they go back to what they were working on.
[02:51:53] SPEAKER_B: And so, yeah, I think we can intervene at that scale.
[02:51:57] SPEAKER_B: Let me ask you more explicitly on the search,
[02:52:01] SPEAKER_A: the SUTI, the Search for Unconventional
[02:52:04] SPEAKER_A: Terrestrial Intelligence.
[02:52:06] SPEAKER_A: What do you hope to do there?
[02:52:08] SPEAKER_A: How do you actually find, try to find
[02:52:12] SPEAKER_A: unconventional intelligence all around us?
[02:52:14] SPEAKER_A: First of all, do you think on Earth
[02:52:16] SPEAKER_A: there is all kinds of incredible intelligence
[02:52:19] SPEAKER_A: we haven't yet discovered?
[02:52:21] SPEAKER_A: I mean, guaranteed, we've already seen in our own bodies.
[02:52:24] SPEAKER_B: And I don't just mean that we are host
[02:52:25] SPEAKER_B: to a bunch of microbiome or any of that.
[02:52:27] SPEAKER_B: I mean that your cells, and we have
[02:52:31] SPEAKER_B: all kinds of work on this.
[02:52:33] SPEAKER_B: Every day they traverse these alien spaces,
[02:52:37] SPEAKER_B: 20,000 dimensional spaces and other spaces.
[02:52:39] SPEAKER_B: They solve problems.
[02:52:41] SPEAKER_B: I think they suffer when they fail to meet their goals.
[02:52:46] SPEAKER_B: They have stress reduction when they meet their goals.
[02:52:48] SPEAKER_B: These things are inside of us.
[02:52:50] SPEAKER_B: They are all around us.
[02:52:51] SPEAKER_B: I think that we are, we have an incredible degree
[02:52:53] SPEAKER_B: of mind blindness to all of the very alien
[02:52:56] SPEAKER_B: kinds of minds around us.
[02:52:58] SPEAKER_B: And I think that, you know, looking for aliens
[02:53:00] SPEAKER_B: off the Earth is awesome and whatever,
[02:53:02] SPEAKER_B: but if we can't recognize the ones
[02:53:05] SPEAKER_B: that are inside our own bodies,
[02:53:07] SPEAKER_B: what chance do we have to really, you know,
[02:53:10] SPEAKER_B: to really recognize the ones that are out there?
[02:53:12] SPEAKER_B: Do you think that could be a measure like IQ
[02:53:16] SPEAKER_A: for mind, what would it be, not mindedness,
[02:53:24] SPEAKER_A: but intelligence that's broadly applicable
[02:53:28] SPEAKER_A: to the unconventional minds,
[02:53:30] SPEAKER_A: that's generalizable to unconventional minds,
[02:53:33] SPEAKER_A: where we could even quantify like,
[02:53:36] SPEAKER_A: holy shit, this discovery is incredible
[02:53:38] SPEAKER_A: because it has this IQ?
[02:53:41] SPEAKER_B: Yeah, yes and no.
[02:53:43] SPEAKER_B: The yes part is that what, as we have shown,
[02:53:47] SPEAKER_B: you can take existing IQ metrics,
[02:53:49] SPEAKER_B: I mean, literally existing kinds of ways
[02:53:52] SPEAKER_B: that people used to measure intelligence
[02:53:54] SPEAKER_B: of animals and humans or whatever,
[02:53:55] SPEAKER_B: and you can apply them to very weird things.
[02:53:57] SPEAKER_B: If you have the imagination to make the interface,
[02:54:00] SPEAKER_B: you can do it and we've done it
[02:54:02] SPEAKER_B: and we've shown creative problem solving
[02:54:03] SPEAKER_B: and all this kind of stuff.
[02:54:05] SPEAKER_B: Like, so yes, however, we have to be humble
[02:54:08] SPEAKER_B: about these things and recognize
[02:54:09] SPEAKER_B: that all of those IQ metrics
[02:54:11] SPEAKER_B: that we've come up with so far
[02:54:13] SPEAKER_B: were derived from an N of one example
[02:54:16] SPEAKER_B: of the evolutionary lineage here on Earth.
[02:54:18] SPEAKER_B: And so we are probably missing a lot of them.
[02:54:21] SPEAKER_B: So I would say we have plenty to start,
[02:54:23] SPEAKER_B: we have so much to start with,
[02:54:25] SPEAKER_B: we could keep tens of thousands of people busy
[02:54:27] SPEAKER_B: just testing things now,
[02:54:29] SPEAKER_B: but we have to be aware that we're probably missing
[02:54:31] SPEAKER_B: a lot of important ones.
[02:54:32] SPEAKER_B: What do you think has more interesting,
[02:54:35] SPEAKER_A: intelligent, unconventional minds inside our body,
[02:54:41] SPEAKER_A: the human body, or like we were talking off mic,
[02:54:44] SPEAKER_A: the Amazon jungle, like nature,
[02:54:47] SPEAKER_A: natural systems outside of the sophisticated
[02:54:53] SPEAKER_A: biological systems we're aware of?
[02:54:55] Yeah, we don't know because it's really hard
[02:54:57] SPEAKER_B: to do experiments on larger systems.
[02:54:59] SPEAKER_B: It's a lot easier to go down than it is to go up.
[02:55:02] SPEAKER_B: But my suspicion is, you know,
[02:55:05] SPEAKER_B: like the Buddhists say, innumerable sentient beings.
[02:55:08] SPEAKER_B: I think by the time you get to that degree of infinity,
[02:55:10] SPEAKER_B: it kind of doesn't matter to compare.
[02:55:13] SPEAKER_B: I suspect there's just massive numbers of them.
[02:55:16] SPEAKER_B: Yeah, I think it really matters which kind of systems
[02:55:19] SPEAKER_A: are amenable to our current methods of scientific inquiry.
[02:55:23] SPEAKER_A: I mean, I spent quite a lot of hours
[02:55:26] SPEAKER_A: just staring at ants when I was in the Amazon.
[02:55:29] SPEAKER_A: And it's such a mysterious,
[02:55:31] SPEAKER_A: wonderful collective intelligence.
[02:55:33] SPEAKER_A: I don't know how amenable it is to research.
[02:55:35] SPEAKER_A: I've seen some folks try.
[02:55:37] SPEAKER_A: You could simulate, you can,
[02:55:39] SPEAKER_A: but I feel like we're missing a lot.
[02:55:41] I'm sure we are.
[02:55:42] SPEAKER_B: But one of my favorite things about that kind of work,
[02:55:45] SPEAKER_B: have you seen, there's at least three or four papers
[02:55:47] SPEAKER_B: showing that ant colonies fall for the same visual illusions
[02:55:50] SPEAKER_B: that we fall for.
[02:55:51] SPEAKER_B: Not the ants, the colonies.
[02:55:53] SPEAKER_B: So if you lay out food in particular patterns,
[02:55:56] SPEAKER_B: they'll do things like complete lines that aren't there
[02:55:58] SPEAKER_B: and like all the same shit that we fall for, they fall.
[02:56:01] SPEAKER_B: So, you know, I don't think it's hopeless,
[02:56:04] SPEAKER_B: but I do think that we need a lot of work to develop tools.
[02:56:08] Do you think all the tooling that we develop
[02:56:11] SPEAKER_A: and the mapping that we've been discussing
[02:56:13] SPEAKER_A: will help us do the SETI part, finding aliens out there?
[02:56:17] SPEAKER_A: I think it's essential.
[02:56:18] SPEAKER_B: I think it's essential.
[02:56:20] SPEAKER_B: We are so parochial in what we expect to find
[02:56:24] SPEAKER_B: in terms of life that we are gonna be
[02:56:27] SPEAKER_B: just completely missing a lot of stuff.
[02:56:29] SPEAKER_B: If we can't even agree on, nevermind definitions of life,
[02:56:34] SPEAKER_B: but, you know, what's actually important.
[02:56:38] SPEAKER_B: I led a paper recently where I asked, whatever,
[02:56:42] SPEAKER_B: 65 or so modern working scientists
[02:56:46] SPEAKER_B: for a definition of life.
[02:56:47] SPEAKER_B: And we had so many different definitions
[02:56:51] SPEAKER_B: across so many different dimensions,
[02:56:52] SPEAKER_B: we had to use AI to make amorphous space out of it.
[02:56:55] SPEAKER_B: And there was zero consensus
[02:56:57] SPEAKER_B: about what actually is important, you know?
[02:57:02] SPEAKER_B: If we're not good at recognizing it here,
[02:57:04] SPEAKER_B: I just don't see how we're gonna be good
[02:57:06] SPEAKER_B: at recognizing it somewhere else.
[02:57:08] SPEAKER_B: So given how miraculous life is here on Earth,
[02:57:11] SPEAKER_A: so it's clear to me that we have so much more work to do.
[02:57:15] SPEAKER_A: That said, would that be exciting to you
[02:57:19] SPEAKER_A: if we find life on other planets in the solar system?
[02:57:25] SPEAKER_A: Like, what would you do with that information?
[02:57:27] SPEAKER_A: Or is that just another life form
[02:57:31] SPEAKER_A: that we don't understand?
[02:57:32] I would be very excited about it
[02:57:33] SPEAKER_B: because it would give us some more
[02:57:36] SPEAKER_B: unconventional embodiments to think about, right?
[02:57:39] SPEAKER_B: A data point that's pretty far away
[02:57:41] SPEAKER_B: from our existing data points,
[02:57:42] SPEAKER_B: at least in this solar system.
[02:57:43] SPEAKER_B: So that'd be cool, I'd be very excited about it.
[02:57:46] SPEAKER_B: But I must admit that my level of,
[02:57:50] SPEAKER_B: my set point for surprise has been pushed so high
[02:57:53] SPEAKER_B: at this point that it would have to,
[02:57:56] SPEAKER_B: you know, it would have to be something really weird
[02:57:58] SPEAKER_B: to make me shocked.
[02:57:59] SPEAKER_B: I mean, the things that we see every day is just, yeah.
[02:58:04] I think you've mentioned in a few places that,
[02:58:08] SPEAKER_A: like, you wrote that the Ingressing Minds paper
[02:58:11] SPEAKER_A: is not the weirdest thing you plan to write.
[02:58:14] SPEAKER_B: Yeah.
[02:58:16] How weird are you gonna get?
[02:58:19] SPEAKER_A: Can you hit, maybe a better question is like,
[02:58:22] SPEAKER_A: in which direction of weirdness
[02:58:25] SPEAKER_A: do you think you will go in your life?
[02:58:29] SPEAKER_A: In which direction of the weird Overton window
[02:58:33] SPEAKER_A: are you going to expand?
[02:58:34] SPEAKER_A: Yeah, well, the kind of a background to this
[02:58:37] SPEAKER_B: is simply that I've had a lot of weird ideas
[02:58:41] SPEAKER_B: for many, many decades.
[02:58:43] SPEAKER_B: And my general policy is not to talk about stuff
[02:58:48] SPEAKER_B: until it becomes actionable.
[02:58:50] SPEAKER_B: And the amazing thing, I mean, I'm really kind of shocked
[02:58:55] SPEAKER_B: is that in my lifetime, the empirical work,
[02:58:58] SPEAKER_B: like, I really didn't think we would get this far.
[02:59:01] SPEAKER_B: And the knob, I have this like mental knob
[02:59:03] SPEAKER_B: of what percentage of the weird things I think
[02:59:06] SPEAKER_B: do I actually say in public, right?
[02:59:08] SPEAKER_B: And every few years when the empirical work moves forward,
[02:59:12] SPEAKER_B: I sort of turn that knob a little, right, as we keep going.
[02:59:15] SPEAKER_B: So I have no idea if we'll continue to be that fortunate
[02:59:18] SPEAKER_B: or how long I can keep doing this
[02:59:20] SPEAKER_B: or however, like, I don't know.
[02:59:21] SPEAKER_B: But just to give you a direction for it,
[02:59:26] SPEAKER_B: it's going to be in the direction of what kinds of things
[02:59:32] SPEAKER_B: do we need to take seriously as other beings
[02:59:35] SPEAKER_B: with which to relate to.
[02:59:37] SPEAKER_B: So I've already pushed it, you know,
[02:59:39] SPEAKER_B: so like we knew brainy things and then we said,
[02:59:43] SPEAKER_B: well, it's not just brains.
[02:59:44] SPEAKER_B: And then we said, well, it's not just,
[02:59:47] SPEAKER_B: so, you know, it's not just in physical space
[02:59:49] SPEAKER_B: and it's not just biologicals and it's not just complexity.
[02:59:52] SPEAKER_B: There's a couple of other steps to take
[02:59:54] SPEAKER_B: that I'm pretty sure are there,
[02:59:58] SPEAKER_B: but we're going to have to wait and see.
[03:00:00] SPEAKER_B: have to do the actual work to make it actionable
[03:00:02] SPEAKER_B: before we really talk about it.
[03:00:04] SPEAKER_B: So that direction.
[03:00:06] SPEAKER_B: I think it's fair to say you're one of the more
[03:00:10] SPEAKER_A: unconventional humans, scientists out there.
[03:00:15] SPEAKER_A: So the interesting question is,
[03:00:17] SPEAKER_A: what's your process of idea generation?
[03:00:19] SPEAKER_A: What's your process of discovery?
[03:00:22] SPEAKER_A: You've done a lot of really incredibly
[03:00:28] SPEAKER_A: interesting, like you said, actionable,
[03:00:29] SPEAKER_A: but interesting out there ideas
[03:00:34] SPEAKER_A: that you've actually engineered with Xenobots
[03:00:37] SPEAKER_A: and Anthrobots, these kinds of things.
[03:00:41] SPEAKER_A: When you go home tonight, go to the lab,
[03:00:45] SPEAKER_A: what's the process?
[03:00:46] SPEAKER_A: Empty sheet of paper when you're thinking through it.
[03:00:50] Well, the mental part is a lot of it,
[03:00:54] SPEAKER_B: much like, funny enough, much like making Xenobots.
[03:00:57] SPEAKER_B: We make Xenobots by releasing constraints, right?
[03:01:00] SPEAKER_B: We don't do anything to them.
[03:01:01] SPEAKER_B: We just release them from the constraints
[03:01:03] SPEAKER_B: they already have.
[03:01:04] SPEAKER_B: And then we say, so a lot of it is releasing
[03:01:07] SPEAKER_B: the constraints that mentally have been placed on us.
[03:01:10] SPEAKER_B: And part of it is my education has been a little weird
[03:01:13] SPEAKER_B: because I was a computer scientist first
[03:01:15] SPEAKER_B: and only later biology.
[03:01:16] SPEAKER_B: And so by the time I heard all the biology things
[03:01:18] SPEAKER_B: that we typically just take on board,
[03:01:20] SPEAKER_B: I was already a little skeptical
[03:01:23] SPEAKER_B: and thinking a little differently.
[03:01:24] SPEAKER_B: But a lot of it comes from releasing constraints.
[03:01:28] SPEAKER_B: And I very specifically think about, okay,
[03:01:30] SPEAKER_B: this is what we know.
[03:01:32] SPEAKER_B: What would things look like if we were wrong?
[03:01:34] SPEAKER_B: Or what would it look like if I was wrong?
[03:01:36] SPEAKER_B: What are we missing?
[03:01:37] SPEAKER_B: What is our worldview specifically not able to see,
[03:01:39] SPEAKER_B: right, whatever model I have?
[03:01:41] SPEAKER_B: Or another way I often think is I'll take two things
[03:01:44] SPEAKER_B: that are considered to be very different things.
[03:01:46] SPEAKER_B: And I'll say, let's just imagine those as two points
[03:01:48] SPEAKER_B: on a continuum.
[03:01:49] SPEAKER_B: What does that look like?
[03:01:50] SPEAKER_B: What does the middle of that continuum look like?
[03:01:52] SPEAKER_B: What's the symmetry there?
[03:01:54] SPEAKER_B: What's the parameter that I can,
[03:01:56] SPEAKER_B: what's the knob I can turn from to get from here to there?
[03:01:59] SPEAKER_B: So those kinds of, I look for symmetries a lot.
[03:02:02] SPEAKER_B: I'm like, okay, this thing is like that way in what way?
[03:02:04] SPEAKER_B: What's the fewest number of things I would have to move
[03:02:07] SPEAKER_B: to make this map onto that, right?
[03:02:08] SPEAKER_B: So those are kind of mental tools.
[03:02:12] SPEAKER_B: The physical process for me is basically,
[03:02:16] SPEAKER_B: I mean, obviously I'm fortunate to have a lot of discussions
[03:02:19] SPEAKER_B: with very smart people.
[03:02:20] SPEAKER_B: And so in my group there's something,
[03:02:22] SPEAKER_B: I've hired some amazing people.
[03:02:23] SPEAKER_B: So we of course have a lot of discussions
[03:02:25] SPEAKER_B: and some stuff comes out of that.
[03:02:27] SPEAKER_B: My process is I do pretty much every morning
[03:02:32] SPEAKER_B: or I'm outside for sunrise and I walk around in nature.
[03:02:37] SPEAKER_B: There's just not really anything better than,
[03:02:40] SPEAKER_B: as inspiration, right, than nature.
[03:02:42] SPEAKER_B: I do photography and I find that
[03:02:45] SPEAKER_B: it's a good meditative tool because it keeps your hands
[03:02:48] SPEAKER_B: and brain just busy enough.
[03:02:50] SPEAKER_B: Like you don't have to think too much,
[03:02:51] SPEAKER_B: but you're sort of twiddling and looking
[03:02:53] SPEAKER_B: and doing some stuff.
[03:02:54] SPEAKER_B: And it keeps your brain off of the linear,
[03:02:57] SPEAKER_B: like logical, like careful train of thought
[03:02:59] SPEAKER_B: enough to release it so that you can ideate a little more
[03:03:02] SPEAKER_B: while your hands are busy.
[03:03:04] SPEAKER_A: So it's not even the thing you're photographing,
[03:03:06] SPEAKER_A: it's the mechanical process of doing the photography.
[03:03:08] SPEAKER_A: And mentally, right?
[03:03:09] SPEAKER_A: So because I'm not walking around thinking,
[03:03:12] SPEAKER_B: okay, let's see, so for this experiment,
[03:03:13] SPEAKER_B: we gotta, you know, I gotta get this piece of equipment
[03:03:16] SPEAKER_B: and this, like that goes away.
[03:03:17] SPEAKER_B: And it's like, okay, what's the lighting
[03:03:19] SPEAKER_B: and what's the, what am I looking at?
[03:03:21] SPEAKER_B: And during that time, when you're not thinking
[03:03:23] SPEAKER_B: about that other stuff, then they say,
[03:03:25] SPEAKER_B: well, yeah, I gotta get, I got a notebook.
[03:03:26] SPEAKER_B: And I'm like, look, this is what we need to do.
[03:03:29] SPEAKER_B: So that kind of stuff.
[03:03:30] SPEAKER_A: And the actual idea of writing down stuff,
[03:03:33] SPEAKER_A: is it notebook, is it computer?
[03:03:36] SPEAKER_A: Are you super organized thinking,
[03:03:38] SPEAKER_A: or is it just like random words here and there
[03:03:40] SPEAKER_A: with drawings and, and also like,
[03:03:46] SPEAKER_A: what is the space of thoughts you have in your head?
[03:03:50] SPEAKER_A: Is this sort of amorphous things that aren't very clear?
[03:03:55] SPEAKER_A: Are you visualizing stuff?
[03:03:58] SPEAKER_A: Is there, is there something you can articulate there?
[03:04:01] SPEAKER_A: I tend to leave myself a lot of voicemails
[03:04:03] SPEAKER_B: because as I'm walking around, I'm like,
[03:04:05] SPEAKER_B: oh man, this, this idea.
[03:04:06] SPEAKER_B: And so I'll, I'll just call my office
[03:04:08] SPEAKER_B: and leave myself a voicemail for later to transcribe.
[03:04:11] SPEAKER_B: I don't have a good enough memory
[03:04:13] SPEAKER_B: to remember any of these things.
[03:04:15] SPEAKER_B: And so what I keep is a mind map.
[03:04:17] SPEAKER_B: So I have a, I have an enormous mind map.
[03:04:19] SPEAKER_B: One piece of it hangs in my, in my lab
[03:04:21] SPEAKER_B: so that people can see like, these are the ideas.
[03:04:23] SPEAKER_B: This is how they link together.
[03:04:24] SPEAKER_B: Here's everybody's project.
[03:04:25] SPEAKER_B: I'm working on this.
[03:04:26] SPEAKER_B: How the hell does this attach to everybody else's
[03:04:28] SPEAKER_B: so they can track it?
[03:04:29] SPEAKER_B: The thing that hangs in the lab is about nine feet wide.
[03:04:31] SPEAKER_B: It's a silk sheet.
[03:04:32] SPEAKER_B: And I, you know, it's, it's out of date
[03:04:34] SPEAKER_B: within a couple of weeks of, of my, of my printing it.
[03:04:36] SPEAKER_B: Cause new stuff keeps moving around.
[03:04:39] SPEAKER_B: And then, and then there's more that isn't, you know,
[03:04:41] SPEAKER_B: isn't for anybody else's view.
[03:04:43] SPEAKER_B: But yeah, I try to, I try to be very organized
[03:04:47] SPEAKER_B: because otherwise, otherwise I forget.
[03:04:49] SPEAKER_B: So, so everything is in the mind map.
[03:04:51] SPEAKER_B: Things are in manuscripts.
[03:04:52] SPEAKER_B: I have something like right now,
[03:04:54] SPEAKER_B: probably 163, 62 open manuscripts
[03:04:58] SPEAKER_B: that are in process of being written at various stages.
[03:05:01] SPEAKER_B: And when things come up,
[03:05:02] SPEAKER_B: I stick them in the right manuscript in the right place
[03:05:04] SPEAKER_B: so that when I'm finally ready to finalize,
[03:05:06] SPEAKER_B: then I'll put words around it, whatever.
[03:05:08] SPEAKER_B: But there's like outlines of everything.
[03:05:10] SPEAKER_B: So I try to be organized cause I can't,
[03:05:12] SPEAKER_B: I don't have to, you know.
[03:05:13] SPEAKER_B: So there's a wide front of manuscripts of work
[03:05:18] SPEAKER_A: that's being done and it's continuously like
[03:05:21] SPEAKER_A: pushing towards completion,
[03:05:23] SPEAKER_A: but you're not clear where it's going to be finished,
[03:05:25] SPEAKER_A: when and how.
[03:05:27] I mean, that's, yes.
[03:05:28] SPEAKER_B: But that's just the,
[03:05:29] SPEAKER_B: that's just the theoretical philosophical stuff.
[03:05:31] SPEAKER_B: The, the empirical work that we're doing with in the lab.
[03:05:34] SPEAKER_B: I mean, those are, we know exactly, you know.
[03:05:36] SPEAKER_B: It's more focused.
[03:05:36] SPEAKER_B: There's like, we know this is, this is, you know,
[03:05:38] SPEAKER_B: anthropod aging, this is limb regeneration.
[03:05:41] SPEAKER_B: This is the new cancer paper.
[03:05:42] SPEAKER_B: This is whatever.
[03:05:43] SPEAKER_B: Yeah, those things are very linear.
[03:05:44] SPEAKER_B: Where do you think ideas come from
[03:05:46] SPEAKER_A: when you're taking a walk
[03:05:48] SPEAKER_A: that eventually materialized in a voicemail?
[03:05:52] SPEAKER_A: Where's that?
[03:05:53] SPEAKER_A: What, is that from you?
[03:05:55] SPEAKER_A: Is that, you know, a lot of really,
[03:05:58] SPEAKER_A: some of the most interesting people
[03:06:00] SPEAKER_A: feel like they're channeling from somewhere else.
[03:06:02] SPEAKER_A: I mean, I hate to bring up the platonic space again,
[03:06:04] SPEAKER_B: but, but I mean, if you talk to any creative,
[03:06:07] SPEAKER_B: that's basically what they, what they'll tell you.
[03:06:09] SPEAKER_B: And, and certainly that's been my experience.
[03:06:11] SPEAKER_B: So I feel, I feel like it's a,
[03:06:13] SPEAKER_B: the way, the way it feels to me is a collaboration.
[03:06:16] SPEAKER_B: So the collaboration is,
[03:06:18] SPEAKER_B: I need to bust my ass and be, be prepped in one,
[03:06:23] SPEAKER_B: A, to, to, to work hard,
[03:06:24] SPEAKER_B: to be able to recognize the idea when it comes
[03:06:27] SPEAKER_B: and B, to actually have an outlet for it
[03:06:29] SPEAKER_B: so that when it does come,
[03:06:30] SPEAKER_B: we have a lab and we have people who can,
[03:06:32] SPEAKER_B: who can help me do it
[03:06:33] SPEAKER_B: and then we can actually get it out, right?
[03:06:35] SPEAKER_B: So that's, that's, that's my part is, you know,
[03:06:37] SPEAKER_B: be up at 4.30 AM doing your thing and be ready for it.
[03:06:40] SPEAKER_B: But the other side of the collaboration is that,
[03:06:42] SPEAKER_B: yeah, when you do that, like amazing ideas come
[03:06:46] SPEAKER_B: and, you know, to say that it's me,
[03:06:47] SPEAKER_B: I don't think would be, would be right.
[03:06:49] SPEAKER_B: I, you know, I think it's,
[03:06:50] SPEAKER_B: it's definitely coming from, from other places.
[03:06:53] What advice would you give to scientists,
[03:06:54] SPEAKER_A: PhD students, grad students, young scientists,
[03:06:57] SPEAKER_A: that are trying to explore the space of ideas,
[03:07:01] SPEAKER_A: given the very unconventional,
[03:07:04] SPEAKER_A: non-standard, unique set of ideas
[03:07:06] SPEAKER_A: you've explored in your life and career?
[03:07:09] SPEAKER_A: Let's see.
[03:07:10] SPEAKER_B: Well, the first and most important thing I've learned
[03:07:13] SPEAKER_B: is not to take too much advice.
[03:07:15] SPEAKER_B: And so I don't like to give too much advice,
[03:07:17] SPEAKER_B: but I do have one technique that I found very useful
[03:07:21] SPEAKER_B: and this isn't for everybody,
[03:07:23] SPEAKER_B: but there's a specific demographic.
[03:07:24] SPEAKER_B: There's a lot of,
[03:07:25] SPEAKER_B: a lot of unconventional people reach out to me
[03:07:27] SPEAKER_B: and I try to respond and help them and so on.
[03:07:31] SPEAKER_B: This is a technique that I think is useful for some people.
[03:07:35] SPEAKER_B: How do I describe it?
[03:07:36] SPEAKER_B: You need to, it's, it's, it's a,
[03:07:38] SPEAKER_B: it's the act of bifurcating your mind
[03:07:41] SPEAKER_B: and you need to have two different regions.
[03:07:44] SPEAKER_B: One region is the practical region of impact.
[03:07:50] SPEAKER_B: In other words, how do I get my idea in,
[03:07:53] SPEAKER_B: out into the world so that other people recognize it?
[03:07:56] SPEAKER_B: What should I say?
[03:07:57] SPEAKER_B: What are people hearing?
[03:07:58] SPEAKER_B: What are they able to hear?
[03:07:59] SPEAKER_B: How do I pivot it?
[03:08:01] SPEAKER_B: What parts do I not talk about?
[03:08:03] SPEAKER_B: Which journal am I gonna publish this in?
[03:08:05] SPEAKER_B: Is it time now?
[03:08:06] SPEAKER_B: Do I wait two years for this?
[03:08:07] SPEAKER_B: Like all the practical stuff
[03:08:09] SPEAKER_B: that is all about how it looks from the outside, right?
[03:08:12] SPEAKER_B: All the stuff that I can't say this
[03:08:13] SPEAKER_B: or I should say this differently
[03:08:14] SPEAKER_B: or this is gonna freak people out
[03:08:16] SPEAKER_B: or this is, you know, this community wants to hear this
[03:08:18] SPEAKER_B: so I can pivot it this way.
[03:08:19] SPEAKER_B: Like all that practical stuff, it's gotta be there.
[03:08:22] SPEAKER_B: Otherwise you're not gonna be in a position
[03:08:23] SPEAKER_B: to follow up any of your ideas.
[03:08:25] SPEAKER_B: You're not gonna have a career.
[03:08:25] SPEAKER_B: You can't, you're not gonna have resources to do anything,
[03:08:28] SPEAKER_B: but it's very important that that can't be the only thing.
[03:08:30] SPEAKER_B: You need another part of your mind
[03:08:31] SPEAKER_B: that ignores all that shit completely
[03:08:33] SPEAKER_B: because this other part of your mind has to be pure.
[03:08:36] SPEAKER_B: It has to be,
[03:08:37] SPEAKER_B: I don't care what anybody else thinks about this.
[03:08:39] SPEAKER_B: I don't care whether this is publishable, describable.
[03:08:41] SPEAKER_B: I don't care if anybody gets it.
[03:08:42] SPEAKER_B: I don't care if anybody thinks it's stupid.
[03:08:44] SPEAKER_B: This is what I think and why,
[03:08:47] SPEAKER_B: and give it space to sort of grow, right?
[03:08:50] SPEAKER_B: And if you keep the, if you try to mush them,
[03:08:52] SPEAKER_B: if you try to mush them together,
[03:08:53] SPEAKER_B: I found that impossible
[03:08:54] SPEAKER_B: because the practical stuff poisons the other stuff.
[03:08:58] SPEAKER_B: If you're too much on the creative end,
[03:09:00] SPEAKER_B: you can be an amazing thinker.
[03:09:02] SPEAKER_B: It's just nothing ever materializes.
[03:09:04] SPEAKER_B: But if you're very practical,
[03:09:05] SPEAKER_B: it tends to poison the other stuff
[03:09:07] SPEAKER_B: because the more you think about how to present things
[03:09:11] SPEAKER_B: so that other people get it,
[03:09:12] SPEAKER_B: it constrains and it bends how you start to think.
[03:09:17] SPEAKER_B: And what I tell my students and others
[03:09:21] SPEAKER_B: is there's two kinds of advice.
[03:09:23] SPEAKER_B: There's very practical, specific things.
[03:09:26] SPEAKER_B: Like somebody says, well, you forgot this control
[03:09:28] SPEAKER_B: or this isn't the right method or you shouldn't be.
[03:09:31] SPEAKER_B: That stuff is gold and you should take that very seriously
[03:09:33] SPEAKER_B: and you should use it to improve your craft, right?
[03:09:36] SPEAKER_B: And that's like super important.
[03:09:37] SPEAKER_B: But then there's the meta advice where people like,
[03:09:39] SPEAKER_B: that's not a good way to think about it.
[03:09:41] SPEAKER_B: Don't work on this.
[03:09:42] SPEAKER_B: This isn't, that stuff is garbage.
[03:09:44] SPEAKER_B: And even very successful people
[03:09:47] SPEAKER_B: often give very constraining, terrible advice.
[03:09:51] SPEAKER_B: Like one of my reviewers in a paper years ago said,
[03:09:53] SPEAKER_B: I love this Freudian slip.
[03:09:55] SPEAKER_B: He said, he's gonna give me constrictive criticism, right?
[03:09:58] SPEAKER_B: And that's exactly what he gave me, was constrictive.
[03:10:00] SPEAKER_B: restrictive criticism. I was like, that's awesome.
[03:10:02] SPEAKER_B: That's a great take.
[03:10:04] SPEAKER_A: It was very true. I mean, that second, the bifurcation of the mind,
[03:10:06] SPEAKER_A: as beautifully put,
[03:10:08] SPEAKER_A: I do think some of the most interesting
[03:10:10] SPEAKER_A: people I've met are
[03:10:12] SPEAKER_A: sometimes
[03:10:14] SPEAKER_A: fall short
[03:10:16] SPEAKER_A: on the normie side,
[03:10:18] SPEAKER_A: on the practical,
[03:10:20] SPEAKER_A: how do I, having the emotional
[03:10:22] SPEAKER_A: intelligence of how do I communicate this with
[03:10:24] SPEAKER_A: people that have a
[03:10:26] SPEAKER_A: very different world view, that
[03:10:28] SPEAKER_A: more conservative and more
[03:10:30] SPEAKER_A: conventional and more kind of
[03:10:32] SPEAKER_A: fit into the norm.
[03:10:34] SPEAKER_A: You have to be able to have the skill to fit in.
[03:10:36] SPEAKER_B: And then you have to,
[03:10:38] SPEAKER_A: again, beautifully put, be able to shut
[03:10:40] SPEAKER_A: that off when you go on your own
[03:10:42] SPEAKER_A: and think. And having
[03:10:44] SPEAKER_A: two skills is very important.
[03:10:46] SPEAKER_A: I think a lot of radical thinkers think that
[03:10:48] SPEAKER_A: they're sacrificing something by learning the
[03:10:50] SPEAKER_A: skill of fitting in. But I think
[03:10:52] SPEAKER_A: if you want to have impact,
[03:10:54] SPEAKER_A: if you want ideas to resonate
[03:10:56] SPEAKER_A: it can actually lead to,
[03:10:58] SPEAKER_A: first of all, be able to build great teams
[03:11:00] SPEAKER_A: that help bring your ideas to life.
[03:11:02] SPEAKER_A: And second of all, for your ideas
[03:11:04] SPEAKER_A: to have impact and to scale
[03:11:06] SPEAKER_A: and to resonate
[03:11:08] SPEAKER_A: with a large number of people, you have to
[03:11:10] SPEAKER_A: have that skill. And those are
[03:11:12] SPEAKER_A: very different.
[03:11:14] SPEAKER_A: Let me ask a
[03:11:16] SPEAKER_A: ridiculous question. You already spoke about it,
[03:11:18] SPEAKER_A: but
[03:11:20] SPEAKER_A: what to you is
[03:11:22] SPEAKER_A: one of the most beautiful ideas that you've
[03:11:24] SPEAKER_A: encountered
[03:11:26] SPEAKER_A: in your various explorations?
[03:11:28] SPEAKER_A: Maybe
[03:11:30] SPEAKER_A: not just beautiful, but one that makes
[03:11:32] SPEAKER_A: you happy to be
[03:11:34] SPEAKER_A: a scientist, to be able to
[03:11:36] SPEAKER_A: be curious humans exploring
[03:11:38] SPEAKER_A: ideas. I mean, I must
[03:11:40] SPEAKER_B: say that I sometimes
[03:11:42] SPEAKER_B: think about these
[03:11:44] SPEAKER_B: ingressions from this
[03:11:46] SPEAKER_B: space as a kind of steganography.
[03:11:48] SPEAKER_B: So steganography
[03:11:50] SPEAKER_B: is when you hide data
[03:11:52] SPEAKER_B: and messages within
[03:11:54] SPEAKER_B: the bits of another pattern that
[03:11:56] SPEAKER_B: don't matter, right? And the rule of steganography
[03:11:58] SPEAKER_B: is you can't mess up the main thing.
[03:12:00] SPEAKER_B: If it's a picture of a cat or whatever, you've got to keep the
[03:12:02] SPEAKER_B: cat, but if there's bits that don't matter, you can
[03:12:04] SPEAKER_B: stick stuff. So I feel like
[03:12:06] SPEAKER_B: all these ingressions are a kind of universal
[03:12:08] SPEAKER_B: steganography, that there's this like, these
[03:12:10] SPEAKER_B: patterns seep into everything, everywhere
[03:12:12] SPEAKER_B: they can, and
[03:12:14] SPEAKER_B: they're kind of shy,
[03:12:16] SPEAKER_B: meaning that they're very subtle,
[03:12:18] SPEAKER_B: not invisible. If you work hard, you can catch
[03:12:20] SPEAKER_B: them, but they're not invisible, but they're
[03:12:22] SPEAKER_B: hard to see. And the
[03:12:24] SPEAKER_B: fact that I think
[03:12:26] SPEAKER_B: they also affect
[03:12:28] SPEAKER_B: quote-unquote machines, as much as
[03:12:30] SPEAKER_B: they certainly affect living
[03:12:32] SPEAKER_B: organisms, I think is incredibly
[03:12:34] SPEAKER_B: beautiful, and I
[03:12:36] SPEAKER_B: personally am happy to be part of
[03:12:38] SPEAKER_B: that same spectrum, and the fact that
[03:12:40] SPEAKER_B: magic is sort of
[03:12:42] SPEAKER_B: applicable to everything.
[03:12:44] SPEAKER_B: A lot of people find
[03:12:46] SPEAKER_B: that extremely disturbing, and that's
[03:12:48] SPEAKER_B: some of the hate mail I get, is like,
[03:12:50] SPEAKER_B: yeah, we were with you on the majesty
[03:12:52] SPEAKER_B: of life thing, until you got to the fact that machines
[03:12:54] SPEAKER_B: get it too, and now, like, terrible,
[03:12:56] SPEAKER_B: right? You're kind of devaluing the
[03:12:58] SPEAKER_B: majesty of life,
[03:13:00] SPEAKER_B: and I don't know,
[03:13:02] SPEAKER_B: the idea that we're now catching these
[03:13:04] SPEAKER_B: patterns, and
[03:13:06] SPEAKER_B: we're able to do meaningful research
[03:13:08] SPEAKER_B: on the interfaces and all that,
[03:13:10] SPEAKER_B: is just, to me, absolutely beautiful,
[03:13:12] SPEAKER_B: and that it's all one spectrum, I think, to me,
[03:13:14] SPEAKER_B: is amazing. I'm
[03:13:16] SPEAKER_B: enriched by it. I agree with you, I think it's incredibly
[03:13:18] SPEAKER_A: beautiful. I lied, there's
[03:13:20] SPEAKER_A: a more ridiculous question.
[03:13:22] SPEAKER_A: So,
[03:13:24] SPEAKER_A: it seems like we are progressing towards
[03:13:26] SPEAKER_A: possibly creating a superintelligence system,
[03:13:28] SPEAKER_A: and
[03:13:30] SPEAKER_A: AGI and ASI,
[03:13:32] SPEAKER_A: if I
[03:13:34] SPEAKER_A: had one, gave it to you, put you in
[03:13:36] SPEAKER_A: the room, what would be the first
[03:13:38] SPEAKER_A: question you ask it? Maybe the first set of
[03:13:40] SPEAKER_A: questions. Like, there's so many topics
[03:13:42] SPEAKER_A: that you've worked on and are interested in.
[03:13:44] SPEAKER_A: Is there
[03:13:46] SPEAKER_A: a first question
[03:13:48] SPEAKER_A: that you really just, if you can get
[03:13:50] SPEAKER_A: an answer, solid answer,
[03:13:52] SPEAKER_A: what would it be? Well, the
[03:13:54] SPEAKER_B: first thing I would ask is,
[03:13:56] SPEAKER_B: how much should I even be talking
[03:13:58] SPEAKER_B: to? For sure,
[03:14:00] SPEAKER_B: because it's
[03:14:02] SPEAKER_B: not clear to me at all that
[03:14:04] SPEAKER_B: getting somebody to tell you an answer
[03:14:06] SPEAKER_B: in the long run is optimal.
[03:14:08] SPEAKER_B: It's the difference between
[03:14:10] SPEAKER_B: when you're a kid learning math
[03:14:12] SPEAKER_B: and having an older sibling
[03:14:14] SPEAKER_B: that'll just tell you the answers.
[03:14:16] SPEAKER_B: Sometimes it's just like, come on, just give me the
[03:14:18] SPEAKER_B: answer, let's move on with this cancer protocol
[03:14:20] SPEAKER_B: and whatever. Great.
[03:14:22] SPEAKER_B: But in the long run,
[03:14:24] SPEAKER_B: the process of discovering it
[03:14:26] SPEAKER_B: yourself, how much of that are we willing
[03:14:28] SPEAKER_B: to give up?
[03:14:30] SPEAKER_B: And by getting a final answer, how much
[03:14:32] SPEAKER_B: have we missed of
[03:14:34] SPEAKER_B: stuff we might have found along the way? Now, I don't know
[03:14:36] SPEAKER_B: what the thing is.
[03:14:38] SPEAKER_B: I don't think it's correct to say
[03:14:40] SPEAKER_B: don't do that at all.
[03:14:42] SPEAKER_B: Take the time in all the blind alleys
[03:14:44] SPEAKER_B: and that may not be optimal either.
[03:14:46] SPEAKER_B: But we don't know what the optimal is.
[03:14:48] SPEAKER_B: We don't know how much we should be
[03:14:50] SPEAKER_B: stumbling around versus having somebody tell us
[03:14:52] SPEAKER_B: the answer. That's actually a brilliant question
[03:14:54] SPEAKER_A: to ask AGI then.
[03:14:56] SPEAKER_B: That's a really good first question.
[03:14:58] SPEAKER_B: Yeah, if it's really an AGI, I'm like,
[03:15:00] SPEAKER_B: tell me what the balance is. How much should I be talking
[03:15:02] SPEAKER_B: to you versus stumbling around in the lab and making
[03:15:04] SPEAKER_B: all my own mistakes?
[03:15:06] SPEAKER_B: Was it 70-30? 10-90? I don't
[03:15:08] SPEAKER_B: know. So that would be the first... And then the AGI
[03:15:10] SPEAKER_A: will say you shouldn't be talking to me.
[03:15:12] SPEAKER_B: It may well be. It may
[03:15:14] SPEAKER_B: say, what the hell did you make me for in the first place?
[03:15:16] SPEAKER_B: You guys are screwed. Like, that's possible.
[03:15:18] SPEAKER_B: Yeah.
[03:15:20] SPEAKER_B: You know, the second question I would ask is
[03:15:22] SPEAKER_B: what's the answer I should
[03:15:24] SPEAKER_B: be... What's the question I should be asking you that
[03:15:26] SPEAKER_B: I probably am not smart enough to ask you?
[03:15:28] SPEAKER_B: That's the other thing I would say.
[03:15:30] This is really complicated. That's
[03:15:32] SPEAKER_A: a really, really strong question.
[03:15:34] SPEAKER_A: But again,
[03:15:36] SPEAKER_A: the answer might be
[03:15:40] SPEAKER_A: you wouldn't understand the question
[03:15:42] SPEAKER_A: it proposes, most likely.
[03:15:44] SPEAKER_A: So I think for me, I would probably,
[03:15:46] SPEAKER_A: assuming you can get a lot of questions,
[03:15:48] SPEAKER_A: I would probably
[03:15:50] SPEAKER_A: go for questions where
[03:15:52] SPEAKER_A: I would understand the answer.
[03:15:54] SPEAKER_A: Like, it would uncover some small
[03:15:56] SPEAKER_A: mystery that I'm super curious about.
[03:15:58] SPEAKER_A: Because if you ask big questions like you did,
[03:16:00] SPEAKER_A: which are really strong questions,
[03:16:02] SPEAKER_A: I just feel like I wouldn't understand
[03:16:04] SPEAKER_A: the answer. If you ask it,
[03:16:06] SPEAKER_A: what question should I be asking you?
[03:16:08] SPEAKER_A: It would probably say something like
[03:16:10] SPEAKER_A: it would say something
[03:16:12] SPEAKER_A: like, what is the shape of the universe?
[03:16:14] SPEAKER_A: And you're like, what? Why is that
[03:16:16] SPEAKER_A: important? Right? You would be very
[03:16:18] SPEAKER_A: confused by the question it proposes.
[03:16:20] SPEAKER_A: Yeah. I would probably want to...
[03:16:22] SPEAKER_A: It would just be nice
[03:16:24] SPEAKER_A: for me to know, straight up, first question.
[03:16:26] SPEAKER_A: How many living,
[03:16:28] SPEAKER_A: intelligent alien civilizations are
[03:16:30] SPEAKER_A: in the observable universe?
[03:16:32] SPEAKER_A: Yeah, that would just be nice.
[03:16:34] Yeah. To know, is it zero?
[03:16:36] SPEAKER_A: Or is it a lot?
[03:16:38] SPEAKER_A: I just want to know that.
[03:16:40] SPEAKER_A: And then, unfortunately, it might answer.
[03:16:42] SPEAKER_A: It might give me
[03:16:44] SPEAKER_A: a Michael Levin answer.
[03:16:46] SPEAKER_B: That's what I was about to say, is that my guess is
[03:16:48] SPEAKER_B: it's going to be exactly the problem
[03:16:50] SPEAKER_B: you said, which is, it's going to say
[03:16:52] SPEAKER_B: oh my god, I mean, right in this room
[03:16:54] SPEAKER_B: you got, you know, oh man.
[03:16:56] SPEAKER_A: Yeah, yeah, yeah.
[03:16:58] SPEAKER_A: Everything you need to know about alien civilizations
[03:17:00] SPEAKER_A: is right here
[03:17:02] SPEAKER_A: in this room. In fact, it's
[03:17:04] SPEAKER_A: inside your own body.
[03:17:06] SPEAKER_A: For starters.
[03:17:08] SPEAKER_A: Thank you, AGI. Thank you.
[03:17:10] SPEAKER_A: Alright, Michael.
[03:17:12] SPEAKER_A: One of my favorite scientists,
[03:17:14] SPEAKER_A: one of my favorite humans. Thank you for everything you do
[03:17:16] SPEAKER_A: in this world. Thank you so much.
[03:17:18] SPEAKER_A: Truly, truly fascinating work. Keep going
[03:17:20] SPEAKER_A: for all of us. You're an inspiration.
[03:17:22] SPEAKER_B: Thank you so much. It's great to see you.
[03:17:24] SPEAKER_B: It's always a good discussion. Thank you so much.
[03:17:26] SPEAKER_A: Thank you.
[03:17:28] Thanks for listening to this conversation
[03:17:30] SPEAKER_A: with Michael Levin. To support this podcast,
[03:17:32] SPEAKER_A: please check out our sponsors in the description
[03:17:34] SPEAKER_A: where you can also find links
[03:17:36] SPEAKER_A: to contact me, ask questions,
[03:17:38] SPEAKER_A: get feedback, and so on.
[03:17:40] SPEAKER_A: And now, let me leave you
[03:17:42] SPEAKER_A: with some words from Albert Einstein.
[03:17:44] SPEAKER_A: The most
[03:17:46] SPEAKER_A: beautiful thing we can experience
[03:17:48] SPEAKER_A: is the mysterious.
[03:17:50] SPEAKER_A: It is the source of all
[03:17:52] SPEAKER_A: true art and science.
[03:17:54] SPEAKER_A: Thank you for listening.
[03:17:56] SPEAKER_A: I hope to see you next time.